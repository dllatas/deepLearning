/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 10000 CIFAR images before starting to train. This will take a few minutes.
2016-05-06 23:00:54.522270: step 0, loss = 6.28 (0.1 examples/sec; 309.940 sec/batch)
2016-05-06 23:07:31.508173: step 1, loss = 6.76 (0.2 examples/sec; 189.258 sec/batch)
2016-05-06 23:11:03.446327: step 2, loss = 6.94 (0.2 examples/sec; 211.938 sec/batch)
2016-05-06 23:14:21.411338: step 3, loss = 6.56 (0.2 examples/sec; 197.965 sec/batch)
2016-05-06 23:17:45.008700: step 4, loss = 6.74 (0.2 examples/sec; 203.597 sec/batch)
2016-05-06 23:21:00.403764: step 5, loss = 6.73 (0.2 examples/sec; 195.395 sec/batch)
2016-05-06 23:24:16.433631: step 6, loss = 6.60 (0.2 examples/sec; 196.030 sec/batch)
2016-05-06 23:27:21.117745: step 7, loss = 6.65 (0.2 examples/sec; 184.684 sec/batch)
2016-05-06 23:30:04.769363: step 8, loss = 6.64 (0.2 examples/sec; 163.652 sec/batch)
2016-05-06 23:32:43.152913: step 9, loss = 6.70 (0.2 examples/sec; 158.383 sec/batch)
2016-05-06 23:35:26.312850: step 10, loss = 6.56 (0.2 examples/sec; 163.160 sec/batch)
2016-05-06 23:41:09.503320: step 11, loss = 6.54 (0.2 examples/sec; 166.652 sec/batch)
2016-05-06 23:43:39.734066: step 12, loss = 6.62 (0.2 examples/sec; 150.231 sec/batch)
2016-05-06 23:45:58.380511: step 13, loss = 6.57 (0.2 examples/sec; 138.646 sec/batch)
2016-05-06 23:48:23.290859: step 14, loss = 6.28 (0.2 examples/sec; 144.910 sec/batch)
2016-05-06 23:50:45.372842: step 15, loss = 6.52 (0.2 examples/sec; 142.082 sec/batch)
2016-05-06 23:53:08.913368: step 16, loss = 6.58 (0.2 examples/sec; 143.540 sec/batch)
2016-05-06 23:55:30.721686: step 17, loss = 6.40 (0.2 examples/sec; 141.808 sec/batch)
2016-05-06 23:57:51.572275: step 18, loss = 6.47 (0.2 examples/sec; 140.851 sec/batch)
2016-05-07 00:00:12.583344: step 19, loss = 6.58 (0.2 examples/sec; 141.011 sec/batch)
2016-05-07 00:02:34.340446: step 20, loss = 6.34 (0.2 examples/sec; 141.757 sec/batch)
2016-05-07 00:07:26.251542: step 21, loss = 6.46 (0.2 examples/sec; 142.120 sec/batch)
2016-05-07 00:09:48.096387: step 22, loss = 6.47 (0.2 examples/sec; 141.845 sec/batch)
2016-05-07 00:12:09.333212: step 23, loss = 7.69 (0.2 examples/sec; 141.237 sec/batch)
2016-05-07 00:14:31.713653: step 24, loss = 6.71 (0.2 examples/sec; 142.380 sec/batch)
2016-05-07 00:16:54.385763: step 25, loss = 6.26 (0.2 examples/sec; 142.672 sec/batch)
2016-05-07 00:19:17.773933: step 26, loss = 6.34 (0.2 examples/sec; 143.388 sec/batch)
2016-05-07 00:21:40.602650: step 27, loss = 6.45 (0.2 examples/sec; 142.829 sec/batch)
2016-05-07 00:24:02.397180: step 28, loss = 6.46 (0.2 examples/sec; 141.794 sec/batch)
2016-05-07 00:26:24.630039: step 29, loss = 6.41 (0.2 examples/sec; 142.233 sec/batch)
2016-05-07 00:28:47.957266: step 30, loss = 6.42 (0.2 examples/sec; 143.327 sec/batch)
2016-05-07 00:33:41.652220: step 31, loss = 6.15 (0.2 examples/sec; 142.529 sec/batch)
2016-05-07 00:36:04.822204: step 32, loss = 6.45 (0.2 examples/sec; 143.170 sec/batch)
2016-05-07 00:38:26.881178: step 33, loss = 6.68 (0.2 examples/sec; 142.059 sec/batch)
2016-05-07 00:40:49.376632: step 34, loss = 6.22 (0.2 examples/sec; 142.495 sec/batch)
2016-05-07 00:43:12.048250: step 35, loss = 6.10 (0.2 examples/sec; 142.672 sec/batch)
2016-05-07 00:45:34.202167: step 36, loss = 6.66 (0.2 examples/sec; 142.154 sec/batch)
2016-05-07 00:47:57.011947: step 37, loss = 8.49 (0.2 examples/sec; 142.810 sec/batch)
2016-05-07 00:50:22.852186: step 38, loss = 6.59 (0.2 examples/sec; 145.840 sec/batch)
2016-05-07 00:52:45.458105: step 39, loss = 6.69 (0.2 examples/sec; 142.606 sec/batch)
2016-05-07 00:55:07.391983: step 40, loss = 6.46 (0.2 examples/sec; 141.934 sec/batch)
2016-05-07 01:00:02.399938: step 41, loss = 6.52 (0.2 examples/sec; 142.393 sec/batch)
2016-05-07 01:02:24.528321: step 42, loss = 6.64 (0.2 examples/sec; 142.128 sec/batch)
2016-05-07 01:04:46.981297: step 43, loss = 6.51 (0.2 examples/sec; 142.453 sec/batch)
2016-05-07 01:07:09.536870: step 44, loss = 6.59 (0.2 examples/sec; 142.555 sec/batch)
2016-05-07 01:09:31.184253: step 45, loss = 6.55 (0.2 examples/sec; 141.647 sec/batch)
2016-05-07 01:11:55.400320: step 46, loss = 8.33 (0.2 examples/sec; 144.216 sec/batch)
2016-05-07 01:14:19.229300: step 47, loss = 17.69 (0.2 examples/sec; 143.829 sec/batch)
2016-05-07 01:16:39.778854: step 48, loss = 6.62 (0.2 examples/sec; 140.549 sec/batch)
2016-05-07 01:19:01.403620: step 49, loss = 6.22 (0.2 examples/sec; 141.625 sec/batch)
2016-05-07 01:21:24.120500: step 50, loss = 6.36 (0.2 examples/sec; 142.717 sec/batch)
2016-05-07 01:26:17.192365: step 51, loss = 6.50 (0.2 examples/sec; 141.705 sec/batch)
2016-05-07 01:28:37.917645: step 52, loss = 6.74 (0.2 examples/sec; 140.725 sec/batch)
2016-05-07 01:30:59.223741: step 53, loss = 6.31 (0.2 examples/sec; 141.306 sec/batch)
2016-05-07 01:33:20.802968: step 54, loss = 6.62 (0.2 examples/sec; 141.579 sec/batch)
2016-05-07 01:35:42.335148: step 55, loss = 6.24 (0.2 examples/sec; 141.532 sec/batch)
2016-05-07 01:38:04.167871: step 56, loss = 6.51 (0.2 examples/sec; 141.833 sec/batch)
2016-05-07 01:40:25.751098: step 57, loss = 6.29 (0.2 examples/sec; 141.583 sec/batch)
2016-05-07 01:42:45.780808: step 58, loss = 6.41 (0.2 examples/sec; 140.030 sec/batch)
2016-05-07 01:45:06.785753: step 59, loss = 6.20 (0.2 examples/sec; 141.005 sec/batch)
2016-05-07 01:47:28.181093: step 60, loss = 6.30 (0.2 examples/sec; 141.395 sec/batch)
2016-05-07 01:52:21.085052: step 61, loss = 6.22 (0.2 examples/sec; 142.716 sec/batch)
2016-05-07 01:54:43.327721: step 62, loss = 6.03 (0.2 examples/sec; 142.243 sec/batch)
2016-05-07 01:57:04.071412: step 63, loss = 6.48 (0.2 examples/sec; 140.744 sec/batch)
2016-05-07 01:59:26.050454: step 64, loss = 6.27 (0.2 examples/sec; 141.979 sec/batch)
2016-05-07 02:01:47.772646: step 65, loss = 6.36 (0.2 examples/sec; 141.722 sec/batch)
2016-05-07 02:04:10.125934: step 66, loss = 6.22 (0.2 examples/sec; 142.353 sec/batch)
2016-05-07 02:06:31.779870: step 67, loss = 6.45 (0.2 examples/sec; 141.654 sec/batch)
2016-05-07 02:08:51.924345: step 68, loss = 6.50 (0.2 examples/sec; 140.144 sec/batch)
2016-05-07 02:11:12.873093: step 69, loss = 6.09 (0.2 examples/sec; 140.949 sec/batch)
2016-05-07 02:13:35.083702: step 70, loss = 6.48 (0.2 examples/sec; 142.211 sec/batch)
2016-05-07 02:18:26.826365: step 71, loss = 6.03 (0.2 examples/sec; 141.559 sec/batch)
2016-05-07 02:20:47.858217: step 72, loss = 6.07 (0.2 examples/sec; 141.032 sec/batch)
2016-05-07 02:23:09.953635: step 73, loss = 6.12 (0.2 examples/sec; 142.095 sec/batch)
2016-05-07 02:25:33.174331: step 74, loss = 6.08 (0.2 examples/sec; 143.221 sec/batch)
2016-05-07 02:27:54.943442: step 75, loss = 6.39 (0.2 examples/sec; 141.769 sec/batch)
2016-05-07 02:30:17.436684: step 76, loss = 6.16 (0.2 examples/sec; 142.493 sec/batch)
2016-05-07 02:32:39.662492: step 77, loss = 6.11 (0.2 examples/sec; 142.226 sec/batch)
2016-05-07 02:35:00.940190: step 78, loss = 6.34 (0.2 examples/sec; 141.278 sec/batch)
2016-05-07 02:37:23.296996: step 79, loss = 6.13 (0.2 examples/sec; 142.357 sec/batch)
2016-05-07 02:39:45.547464: step 80, loss = 6.26 (0.2 examples/sec; 142.250 sec/batch)
2016-05-07 02:44:37.371332: step 81, loss = 6.30 (0.2 examples/sec; 141.109 sec/batch)
2016-05-07 02:46:59.945789: step 82, loss = 6.09 (0.2 examples/sec; 142.574 sec/batch)
2016-05-07 02:49:21.647837: step 83, loss = 6.22 (0.2 examples/sec; 141.702 sec/batch)
2016-05-07 02:51:43.128457: step 84, loss = 6.20 (0.2 examples/sec; 141.481 sec/batch)
2016-05-07 02:54:05.795200: step 85, loss = 6.35 (0.2 examples/sec; 142.667 sec/batch)
2016-05-07 02:56:29.350347: step 86, loss = 6.04 (0.2 examples/sec; 143.555 sec/batch)
2016-05-07 02:58:51.569969: step 87, loss = 6.09 (0.2 examples/sec; 142.220 sec/batch)
2016-05-07 03:01:38.554312: step 88, loss = 6.22 (0.2 examples/sec; 166.984 sec/batch)
2016-05-07 03:04:45.710537: step 89, loss = 7.63 (0.2 examples/sec; 187.156 sec/batch)
2016-05-07 03:07:07.897893: step 90, loss = 6.25 (0.2 examples/sec; 142.187 sec/batch)
2016-05-07 03:12:02.146585: step 91, loss = 6.18 (0.2 examples/sec; 142.972 sec/batch)
2016-05-07 03:14:22.647058: step 92, loss = 12.20 (0.2 examples/sec; 140.500 sec/batch)
2016-05-07 03:16:43.500103: step 93, loss = 7.17 (0.2 examples/sec; 140.853 sec/batch)
2016-05-07 03:19:06.446723: step 94, loss = 7.08 (0.2 examples/sec; 142.947 sec/batch)
2016-05-07 03:21:28.095378: step 95, loss = 6.64 (0.2 examples/sec; 141.649 sec/batch)
2016-05-07 03:23:49.844560: step 96, loss = 6.22 (0.2 examples/sec; 141.749 sec/batch)
2016-05-07 03:26:11.469331: step 97, loss = 6.49 (0.2 examples/sec; 141.625 sec/batch)
2016-05-07 03:28:33.663946: step 98, loss = 6.53 (0.2 examples/sec; 142.195 sec/batch)
2016-05-07 03:30:56.556201: step 99, loss = 6.32 (0.2 examples/sec; 142.892 sec/batch)

Process finished with exit code 0
