/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-30 09:40:10.652077: step 0, loss = 63.93 (0.3 examples/sec; 199.156 sec/batch)
2016-04-30 09:40:22.034961: step 1, loss = 63.68 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 09:40:26.918829: step 2, loss = 63.56 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 09:40:31.986874: step 3, loss = 63.46 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 09:40:36.739163: step 4, loss = 63.28 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 09:40:41.518614: step 5, loss = 63.58 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 09:40:46.347476: step 6, loss = 63.69 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 09:40:51.252940: step 7, loss = 63.52 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 09:40:55.950479: step 8, loss = 63.31 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 09:41:01.412519: step 9, loss = 63.25 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 09:41:06.195696: step 10, loss = 63.40 (13.4 examples/sec; 4.783 sec/batch)
2016-04-30 09:41:17.479526: step 11, loss = 63.14 (13.6 examples/sec; 4.719 sec/batch)
2016-04-30 09:41:22.282154: step 12, loss = 62.89 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 09:41:27.087341: step 13, loss = 62.97 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 09:41:31.687080: step 14, loss = 64.71 (13.9 examples/sec; 4.600 sec/batch)
2016-04-30 09:41:37.186343: step 15, loss = 65.13 (11.6 examples/sec; 5.499 sec/batch)
2016-04-30 09:41:42.134784: step 16, loss = 62.97 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 09:41:48.680767: step 17, loss = 62.88 (9.8 examples/sec; 6.546 sec/batch)
2016-04-30 09:41:53.302812: step 18, loss = 62.89 (13.8 examples/sec; 4.622 sec/batch)
2016-04-30 09:41:58.099700: step 19, loss = 62.71 (13.3 examples/sec; 4.797 sec/batch)
2016-04-30 09:42:05.132989: step 20, loss = 62.82 (9.1 examples/sec; 7.033 sec/batch)
2016-04-30 09:42:24.252386: step 21, loss = 62.51 (8.3 examples/sec; 7.673 sec/batch)
2016-04-30 09:42:31.457218: step 22, loss = 62.43 (8.9 examples/sec; 7.205 sec/batch)
2016-04-30 09:42:40.333298: step 23, loss = 62.68 (7.2 examples/sec; 8.876 sec/batch)
2016-04-30 09:42:46.809972: step 24, loss = 62.45 (9.9 examples/sec; 6.477 sec/batch)
2016-04-30 09:42:53.740823: step 25, loss = 62.34 (9.2 examples/sec; 6.931 sec/batch)
2016-04-30 09:43:00.200353: step 26, loss = 62.56 (9.9 examples/sec; 6.459 sec/batch)
2016-04-30 09:43:06.111652: step 27, loss = 62.55 (10.8 examples/sec; 5.911 sec/batch)
2016-04-30 09:43:12.026375: step 28, loss = 62.45 (10.8 examples/sec; 5.915 sec/batch)
2016-04-30 09:43:18.831871: step 29, loss = 62.70 (9.4 examples/sec; 6.805 sec/batch)
2016-04-30 09:43:25.061933: step 30, loss = 62.44 (10.3 examples/sec; 6.230 sec/batch)
2016-04-30 09:43:39.234926: step 31, loss = 62.08 (10.8 examples/sec; 5.930 sec/batch)
2016-04-30 09:43:45.095161: step 32, loss = 62.02 (10.9 examples/sec; 5.860 sec/batch)
2016-04-30 09:43:51.727361: step 33, loss = 64.11 (9.7 examples/sec; 6.632 sec/batch)
2016-04-30 09:43:57.606612: step 34, loss = 63.19 (10.9 examples/sec; 5.879 sec/batch)
2016-04-30 09:44:03.730786: step 35, loss = 61.88 (10.5 examples/sec; 6.124 sec/batch)
2016-04-30 09:44:09.910804: step 36, loss = 61.95 (10.4 examples/sec; 6.180 sec/batch)
2016-04-30 09:44:15.988119: step 37, loss = 62.10 (10.5 examples/sec; 6.077 sec/batch)
2016-04-30 09:44:22.640643: step 38, loss = 61.85 (9.6 examples/sec; 6.652 sec/batch)
2016-04-30 09:44:28.647413: step 39, loss = 61.68 (10.7 examples/sec; 6.007 sec/batch)
2016-04-30 09:44:34.604042: step 40, loss = 61.71 (10.7 examples/sec; 5.957 sec/batch)
2016-04-30 09:44:48.600963: step 41, loss = 61.80 (10.9 examples/sec; 5.854 sec/batch)
2016-04-30 09:44:55.255284: step 42, loss = 61.72 (9.6 examples/sec; 6.654 sec/batch)
2016-04-30 09:45:01.332804: step 43, loss = 61.53 (10.5 examples/sec; 6.077 sec/batch)
2016-04-30 09:45:07.255703: step 44, loss = 61.69 (10.8 examples/sec; 5.923 sec/batch)
2016-04-30 09:45:13.277771: step 45, loss = 61.57 (10.6 examples/sec; 6.022 sec/batch)
2016-04-30 09:45:19.352649: step 46, loss = 61.47 (10.5 examples/sec; 6.075 sec/batch)
2016-04-30 09:45:25.960463: step 47, loss = 61.36 (9.7 examples/sec; 6.608 sec/batch)
2016-04-30 09:45:31.770771: step 48, loss = 61.37 (11.0 examples/sec; 5.810 sec/batch)
2016-04-30 09:45:37.779580: step 49, loss = 61.34 (10.7 examples/sec; 6.009 sec/batch)
2016-04-30 09:45:43.844476: step 50, loss = 61.37 (10.6 examples/sec; 6.065 sec/batch)
2016-04-30 09:45:57.763706: step 51, loss = 61.36 (10.8 examples/sec; 5.932 sec/batch)
2016-04-30 09:46:04.321262: step 52, loss = 61.22 (9.8 examples/sec; 6.557 sec/batch)
2016-04-30 09:46:10.358108: step 53, loss = 61.13 (10.6 examples/sec; 6.037 sec/batch)
2016-04-30 09:46:16.340783: step 54, loss = 61.20 (10.7 examples/sec; 5.983 sec/batch)
2016-04-30 09:46:22.092613: step 55, loss = 61.00 (11.1 examples/sec; 5.752 sec/batch)
2016-04-30 09:46:28.120098: step 56, loss = 60.81 (10.6 examples/sec; 6.027 sec/batch)
2016-04-30 09:46:34.745893: step 57, loss = 61.00 (9.7 examples/sec; 6.626 sec/batch)
2016-04-30 09:46:40.815234: step 58, loss = 60.94 (10.5 examples/sec; 6.069 sec/batch)
2016-04-30 09:46:46.688904: step 59, loss = 60.71 (10.9 examples/sec; 5.874 sec/batch)
2016-04-30 09:46:52.605910: step 60, loss = 60.84 (10.8 examples/sec; 5.917 sec/batch)
2016-04-30 09:47:07.289755: step 61, loss = 60.68 (10.1 examples/sec; 6.312 sec/batch)
2016-04-30 09:47:13.247921: step 62, loss = 60.67 (10.7 examples/sec; 5.958 sec/batch)
2016-04-30 09:47:19.155242: step 63, loss = 60.76 (10.8 examples/sec; 5.907 sec/batch)
2016-04-30 09:47:25.692557: step 64, loss = 60.74 (9.8 examples/sec; 6.537 sec/batch)
2016-04-30 09:47:34.615143: step 65, loss = 60.67 (7.2 examples/sec; 8.922 sec/batch)
2016-04-30 09:47:43.859637: step 66, loss = 60.47 (6.9 examples/sec; 9.244 sec/batch)
2016-04-30 09:47:52.577345: step 67, loss = 60.49 (7.3 examples/sec; 8.718 sec/batch)
2016-04-30 09:48:01.406249: step 68, loss = 60.52 (7.2 examples/sec; 8.829 sec/batch)
2016-04-30 09:48:10.311888: step 69, loss = 60.48 (7.2 examples/sec; 8.906 sec/batch)
2016-04-30 09:48:19.419534: step 70, loss = 60.45 (7.0 examples/sec; 9.108 sec/batch)
2016-04-30 09:48:39.537368: step 71, loss = 60.36 (7.5 examples/sec; 8.534 sec/batch)
2016-04-30 09:48:48.672470: step 72, loss = 60.24 (7.0 examples/sec; 9.135 sec/batch)
2016-04-30 09:48:57.256976: step 73, loss = 60.10 (7.5 examples/sec; 8.584 sec/batch)
2016-04-30 09:49:05.951871: step 74, loss = 60.22 (7.4 examples/sec; 8.695 sec/batch)
2016-04-30 09:49:14.424853: step 75, loss = 59.92 (7.6 examples/sec; 8.473 sec/batch)
2016-04-30 09:49:23.613640: step 76, loss = 60.10 (7.0 examples/sec; 9.189 sec/batch)
2016-04-30 09:49:30.560444: step 77, loss = 59.94 (9.2 examples/sec; 6.947 sec/batch)
2016-04-30 09:49:35.769614: step 78, loss = 59.70 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 09:49:40.966674: step 79, loss = 59.67 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 09:49:46.332480: step 80, loss = 59.84 (11.9 examples/sec; 5.366 sec/batch)
2016-04-30 09:49:58.766005: step 81, loss = 59.88 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 09:50:04.274160: step 82, loss = 59.68 (11.6 examples/sec; 5.508 sec/batch)
2016-04-30 09:50:09.606381: step 83, loss = 59.57 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 09:50:14.789802: step 84, loss = 59.53 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 09:50:20.289377: step 85, loss = 59.73 (11.6 examples/sec; 5.499 sec/batch)
2016-04-30 09:50:26.299272: step 86, loss = 59.57 (10.6 examples/sec; 6.010 sec/batch)
2016-04-30 09:50:31.584485: step 87, loss = 59.45 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 09:50:37.025683: step 88, loss = 59.33 (11.8 examples/sec; 5.441 sec/batch)
2016-04-30 09:50:42.695232: step 89, loss = 59.34 (11.3 examples/sec; 5.669 sec/batch)
2016-04-30 09:50:48.102660: step 90, loss = 59.32 (11.8 examples/sec; 5.407 sec/batch)
2016-04-30 09:51:01.380339: step 91, loss = 59.27 (10.7 examples/sec; 5.954 sec/batch)
2016-04-30 09:51:06.714198: step 92, loss = 59.43 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 09:51:11.969954: step 93, loss = 59.14 (12.2 examples/sec; 5.256 sec/batch)
2016-04-30 09:51:17.327756: step 94, loss = 59.09 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 09:51:23.218233: step 95, loss = 59.15 (10.9 examples/sec; 5.890 sec/batch)
2016-04-30 09:51:28.860924: step 96, loss = 59.14 (11.3 examples/sec; 5.643 sec/batch)
2016-04-30 09:51:34.833756: step 97, loss = 59.07 (10.7 examples/sec; 5.973 sec/batch)
2016-04-30 09:51:40.199046: step 98, loss = 59.19 (11.9 examples/sec; 5.365 sec/batch)
2016-04-30 09:51:45.669572: step 99, loss = 58.97 (11.7 examples/sec; 5.470 sec/batch)
2016-04-30 09:51:50.945931: step 100, loss = 59.00 (12.1 examples/sec; 5.276 sec/batch)
2016-04-30 09:52:04.209709: step 101, loss = 58.69 (10.7 examples/sec; 5.989 sec/batch)
2016-04-30 09:52:09.929427: step 102, loss = 58.59 (11.2 examples/sec; 5.720 sec/batch)
2016-04-30 09:52:15.413134: step 103, loss = 58.74 (11.7 examples/sec; 5.484 sec/batch)
2016-04-30 09:52:20.875047: step 104, loss = 58.61 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 09:52:26.230990: step 105, loss = 58.64 (11.9 examples/sec; 5.356 sec/batch)
2016-04-30 09:52:31.603476: step 106, loss = 58.56 (11.9 examples/sec; 5.372 sec/batch)
2016-04-30 09:52:37.510339: step 107, loss = 58.42 (10.8 examples/sec; 5.907 sec/batch)
2016-04-30 09:52:43.136914: step 108, loss = 58.47 (11.4 examples/sec; 5.626 sec/batch)
2016-04-30 09:52:48.545923: step 109, loss = 58.49 (11.8 examples/sec; 5.409 sec/batch)
2016-04-30 09:52:53.695959: step 110, loss = 58.52 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 09:53:06.544022: step 111, loss = 58.42 (11.7 examples/sec; 5.477 sec/batch)
2016-04-30 09:53:13.200086: step 112, loss = 58.26 (9.6 examples/sec; 6.656 sec/batch)
2016-04-30 09:53:19.453464: step 113, loss = 58.28 (10.2 examples/sec; 6.253 sec/batch)
2016-04-30 09:53:25.307542: step 114, loss = 58.24 (10.9 examples/sec; 5.854 sec/batch)
2016-04-30 09:53:30.952748: step 115, loss = 58.19 (11.3 examples/sec; 5.645 sec/batch)
2016-04-30 09:53:36.633495: step 116, loss = 58.13 (11.3 examples/sec; 5.681 sec/batch)
2016-04-30 09:53:41.751368: step 117, loss = 58.10 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 09:53:47.653198: step 118, loss = 58.10 (10.8 examples/sec; 5.902 sec/batch)
2016-04-30 09:53:53.193053: step 119, loss = 57.97 (11.6 examples/sec; 5.540 sec/batch)
2016-04-30 09:53:58.555173: step 120, loss = 57.82 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 09:54:11.224475: step 121, loss = 57.92 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 09:54:17.124341: step 122, loss = 57.83 (10.8 examples/sec; 5.900 sec/batch)
2016-04-30 09:54:22.783208: step 123, loss = 57.95 (11.3 examples/sec; 5.659 sec/batch)
2016-04-30 09:54:28.264458: step 124, loss = 57.89 (11.7 examples/sec; 5.481 sec/batch)
2016-04-30 09:54:33.733817: step 125, loss = 57.91 (11.7 examples/sec; 5.469 sec/batch)
2016-04-30 09:54:39.003925: step 126, loss = 57.66 (12.1 examples/sec; 5.270 sec/batch)
2016-04-30 09:54:44.165265: step 127, loss = 57.72 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 09:54:50.220090: step 128, loss = 57.71 (10.6 examples/sec; 6.055 sec/batch)
2016-04-30 09:54:55.708297: step 129, loss = 57.58 (11.7 examples/sec; 5.488 sec/batch)
2016-04-30 09:55:01.308847: step 130, loss = 57.53 (11.4 examples/sec; 5.600 sec/batch)
2016-04-30 09:55:13.720416: step 131, loss = 57.28 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 09:55:19.477008: step 132, loss = 57.39 (11.1 examples/sec; 5.757 sec/batch)
2016-04-30 09:55:25.491996: step 133, loss = 57.28 (10.6 examples/sec; 6.015 sec/batch)
2016-04-30 09:55:30.934314: step 134, loss = 57.31 (11.8 examples/sec; 5.442 sec/batch)
2016-04-30 09:55:36.426119: step 135, loss = 57.32 (11.7 examples/sec; 5.492 sec/batch)
2016-04-30 09:55:41.473754: step 136, loss = 57.31 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 09:55:46.832471: step 137, loss = 57.21 (11.9 examples/sec; 5.359 sec/batch)
2016-04-30 09:55:52.334969: step 138, loss = 57.10 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 09:55:58.417266: step 139, loss = 57.11 (10.5 examples/sec; 6.082 sec/batch)
2016-04-30 09:56:03.907609: step 140, loss = 56.96 (11.7 examples/sec; 5.490 sec/batch)
2016-04-30 09:56:16.347572: step 141, loss = 56.97 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 09:56:22.070025: step 142, loss = 57.06 (11.2 examples/sec; 5.722 sec/batch)
2016-04-30 09:56:27.494448: step 143, loss = 56.95 (11.8 examples/sec; 5.424 sec/batch)
2016-04-30 09:56:33.268041: step 144, loss = 56.84 (11.1 examples/sec; 5.774 sec/batch)
2016-04-30 09:56:38.626038: step 145, loss = 56.76 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 09:56:43.982535: step 146, loss = 56.75 (11.9 examples/sec; 5.356 sec/batch)
2016-04-30 09:56:49.391615: step 147, loss = 56.89 (11.8 examples/sec; 5.409 sec/batch)
2016-04-30 09:56:54.864029: step 148, loss = 56.77 (11.7 examples/sec; 5.472 sec/batch)
2016-04-30 09:57:01.255068: step 149, loss = 56.64 (10.0 examples/sec; 6.391 sec/batch)
2016-04-30 09:57:07.105258: step 150, loss = 56.45 (10.9 examples/sec; 5.850 sec/batch)
2016-04-30 09:57:19.632342: step 151, loss = 56.55 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 09:57:25.026339: step 152, loss = 56.56 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 09:57:30.403748: step 153, loss = 56.58 (11.9 examples/sec; 5.377 sec/batch)
2016-04-30 09:57:36.371644: step 154, loss = 56.20 (10.7 examples/sec; 5.968 sec/batch)
2016-04-30 09:57:41.430405: step 155, loss = 56.41 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 09:57:46.870005: step 156, loss = 56.27 (11.8 examples/sec; 5.439 sec/batch)
2016-04-30 09:57:52.182341: step 157, loss = 56.29 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 09:57:57.393988: step 158, loss = 56.04 (12.3 examples/sec; 5.212 sec/batch)
2016-04-30 09:58:02.660209: step 159, loss = 56.22 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 09:58:08.689960: step 160, loss = 56.18 (10.6 examples/sec; 6.030 sec/batch)
2016-04-30 09:58:21.453949: step 161, loss = 56.11 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 09:58:26.601144: step 162, loss = 55.92 (12.4 examples/sec; 5.147 sec/batch)
2016-04-30 09:58:31.805620: step 163, loss = 56.13 (12.3 examples/sec; 5.204 sec/batch)
2016-04-30 09:58:37.138696: step 164, loss = 56.14 (12.0 examples/sec; 5.333 sec/batch)
2016-04-30 09:58:43.342472: step 165, loss = 55.95 (10.3 examples/sec; 6.204 sec/batch)
2016-04-30 09:58:48.872656: step 166, loss = 55.98 (11.6 examples/sec; 5.530 sec/batch)
2016-04-30 09:58:54.492152: step 167, loss = 56.15 (11.4 examples/sec; 5.619 sec/batch)
2016-04-30 09:58:59.643125: step 168, loss = 55.84 (12.4 examples/sec; 5.151 sec/batch)
2016-04-30 09:59:05.098087: step 169, loss = 55.57 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 09:59:10.697179: step 170, loss = 55.84 (11.4 examples/sec; 5.599 sec/batch)
2016-04-30 09:59:23.935280: step 171, loss = 55.55 (12.7 examples/sec; 5.045 sec/batch)
2016-04-30 09:59:29.279068: step 172, loss = 55.55 (12.0 examples/sec; 5.344 sec/batch)
2016-04-30 09:59:34.881156: step 173, loss = 55.68 (11.4 examples/sec; 5.602 sec/batch)
2016-04-30 09:59:40.240413: step 174, loss = 55.69 (11.9 examples/sec; 5.359 sec/batch)
2016-04-30 09:59:45.968935: step 175, loss = 55.50 (11.2 examples/sec; 5.728 sec/batch)
2016-04-30 09:59:51.507434: step 176, loss = 55.36 (11.6 examples/sec; 5.538 sec/batch)
2016-04-30 09:59:56.489501: step 177, loss = 55.51 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 10:00:02.048452: step 178, loss = 55.48 (11.5 examples/sec; 5.559 sec/batch)
2016-04-30 10:00:07.866903: step 179, loss = 55.31 (11.0 examples/sec; 5.818 sec/batch)
2016-04-30 10:00:14.230572: step 180, loss = 55.21 (10.1 examples/sec; 6.364 sec/batch)
2016-04-30 10:00:27.922101: step 181, loss = 55.27 (11.7 examples/sec; 5.486 sec/batch)
2016-04-30 10:00:33.306278: step 182, loss = 55.13 (11.9 examples/sec; 5.384 sec/batch)
2016-04-30 10:00:38.893704: step 183, loss = 55.12 (11.5 examples/sec; 5.587 sec/batch)
2016-04-30 10:00:44.288453: step 184, loss = 55.11 (11.9 examples/sec; 5.395 sec/batch)
2016-04-30 10:00:49.956471: step 185, loss = 54.95 (11.3 examples/sec; 5.668 sec/batch)
2016-04-30 10:00:56.179148: step 186, loss = 55.02 (10.3 examples/sec; 6.223 sec/batch)
2016-04-30 10:01:02.376007: step 187, loss = 55.01 (10.3 examples/sec; 6.197 sec/batch)
2016-04-30 10:01:07.885876: step 188, loss = 54.75 (11.6 examples/sec; 5.510 sec/batch)
2016-04-30 10:01:13.459161: step 189, loss = 54.89 (11.5 examples/sec; 5.573 sec/batch)
2016-04-30 10:01:18.884471: step 190, loss = 54.64 (11.8 examples/sec; 5.425 sec/batch)
2016-04-30 10:01:31.890183: step 191, loss = 54.66 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 10:01:37.302821: step 192, loss = 54.77 (11.8 examples/sec; 5.413 sec/batch)
2016-04-30 10:01:42.789368: step 193, loss = 54.76 (11.7 examples/sec; 5.486 sec/batch)
2016-04-30 10:01:47.845430: step 194, loss = 54.72 (12.7 examples/sec; 5.056 sec/batch)
2016-04-30 10:01:53.456191: step 195, loss = 54.75 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 10:01:59.665712: step 196, loss = 54.71 (10.3 examples/sec; 6.209 sec/batch)
2016-04-30 10:02:05.301765: step 197, loss = 54.53 (11.4 examples/sec; 5.636 sec/batch)
2016-04-30 10:02:10.623093: step 198, loss = 54.36 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 10:02:16.161103: step 199, loss = 54.52 (11.6 examples/sec; 5.538 sec/batch)
2016-04-30 10:02:21.925759: step 200, loss = 54.38 (11.1 examples/sec; 5.765 sec/batch)
2016-04-30 10:02:35.245019: step 201, loss = 54.10 (10.8 examples/sec; 5.926 sec/batch)
2016-04-30 10:02:40.638845: step 202, loss = 54.42 (11.9 examples/sec; 5.393 sec/batch)
2016-04-30 10:02:46.097441: step 203, loss = 54.37 (11.7 examples/sec; 5.458 sec/batch)
2016-04-30 10:02:51.807837: step 204, loss = 54.18 (11.2 examples/sec; 5.710 sec/batch)
2016-04-30 10:02:56.917969: step 205, loss = 54.12 (12.5 examples/sec; 5.110 sec/batch)
2016-04-30 10:03:02.421747: step 206, loss = 54.13 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 10:03:08.482033: step 207, loss = 54.25 (10.6 examples/sec; 6.060 sec/batch)
2016-04-30 10:03:13.861609: step 208, loss = 54.24 (11.9 examples/sec; 5.379 sec/batch)
2016-04-30 10:03:19.249986: step 209, loss = 54.05 (11.9 examples/sec; 5.388 sec/batch)
2016-04-30 10:03:24.682931: step 210, loss = 54.22 (11.8 examples/sec; 5.433 sec/batch)
2016-04-30 10:03:38.051176: step 211, loss = 53.92 (10.4 examples/sec; 6.131 sec/batch)
2016-04-30 10:03:43.508042: step 212, loss = 54.04 (11.7 examples/sec; 5.457 sec/batch)
2016-04-30 10:03:49.004620: step 213, loss = 53.95 (11.6 examples/sec; 5.496 sec/batch)
2016-04-30 10:03:54.594969: step 214, loss = 53.96 (11.4 examples/sec; 5.590 sec/batch)
2016-04-30 10:04:00.632484: step 215, loss = 53.87 (10.6 examples/sec; 6.037 sec/batch)
2016-04-30 10:04:07.156200: step 216, loss = 53.87 (9.8 examples/sec; 6.524 sec/batch)
2016-04-30 10:04:13.470457: step 217, loss = 53.50 (10.1 examples/sec; 6.314 sec/batch)
2016-04-30 10:04:19.310878: step 218, loss = 53.68 (11.0 examples/sec; 5.840 sec/batch)
2016-04-30 10:04:25.003937: step 219, loss = 53.71 (11.2 examples/sec; 5.693 sec/batch)
2016-04-30 10:04:30.332911: step 220, loss = 53.49 (12.0 examples/sec; 5.329 sec/batch)
2016-04-30 10:04:43.662200: step 221, loss = 53.76 (10.9 examples/sec; 5.870 sec/batch)
2016-04-30 10:04:49.137302: step 222, loss = 53.52 (11.7 examples/sec; 5.475 sec/batch)
2016-04-30 10:04:54.588889: step 223, loss = 53.54 (11.7 examples/sec; 5.451 sec/batch)
2016-04-30 10:04:59.876342: step 224, loss = 53.26 (12.1 examples/sec; 5.287 sec/batch)
2016-04-30 10:05:05.335956: step 225, loss = 53.46 (11.7 examples/sec; 5.460 sec/batch)
2016-04-30 10:05:10.746798: step 226, loss = 53.45 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 10:05:17.041803: step 227, loss = 53.26 (10.2 examples/sec; 6.295 sec/batch)
2016-04-30 10:05:22.541012: step 228, loss = 53.10 (11.6 examples/sec; 5.499 sec/batch)
2016-04-30 10:05:28.045353: step 229, loss = 53.17 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 10:05:33.393989: step 230, loss = 53.17 (12.0 examples/sec; 5.349 sec/batch)
2016-04-30 10:05:45.780423: step 231, loss = 53.20 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 10:05:51.789363: step 232, loss = 53.05 (10.7 examples/sec; 6.009 sec/batch)
2016-04-30 10:05:56.784492: step 233, loss = 53.03 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 10:06:02.127592: step 234, loss = 52.93 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 10:06:07.496945: step 235, loss = 52.90 (11.9 examples/sec; 5.369 sec/batch)
2016-04-30 10:06:12.946492: step 236, loss = 52.87 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 10:06:18.488542: step 237, loss = 52.98 (11.5 examples/sec; 5.542 sec/batch)
2016-04-30 10:06:24.613608: step 238, loss = 52.91 (10.4 examples/sec; 6.125 sec/batch)
2016-04-30 10:06:29.725206: step 239, loss = 52.73 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 10:06:35.199026: step 240, loss = 52.75 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 10:06:47.534443: step 241, loss = 52.65 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 10:06:52.917255: step 242, loss = 52.69 (11.9 examples/sec; 5.383 sec/batch)
2016-04-30 10:06:59.059496: step 243, loss = 52.60 (10.4 examples/sec; 6.142 sec/batch)
2016-04-30 10:07:04.588214: step 244, loss = 52.59 (11.6 examples/sec; 5.529 sec/batch)
2016-04-30 10:07:10.125432: step 245, loss = 52.73 (11.6 examples/sec; 5.537 sec/batch)
2016-04-30 10:07:15.526977: step 246, loss = 52.36 (11.8 examples/sec; 5.401 sec/batch)
2016-04-30 10:07:20.763319: step 247, loss = 52.36 (12.2 examples/sec; 5.236 sec/batch)
2016-04-30 10:07:26.180224: step 248, loss = 52.60 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 10:07:32.141474: step 249, loss = 52.28 (10.7 examples/sec; 5.961 sec/batch)
2016-04-30 10:07:37.537083: step 250, loss = 52.53 (11.9 examples/sec; 5.396 sec/batch)
2016-04-30 10:07:49.958248: step 251, loss = 52.34 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 10:07:55.454579: step 252, loss = 52.26 (11.6 examples/sec; 5.496 sec/batch)
2016-04-30 10:08:01.473348: step 253, loss = 52.26 (10.6 examples/sec; 6.019 sec/batch)
2016-04-30 10:08:06.888594: step 254, loss = 52.20 (11.8 examples/sec; 5.415 sec/batch)
2016-04-30 10:08:11.943618: step 255, loss = 52.18 (12.7 examples/sec; 5.055 sec/batch)
2016-04-30 10:08:17.127589: step 256, loss = 52.19 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 10:08:22.473145: step 257, loss = 52.16 (12.0 examples/sec; 5.345 sec/batch)
2016-04-30 10:08:28.047572: step 258, loss = 52.02 (11.5 examples/sec; 5.574 sec/batch)
2016-04-30 10:08:33.332809: step 259, loss = 51.99 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 10:08:39.219215: step 260, loss = 51.98 (10.9 examples/sec; 5.886 sec/batch)
2016-04-30 10:08:51.644292: step 261, loss = 51.76 (12.5 examples/sec; 5.132 sec/batch)
2016-04-30 10:08:56.619489: step 262, loss = 51.71 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 10:09:02.375214: step 263, loss = 51.80 (11.1 examples/sec; 5.756 sec/batch)
2016-04-30 10:09:09.197786: step 264, loss = 51.78 (9.4 examples/sec; 6.822 sec/batch)
2016-04-30 10:09:15.570384: step 265, loss = 51.67 (10.0 examples/sec; 6.372 sec/batch)
2016-04-30 10:09:21.055985: step 266, loss = 51.68 (11.7 examples/sec; 5.485 sec/batch)
2016-04-30 10:09:26.447159: step 267, loss = 51.80 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 10:09:32.005280: step 268, loss = 51.54 (11.5 examples/sec; 5.558 sec/batch)
2016-04-30 10:09:37.369266: step 269, loss = 51.60 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 10:09:43.488938: step 270, loss = 51.70 (10.5 examples/sec; 6.120 sec/batch)
2016-04-30 10:09:56.061182: step 271, loss = 51.43 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 10:10:01.759337: step 272, loss = 51.43 (11.2 examples/sec; 5.698 sec/batch)
2016-04-30 10:10:07.332339: step 273, loss = 51.41 (11.5 examples/sec; 5.573 sec/batch)
2016-04-30 10:10:13.219972: step 274, loss = 51.40 (10.9 examples/sec; 5.888 sec/batch)
2016-04-30 10:10:19.163915: step 275, loss = 51.26 (10.8 examples/sec; 5.944 sec/batch)
2016-04-30 10:10:24.771275: step 276, loss = 51.35 (11.4 examples/sec; 5.607 sec/batch)
2016-04-30 10:10:30.075421: step 277, loss = 51.30 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 10:10:35.652724: step 278, loss = 51.20 (11.5 examples/sec; 5.577 sec/batch)
2016-04-30 10:10:41.115573: step 279, loss = 51.21 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 10:10:47.257882: step 280, loss = 51.19 (10.4 examples/sec; 6.142 sec/batch)
2016-04-30 10:11:00.270323: step 281, loss = 50.99 (12.1 examples/sec; 5.289 sec/batch)
2016-04-30 10:11:05.745779: step 282, loss = 51.13 (11.7 examples/sec; 5.475 sec/batch)
2016-04-30 10:11:11.172163: step 283, loss = 50.97 (11.8 examples/sec; 5.426 sec/batch)
2016-04-30 10:11:16.603612: step 284, loss = 50.93 (11.8 examples/sec; 5.431 sec/batch)
2016-04-30 10:11:22.669238: step 285, loss = 50.79 (10.6 examples/sec; 6.066 sec/batch)
2016-04-30 10:11:28.329883: step 286, loss = 50.97 (11.3 examples/sec; 5.661 sec/batch)
2016-04-30 10:11:33.893873: step 287, loss = 50.78 (11.5 examples/sec; 5.564 sec/batch)
2016-04-30 10:11:39.248361: step 288, loss = 50.92 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 10:11:44.568731: step 289, loss = 50.77 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 10:11:50.042556: step 290, loss = 50.68 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 10:12:03.368320: step 291, loss = 50.79 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 10:12:08.632043: step 292, loss = 50.63 (12.2 examples/sec; 5.264 sec/batch)
2016-04-30 10:12:14.112348: step 293, loss = 50.76 (11.7 examples/sec; 5.480 sec/batch)
2016-04-30 10:12:19.692911: step 294, loss = 50.61 (11.5 examples/sec; 5.580 sec/batch)
2016-04-30 10:12:25.697176: step 295, loss = 50.50 (10.7 examples/sec; 6.004 sec/batch)
2016-04-30 10:12:31.320831: step 296, loss = 50.53 (11.4 examples/sec; 5.624 sec/batch)
2016-04-30 10:12:36.696657: step 297, loss = 50.46 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 10:12:41.844477: step 298, loss = 50.45 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 10:12:47.164593: step 299, loss = 50.21 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 10:12:52.743342: step 300, loss = 50.26 (11.5 examples/sec; 5.579 sec/batch)
2016-04-30 10:13:06.286475: step 301, loss = 50.33 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 10:13:11.659986: step 302, loss = 50.28 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 10:13:17.053989: step 303, loss = 50.20 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 10:13:22.769183: step 304, loss = 50.10 (11.2 examples/sec; 5.715 sec/batch)
2016-04-30 10:13:28.740804: step 305, loss = 50.13 (10.7 examples/sec; 5.972 sec/batch)
2016-04-30 10:13:34.765342: step 306, loss = 50.11 (10.6 examples/sec; 6.024 sec/batch)
2016-04-30 10:13:40.387983: step 307, loss = 50.16 (11.4 examples/sec; 5.623 sec/batch)
2016-04-30 10:13:45.790203: step 308, loss = 50.14 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 10:13:51.141542: step 309, loss = 50.09 (12.0 examples/sec; 5.351 sec/batch)
2016-04-30 10:13:56.568745: step 310, loss = 49.73 (11.8 examples/sec; 5.427 sec/batch)
2016-04-30 10:14:10.178382: step 311, loss = 49.93 (11.9 examples/sec; 5.399 sec/batch)
2016-04-30 10:14:15.699305: step 312, loss = 49.89 (11.6 examples/sec; 5.521 sec/batch)
2016-04-30 10:14:20.981048: step 313, loss = 49.98 (12.1 examples/sec; 5.282 sec/batch)
2016-04-30 10:14:26.367091: step 314, loss = 49.80 (11.9 examples/sec; 5.386 sec/batch)
2016-04-30 10:14:31.763788: step 315, loss = 49.77 (11.9 examples/sec; 5.397 sec/batch)
2016-04-30 10:14:38.253056: step 316, loss = 49.77 (9.9 examples/sec; 6.488 sec/batch)
2016-04-30 10:14:43.820590: step 317, loss = 49.72 (11.5 examples/sec; 5.567 sec/batch)
2016-04-30 10:14:49.454466: step 318, loss = 49.79 (11.4 examples/sec; 5.634 sec/batch)
2016-04-30 10:14:54.969304: step 319, loss = 49.55 (11.6 examples/sec; 5.515 sec/batch)
2016-04-30 10:15:00.032260: step 320, loss = 49.55 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 10:15:13.549953: step 321, loss = 49.64 (10.8 examples/sec; 5.916 sec/batch)
2016-04-30 10:15:19.567090: step 322, loss = 49.45 (10.6 examples/sec; 6.017 sec/batch)
2016-04-30 10:15:25.693667: step 323, loss = 49.46 (10.4 examples/sec; 6.126 sec/batch)
2016-04-30 10:15:32.290123: step 324, loss = 49.48 (9.7 examples/sec; 6.596 sec/batch)
2016-04-30 10:15:37.823483: step 325, loss = 49.37 (11.6 examples/sec; 5.533 sec/batch)
2016-04-30 10:15:44.285628: step 326, loss = 49.29 (9.9 examples/sec; 6.462 sec/batch)
2016-04-30 10:15:49.818066: step 327, loss = 49.30 (11.6 examples/sec; 5.532 sec/batch)
2016-04-30 10:15:55.468523: step 328, loss = 49.23 (11.3 examples/sec; 5.650 sec/batch)
2016-04-30 10:16:01.184459: step 329, loss = 49.22 (11.2 examples/sec; 5.716 sec/batch)
2016-04-30 10:16:06.651591: step 330, loss = 49.12 (11.7 examples/sec; 5.467 sec/batch)
2016-04-30 10:16:19.734269: step 331, loss = 49.10 (11.1 examples/sec; 5.746 sec/batch)
2016-04-30 10:16:25.232209: step 332, loss = 49.08 (11.6 examples/sec; 5.498 sec/batch)
2016-04-30 10:16:30.726554: step 333, loss = 49.00 (11.6 examples/sec; 5.494 sec/batch)
2016-04-30 10:16:35.899618: step 334, loss = 48.88 (12.4 examples/sec; 5.173 sec/batch)
2016-04-30 10:16:41.501225: step 335, loss = 48.84 (11.4 examples/sec; 5.602 sec/batch)
2016-04-30 10:16:46.886231: step 336, loss = 49.09 (11.9 examples/sec; 5.385 sec/batch)
2016-04-30 10:16:52.791755: step 337, loss = 49.02 (10.8 examples/sec; 5.905 sec/batch)
2016-04-30 10:16:58.508839: step 338, loss = 48.80 (11.2 examples/sec; 5.717 sec/batch)
2016-04-30 10:17:04.188166: step 339, loss = 48.83 (11.3 examples/sec; 5.679 sec/batch)
2016-04-30 10:17:09.448603: step 340, loss = 48.88 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 10:17:22.177139: step 341, loss = 48.82 (11.5 examples/sec; 5.578 sec/batch)
2016-04-30 10:17:27.882330: step 342, loss = 48.80 (11.2 examples/sec; 5.705 sec/batch)
2016-04-30 10:17:33.286017: step 343, loss = 48.48 (11.8 examples/sec; 5.404 sec/batch)
2016-04-30 10:17:38.528462: step 344, loss = 48.34 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 10:17:43.929298: step 345, loss = 48.63 (11.9 examples/sec; 5.401 sec/batch)
2016-04-30 10:17:49.335076: step 346, loss = 48.48 (11.8 examples/sec; 5.406 sec/batch)
2016-04-30 10:17:54.718319: step 347, loss = 48.65 (11.9 examples/sec; 5.383 sec/batch)
2016-04-30 10:18:00.767207: step 348, loss = 48.54 (10.6 examples/sec; 6.049 sec/batch)
2016-04-30 10:18:05.995194: step 349, loss = 48.44 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 10:18:11.479412: step 350, loss = 48.36 (11.7 examples/sec; 5.484 sec/batch)
2016-04-30 10:18:24.476205: step 351, loss = 48.59 (12.1 examples/sec; 5.306 sec/batch)
2016-04-30 10:18:30.759689: step 352, loss = 48.41 (10.2 examples/sec; 6.283 sec/batch)
2016-04-30 10:18:35.990303: step 353, loss = 48.23 (12.2 examples/sec; 5.231 sec/batch)
2016-04-30 10:18:41.506186: step 354, loss = 48.34 (11.6 examples/sec; 5.516 sec/batch)
2016-04-30 10:18:46.871587: step 355, loss = 48.14 (11.9 examples/sec; 5.365 sec/batch)
2016-04-30 10:18:52.692859: step 356, loss = 48.22 (11.0 examples/sec; 5.821 sec/batch)
2016-04-30 10:18:58.178515: step 357, loss = 48.18 (11.7 examples/sec; 5.486 sec/batch)
2016-04-30 10:19:04.479792: step 358, loss = 48.16 (10.2 examples/sec; 6.301 sec/batch)
2016-04-30 10:19:10.055259: step 359, loss = 48.20 (11.5 examples/sec; 5.575 sec/batch)
2016-04-30 10:19:15.449646: step 360, loss = 48.09 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 10:19:28.047556: step 361, loss = 48.02 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 10:19:33.274470: step 362, loss = 47.96 (12.2 examples/sec; 5.227 sec/batch)
2016-04-30 10:19:39.363154: step 363, loss = 47.99 (10.5 examples/sec; 6.089 sec/batch)
2016-04-30 10:19:44.825950: step 364, loss = 47.69 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 10:19:50.343763: step 365, loss = 47.79 (11.6 examples/sec; 5.518 sec/batch)
2016-04-30 10:19:56.029330: step 366, loss = 47.72 (11.3 examples/sec; 5.685 sec/batch)
2016-04-30 10:20:01.757925: step 367, loss = 47.73 (11.2 examples/sec; 5.728 sec/batch)
2016-04-30 10:20:07.539970: step 368, loss = 47.63 (11.1 examples/sec; 5.782 sec/batch)
2016-04-30 10:20:13.422906: step 369, loss = 48.05 (10.9 examples/sec; 5.883 sec/batch)
2016-04-30 10:20:18.972309: step 370, loss = 47.76 (11.5 examples/sec; 5.549 sec/batch)
2016-04-30 10:20:31.596025: step 371, loss = 47.67 (11.9 examples/sec; 5.386 sec/batch)
2016-04-30 10:20:37.452021: step 372, loss = 47.67 (10.9 examples/sec; 5.856 sec/batch)
2016-04-30 10:20:43.576715: step 373, loss = 47.49 (10.4 examples/sec; 6.125 sec/batch)
2016-04-30 10:20:48.937497: step 374, loss = 47.40 (11.9 examples/sec; 5.361 sec/batch)
2016-04-30 10:20:54.416495: step 375, loss = 47.36 (11.7 examples/sec; 5.479 sec/batch)
2016-04-30 10:21:00.846571: step 376, loss = 47.45 (10.0 examples/sec; 6.430 sec/batch)
2016-04-30 10:21:07.239699: step 377, loss = 47.48 (10.0 examples/sec; 6.393 sec/batch)
2016-04-30 10:21:13.251997: step 378, loss = 47.38 (10.6 examples/sec; 6.012 sec/batch)
2016-04-30 10:21:19.528908: step 379, loss = 47.18 (10.2 examples/sec; 6.277 sec/batch)
2016-04-30 10:21:25.616443: step 380, loss = 47.43 (10.5 examples/sec; 6.087 sec/batch)
2016-04-30 10:21:38.629306: step 381, loss = 47.42 (11.8 examples/sec; 5.420 sec/batch)
2016-04-30 10:21:44.513250: step 382, loss = 47.42 (10.9 examples/sec; 5.884 sec/batch)
2016-04-30 10:21:51.109006: step 383, loss = 47.19 (9.7 examples/sec; 6.596 sec/batch)
2016-04-30 10:21:56.748221: step 384, loss = 47.08 (11.3 examples/sec; 5.639 sec/batch)
2016-04-30 10:22:02.556607: step 385, loss = 47.16 (11.0 examples/sec; 5.808 sec/batch)
2016-04-30 10:22:08.237827: step 386, loss = 47.08 (11.3 examples/sec; 5.681 sec/batch)
2016-04-30 10:22:13.945412: step 387, loss = 47.02 (11.2 examples/sec; 5.707 sec/batch)
2016-04-30 10:22:20.074111: step 388, loss = 47.07 (10.4 examples/sec; 6.129 sec/batch)
2016-04-30 10:22:25.766554: step 389, loss = 46.91 (11.2 examples/sec; 5.692 sec/batch)
2016-04-30 10:22:31.623128: step 390, loss = 46.85 (10.9 examples/sec; 5.856 sec/batch)
2016-04-30 10:22:44.557287: step 391, loss = 47.01 (11.7 examples/sec; 5.493 sec/batch)
2016-04-30 10:22:50.153821: step 392, loss = 46.96 (11.4 examples/sec; 5.596 sec/batch)
2016-04-30 10:22:56.344614: step 393, loss = 46.84 (10.3 examples/sec; 6.191 sec/batch)
2016-04-30 10:23:02.310542: step 394, loss = 46.78 (10.7 examples/sec; 5.966 sec/batch)
2016-04-30 10:23:07.804639: step 395, loss = 46.71 (11.6 examples/sec; 5.494 sec/batch)
2016-04-30 10:23:13.583449: step 396, loss = 46.67 (11.1 examples/sec; 5.779 sec/batch)
2016-04-30 10:23:19.238999: step 397, loss = 46.79 (11.3 examples/sec; 5.655 sec/batch)
2016-04-30 10:23:24.951393: step 398, loss = 46.64 (11.2 examples/sec; 5.712 sec/batch)
2016-04-30 10:23:31.005970: step 399, loss = 46.72 (10.6 examples/sec; 6.054 sec/batch)
2016-04-30 10:23:36.672039: step 400, loss = 46.50 (11.3 examples/sec; 5.666 sec/batch)
2016-04-30 10:23:49.610747: step 401, loss = 46.60 (11.9 examples/sec; 5.389 sec/batch)
2016-04-30 10:23:55.221462: step 402, loss = 46.55 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 10:24:01.564175: step 403, loss = 46.34 (10.1 examples/sec; 6.343 sec/batch)
2016-04-30 10:24:07.227149: step 404, loss = 46.49 (11.3 examples/sec; 5.663 sec/batch)
2016-04-30 10:24:14.173603: step 405, loss = 46.57 (9.2 examples/sec; 6.940 sec/batch)
2016-04-30 10:24:20.111544: step 406, loss = 46.39 (10.8 examples/sec; 5.935 sec/batch)
2016-04-30 10:24:25.898839: step 407, loss = 46.17 (11.1 examples/sec; 5.787 sec/batch)
2016-04-30 10:24:32.323225: step 408, loss = 46.36 (10.0 examples/sec; 6.424 sec/batch)
2016-04-30 10:24:38.121660: step 409, loss = 46.34 (11.0 examples/sec; 5.798 sec/batch)
2016-04-30 10:24:43.663073: step 410, loss = 46.13 (11.5 examples/sec; 5.541 sec/batch)
2016-04-30 10:24:56.609878: step 411, loss = 46.10 (11.6 examples/sec; 5.503 sec/batch)
2016-04-30 10:25:02.301595: step 412, loss = 46.03 (11.2 examples/sec; 5.692 sec/batch)
2016-04-30 10:25:08.471777: step 413, loss = 46.00 (10.4 examples/sec; 6.170 sec/batch)
2016-04-30 10:25:14.158029: step 414, loss = 45.95 (11.3 examples/sec; 5.686 sec/batch)
2016-04-30 10:25:19.887490: step 415, loss = 46.09 (11.2 examples/sec; 5.729 sec/batch)
2016-04-30 10:25:25.537533: step 416, loss = 46.12 (11.3 examples/sec; 5.650 sec/batch)
2016-04-30 10:25:30.858214: step 417, loss = 45.94 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 10:25:36.400153: step 418, loss = 45.92 (11.5 examples/sec; 5.542 sec/batch)
2016-04-30 10:25:42.380441: step 419, loss = 45.85 (10.7 examples/sec; 5.980 sec/batch)
2016-04-30 10:25:47.874384: step 420, loss = 45.90 (11.6 examples/sec; 5.494 sec/batch)
2016-04-30 10:26:01.097999: step 421, loss = 45.85 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 10:26:06.413442: step 422, loss = 45.73 (12.0 examples/sec; 5.315 sec/batch)
2016-04-30 10:26:12.966223: step 423, loss = 45.59 (9.8 examples/sec; 6.553 sec/batch)
2016-04-30 10:26:18.414926: step 424, loss = 45.83 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 10:26:23.850290: step 425, loss = 45.63 (11.8 examples/sec; 5.435 sec/batch)
2016-04-30 10:26:29.352551: step 426, loss = 45.71 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 10:26:35.322998: step 427, loss = 45.65 (10.7 examples/sec; 5.970 sec/batch)
2016-04-30 10:26:41.421712: step 428, loss = 45.54 (10.5 examples/sec; 6.099 sec/batch)
2016-04-30 10:26:48.268817: step 429, loss = 45.48 (9.3 examples/sec; 6.847 sec/batch)
2016-04-30 10:26:53.952961: step 430, loss = 45.57 (11.3 examples/sec; 5.684 sec/batch)
2016-04-30 10:27:07.063704: step 431, loss = 45.39 (12.2 examples/sec; 5.225 sec/batch)
2016-04-30 10:27:12.318738: step 432, loss = 45.30 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 10:27:18.456476: step 433, loss = 45.10 (10.4 examples/sec; 6.138 sec/batch)
2016-04-30 10:27:23.818672: step 434, loss = 45.39 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 10:27:29.299210: step 435, loss = 45.32 (11.7 examples/sec; 5.480 sec/batch)
2016-04-30 10:27:34.749429: step 436, loss = 45.03 (11.7 examples/sec; 5.450 sec/batch)
2016-04-30 10:27:40.182644: step 437, loss = 45.28 (11.8 examples/sec; 5.433 sec/batch)
2016-04-30 10:27:45.800582: step 438, loss = 45.08 (11.4 examples/sec; 5.618 sec/batch)
2016-04-30 10:27:51.607802: step 439, loss = 45.36 (11.0 examples/sec; 5.807 sec/batch)
2016-04-30 10:27:56.803646: step 440, loss = 45.13 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 10:28:09.252500: step 441, loss = 45.05 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 10:28:14.735714: step 442, loss = 45.16 (11.7 examples/sec; 5.483 sec/batch)
2016-04-30 10:28:20.212847: step 443, loss = 44.89 (11.7 examples/sec; 5.477 sec/batch)
2016-04-30 10:28:26.223479: step 444, loss = 44.97 (10.6 examples/sec; 6.011 sec/batch)
2016-04-30 10:28:31.607599: step 445, loss = 44.79 (11.9 examples/sec; 5.384 sec/batch)
2016-04-30 10:28:36.630781: step 446, loss = 44.94 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 10:28:41.694926: step 447, loss = 44.85 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 10:28:46.940455: step 448, loss = 44.95 (12.2 examples/sec; 5.245 sec/batch)
2016-04-30 10:28:52.256967: step 449, loss = 44.61 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 10:28:58.375542: step 450, loss = 44.84 (10.5 examples/sec; 6.118 sec/batch)
2016-04-30 10:29:11.857797: step 451, loss = 45.03 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 10:29:17.175415: step 452, loss = 44.70 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 10:29:22.518907: step 453, loss = 44.50 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 10:29:28.505268: step 454, loss = 44.66 (10.7 examples/sec; 5.986 sec/batch)
2016-04-30 10:29:34.159205: step 455, loss = 44.52 (11.3 examples/sec; 5.654 sec/batch)
2016-04-30 10:29:39.199402: step 456, loss = 44.68 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 10:29:44.449065: step 457, loss = 44.65 (12.2 examples/sec; 5.250 sec/batch)
2016-04-30 10:29:49.888127: step 458, loss = 44.47 (11.8 examples/sec; 5.439 sec/batch)
2016-04-30 10:29:55.446626: step 459, loss = 44.47 (11.5 examples/sec; 5.558 sec/batch)
2016-04-30 10:30:00.964319: step 460, loss = 44.50 (11.6 examples/sec; 5.518 sec/batch)
2016-04-30 10:30:13.986934: step 461, loss = 44.48 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 10:30:19.570629: step 462, loss = 44.53 (11.5 examples/sec; 5.584 sec/batch)
2016-04-30 10:30:24.936484: step 463, loss = 44.40 (11.9 examples/sec; 5.366 sec/batch)
2016-04-30 10:30:30.178224: step 464, loss = 44.16 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 10:30:36.056503: step 465, loss = 44.13 (10.9 examples/sec; 5.878 sec/batch)
2016-04-30 10:30:41.501795: step 466, loss = 44.37 (11.8 examples/sec; 5.445 sec/batch)
2016-04-30 10:30:46.850062: step 467, loss = 44.18 (12.0 examples/sec; 5.348 sec/batch)
2016-04-30 10:30:52.140694: step 468, loss = 44.22 (12.1 examples/sec; 5.291 sec/batch)
2016-04-30 10:30:57.193142: step 469, loss = 44.21 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 10:31:02.985012: step 470, loss = 44.18 (11.1 examples/sec; 5.792 sec/batch)
2016-04-30 10:31:16.910922: step 471, loss = 44.13 (11.3 examples/sec; 5.681 sec/batch)
2016-04-30 10:31:23.349306: step 472, loss = 44.08 (9.9 examples/sec; 6.438 sec/batch)
2016-04-30 10:31:28.970265: step 473, loss = 44.04 (11.4 examples/sec; 5.621 sec/batch)
2016-04-30 10:31:34.718323: step 474, loss = 43.93 (11.1 examples/sec; 5.748 sec/batch)
2016-04-30 10:31:41.010001: step 475, loss = 43.87 (10.2 examples/sec; 6.292 sec/batch)
2016-04-30 10:31:46.660217: step 476, loss = 43.87 (11.3 examples/sec; 5.650 sec/batch)
2016-04-30 10:31:52.050924: step 477, loss = 43.89 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 10:31:57.344688: step 478, loss = 43.87 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 10:32:02.810454: step 479, loss = 43.97 (11.7 examples/sec; 5.466 sec/batch)
2016-04-30 10:32:08.417809: step 480, loss = 43.74 (11.4 examples/sec; 5.607 sec/batch)
2016-04-30 10:32:21.475629: step 481, loss = 43.52 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 10:32:26.739672: step 482, loss = 43.69 (12.2 examples/sec; 5.264 sec/batch)
2016-04-30 10:32:32.170195: step 483, loss = 43.49 (11.8 examples/sec; 5.430 sec/batch)
2016-04-30 10:32:37.560834: step 484, loss = 43.64 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 10:32:43.270992: step 485, loss = 43.57 (11.2 examples/sec; 5.710 sec/batch)
2016-04-30 10:32:49.183635: step 486, loss = 43.54 (10.8 examples/sec; 5.913 sec/batch)
2016-04-30 10:32:54.511521: step 487, loss = 43.50 (12.0 examples/sec; 5.328 sec/batch)
2016-04-30 10:32:59.843666: step 488, loss = 43.55 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 10:33:05.579149: step 489, loss = 43.55 (11.2 examples/sec; 5.735 sec/batch)
2016-04-30 10:33:11.022151: step 490, loss = 43.42 (11.8 examples/sec; 5.443 sec/batch)
2016-04-30 10:33:24.112757: step 491, loss = 43.38 (11.0 examples/sec; 5.824 sec/batch)
2016-04-30 10:33:29.591521: step 492, loss = 43.38 (11.7 examples/sec; 5.478 sec/batch)
2016-04-30 10:33:35.019593: step 493, loss = 43.42 (11.8 examples/sec; 5.428 sec/batch)
2016-04-30 10:33:40.482544: step 494, loss = 43.36 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 10:33:45.761631: step 495, loss = 43.10 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 10:33:51.120516: step 496, loss = 43.28 (11.9 examples/sec; 5.359 sec/batch)
2016-04-30 10:33:57.266908: step 497, loss = 43.17 (10.4 examples/sec; 6.146 sec/batch)
2016-04-30 10:34:02.930504: step 498, loss = 43.17 (11.3 examples/sec; 5.663 sec/batch)
2016-04-30 10:34:08.297917: step 499, loss = 43.17 (11.9 examples/sec; 5.367 sec/batch)
2016-04-30 10:34:13.972019: step 500, loss = 42.99 (11.3 examples/sec; 5.674 sec/batch)
2016-04-30 10:34:27.106499: step 501, loss = 43.02 (10.8 examples/sec; 5.913 sec/batch)
2016-04-30 10:34:32.706437: step 502, loss = 42.96 (11.4 examples/sec; 5.600 sec/batch)
2016-04-30 10:34:38.289380: step 503, loss = 42.96 (11.5 examples/sec; 5.583 sec/batch)
2016-04-30 10:34:44.244940: step 504, loss = 42.94 (10.7 examples/sec; 5.955 sec/batch)
2016-04-30 10:34:50.052529: step 505, loss = 42.99 (11.0 examples/sec; 5.807 sec/batch)
2016-04-30 10:34:56.208001: step 506, loss = 43.10 (10.4 examples/sec; 6.155 sec/batch)
2016-04-30 10:35:02.232408: step 507, loss = 42.83 (10.6 examples/sec; 6.024 sec/batch)
2016-04-30 10:35:07.622477: step 508, loss = 42.87 (11.9 examples/sec; 5.390 sec/batch)
2016-04-30 10:35:13.167322: step 509, loss = 42.82 (11.5 examples/sec; 5.545 sec/batch)
2016-04-30 10:35:18.198026: step 510, loss = 42.72 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 10:35:30.607109: step 511, loss = 42.78 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 10:35:36.592096: step 512, loss = 42.80 (10.7 examples/sec; 5.985 sec/batch)
2016-04-30 10:35:41.705285: step 513, loss = 42.62 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 10:35:47.062280: step 514, loss = 42.50 (11.9 examples/sec; 5.357 sec/batch)
2016-04-30 10:35:52.564345: step 515, loss = 42.60 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 10:35:57.555408: step 516, loss = 42.53 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 10:36:02.902765: step 517, loss = 42.72 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 10:36:08.639831: step 518, loss = 42.56 (11.2 examples/sec; 5.737 sec/batch)
2016-04-30 10:36:13.837947: step 519, loss = 42.43 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 10:36:19.373542: step 520, loss = 42.51 (11.6 examples/sec; 5.536 sec/batch)
2016-04-30 10:36:31.430968: step 521, loss = 42.49 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 10:36:36.518120: step 522, loss = 42.33 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 10:36:42.219259: step 523, loss = 42.14 (11.2 examples/sec; 5.701 sec/batch)
2016-04-30 10:36:47.436342: step 524, loss = 42.17 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 10:36:52.595273: step 525, loss = 42.31 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 10:36:57.638946: step 526, loss = 42.20 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 10:37:02.822838: step 527, loss = 42.31 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 10:37:07.968351: step 528, loss = 42.16 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 10:37:13.809307: step 529, loss = 42.00 (11.0 examples/sec; 5.841 sec/batch)
2016-04-30 10:37:18.942998: step 530, loss = 42.13 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 10:37:31.127283: step 531, loss = 42.02 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 10:37:36.232011: step 532, loss = 42.08 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 10:37:41.430906: step 533, loss = 42.06 (12.3 examples/sec; 5.199 sec/batch)
2016-04-30 10:37:47.175799: step 534, loss = 41.97 (11.1 examples/sec; 5.745 sec/batch)
2016-04-30 10:37:52.410250: step 535, loss = 41.93 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 10:37:57.305548: step 536, loss = 41.92 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 10:38:02.924459: step 537, loss = 41.97 (11.4 examples/sec; 5.619 sec/batch)
2016-04-30 10:38:08.409884: step 538, loss = 41.73 (11.7 examples/sec; 5.485 sec/batch)
2016-04-30 10:38:13.950515: step 539, loss = 41.84 (11.6 examples/sec; 5.541 sec/batch)
2016-04-30 10:38:19.735612: step 540, loss = 41.88 (11.1 examples/sec; 5.785 sec/batch)
2016-04-30 10:38:31.637106: step 541, loss = 41.69 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 10:38:36.572068: step 542, loss = 41.82 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 10:38:41.647435: step 543, loss = 41.71 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 10:38:46.780637: step 544, loss = 41.53 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 10:38:52.334614: step 545, loss = 41.63 (11.5 examples/sec; 5.554 sec/batch)
2016-04-30 10:38:57.295688: step 546, loss = 41.54 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 10:39:02.629589: step 547, loss = 41.54 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 10:39:07.906961: step 548, loss = 41.47 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 10:39:13.059451: step 549, loss = 41.46 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 10:39:18.277221: step 550, loss = 41.23 (12.3 examples/sec; 5.218 sec/batch)
2016-04-30 10:39:31.063667: step 551, loss = 41.41 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 10:39:36.224446: step 552, loss = 41.50 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 10:39:41.451447: step 553, loss = 41.21 (12.2 examples/sec; 5.227 sec/batch)
2016-04-30 10:39:46.749239: step 554, loss = 41.42 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 10:39:51.546572: step 555, loss = 41.31 (13.3 examples/sec; 4.797 sec/batch)
2016-04-30 10:39:57.077566: step 556, loss = 41.29 (11.6 examples/sec; 5.531 sec/batch)
2016-04-30 10:40:02.533736: step 557, loss = 41.17 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 10:40:07.708163: step 558, loss = 41.19 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 10:40:12.580558: step 559, loss = 41.07 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 10:40:17.642372: step 560, loss = 41.12 (12.6 examples/sec; 5.062 sec/batch)
2016-04-30 10:40:30.146208: step 561, loss = 41.15 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 10:40:35.337067: step 562, loss = 41.10 (12.3 examples/sec; 5.191 sec/batch)
2016-04-30 10:40:40.459934: step 563, loss = 41.05 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 10:40:45.417605: step 564, loss = 40.86 (12.9 examples/sec; 4.958 sec/batch)
2016-04-30 10:40:50.512334: step 565, loss = 41.09 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 10:40:55.958628: step 566, loss = 41.03 (11.8 examples/sec; 5.446 sec/batch)
2016-04-30 10:41:01.313430: step 567, loss = 40.95 (12.0 examples/sec; 5.355 sec/batch)
2016-04-30 10:41:06.653269: step 568, loss = 40.89 (12.0 examples/sec; 5.340 sec/batch)
2016-04-30 10:41:11.802537: step 569, loss = 40.92 (12.4 examples/sec; 5.149 sec/batch)
2016-04-30 10:41:16.895722: step 570, loss = 40.83 (12.6 examples/sec; 5.093 sec/batch)
2016-04-30 10:41:29.074900: step 571, loss = 40.81 (12.5 examples/sec; 5.108 sec/batch)
2016-04-30 10:41:35.464073: step 572, loss = 40.93 (10.0 examples/sec; 6.389 sec/batch)
2016-04-30 10:41:41.528703: step 573, loss = 40.70 (10.6 examples/sec; 6.065 sec/batch)
2016-04-30 10:41:47.477834: step 574, loss = 40.59 (10.8 examples/sec; 5.949 sec/batch)
2016-04-30 10:41:53.119202: step 575, loss = 40.57 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 10:41:58.786807: step 576, loss = 40.54 (11.3 examples/sec; 5.668 sec/batch)
2016-04-30 10:42:04.422248: step 577, loss = 40.69 (11.4 examples/sec; 5.635 sec/batch)
2016-04-30 10:42:10.300237: step 578, loss = 40.65 (10.9 examples/sec; 5.878 sec/batch)
2016-04-30 10:42:15.715746: step 579, loss = 40.65 (11.8 examples/sec; 5.415 sec/batch)
2016-04-30 10:42:20.920172: step 580, loss = 40.46 (12.3 examples/sec; 5.204 sec/batch)
2016-04-30 10:42:32.950956: step 581, loss = 40.36 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 10:42:38.447132: step 582, loss = 40.37 (11.6 examples/sec; 5.496 sec/batch)
2016-04-30 10:42:44.303259: step 583, loss = 40.40 (10.9 examples/sec; 5.856 sec/batch)
2016-04-30 10:42:49.652095: step 584, loss = 40.31 (12.0 examples/sec; 5.349 sec/batch)
2016-04-30 10:42:54.763498: step 585, loss = 40.31 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 10:42:59.923149: step 586, loss = 40.18 (12.4 examples/sec; 5.160 sec/batch)
2016-04-30 10:43:05.376339: step 587, loss = 40.31 (11.7 examples/sec; 5.453 sec/batch)
2016-04-30 10:43:10.742527: step 588, loss = 40.05 (11.9 examples/sec; 5.366 sec/batch)
2016-04-30 10:43:16.589903: step 589, loss = 40.15 (10.9 examples/sec; 5.847 sec/batch)
2016-04-30 10:43:21.662340: step 590, loss = 40.19 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 10:43:33.753888: step 591, loss = 40.28 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 10:43:38.986969: step 592, loss = 40.05 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 10:43:44.350799: step 593, loss = 40.02 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 10:43:50.143999: step 594, loss = 39.97 (11.0 examples/sec; 5.793 sec/batch)
2016-04-30 10:43:55.398769: step 595, loss = 40.02 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 10:44:01.098850: step 596, loss = 40.01 (11.2 examples/sec; 5.700 sec/batch)
2016-04-30 10:44:06.001933: step 597, loss = 40.02 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 10:44:11.348092: step 598, loss = 39.96 (12.0 examples/sec; 5.346 sec/batch)
2016-04-30 10:44:16.647285: step 599, loss = 39.99 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 10:44:22.497581: step 600, loss = 39.64 (10.9 examples/sec; 5.850 sec/batch)
2016-04-30 10:44:34.769054: step 601, loss = 39.76 (12.5 examples/sec; 5.130 sec/batch)
2016-04-30 10:44:39.835362: step 602, loss = 39.63 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 10:44:44.903796: step 603, loss = 39.80 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 10:44:50.395542: step 604, loss = 39.75 (11.7 examples/sec; 5.492 sec/batch)
2016-04-30 10:44:56.260055: step 605, loss = 39.67 (10.9 examples/sec; 5.864 sec/batch)
2016-04-30 10:45:01.843843: step 606, loss = 39.84 (11.5 examples/sec; 5.584 sec/batch)
2016-04-30 10:45:07.244672: step 607, loss = 39.82 (11.9 examples/sec; 5.401 sec/batch)
2016-04-30 10:45:12.561105: step 608, loss = 39.79 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 10:45:18.039530: step 609, loss = 39.75 (11.7 examples/sec; 5.478 sec/batch)
2016-04-30 10:45:23.589405: step 610, loss = 39.67 (11.5 examples/sec; 5.550 sec/batch)
2016-04-30 10:45:36.428759: step 611, loss = 39.57 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 10:45:41.797177: step 612, loss = 39.60 (11.9 examples/sec; 5.368 sec/batch)
2016-04-30 10:45:47.052426: step 613, loss = 39.31 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 10:45:52.378439: step 614, loss = 39.42 (12.0 examples/sec; 5.326 sec/batch)
2016-04-30 10:45:57.417433: step 615, loss = 39.52 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 10:46:03.442525: step 616, loss = 39.47 (10.6 examples/sec; 6.025 sec/batch)
2016-04-30 10:46:08.656600: step 617, loss = 39.30 (12.3 examples/sec; 5.214 sec/batch)
2016-04-30 10:46:13.977773: step 618, loss = 39.37 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 10:46:19.376477: step 619, loss = 39.31 (11.9 examples/sec; 5.399 sec/batch)
2016-04-30 10:46:24.320462: step 620, loss = 39.18 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 10:46:37.097528: step 621, loss = 39.18 (11.7 examples/sec; 5.459 sec/batch)
2016-04-30 10:46:42.509306: step 622, loss = 39.22 (11.8 examples/sec; 5.412 sec/batch)
2016-04-30 10:46:48.048561: step 623, loss = 39.15 (11.6 examples/sec; 5.539 sec/batch)
2016-04-30 10:46:54.166988: step 624, loss = 39.10 (10.5 examples/sec; 6.118 sec/batch)
2016-04-30 10:46:59.501267: step 625, loss = 39.18 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 10:47:05.635065: step 626, loss = 39.19 (10.4 examples/sec; 6.134 sec/batch)
2016-04-30 10:47:10.938680: step 627, loss = 39.29 (12.1 examples/sec; 5.303 sec/batch)
2016-04-30 10:47:16.242747: step 628, loss = 38.97 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 10:47:21.398911: step 629, loss = 38.92 (12.4 examples/sec; 5.156 sec/batch)
2016-04-30 10:47:26.724565: step 630, loss = 38.98 (12.0 examples/sec; 5.326 sec/batch)
2016-04-30 10:47:39.341961: step 631, loss = 38.89 (11.4 examples/sec; 5.600 sec/batch)
2016-04-30 10:47:44.652222: step 632, loss = 38.81 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 10:47:50.220379: step 633, loss = 38.62 (11.5 examples/sec; 5.568 sec/batch)
2016-04-30 10:47:55.600478: step 634, loss = 38.96 (11.9 examples/sec; 5.380 sec/batch)
2016-04-30 10:48:01.070112: step 635, loss = 38.87 (11.7 examples/sec; 5.470 sec/batch)
2016-04-30 10:48:06.031495: step 636, loss = 38.66 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 10:48:11.908783: step 637, loss = 38.64 (10.9 examples/sec; 5.877 sec/batch)
2016-04-30 10:48:16.999900: step 638, loss = 38.68 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 10:48:22.271225: step 639, loss = 38.71 (12.1 examples/sec; 5.271 sec/batch)
2016-04-30 10:48:27.308818: step 640, loss = 38.77 (12.7 examples/sec; 5.037 sec/batch)
2016-04-30 10:48:39.317404: step 641, loss = 38.57 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 10:48:45.204188: step 642, loss = 38.65 (10.9 examples/sec; 5.887 sec/batch)
2016-04-30 10:48:50.547727: step 643, loss = 38.35 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 10:48:55.800661: step 644, loss = 38.56 (12.2 examples/sec; 5.253 sec/batch)
2016-04-30 10:49:01.089988: step 645, loss = 38.50 (12.1 examples/sec; 5.289 sec/batch)
2016-04-30 10:49:06.151882: step 646, loss = 38.48 (12.6 examples/sec; 5.062 sec/batch)
2016-04-30 10:49:11.546446: step 647, loss = 38.50 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 10:49:17.391552: step 648, loss = 38.40 (10.9 examples/sec; 5.845 sec/batch)
2016-04-30 10:49:22.436438: step 649, loss = 38.42 (12.7 examples/sec; 5.045 sec/batch)
2016-04-30 10:49:27.429555: step 650, loss = 38.49 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 10:49:39.500243: step 651, loss = 38.42 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 10:49:44.716461: step 652, loss = 38.26 (12.3 examples/sec; 5.216 sec/batch)
2016-04-30 10:49:50.540408: step 653, loss = 38.36 (11.0 examples/sec; 5.824 sec/batch)
2016-04-30 10:49:55.678545: step 654, loss = 38.15 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 10:50:00.815437: step 655, loss = 38.17 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 10:50:06.041258: step 656, loss = 38.19 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 10:50:11.195058: step 657, loss = 38.27 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 10:50:16.496408: step 658, loss = 38.36 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 10:50:21.464177: step 659, loss = 38.19 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 10:50:27.275764: step 660, loss = 38.04 (11.0 examples/sec; 5.812 sec/batch)
2016-04-30 10:50:39.328636: step 661, loss = 37.96 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 10:50:44.632688: step 662, loss = 37.95 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 10:50:49.979508: step 663, loss = 38.05 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 10:50:55.004461: step 664, loss = 37.97 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 10:51:00.917552: step 665, loss = 38.01 (10.8 examples/sec; 5.913 sec/batch)
2016-04-30 10:51:06.057478: step 666, loss = 37.76 (12.5 examples/sec; 5.140 sec/batch)
2016-04-30 10:51:11.400843: step 667, loss = 37.95 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 10:51:16.623040: step 668, loss = 37.87 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 10:51:21.662524: step 669, loss = 37.86 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 10:51:26.811604: step 670, loss = 37.87 (12.4 examples/sec; 5.149 sec/batch)
2016-04-30 10:51:39.375042: step 671, loss = 37.62 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 10:51:44.957567: step 672, loss = 37.82 (11.5 examples/sec; 5.582 sec/batch)
2016-04-30 10:51:50.347975: step 673, loss = 37.64 (11.9 examples/sec; 5.390 sec/batch)
2016-04-30 10:51:55.589924: step 674, loss = 37.67 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 10:52:00.624727: step 675, loss = 37.70 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 10:52:06.374020: step 676, loss = 37.57 (11.1 examples/sec; 5.749 sec/batch)
2016-04-30 10:52:11.751376: step 677, loss = 37.46 (11.9 examples/sec; 5.377 sec/batch)
2016-04-30 10:52:16.834175: step 678, loss = 37.57 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 10:52:21.668181: step 679, loss = 37.50 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 10:52:26.757067: step 680, loss = 37.36 (12.6 examples/sec; 5.089 sec/batch)
2016-04-30 10:52:39.180463: step 681, loss = 37.45 (11.5 examples/sec; 5.564 sec/batch)
2016-04-30 10:52:44.422345: step 682, loss = 37.39 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 10:52:49.606503: step 683, loss = 37.29 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 10:52:54.634213: step 684, loss = 37.32 (12.7 examples/sec; 5.028 sec/batch)
2016-04-30 10:52:59.847991: step 685, loss = 37.36 (12.3 examples/sec; 5.214 sec/batch)
2016-04-30 10:53:05.093771: step 686, loss = 37.22 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 10:53:10.996084: step 687, loss = 37.26 (10.8 examples/sec; 5.902 sec/batch)
2016-04-30 10:53:16.091413: step 688, loss = 37.14 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 10:53:21.451085: step 689, loss = 37.34 (11.9 examples/sec; 5.360 sec/batch)
2016-04-30 10:53:26.965192: step 690, loss = 37.18 (11.6 examples/sec; 5.514 sec/batch)
2016-04-30 10:53:40.002572: step 691, loss = 37.23 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 10:53:46.324341: step 692, loss = 37.01 (10.1 examples/sec; 6.322 sec/batch)
2016-04-30 10:53:51.266662: step 693, loss = 37.09 (12.9 examples/sec; 4.942 sec/batch)
2016-04-30 10:53:56.470843: step 694, loss = 37.01 (12.3 examples/sec; 5.204 sec/batch)
2016-04-30 10:54:01.908140: step 695, loss = 37.25 (11.8 examples/sec; 5.437 sec/batch)
2016-04-30 10:54:06.891276: step 696, loss = 37.08 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 10:54:12.055470: step 697, loss = 36.99 (12.4 examples/sec; 5.164 sec/batch)
2016-04-30 10:54:17.703562: step 698, loss = 36.97 (11.3 examples/sec; 5.648 sec/batch)
2016-04-30 10:54:22.890429: step 699, loss = 36.96 (12.3 examples/sec; 5.187 sec/batch)
2016-04-30 10:54:27.983614: step 700, loss = 37.00 (12.6 examples/sec; 5.093 sec/batch)
2016-04-30 10:54:40.108840: step 701, loss = 36.78 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 10:54:45.201189: step 702, loss = 36.74 (12.6 examples/sec; 5.092 sec/batch)
2016-04-30 10:54:51.091032: step 703, loss = 36.78 (10.9 examples/sec; 5.890 sec/batch)
2016-04-30 10:54:56.316188: step 704, loss = 36.84 (12.2 examples/sec; 5.225 sec/batch)
2016-04-30 10:55:01.732461: step 705, loss = 36.61 (11.8 examples/sec; 5.416 sec/batch)
2016-04-30 10:55:06.830952: step 706, loss = 36.73 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 10:55:11.942056: step 707, loss = 36.70 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 10:55:17.235443: step 708, loss = 36.76 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 10:55:23.055325: step 709, loss = 36.57 (11.0 examples/sec; 5.820 sec/batch)
2016-04-30 10:55:28.243253: step 710, loss = 36.56 (12.3 examples/sec; 5.188 sec/batch)
2016-04-30 10:55:40.158712: step 711, loss = 36.64 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 10:55:45.112173: step 712, loss = 36.56 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 10:55:50.332676: step 713, loss = 36.58 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 10:55:56.232351: step 714, loss = 36.54 (10.8 examples/sec; 5.900 sec/batch)
2016-04-30 10:56:01.689621: step 715, loss = 36.50 (11.7 examples/sec; 5.457 sec/batch)
2016-04-30 10:56:06.611001: step 716, loss = 36.40 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 10:56:11.870025: step 717, loss = 36.47 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 10:56:17.240824: step 718, loss = 36.34 (11.9 examples/sec; 5.371 sec/batch)
2016-04-30 10:56:22.614580: step 719, loss = 36.30 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 10:56:28.502835: step 720, loss = 36.48 (10.9 examples/sec; 5.888 sec/batch)
2016-04-30 10:56:40.315205: step 721, loss = 36.15 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 10:56:45.251936: step 722, loss = 36.35 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 10:56:50.450006: step 723, loss = 36.32 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 10:56:55.760714: step 724, loss = 36.04 (12.1 examples/sec; 5.311 sec/batch)
2016-04-30 10:57:01.751052: step 725, loss = 36.09 (10.7 examples/sec; 5.990 sec/batch)
2016-04-30 10:57:06.634003: step 726, loss = 36.17 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 10:57:11.763564: step 727, loss = 36.20 (12.5 examples/sec; 5.129 sec/batch)
2016-04-30 10:57:16.800946: step 728, loss = 36.20 (12.7 examples/sec; 5.037 sec/batch)
2016-04-30 10:57:21.666912: step 729, loss = 36.27 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 10:57:26.643973: step 730, loss = 36.09 (12.9 examples/sec; 4.977 sec/batch)
2016-04-30 10:57:39.922297: step 731, loss = 36.03 (12.0 examples/sec; 5.335 sec/batch)
2016-04-30 10:57:45.952659: step 732, loss = 36.22 (10.6 examples/sec; 6.030 sec/batch)
2016-04-30 10:57:51.075463: step 733, loss = 35.92 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 10:57:56.724327: step 734, loss = 35.89 (11.3 examples/sec; 5.649 sec/batch)
2016-04-30 10:58:01.942247: step 735, loss = 36.06 (12.3 examples/sec; 5.218 sec/batch)
2016-04-30 10:58:07.698627: step 736, loss = 35.92 (11.1 examples/sec; 5.756 sec/batch)
2016-04-30 10:58:12.682306: step 737, loss = 35.87 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 10:58:18.184468: step 738, loss = 35.88 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 10:58:23.501618: step 739, loss = 35.81 (12.0 examples/sec; 5.317 sec/batch)
2016-04-30 10:58:28.614668: step 740, loss = 35.86 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 10:58:41.150899: step 741, loss = 35.76 (11.5 examples/sec; 5.545 sec/batch)
2016-04-30 10:58:46.547431: step 742, loss = 35.72 (11.9 examples/sec; 5.396 sec/batch)
2016-04-30 10:58:51.525547: step 743, loss = 35.82 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 10:58:56.810038: step 744, loss = 35.77 (12.1 examples/sec; 5.284 sec/batch)
2016-04-30 10:59:02.078534: step 745, loss = 35.59 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 10:59:07.155229: step 746, loss = 35.46 (12.6 examples/sec; 5.077 sec/batch)
2016-04-30 10:59:12.711779: step 747, loss = 35.56 (11.5 examples/sec; 5.556 sec/batch)
2016-04-30 10:59:17.800453: step 748, loss = 35.66 (12.6 examples/sec; 5.089 sec/batch)
2016-04-30 10:59:22.981100: step 749, loss = 35.49 (12.4 examples/sec; 5.181 sec/batch)
2016-04-30 10:59:28.228346: step 750, loss = 35.45 (12.2 examples/sec; 5.247 sec/batch)
2016-04-30 10:59:40.053771: step 751, loss = 35.45 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 10:59:45.677664: step 752, loss = 35.59 (11.4 examples/sec; 5.623 sec/batch)
2016-04-30 10:59:50.878641: step 753, loss = 35.39 (12.3 examples/sec; 5.201 sec/batch)
2016-04-30 10:59:56.069859: step 754, loss = 35.39 (12.3 examples/sec; 5.191 sec/batch)
2016-04-30 11:00:01.499914: step 755, loss = 35.33 (11.8 examples/sec; 5.430 sec/batch)
2016-04-30 11:00:06.388041: step 756, loss = 35.35 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 11:00:11.689056: step 757, loss = 35.30 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 11:00:17.242405: step 758, loss = 35.23 (11.5 examples/sec; 5.553 sec/batch)
2016-04-30 11:00:22.353253: step 759, loss = 35.31 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 11:00:27.140896: step 760, loss = 35.11 (13.4 examples/sec; 4.788 sec/batch)
2016-04-30 11:00:38.823147: step 761, loss = 35.16 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 11:00:44.101068: step 762, loss = 35.25 (12.1 examples/sec; 5.278 sec/batch)
2016-04-30 11:00:49.122528: step 763, loss = 35.19 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 11:00:54.692967: step 764, loss = 34.98 (11.5 examples/sec; 5.570 sec/batch)
2016-04-30 11:00:59.932888: step 765, loss = 35.13 (12.2 examples/sec; 5.240 sec/batch)
2016-04-30 11:01:05.254014: step 766, loss = 35.03 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 11:01:10.432472: step 767, loss = 35.05 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 11:01:15.465213: step 768, loss = 35.16 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 11:01:20.682474: step 769, loss = 34.76 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 11:01:26.355062: step 770, loss = 35.02 (11.3 examples/sec; 5.672 sec/batch)
2016-04-30 11:01:38.351616: step 771, loss = 34.99 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 11:01:43.848514: step 772, loss = 34.90 (11.6 examples/sec; 5.497 sec/batch)
2016-04-30 11:01:48.724472: step 773, loss = 34.85 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 11:01:53.861014: step 774, loss = 34.90 (12.5 examples/sec; 5.136 sec/batch)
2016-04-30 11:01:59.519980: step 775, loss = 34.91 (11.3 examples/sec; 5.659 sec/batch)
2016-04-30 11:02:04.832142: step 776, loss = 35.00 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 11:02:09.660633: step 777, loss = 34.82 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 11:02:14.919373: step 778, loss = 34.80 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 11:02:20.101416: step 779, loss = 34.72 (12.4 examples/sec; 5.182 sec/batch)
2016-04-30 11:02:25.226143: step 780, loss = 34.69 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 11:02:37.728310: step 781, loss = 34.73 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 11:02:42.701099: step 782, loss = 34.71 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 11:02:48.052344: step 783, loss = 34.53 (12.0 examples/sec; 5.351 sec/batch)
2016-04-30 11:02:53.832360: step 784, loss = 34.70 (11.1 examples/sec; 5.780 sec/batch)
2016-04-30 11:03:00.135684: step 785, loss = 34.59 (10.2 examples/sec; 6.303 sec/batch)
2016-04-30 11:03:06.268471: step 786, loss = 34.67 (10.4 examples/sec; 6.133 sec/batch)
2016-04-30 11:03:11.582401: step 787, loss = 34.47 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 11:03:16.924084: step 788, loss = 34.35 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 11:03:22.034588: step 789, loss = 34.51 (12.5 examples/sec; 5.110 sec/batch)
2016-04-30 11:03:27.187500: step 790, loss = 34.54 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 11:03:39.697060: step 791, loss = 34.45 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 11:03:44.938245: step 792, loss = 34.18 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 11:03:50.182794: step 793, loss = 34.46 (12.2 examples/sec; 5.244 sec/batch)
2016-04-30 11:03:55.371443: step 794, loss = 34.23 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 11:04:00.532183: step 795, loss = 34.47 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 11:04:05.720461: step 796, loss = 34.37 (12.3 examples/sec; 5.188 sec/batch)
2016-04-30 11:04:11.493809: step 797, loss = 34.41 (11.1 examples/sec; 5.773 sec/batch)
2016-04-30 11:04:16.896907: step 798, loss = 34.22 (11.8 examples/sec; 5.403 sec/batch)
2016-04-30 11:04:21.774495: step 799, loss = 34.24 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 11:04:27.282858: step 800, loss = 34.02 (11.6 examples/sec; 5.508 sec/batch)
2016-04-30 11:04:39.364853: step 801, loss = 34.11 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 11:04:45.202402: step 802, loss = 33.91 (11.0 examples/sec; 5.837 sec/batch)
2016-04-30 11:04:50.298932: step 803, loss = 34.11 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 11:04:55.673674: step 804, loss = 33.99 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 11:05:00.776456: step 805, loss = 34.00 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 11:05:06.001285: step 806, loss = 33.95 (12.2 examples/sec; 5.225 sec/batch)
2016-04-30 11:05:11.120289: step 807, loss = 34.04 (12.5 examples/sec; 5.119 sec/batch)
2016-04-30 11:05:16.912509: step 808, loss = 33.95 (11.0 examples/sec; 5.792 sec/batch)
2016-04-30 11:05:22.009922: step 809, loss = 34.02 (12.6 examples/sec; 5.097 sec/batch)
2016-04-30 11:05:26.966344: step 810, loss = 33.83 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 11:05:38.863886: step 811, loss = 33.88 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 11:05:44.067153: step 812, loss = 33.87 (12.3 examples/sec; 5.203 sec/batch)
2016-04-30 11:05:49.771801: step 813, loss = 33.66 (11.2 examples/sec; 5.705 sec/batch)
2016-04-30 11:05:54.779154: step 814, loss = 33.76 (12.8 examples/sec; 5.007 sec/batch)
2016-04-30 11:05:59.782182: step 815, loss = 33.59 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 11:06:05.038673: step 816, loss = 33.78 (12.2 examples/sec; 5.256 sec/batch)
2016-04-30 11:06:09.934058: step 817, loss = 33.81 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 11:06:15.099324: step 818, loss = 33.59 (12.4 examples/sec; 5.165 sec/batch)
2016-04-30 11:06:20.703691: step 819, loss = 33.78 (11.4 examples/sec; 5.604 sec/batch)
2016-04-30 11:06:25.962382: step 820, loss = 33.63 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 11:06:37.731733: step 821, loss = 33.69 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 11:06:42.721517: step 822, loss = 33.60 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 11:06:47.787271: step 823, loss = 33.60 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 11:06:53.704098: step 824, loss = 33.53 (10.8 examples/sec; 5.917 sec/batch)
2016-04-30 11:06:58.927503: step 825, loss = 33.29 (12.3 examples/sec; 5.223 sec/batch)
2016-04-30 11:07:04.124342: step 826, loss = 33.47 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 11:07:09.250676: step 827, loss = 33.53 (12.5 examples/sec; 5.126 sec/batch)
2016-04-30 11:07:14.346210: step 828, loss = 33.42 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 11:07:19.460128: step 829, loss = 33.55 (12.5 examples/sec; 5.114 sec/batch)
2016-04-30 11:07:24.541929: step 830, loss = 33.22 (12.6 examples/sec; 5.082 sec/batch)
2016-04-30 11:07:37.056116: step 831, loss = 33.56 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 11:07:42.113723: step 832, loss = 33.25 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 11:07:47.228711: step 833, loss = 33.25 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 11:07:52.088215: step 834, loss = 33.26 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 11:07:57.078597: step 835, loss = 33.31 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 11:08:03.174461: step 836, loss = 33.18 (10.5 examples/sec; 6.096 sec/batch)
2016-04-30 11:08:08.249450: step 837, loss = 33.23 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 11:08:13.263289: step 838, loss = 33.23 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 11:08:18.209403: step 839, loss = 33.25 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 11:08:23.357128: step 840, loss = 32.97 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 11:08:35.888022: step 841, loss = 33.26 (11.6 examples/sec; 5.536 sec/batch)
2016-04-30 11:08:41.146383: step 842, loss = 33.19 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 11:08:46.698232: step 843, loss = 33.03 (11.5 examples/sec; 5.552 sec/batch)
2016-04-30 11:08:51.913112: step 844, loss = 33.08 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 11:08:57.849429: step 845, loss = 32.99 (10.8 examples/sec; 5.936 sec/batch)
2016-04-30 11:09:03.295670: step 846, loss = 32.91 (11.8 examples/sec; 5.446 sec/batch)
2016-04-30 11:09:09.223607: step 847, loss = 33.11 (10.8 examples/sec; 5.928 sec/batch)
2016-04-30 11:09:14.656290: step 848, loss = 32.90 (11.8 examples/sec; 5.433 sec/batch)
2016-04-30 11:09:19.999952: step 849, loss = 32.79 (12.0 examples/sec; 5.344 sec/batch)
2016-04-30 11:09:25.012149: step 850, loss = 33.10 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 11:09:37.218802: step 851, loss = 32.69 (12.6 examples/sec; 5.062 sec/batch)
2016-04-30 11:09:43.060939: step 852, loss = 32.77 (11.0 examples/sec; 5.842 sec/batch)
2016-04-30 11:09:48.273908: step 853, loss = 32.66 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 11:09:53.566369: step 854, loss = 32.72 (12.1 examples/sec; 5.292 sec/batch)
2016-04-30 11:09:58.834555: step 855, loss = 32.80 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 11:10:04.027948: step 856, loss = 32.64 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 11:10:09.112693: step 857, loss = 32.54 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 11:10:14.753747: step 858, loss = 32.73 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 11:10:19.990844: step 859, loss = 32.73 (12.2 examples/sec; 5.237 sec/batch)
2016-04-30 11:10:25.088848: step 860, loss = 32.66 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 11:10:36.789806: step 861, loss = 32.42 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 11:10:41.982749: step 862, loss = 32.59 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 11:10:47.721886: step 863, loss = 32.61 (11.2 examples/sec; 5.739 sec/batch)
2016-04-30 11:10:52.815552: step 864, loss = 32.54 (12.6 examples/sec; 5.094 sec/batch)
2016-04-30 11:10:57.699414: step 865, loss = 32.58 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 11:11:02.880624: step 866, loss = 32.50 (12.4 examples/sec; 5.181 sec/batch)
2016-04-30 11:11:08.074790: step 867, loss = 32.52 (12.3 examples/sec; 5.194 sec/batch)
2016-04-30 11:11:12.896214: step 868, loss = 32.50 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 11:11:18.588691: step 869, loss = 32.47 (11.2 examples/sec; 5.692 sec/batch)
2016-04-30 11:11:23.746679: step 870, loss = 32.33 (12.4 examples/sec; 5.158 sec/batch)
2016-04-30 11:11:35.742742: step 871, loss = 32.28 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 11:11:40.925924: step 872, loss = 32.32 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 11:11:45.876892: step 873, loss = 32.39 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 11:11:51.784445: step 874, loss = 32.16 (10.8 examples/sec; 5.907 sec/batch)
2016-04-30 11:11:57.100462: step 875, loss = 32.22 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 11:12:02.302249: step 876, loss = 32.10 (12.3 examples/sec; 5.202 sec/batch)
2016-04-30 11:12:07.321855: step 877, loss = 32.10 (12.8 examples/sec; 5.020 sec/batch)
2016-04-30 11:12:12.242163: step 878, loss = 32.20 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 11:12:17.408023: step 879, loss = 32.27 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 11:12:22.625549: step 880, loss = 32.22 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 11:12:34.978350: step 881, loss = 32.31 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 11:12:39.877975: step 882, loss = 32.13 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 11:12:45.119266: step 883, loss = 32.13 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 11:12:50.247827: step 884, loss = 31.97 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 11:12:55.339196: step 885, loss = 32.26 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 11:13:01.479230: step 886, loss = 31.87 (10.4 examples/sec; 6.140 sec/batch)
2016-04-30 11:13:06.903807: step 887, loss = 32.00 (11.8 examples/sec; 5.424 sec/batch)
2016-04-30 11:13:12.790527: step 888, loss = 31.92 (10.9 examples/sec; 5.887 sec/batch)
2016-04-30 11:13:18.138903: step 889, loss = 31.90 (12.0 examples/sec; 5.348 sec/batch)
2016-04-30 11:13:23.493004: step 890, loss = 31.89 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 11:13:35.923261: step 891, loss = 31.90 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 11:13:41.039921: step 892, loss = 31.94 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 11:13:46.010152: step 893, loss = 31.65 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 11:13:51.147262: step 894, loss = 31.73 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 11:13:56.472024: step 895, loss = 31.72 (12.0 examples/sec; 5.325 sec/batch)
2016-04-30 11:14:01.845644: step 896, loss = 31.91 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 11:14:07.108529: step 897, loss = 31.62 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 11:14:12.228747: step 898, loss = 31.79 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 11:14:17.471008: step 899, loss = 31.60 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 11:14:22.605843: step 900, loss = 31.67 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 11:14:34.506605: step 901, loss = 31.67 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 11:14:40.007574: step 902, loss = 31.65 (11.6 examples/sec; 5.501 sec/batch)
2016-04-30 11:14:45.165634: step 903, loss = 31.51 (12.4 examples/sec; 5.158 sec/batch)
2016-04-30 11:14:50.430044: step 904, loss = 31.42 (12.2 examples/sec; 5.264 sec/batch)
2016-04-30 11:14:55.416259: step 905, loss = 31.44 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 11:15:00.594650: step 906, loss = 31.43 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 11:15:05.719515: step 907, loss = 31.45 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 11:15:11.643271: step 908, loss = 31.50 (10.8 examples/sec; 5.924 sec/batch)
2016-04-30 11:15:16.835419: step 909, loss = 31.50 (12.3 examples/sec; 5.192 sec/batch)
2016-04-30 11:15:21.832426: step 910, loss = 31.43 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 11:15:33.682680: step 911, loss = 31.43 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 11:15:38.855014: step 912, loss = 31.23 (12.4 examples/sec; 5.172 sec/batch)
2016-04-30 11:15:44.805043: step 913, loss = 31.34 (10.8 examples/sec; 5.950 sec/batch)
2016-04-30 11:15:50.099868: step 914, loss = 31.37 (12.1 examples/sec; 5.295 sec/batch)
2016-04-30 11:15:55.073806: step 915, loss = 31.18 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 11:16:00.145896: step 916, loss = 31.22 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 11:16:05.616483: step 917, loss = 31.16 (11.7 examples/sec; 5.470 sec/batch)
2016-04-30 11:16:10.788328: step 918, loss = 31.25 (12.4 examples/sec; 5.172 sec/batch)
2016-04-30 11:16:16.395891: step 919, loss = 31.11 (11.4 examples/sec; 5.607 sec/batch)
2016-04-30 11:16:21.519145: step 920, loss = 31.15 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 11:16:33.238519: step 921, loss = 31.10 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 11:16:38.476106: step 922, loss = 31.18 (12.2 examples/sec; 5.237 sec/batch)
2016-04-30 11:16:43.526832: step 923, loss = 30.92 (12.7 examples/sec; 5.051 sec/batch)
2016-04-30 11:16:49.054286: step 924, loss = 31.14 (11.6 examples/sec; 5.527 sec/batch)
2016-04-30 11:16:54.176458: step 925, loss = 31.05 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 11:16:59.331895: step 926, loss = 31.02 (12.4 examples/sec; 5.155 sec/batch)
2016-04-30 11:17:04.734509: step 927, loss = 31.09 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 11:17:09.571133: step 928, loss = 30.85 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 11:17:14.764290: step 929, loss = 30.99 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 11:17:20.140336: step 930, loss = 31.08 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 11:17:32.277048: step 931, loss = 30.78 (13.0 examples/sec; 4.927 sec/batch)
2016-04-30 11:17:37.063287: step 932, loss = 30.77 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 11:17:42.353825: step 933, loss = 30.74 (12.1 examples/sec; 5.290 sec/batch)
2016-04-30 11:17:47.499534: step 934, loss = 30.80 (12.4 examples/sec; 5.146 sec/batch)
2016-04-30 11:17:52.813931: step 935, loss = 30.79 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 11:17:58.395955: step 936, loss = 30.72 (11.5 examples/sec; 5.582 sec/batch)
2016-04-30 11:18:03.520726: step 937, loss = 30.93 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 11:18:08.688061: step 938, loss = 30.79 (12.4 examples/sec; 5.167 sec/batch)
2016-04-30 11:18:13.702075: step 939, loss = 30.43 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 11:18:18.557635: step 940, loss = 30.64 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 11:18:30.956004: step 941, loss = 30.73 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 11:18:36.575395: step 942, loss = 30.69 (11.4 examples/sec; 5.619 sec/batch)
2016-04-30 11:18:42.671530: step 943, loss = 30.65 (10.5 examples/sec; 6.096 sec/batch)
2016-04-30 11:18:48.289221: step 944, loss = 30.59 (11.4 examples/sec; 5.618 sec/batch)
2016-04-30 11:18:53.587796: step 945, loss = 30.44 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 11:18:59.242849: step 946, loss = 30.54 (11.3 examples/sec; 5.655 sec/batch)
2016-04-30 11:19:05.010094: step 947, loss = 30.60 (11.1 examples/sec; 5.767 sec/batch)
2016-04-30 11:19:10.031398: step 948, loss = 30.27 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 11:19:15.301825: step 949, loss = 30.52 (12.1 examples/sec; 5.270 sec/batch)
2016-04-30 11:19:20.524327: step 950, loss = 30.18 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 11:19:33.196807: step 951, loss = 30.45 (11.2 examples/sec; 5.727 sec/batch)
2016-04-30 11:19:38.658813: step 952, loss = 30.49 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 11:19:43.930175: step 953, loss = 30.15 (12.1 examples/sec; 5.271 sec/batch)
2016-04-30 11:19:48.851043: step 954, loss = 30.32 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 11:19:54.072806: step 955, loss = 30.32 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 11:19:59.297352: step 956, loss = 30.37 (12.3 examples/sec; 5.224 sec/batch)
2016-04-30 11:20:05.004401: step 957, loss = 30.34 (11.2 examples/sec; 5.707 sec/batch)
2016-04-30 11:20:10.533204: step 958, loss = 30.31 (11.6 examples/sec; 5.529 sec/batch)
2016-04-30 11:20:15.560753: step 959, loss = 30.15 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 11:20:20.763038: step 960, loss = 30.24 (12.3 examples/sec; 5.202 sec/batch)
2016-04-30 11:20:32.828244: step 961, loss = 30.16 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 11:20:38.558090: step 962, loss = 30.10 (11.2 examples/sec; 5.730 sec/batch)
2016-04-30 11:20:43.929035: step 963, loss = 30.36 (11.9 examples/sec; 5.371 sec/batch)
2016-04-30 11:20:48.752461: step 964, loss = 30.24 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 11:20:54.012446: step 965, loss = 29.99 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 11:20:59.243316: step 966, loss = 30.06 (12.2 examples/sec; 5.231 sec/batch)
2016-04-30 11:21:04.355452: step 967, loss = 30.28 (12.5 examples/sec; 5.112 sec/batch)
2016-04-30 11:21:09.507105: step 968, loss = 29.97 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 11:21:15.345663: step 969, loss = 30.04 (11.0 examples/sec; 5.838 sec/batch)
2016-04-30 11:21:20.725579: step 970, loss = 29.94 (11.9 examples/sec; 5.380 sec/batch)
2016-04-30 11:21:32.599086: step 971, loss = 29.90 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 11:21:37.516073: step 972, loss = 30.15 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 11:21:42.454337: step 973, loss = 29.72 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 11:21:48.233748: step 974, loss = 29.85 (11.1 examples/sec; 5.779 sec/batch)
2016-04-30 11:21:53.581786: step 975, loss = 29.75 (12.0 examples/sec; 5.348 sec/batch)
2016-04-30 11:21:58.532232: step 976, loss = 29.73 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 11:22:03.807556: step 977, loss = 29.79 (12.1 examples/sec; 5.275 sec/batch)
2016-04-30 11:22:08.878369: step 978, loss = 29.87 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 11:22:13.932472: step 979, loss = 29.81 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 11:22:19.954165: step 980, loss = 29.77 (10.6 examples/sec; 6.022 sec/batch)
2016-04-30 11:22:31.664572: step 981, loss = 29.78 (13.0 examples/sec; 4.925 sec/batch)
2016-04-30 11:22:36.466084: step 982, loss = 29.78 (13.3 examples/sec; 4.801 sec/batch)
2016-04-30 11:22:41.662443: step 983, loss = 29.84 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 11:22:46.823252: step 984, loss = 29.65 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 11:22:52.091495: step 985, loss = 29.54 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 11:22:57.257403: step 986, loss = 29.58 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 11:23:02.551063: step 987, loss = 29.63 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 11:23:07.444704: step 988, loss = 29.54 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 11:23:12.292213: step 989, loss = 29.66 (13.2 examples/sec; 4.847 sec/batch)
2016-04-30 11:23:17.335580: step 990, loss = 29.51 (12.7 examples/sec; 5.043 sec/batch)
2016-04-30 11:23:29.690099: step 991, loss = 29.54 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 11:23:34.989102: step 992, loss = 29.56 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 11:23:39.856466: step 993, loss = 29.44 (13.1 examples/sec; 4.867 sec/batch)
2016-04-30 11:23:45.249323: step 994, loss = 29.39 (11.9 examples/sec; 5.393 sec/batch)
2016-04-30 11:23:51.086809: step 995, loss = 29.28 (11.0 examples/sec; 5.837 sec/batch)
2016-04-30 11:23:57.422447: step 996, loss = 29.53 (10.1 examples/sec; 6.336 sec/batch)
2016-04-30 11:24:03.731916: step 997, loss = 29.27 (10.1 examples/sec; 6.309 sec/batch)
2016-04-30 11:24:09.443900: step 998, loss = 29.30 (11.2 examples/sec; 5.712 sec/batch)
2016-04-30 11:24:15.601419: step 999, loss = 29.31 (10.4 examples/sec; 6.157 sec/batch)
2016-04-30 11:24:20.772132: step 1000, loss = 29.29 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 11:24:33.634195: step 1001, loss = 29.43 (11.1 examples/sec; 5.755 sec/batch)
2016-04-30 11:24:38.937788: step 1002, loss = 29.34 (12.1 examples/sec; 5.303 sec/batch)
2016-04-30 11:24:44.199515: step 1003, loss = 29.32 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 11:24:49.445255: step 1004, loss = 29.30 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 11:24:54.504660: step 1005, loss = 29.21 (12.6 examples/sec; 5.059 sec/batch)
2016-04-30 11:24:59.618413: step 1006, loss = 29.16 (12.5 examples/sec; 5.114 sec/batch)
2016-04-30 11:25:05.486331: step 1007, loss = 29.19 (10.9 examples/sec; 5.868 sec/batch)
2016-04-30 11:25:10.489510: step 1008, loss = 29.19 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 11:25:15.648196: step 1009, loss = 28.99 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 11:25:20.762121: step 1010, loss = 29.02 (12.5 examples/sec; 5.114 sec/batch)
2016-04-30 11:25:32.659040: step 1011, loss = 29.04 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 11:25:38.321850: step 1012, loss = 28.94 (11.3 examples/sec; 5.663 sec/batch)
2016-04-30 11:25:43.315946: step 1013, loss = 29.14 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 11:25:48.506943: step 1014, loss = 28.99 (12.3 examples/sec; 5.191 sec/batch)
2016-04-30 11:25:53.684561: step 1015, loss = 28.89 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 11:25:58.825438: step 1016, loss = 28.90 (12.4 examples/sec; 5.141 sec/batch)
2016-04-30 11:26:03.901527: step 1017, loss = 28.85 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 11:26:09.624799: step 1018, loss = 28.98 (11.2 examples/sec; 5.723 sec/batch)
2016-04-30 11:26:14.755942: step 1019, loss = 28.86 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 11:26:19.914156: step 1020, loss = 28.93 (12.4 examples/sec; 5.158 sec/batch)
2016-04-30 11:26:32.196537: step 1021, loss = 28.77 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 11:26:37.085569: step 1022, loss = 28.96 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 11:26:42.736971: step 1023, loss = 28.84 (11.3 examples/sec; 5.651 sec/batch)
2016-04-30 11:26:47.827537: step 1024, loss = 28.73 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 11:26:52.928923: step 1025, loss = 28.66 (12.5 examples/sec; 5.101 sec/batch)
2016-04-30 11:26:57.804211: step 1026, loss = 28.62 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 11:27:03.165939: step 1027, loss = 28.80 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 11:27:08.446039: step 1028, loss = 28.80 (12.1 examples/sec; 5.280 sec/batch)
2016-04-30 11:27:13.261651: step 1029, loss = 28.55 (13.3 examples/sec; 4.816 sec/batch)
2016-04-30 11:27:18.958592: step 1030, loss = 28.50 (11.2 examples/sec; 5.697 sec/batch)
2016-04-30 11:27:30.705609: step 1031, loss = 28.49 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 11:27:35.885308: step 1032, loss = 28.56 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 11:27:40.982790: step 1033, loss = 28.59 (12.6 examples/sec; 5.097 sec/batch)
2016-04-30 11:27:46.159847: step 1034, loss = 28.63 (12.4 examples/sec; 5.177 sec/batch)
2016-04-30 11:27:52.053358: step 1035, loss = 28.34 (10.9 examples/sec; 5.893 sec/batch)
2016-04-30 11:27:57.217059: step 1036, loss = 28.30 (12.4 examples/sec; 5.164 sec/batch)
2016-04-30 11:28:02.582683: step 1037, loss = 28.56 (11.9 examples/sec; 5.365 sec/batch)
2016-04-30 11:28:07.537219: step 1038, loss = 28.50 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 11:28:12.841833: step 1039, loss = 28.52 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 11:28:18.531564: step 1040, loss = 28.32 (11.2 examples/sec; 5.690 sec/batch)
2016-04-30 11:28:32.287491: step 1041, loss = 28.49 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 11:28:37.677570: step 1042, loss = 28.36 (11.9 examples/sec; 5.390 sec/batch)
2016-04-30 11:28:42.679943: step 1043, loss = 28.41 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 11:28:48.095776: step 1044, loss = 28.44 (11.8 examples/sec; 5.416 sec/batch)
2016-04-30 11:28:54.291453: step 1045, loss = 28.26 (10.3 examples/sec; 6.196 sec/batch)
2016-04-30 11:28:59.857092: step 1046, loss = 28.28 (11.5 examples/sec; 5.566 sec/batch)
2016-04-30 11:29:05.508669: step 1047, loss = 28.26 (11.3 examples/sec; 5.651 sec/batch)
2016-04-30 11:29:10.667758: step 1048, loss = 28.45 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 11:29:15.730640: step 1049, loss = 28.11 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 11:29:20.948759: step 1050, loss = 28.25 (12.3 examples/sec; 5.218 sec/batch)
2016-04-30 11:29:33.846631: step 1051, loss = 28.17 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 11:29:39.232779: step 1052, loss = 28.14 (11.9 examples/sec; 5.386 sec/batch)
2016-04-30 11:29:44.681234: step 1053, loss = 28.17 (11.7 examples/sec; 5.448 sec/batch)
2016-04-30 11:29:49.910143: step 1054, loss = 28.07 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 11:29:55.038537: step 1055, loss = 28.25 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 11:30:01.110470: step 1056, loss = 28.09 (10.5 examples/sec; 6.072 sec/batch)
2016-04-30 11:30:06.376387: step 1057, loss = 28.06 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 11:30:11.661833: step 1058, loss = 28.15 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 11:30:16.870491: step 1059, loss = 28.15 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 11:30:21.771153: step 1060, loss = 28.11 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 11:30:34.564621: step 1061, loss = 27.83 (11.6 examples/sec; 5.533 sec/batch)
2016-04-30 11:30:39.677034: step 1062, loss = 27.87 (12.5 examples/sec; 5.112 sec/batch)
2016-04-30 11:30:45.248067: step 1063, loss = 27.99 (11.5 examples/sec; 5.571 sec/batch)
2016-04-30 11:30:50.583709: step 1064, loss = 28.04 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 11:30:55.874576: step 1065, loss = 27.92 (12.1 examples/sec; 5.291 sec/batch)
2016-04-30 11:31:00.979034: step 1066, loss = 27.93 (12.5 examples/sec; 5.104 sec/batch)
2016-04-30 11:31:06.901777: step 1067, loss = 27.97 (10.8 examples/sec; 5.923 sec/batch)
2016-04-30 11:31:12.041805: step 1068, loss = 27.74 (12.5 examples/sec; 5.140 sec/batch)
2016-04-30 11:31:17.279979: step 1069, loss = 27.82 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 11:31:22.140010: step 1070, loss = 27.82 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 11:31:34.330523: step 1071, loss = 27.86 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 11:31:40.128331: step 1072, loss = 27.96 (11.0 examples/sec; 5.798 sec/batch)
2016-04-30 11:31:45.517076: step 1073, loss = 27.59 (11.9 examples/sec; 5.388 sec/batch)
2016-04-30 11:31:50.740456: step 1074, loss = 27.56 (12.3 examples/sec; 5.223 sec/batch)
2016-04-30 11:31:55.864800: step 1075, loss = 27.76 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 11:32:01.014904: step 1076, loss = 27.57 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 11:32:06.545622: step 1077, loss = 27.67 (11.6 examples/sec; 5.531 sec/batch)
2016-04-30 11:32:12.673460: step 1078, loss = 27.62 (10.4 examples/sec; 6.128 sec/batch)
2016-04-30 11:32:17.963023: step 1079, loss = 27.68 (12.1 examples/sec; 5.289 sec/batch)
2016-04-30 11:32:23.242048: step 1080, loss = 27.57 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 11:32:35.173784: step 1081, loss = 27.49 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 11:32:40.207707: step 1082, loss = 27.46 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 11:32:46.047993: step 1083, loss = 27.39 (11.0 examples/sec; 5.840 sec/batch)
2016-04-30 11:32:51.244525: step 1084, loss = 27.61 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 11:32:56.563264: step 1085, loss = 27.40 (12.0 examples/sec; 5.319 sec/batch)
2016-04-30 11:33:01.726574: step 1086, loss = 27.51 (12.4 examples/sec; 5.163 sec/batch)
2016-04-30 11:33:06.756855: step 1087, loss = 27.29 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 11:33:11.991768: step 1088, loss = 27.41 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 11:33:17.011678: step 1089, loss = 27.45 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 11:33:22.494838: step 1090, loss = 27.34 (11.7 examples/sec; 5.483 sec/batch)
2016-04-30 11:33:34.259283: step 1091, loss = 27.31 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 11:33:39.665025: step 1092, loss = 27.37 (11.8 examples/sec; 5.406 sec/batch)
2016-04-30 11:33:44.865022: step 1093, loss = 27.29 (12.3 examples/sec; 5.200 sec/batch)
2016-04-30 11:33:50.933628: step 1094, loss = 27.41 (10.5 examples/sec; 6.069 sec/batch)
2016-04-30 11:33:56.803014: step 1095, loss = 27.26 (10.9 examples/sec; 5.869 sec/batch)
2016-04-30 11:34:03.029726: step 1096, loss = 27.23 (10.3 examples/sec; 6.227 sec/batch)
2016-04-30 11:34:08.277969: step 1097, loss = 27.15 (12.2 examples/sec; 5.248 sec/batch)
2016-04-30 11:34:13.695247: step 1098, loss = 27.09 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 11:34:18.899627: step 1099, loss = 27.23 (12.3 examples/sec; 5.204 sec/batch)
2016-04-30 11:34:24.883964: step 1100, loss = 27.01 (10.7 examples/sec; 5.984 sec/batch)
2016-04-30 11:34:37.199041: step 1101, loss = 27.20 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 11:34:42.475510: step 1102, loss = 27.07 (12.1 examples/sec; 5.276 sec/batch)
2016-04-30 11:34:47.880478: step 1103, loss = 27.02 (11.8 examples/sec; 5.405 sec/batch)
2016-04-30 11:34:53.032453: step 1104, loss = 26.96 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 11:34:59.071321: step 1105, loss = 27.12 (10.6 examples/sec; 6.039 sec/batch)
2016-04-30 11:35:04.493173: step 1106, loss = 27.16 (11.8 examples/sec; 5.422 sec/batch)
2016-04-30 11:35:09.817787: step 1107, loss = 26.98 (12.0 examples/sec; 5.325 sec/batch)
2016-04-30 11:35:15.050372: step 1108, loss = 27.25 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 11:35:20.439073: step 1109, loss = 27.01 (11.9 examples/sec; 5.389 sec/batch)
2016-04-30 11:35:25.377225: step 1110, loss = 27.02 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 11:35:38.129681: step 1111, loss = 27.05 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 11:35:43.380471: step 1112, loss = 26.88 (12.2 examples/sec; 5.251 sec/batch)
2016-04-30 11:35:48.600084: step 1113, loss = 26.87 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 11:35:53.895210: step 1114, loss = 26.94 (12.1 examples/sec; 5.295 sec/batch)
2016-04-30 11:35:59.165070: step 1115, loss = 26.79 (12.1 examples/sec; 5.270 sec/batch)
2016-04-30 11:36:05.148553: step 1116, loss = 26.84 (10.7 examples/sec; 5.983 sec/batch)
2016-04-30 11:36:10.093641: step 1117, loss = 26.71 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 11:36:15.370706: step 1118, loss = 26.77 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 11:36:20.518532: step 1119, loss = 26.72 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 11:36:25.662101: step 1120, loss = 26.75 (12.4 examples/sec; 5.143 sec/batch)
2016-04-30 11:36:38.263879: step 1121, loss = 26.71 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 11:36:43.282678: step 1122, loss = 26.92 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 11:36:48.432753: step 1123, loss = 26.78 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 11:36:53.619944: step 1124, loss = 26.60 (12.3 examples/sec; 5.187 sec/batch)
2016-04-30 11:36:58.531689: step 1125, loss = 26.70 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 11:37:03.754373: step 1126, loss = 26.65 (12.3 examples/sec; 5.223 sec/batch)
2016-04-30 11:37:09.557090: step 1127, loss = 26.60 (11.0 examples/sec; 5.803 sec/batch)
2016-04-30 11:37:14.790461: step 1128, loss = 26.49 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 11:37:20.082414: step 1129, loss = 26.46 (12.1 examples/sec; 5.292 sec/batch)
2016-04-30 11:37:25.029745: step 1130, loss = 26.63 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 11:37:36.915872: step 1131, loss = 26.49 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 11:37:42.597595: step 1132, loss = 26.68 (11.3 examples/sec; 5.682 sec/batch)
2016-04-30 11:37:47.751158: step 1133, loss = 26.54 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 11:37:52.825747: step 1134, loss = 26.35 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 11:37:57.855500: step 1135, loss = 26.44 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 11:38:03.509235: step 1136, loss = 26.51 (11.3 examples/sec; 5.654 sec/batch)
2016-04-30 11:38:08.783770: step 1137, loss = 26.16 (12.1 examples/sec; 5.274 sec/batch)
2016-04-30 11:38:13.980864: step 1138, loss = 26.49 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 11:38:19.589822: step 1139, loss = 26.30 (11.4 examples/sec; 5.609 sec/batch)
2016-04-30 11:38:24.768403: step 1140, loss = 26.33 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 11:38:36.660425: step 1141, loss = 26.43 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 11:38:42.004455: step 1142, loss = 26.18 (12.0 examples/sec; 5.344 sec/batch)
2016-04-30 11:38:47.975396: step 1143, loss = 26.20 (10.7 examples/sec; 5.971 sec/batch)
2016-04-30 11:38:54.433227: step 1144, loss = 26.17 (9.9 examples/sec; 6.458 sec/batch)
2016-04-30 11:38:59.976022: step 1145, loss = 26.26 (11.5 examples/sec; 5.543 sec/batch)
2016-04-30 11:39:05.622642: step 1146, loss = 26.35 (11.3 examples/sec; 5.647 sec/batch)
2016-04-30 11:39:10.944446: step 1147, loss = 26.42 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 11:39:16.150361: step 1148, loss = 26.24 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 11:39:22.194538: step 1149, loss = 26.15 (10.6 examples/sec; 6.044 sec/batch)
2016-04-30 11:39:27.482618: step 1150, loss = 25.99 (12.1 examples/sec; 5.288 sec/batch)
2016-04-30 11:39:39.679684: step 1151, loss = 26.11 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 11:39:45.073856: step 1152, loss = 26.18 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 11:39:50.461419: step 1153, loss = 26.18 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 11:39:56.349191: step 1154, loss = 26.12 (10.9 examples/sec; 5.888 sec/batch)
2016-04-30 11:40:01.578198: step 1155, loss = 26.35 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 11:40:06.923385: step 1156, loss = 26.11 (12.0 examples/sec; 5.345 sec/batch)
2016-04-30 11:40:12.156449: step 1157, loss = 25.89 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 11:40:17.366116: step 1158, loss = 25.75 (12.3 examples/sec; 5.210 sec/batch)
2016-04-30 11:40:22.207857: step 1159, loss = 25.99 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 11:40:28.091782: step 1160, loss = 25.87 (10.9 examples/sec; 5.884 sec/batch)
2016-04-30 11:40:40.045377: step 1161, loss = 26.07 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 11:40:45.311392: step 1162, loss = 25.93 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 11:40:50.545039: step 1163, loss = 25.85 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 11:40:55.478976: step 1164, loss = 25.77 (13.0 examples/sec; 4.934 sec/batch)
2016-04-30 11:41:01.391168: step 1165, loss = 25.89 (10.8 examples/sec; 5.912 sec/batch)
2016-04-30 11:41:06.523053: step 1166, loss = 25.78 (12.5 examples/sec; 5.132 sec/batch)
2016-04-30 11:41:11.661855: step 1167, loss = 25.87 (12.5 examples/sec; 5.139 sec/batch)
2016-04-30 11:41:16.778576: step 1168, loss = 25.70 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 11:41:21.928655: step 1169, loss = 25.58 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 11:41:27.111154: step 1170, loss = 25.71 (12.3 examples/sec; 5.182 sec/batch)
2016-04-30 11:41:39.567432: step 1171, loss = 25.83 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 11:41:44.907746: step 1172, loss = 25.59 (12.0 examples/sec; 5.340 sec/batch)
2016-04-30 11:41:50.193484: step 1173, loss = 25.67 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 11:41:55.239909: step 1174, loss = 25.46 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 11:42:00.833505: step 1175, loss = 25.57 (11.4 examples/sec; 5.594 sec/batch)
2016-04-30 11:42:06.581390: step 1176, loss = 25.62 (11.1 examples/sec; 5.748 sec/batch)
2016-04-30 11:42:11.913293: step 1177, loss = 25.68 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 11:42:17.122756: step 1178, loss = 25.62 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 11:42:21.934803: step 1179, loss = 25.70 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 11:42:27.175523: step 1180, loss = 25.39 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 11:42:39.606697: step 1181, loss = 25.59 (11.5 examples/sec; 5.585 sec/batch)
2016-04-30 11:42:44.780743: step 1182, loss = 25.57 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 11:42:49.999838: step 1183, loss = 25.37 (12.3 examples/sec; 5.219 sec/batch)
2016-04-30 11:42:54.958775: step 1184, loss = 25.49 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 11:43:00.269105: step 1185, loss = 25.33 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 11:43:05.615027: step 1186, loss = 25.46 (12.0 examples/sec; 5.346 sec/batch)
2016-04-30 11:43:10.622889: step 1187, loss = 25.59 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 11:43:16.356513: step 1188, loss = 25.32 (11.2 examples/sec; 5.734 sec/batch)
2016-04-30 11:43:22.046677: step 1189, loss = 25.43 (11.2 examples/sec; 5.690 sec/batch)
2016-04-30 11:43:27.691751: step 1190, loss = 25.27 (11.3 examples/sec; 5.645 sec/batch)
2016-04-30 11:43:40.730086: step 1191, loss = 25.37 (12.8 examples/sec; 5.010 sec/batch)
2016-04-30 11:43:46.406326: step 1192, loss = 25.34 (11.3 examples/sec; 5.676 sec/batch)
2016-04-30 11:43:51.807154: step 1193, loss = 25.48 (11.9 examples/sec; 5.401 sec/batch)
2016-04-30 11:43:57.217927: step 1194, loss = 25.41 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 11:44:02.656625: step 1195, loss = 25.26 (11.8 examples/sec; 5.439 sec/batch)
2016-04-30 11:44:07.730494: step 1196, loss = 25.22 (12.6 examples/sec; 5.074 sec/batch)
2016-04-30 11:44:12.860433: step 1197, loss = 25.03 (12.5 examples/sec; 5.130 sec/batch)
2016-04-30 11:44:18.695783: step 1198, loss = 25.03 (11.0 examples/sec; 5.835 sec/batch)
2016-04-30 11:44:23.989300: step 1199, loss = 25.25 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 11:44:29.435043: step 1200, loss = 25.21 (11.8 examples/sec; 5.446 sec/batch)
2016-04-30 11:44:41.370293: step 1201, loss = 25.22 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 11:44:46.247430: step 1202, loss = 25.11 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 11:44:52.039816: step 1203, loss = 25.23 (11.0 examples/sec; 5.792 sec/batch)
2016-04-30 11:44:57.225965: step 1204, loss = 25.17 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 11:45:02.551974: step 1205, loss = 25.18 (12.0 examples/sec; 5.326 sec/batch)
2016-04-30 11:45:07.645134: step 1206, loss = 25.04 (12.6 examples/sec; 5.093 sec/batch)
2016-04-30 11:45:13.524097: step 1207, loss = 24.92 (10.9 examples/sec; 5.879 sec/batch)
2016-04-30 11:45:18.859745: step 1208, loss = 25.09 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 11:45:25.242071: step 1209, loss = 24.90 (10.0 examples/sec; 6.382 sec/batch)
2016-04-30 11:45:30.736884: step 1210, loss = 25.17 (11.6 examples/sec; 5.495 sec/batch)
2016-04-30 11:45:42.500543: step 1211, loss = 25.01 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 11:45:47.651083: step 1212, loss = 24.94 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 11:45:52.509190: step 1213, loss = 24.89 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 11:45:58.189863: step 1214, loss = 24.82 (11.3 examples/sec; 5.681 sec/batch)
2016-04-30 11:46:03.740010: step 1215, loss = 24.82 (11.5 examples/sec; 5.550 sec/batch)
2016-04-30 11:46:08.970325: step 1216, loss = 25.00 (12.2 examples/sec; 5.230 sec/batch)
2016-04-30 11:46:14.256508: step 1217, loss = 24.93 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 11:46:19.265255: step 1218, loss = 24.85 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 11:46:24.665756: step 1219, loss = 25.01 (11.9 examples/sec; 5.400 sec/batch)
2016-04-30 11:46:30.477175: step 1220, loss = 24.84 (11.0 examples/sec; 5.811 sec/batch)
2016-04-30 11:46:42.355594: step 1221, loss = 24.66 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 11:46:47.614323: step 1222, loss = 24.63 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 11:46:52.491307: step 1223, loss = 24.85 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 11:46:57.606514: step 1224, loss = 24.64 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 11:47:03.377558: step 1225, loss = 24.81 (11.1 examples/sec; 5.771 sec/batch)
2016-04-30 11:47:08.685113: step 1226, loss = 24.75 (12.1 examples/sec; 5.307 sec/batch)
2016-04-30 11:47:13.710987: step 1227, loss = 24.70 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 11:47:18.779488: step 1228, loss = 24.58 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 11:47:24.115144: step 1229, loss = 24.55 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 11:47:29.126262: step 1230, loss = 24.60 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 11:47:41.381248: step 1231, loss = 24.78 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 11:47:46.439557: step 1232, loss = 24.64 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 11:47:51.782825: step 1233, loss = 24.53 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 11:47:57.037946: step 1234, loss = 24.65 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 11:48:02.259921: step 1235, loss = 24.44 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 11:48:07.098182: step 1236, loss = 24.42 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 11:48:12.765762: step 1237, loss = 24.62 (11.3 examples/sec; 5.667 sec/batch)
2016-04-30 11:48:17.880988: step 1238, loss = 24.24 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 11:48:23.573963: step 1239, loss = 24.61 (11.2 examples/sec; 5.693 sec/batch)
2016-04-30 11:48:29.216673: step 1240, loss = 24.40 (11.3 examples/sec; 5.643 sec/batch)
2016-04-30 11:48:41.776644: step 1241, loss = 24.40 (12.2 examples/sec; 5.248 sec/batch)
2016-04-30 11:48:47.468358: step 1242, loss = 24.39 (11.2 examples/sec; 5.692 sec/batch)
2016-04-30 11:48:52.457833: step 1243, loss = 24.25 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 11:48:57.596171: step 1244, loss = 24.47 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 11:49:02.853201: step 1245, loss = 24.46 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 11:49:07.547110: step 1246, loss = 24.30 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 11:49:12.796555: step 1247, loss = 24.33 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 11:49:18.391981: step 1248, loss = 24.26 (11.4 examples/sec; 5.595 sec/batch)
2016-04-30 11:49:23.420663: step 1249, loss = 24.11 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 11:49:28.251470: step 1250, loss = 24.35 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 11:49:39.995914: step 1251, loss = 24.27 (13.9 examples/sec; 4.615 sec/batch)
2016-04-30 11:49:45.133460: step 1252, loss = 23.95 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 11:49:50.890383: step 1253, loss = 24.23 (11.1 examples/sec; 5.757 sec/batch)
2016-04-30 11:49:55.789326: step 1254, loss = 24.12 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 11:50:01.056437: step 1255, loss = 24.16 (12.2 examples/sec; 5.267 sec/batch)
2016-04-30 11:50:06.202689: step 1256, loss = 24.15 (12.4 examples/sec; 5.146 sec/batch)
2016-04-30 11:50:11.369424: step 1257, loss = 24.17 (12.4 examples/sec; 5.167 sec/batch)
2016-04-30 11:50:16.240422: step 1258, loss = 23.94 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 11:50:21.911886: step 1259, loss = 23.87 (11.3 examples/sec; 5.671 sec/batch)
2016-04-30 11:50:27.053584: step 1260, loss = 24.18 (12.4 examples/sec; 5.142 sec/batch)
2016-04-30 11:50:38.778575: step 1261, loss = 24.27 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 11:50:43.786312: step 1262, loss = 24.05 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 11:50:48.884980: step 1263, loss = 23.82 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 11:50:54.385355: step 1264, loss = 24.09 (11.6 examples/sec; 5.500 sec/batch)
2016-04-30 11:50:59.465244: step 1265, loss = 23.84 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 11:51:04.481309: step 1266, loss = 23.84 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 11:51:09.490654: step 1267, loss = 24.01 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 11:51:14.649650: step 1268, loss = 23.77 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 11:51:19.655903: step 1269, loss = 23.97 (12.8 examples/sec; 5.006 sec/batch)
2016-04-30 11:51:25.053166: step 1270, loss = 23.99 (11.9 examples/sec; 5.397 sec/batch)
2016-04-30 11:51:41.638105: step 1271, loss = 23.66 (9.4 examples/sec; 6.784 sec/batch)
2016-04-30 11:51:48.496709: step 1272, loss = 23.86 (9.3 examples/sec; 6.858 sec/batch)
2016-04-30 11:51:55.181366: step 1273, loss = 24.00 (9.6 examples/sec; 6.685 sec/batch)
2016-04-30 11:52:02.999169: step 1274, loss = 23.75 (8.2 examples/sec; 7.818 sec/batch)
2016-04-30 11:52:09.167848: step 1275, loss = 23.89 (10.4 examples/sec; 6.169 sec/batch)
2016-04-30 11:52:15.394163: step 1276, loss = 23.77 (10.3 examples/sec; 6.226 sec/batch)
2016-04-30 11:52:21.866989: step 1277, loss = 23.87 (9.9 examples/sec; 6.473 sec/batch)
2016-04-30 11:52:29.632001: step 1278, loss = 23.69 (8.2 examples/sec; 7.765 sec/batch)
2016-04-30 11:52:38.692250: step 1279, loss = 23.74 (7.1 examples/sec; 9.060 sec/batch)
2016-04-30 11:52:46.918323: step 1280, loss = 23.63 (7.8 examples/sec; 8.226 sec/batch)
2016-04-30 11:53:03.840974: step 1281, loss = 23.59 (9.4 examples/sec; 6.777 sec/batch)
2016-04-30 11:53:09.931160: step 1282, loss = 23.57 (10.5 examples/sec; 6.090 sec/batch)
2016-04-30 11:53:15.654191: step 1283, loss = 23.47 (11.2 examples/sec; 5.723 sec/batch)
2016-04-30 11:53:21.563346: step 1284, loss = 23.67 (10.8 examples/sec; 5.909 sec/batch)
2016-04-30 11:53:27.563218: step 1285, loss = 23.73 (10.7 examples/sec; 6.000 sec/batch)
2016-04-30 11:53:33.607826: step 1286, loss = 23.62 (10.6 examples/sec; 6.045 sec/batch)
2016-04-30 11:53:39.740543: step 1287, loss = 23.62 (10.4 examples/sec; 6.133 sec/batch)
2016-04-30 11:53:44.971258: step 1288, loss = 23.53 (12.2 examples/sec; 5.231 sec/batch)
2016-04-30 11:53:50.317063: step 1289, loss = 23.53 (12.0 examples/sec; 5.346 sec/batch)
2016-04-30 11:53:55.383234: step 1290, loss = 23.41 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 11:54:07.476556: step 1291, loss = 23.44 (13.3 examples/sec; 4.799 sec/batch)
2016-04-30 11:54:13.688466: step 1292, loss = 23.44 (10.3 examples/sec; 6.212 sec/batch)
2016-04-30 11:54:19.157691: step 1293, loss = 23.43 (11.7 examples/sec; 5.469 sec/batch)
2016-04-30 11:54:24.663932: step 1294, loss = 23.44 (11.6 examples/sec; 5.506 sec/batch)
2016-04-30 11:54:29.931603: step 1295, loss = 23.35 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 11:54:34.856435: step 1296, loss = 23.40 (13.0 examples/sec; 4.925 sec/batch)
2016-04-30 11:54:40.123019: step 1297, loss = 23.40 (12.2 examples/sec; 5.267 sec/batch)
2016-04-30 11:54:45.893496: step 1298, loss = 23.51 (11.1 examples/sec; 5.770 sec/batch)
2016-04-30 11:54:51.125780: step 1299, loss = 23.43 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 11:54:56.233303: step 1300, loss = 23.18 (12.5 examples/sec; 5.107 sec/batch)
2016-04-30 11:55:08.086361: step 1301, loss = 23.25 (13.5 examples/sec; 4.739 sec/batch)
2016-04-30 11:55:13.118013: step 1302, loss = 23.44 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 11:55:18.809777: step 1303, loss = 23.20 (11.2 examples/sec; 5.692 sec/batch)
2016-04-30 11:55:24.140037: step 1304, loss = 23.26 (12.0 examples/sec; 5.330 sec/batch)
2016-04-30 11:55:29.337534: step 1305, loss = 23.48 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 11:55:34.443622: step 1306, loss = 23.22 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 11:55:39.634025: step 1307, loss = 23.10 (12.3 examples/sec; 5.190 sec/batch)
2016-04-30 11:55:44.873224: step 1308, loss = 23.08 (12.2 examples/sec; 5.239 sec/batch)
2016-04-30 11:55:49.875053: step 1309, loss = 23.41 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 11:55:55.689484: step 1310, loss = 23.15 (11.0 examples/sec; 5.814 sec/batch)
2016-04-30 11:56:07.962662: step 1311, loss = 23.18 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 11:56:13.117172: step 1312, loss = 23.21 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 11:56:18.444183: step 1313, loss = 23.04 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 11:56:24.020840: step 1314, loss = 23.19 (11.5 examples/sec; 5.577 sec/batch)
2016-04-30 11:56:29.497768: step 1315, loss = 23.00 (11.7 examples/sec; 5.477 sec/batch)
2016-04-30 11:56:34.458260: step 1316, loss = 23.02 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 11:56:39.833101: step 1317, loss = 22.97 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 11:56:45.119841: step 1318, loss = 23.00 (12.1 examples/sec; 5.287 sec/batch)
2016-04-30 11:56:50.390939: step 1319, loss = 22.97 (12.1 examples/sec; 5.271 sec/batch)
2016-04-30 11:56:55.432450: step 1320, loss = 23.01 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 11:57:08.595757: step 1321, loss = 23.00 (12.6 examples/sec; 5.073 sec/batch)
2016-04-30 11:57:13.533009: step 1322, loss = 23.00 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 11:57:18.921299: step 1323, loss = 22.94 (11.9 examples/sec; 5.388 sec/batch)
2016-04-30 11:57:24.306496: step 1324, loss = 23.02 (11.9 examples/sec; 5.385 sec/batch)
2016-04-30 11:57:29.795906: step 1325, loss = 22.72 (11.7 examples/sec; 5.489 sec/batch)
2016-04-30 11:57:35.437099: step 1326, loss = 22.88 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 11:57:40.298473: step 1327, loss = 22.88 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 11:57:45.656408: step 1328, loss = 22.83 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 11:57:50.867423: step 1329, loss = 22.86 (12.3 examples/sec; 5.211 sec/batch)
2016-04-30 11:57:56.274279: step 1330, loss = 22.93 (11.8 examples/sec; 5.407 sec/batch)
2016-04-30 11:58:09.049808: step 1331, loss = 22.74 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 11:58:14.576893: step 1332, loss = 22.61 (11.6 examples/sec; 5.527 sec/batch)
2016-04-30 11:58:20.197188: step 1333, loss = 22.74 (11.4 examples/sec; 5.620 sec/batch)
2016-04-30 11:58:26.447781: step 1334, loss = 22.85 (10.2 examples/sec; 6.250 sec/batch)
2016-04-30 11:58:31.568746: step 1335, loss = 22.69 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 11:58:37.568293: step 1336, loss = 22.89 (10.7 examples/sec; 5.999 sec/batch)
2016-04-30 11:58:43.175918: step 1337, loss = 22.74 (11.4 examples/sec; 5.608 sec/batch)
2016-04-30 11:58:48.476565: step 1338, loss = 22.75 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 11:58:53.826627: step 1339, loss = 22.77 (12.0 examples/sec; 5.350 sec/batch)
2016-04-30 11:58:58.845333: step 1340, loss = 22.53 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 11:59:11.830354: step 1341, loss = 22.75 (11.4 examples/sec; 5.590 sec/batch)
2016-04-30 11:59:16.833550: step 1342, loss = 22.50 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 11:59:21.890288: step 1343, loss = 22.60 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 11:59:27.131832: step 1344, loss = 22.59 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 11:59:32.495643: step 1345, loss = 22.44 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 11:59:37.386397: step 1346, loss = 22.49 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 11:59:43.319741: step 1347, loss = 22.58 (10.8 examples/sec; 5.933 sec/batch)
2016-04-30 11:59:48.553636: step 1348, loss = 22.51 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 11:59:53.744470: step 1349, loss = 22.48 (12.3 examples/sec; 5.191 sec/batch)
2016-04-30 11:59:58.735772: step 1350, loss = 22.55 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 12:00:10.722276: step 1351, loss = 22.50 (13.7 examples/sec; 4.686 sec/batch)
2016-04-30 12:00:16.516704: step 1352, loss = 22.36 (11.0 examples/sec; 5.794 sec/batch)
2016-04-30 12:00:21.845025: step 1353, loss = 22.56 (12.0 examples/sec; 5.328 sec/batch)
2016-04-30 12:00:27.093932: step 1354, loss = 22.34 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 12:00:32.042993: step 1355, loss = 22.30 (12.9 examples/sec; 4.949 sec/batch)
2016-04-30 12:00:37.034238: step 1356, loss = 22.28 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 12:00:42.115681: step 1357, loss = 22.57 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 12:00:46.953465: step 1358, loss = 22.32 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 12:00:52.750547: step 1359, loss = 22.28 (11.0 examples/sec; 5.797 sec/batch)
2016-04-30 12:00:57.851592: step 1360, loss = 22.17 (12.5 examples/sec; 5.101 sec/batch)
2016-04-30 12:01:10.098671: step 1361, loss = 22.32 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 12:01:15.353598: step 1362, loss = 22.22 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 12:01:20.873331: step 1363, loss = 22.26 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 12:01:26.432084: step 1364, loss = 22.22 (11.5 examples/sec; 5.559 sec/batch)
2016-04-30 12:01:31.619832: step 1365, loss = 22.36 (12.3 examples/sec; 5.188 sec/batch)
2016-04-30 12:01:36.882707: step 1366, loss = 22.20 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 12:01:42.186688: step 1367, loss = 22.11 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 12:01:47.246145: step 1368, loss = 22.07 (12.6 examples/sec; 5.059 sec/batch)
2016-04-30 12:01:52.257220: step 1369, loss = 22.27 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 12:01:57.937229: step 1370, loss = 22.05 (11.3 examples/sec; 5.680 sec/batch)
2016-04-30 12:02:10.220471: step 1371, loss = 22.16 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 12:02:15.536491: step 1372, loss = 22.16 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 12:02:20.966984: step 1373, loss = 22.04 (11.8 examples/sec; 5.430 sec/batch)
2016-04-30 12:02:25.929595: step 1374, loss = 22.17 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 12:02:31.597836: step 1375, loss = 22.12 (11.3 examples/sec; 5.668 sec/batch)
2016-04-30 12:02:36.882343: step 1376, loss = 22.09 (12.1 examples/sec; 5.284 sec/batch)
2016-04-30 12:02:42.049297: step 1377, loss = 22.09 (12.4 examples/sec; 5.167 sec/batch)
2016-04-30 12:02:47.184178: step 1378, loss = 22.21 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 12:02:52.191670: step 1379, loss = 22.11 (12.8 examples/sec; 5.007 sec/batch)
2016-04-30 12:02:57.448045: step 1380, loss = 22.11 (12.2 examples/sec; 5.256 sec/batch)
2016-04-30 12:03:11.875381: step 1381, loss = 21.97 (10.8 examples/sec; 5.939 sec/batch)
2016-04-30 12:03:17.271472: step 1382, loss = 21.99 (11.9 examples/sec; 5.396 sec/batch)
2016-04-30 12:03:22.718143: step 1383, loss = 21.82 (11.8 examples/sec; 5.447 sec/batch)
2016-04-30 12:03:27.946071: step 1384, loss = 21.99 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 12:03:34.192872: step 1385, loss = 22.00 (10.2 examples/sec; 6.247 sec/batch)
2016-04-30 12:03:39.803341: step 1386, loss = 21.79 (11.4 examples/sec; 5.610 sec/batch)
2016-04-30 12:03:45.178325: step 1387, loss = 21.93 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 12:03:50.741532: step 1388, loss = 21.78 (11.5 examples/sec; 5.563 sec/batch)
2016-04-30 12:03:55.942088: step 1389, loss = 21.86 (12.3 examples/sec; 5.200 sec/batch)
2016-04-30 12:04:01.253141: step 1390, loss = 22.07 (12.1 examples/sec; 5.311 sec/batch)
2016-04-30 12:04:14.115131: step 1391, loss = 21.78 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 12:04:19.339699: step 1392, loss = 21.67 (12.2 examples/sec; 5.224 sec/batch)
2016-04-30 12:04:24.670786: step 1393, loss = 21.81 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 12:04:30.083322: step 1394, loss = 21.83 (11.8 examples/sec; 5.412 sec/batch)
2016-04-30 12:04:35.228059: step 1395, loss = 21.62 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 12:04:41.071353: step 1396, loss = 21.72 (11.0 examples/sec; 5.843 sec/batch)
2016-04-30 12:04:46.240442: step 1397, loss = 21.67 (12.4 examples/sec; 5.169 sec/batch)
2016-04-30 12:04:51.578187: step 1398, loss = 21.61 (12.0 examples/sec; 5.338 sec/batch)
2016-04-30 12:04:56.856328: step 1399, loss = 21.69 (12.1 examples/sec; 5.278 sec/batch)
2016-04-30 12:05:02.102842: step 1400, loss = 21.83 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 12:05:14.611518: step 1401, loss = 21.67 (11.4 examples/sec; 5.616 sec/batch)
2016-04-30 12:05:19.640754: step 1402, loss = 21.63 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 12:05:24.853630: step 1403, loss = 21.67 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 12:05:30.201814: step 1404, loss = 21.56 (12.0 examples/sec; 5.348 sec/batch)
2016-04-30 12:05:35.255876: step 1405, loss = 21.61 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 12:05:40.283186: step 1406, loss = 21.45 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 12:05:46.065122: step 1407, loss = 21.49 (11.1 examples/sec; 5.782 sec/batch)
2016-04-30 12:05:51.366261: step 1408, loss = 21.59 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 12:05:56.323055: step 1409, loss = 21.51 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 12:06:01.615740: step 1410, loss = 21.58 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 12:06:13.534657: step 1411, loss = 21.51 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 12:06:19.810791: step 1412, loss = 21.33 (10.2 examples/sec; 6.276 sec/batch)
2016-04-30 12:06:25.490626: step 1413, loss = 21.57 (11.3 examples/sec; 5.680 sec/batch)
2016-04-30 12:06:31.685801: step 1414, loss = 21.39 (10.3 examples/sec; 6.195 sec/batch)
2016-04-30 12:06:37.115061: step 1415, loss = 21.55 (11.8 examples/sec; 5.429 sec/batch)
2016-04-30 12:06:42.430634: step 1416, loss = 21.35 (12.0 examples/sec; 5.315 sec/batch)
2016-04-30 12:06:47.748902: step 1417, loss = 21.30 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 12:06:53.746335: step 1418, loss = 21.39 (10.7 examples/sec; 5.997 sec/batch)
2016-04-30 12:06:58.922682: step 1419, loss = 21.22 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 12:07:04.447188: step 1420, loss = 21.40 (11.6 examples/sec; 5.524 sec/batch)
2016-04-30 12:07:16.567505: step 1421, loss = 21.33 (13.4 examples/sec; 4.767 sec/batch)
2016-04-30 12:07:22.095352: step 1422, loss = 21.29 (11.6 examples/sec; 5.528 sec/batch)
2016-04-30 12:07:28.052502: step 1423, loss = 21.26 (10.7 examples/sec; 5.957 sec/batch)
2016-04-30 12:07:33.351016: step 1424, loss = 21.39 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 12:07:38.869836: step 1425, loss = 21.22 (11.6 examples/sec; 5.519 sec/batch)
2016-04-30 12:07:44.042629: step 1426, loss = 21.37 (12.4 examples/sec; 5.173 sec/batch)
2016-04-30 12:07:49.468783: step 1427, loss = 21.30 (11.8 examples/sec; 5.426 sec/batch)
2016-04-30 12:07:54.918414: step 1428, loss = 21.22 (11.7 examples/sec; 5.450 sec/batch)
2016-04-30 12:08:01.013477: step 1429, loss = 21.08 (10.5 examples/sec; 6.095 sec/batch)
2016-04-30 12:08:06.246380: step 1430, loss = 21.12 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 12:08:18.576425: step 1431, loss = 21.17 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 12:08:23.874352: step 1432, loss = 21.22 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 12:08:29.021103: step 1433, loss = 20.84 (12.4 examples/sec; 5.147 sec/batch)
2016-04-30 12:08:34.782738: step 1434, loss = 21.13 (11.1 examples/sec; 5.762 sec/batch)
2016-04-30 12:08:40.199222: step 1435, loss = 21.17 (11.8 examples/sec; 5.416 sec/batch)
2016-04-30 12:08:45.608458: step 1436, loss = 21.08 (11.8 examples/sec; 5.409 sec/batch)
2016-04-30 12:08:51.000446: step 1437, loss = 21.10 (11.9 examples/sec; 5.392 sec/batch)
2016-04-30 12:08:56.003604: step 1438, loss = 21.14 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 12:09:01.377672: step 1439, loss = 21.01 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 12:09:07.225112: step 1440, loss = 21.14 (10.9 examples/sec; 5.847 sec/batch)
2016-04-30 12:09:19.533874: step 1441, loss = 20.93 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 12:09:24.957347: step 1442, loss = 20.87 (11.8 examples/sec; 5.423 sec/batch)
2016-04-30 12:09:30.153811: step 1443, loss = 20.96 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 12:09:35.176132: step 1444, loss = 21.02 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 12:09:40.945650: step 1445, loss = 20.85 (11.1 examples/sec; 5.769 sec/batch)
2016-04-30 12:09:46.204234: step 1446, loss = 21.03 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 12:09:51.618721: step 1447, loss = 20.86 (11.8 examples/sec; 5.414 sec/batch)
2016-04-30 12:09:56.908460: step 1448, loss = 20.85 (12.1 examples/sec; 5.290 sec/batch)
2016-04-30 12:10:02.434436: step 1449, loss = 20.70 (11.6 examples/sec; 5.526 sec/batch)
2016-04-30 12:10:07.567350: step 1450, loss = 21.06 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 12:10:20.189359: step 1451, loss = 20.90 (13.1 examples/sec; 4.867 sec/batch)
2016-04-30 12:10:25.273032: step 1452, loss = 20.83 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 12:10:30.443867: step 1453, loss = 20.89 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 12:10:35.783493: step 1454, loss = 20.90 (12.0 examples/sec; 5.340 sec/batch)
2016-04-30 12:10:40.799715: step 1455, loss = 20.76 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 12:10:46.626442: step 1456, loss = 20.88 (11.0 examples/sec; 5.827 sec/batch)
2016-04-30 12:10:52.015503: step 1457, loss = 20.84 (11.9 examples/sec; 5.389 sec/batch)
2016-04-30 12:10:57.346749: step 1458, loss = 21.04 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 12:11:02.578813: step 1459, loss = 20.85 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 12:11:07.575199: step 1460, loss = 20.57 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 12:11:20.356601: step 1461, loss = 20.66 (12.5 examples/sec; 5.127 sec/batch)
2016-04-30 12:11:25.518057: step 1462, loss = 20.56 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 12:11:30.833998: step 1463, loss = 20.43 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 12:11:36.155239: step 1464, loss = 20.46 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 12:11:41.365230: step 1465, loss = 20.67 (12.3 examples/sec; 5.210 sec/batch)
2016-04-30 12:11:46.569102: step 1466, loss = 20.49 (12.3 examples/sec; 5.204 sec/batch)
2016-04-30 12:11:52.540190: step 1467, loss = 20.69 (10.7 examples/sec; 5.971 sec/batch)
2016-04-30 12:11:57.837184: step 1468, loss = 20.56 (12.1 examples/sec; 5.297 sec/batch)
2016-04-30 12:12:03.339638: step 1469, loss = 20.54 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 12:12:08.705424: step 1470, loss = 20.47 (11.9 examples/sec; 5.366 sec/batch)
2016-04-30 12:12:21.023495: step 1471, loss = 20.48 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 12:12:27.064011: step 1472, loss = 20.51 (10.6 examples/sec; 6.040 sec/batch)
2016-04-30 12:12:33.017256: step 1473, loss = 20.49 (10.8 examples/sec; 5.953 sec/batch)
2016-04-30 12:12:39.125547: step 1474, loss = 20.46 (10.5 examples/sec; 6.108 sec/batch)
2016-04-30 12:12:44.615372: step 1475, loss = 20.50 (11.7 examples/sec; 5.490 sec/batch)
2016-04-30 12:12:49.797151: step 1476, loss = 20.27 (12.4 examples/sec; 5.182 sec/batch)
2016-04-30 12:12:55.772437: step 1477, loss = 20.37 (10.7 examples/sec; 5.975 sec/batch)
2016-04-30 12:13:01.519788: step 1478, loss = 20.30 (11.1 examples/sec; 5.747 sec/batch)
2016-04-30 12:13:07.029211: step 1479, loss = 20.51 (11.6 examples/sec; 5.509 sec/batch)
2016-04-30 12:13:12.350585: step 1480, loss = 20.28 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 12:13:24.627777: step 1481, loss = 20.30 (11.9 examples/sec; 5.400 sec/batch)
2016-04-30 12:13:30.407424: step 1482, loss = 20.25 (11.1 examples/sec; 5.780 sec/batch)
2016-04-30 12:13:35.727029: step 1483, loss = 20.42 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 12:13:40.934504: step 1484, loss = 20.50 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 12:13:46.198028: step 1485, loss = 20.39 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 12:13:51.484227: step 1486, loss = 20.34 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 12:13:56.628479: step 1487, loss = 20.39 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 12:14:02.775082: step 1488, loss = 20.12 (10.4 examples/sec; 6.147 sec/batch)
2016-04-30 12:14:07.767751: step 1489, loss = 20.38 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 12:14:13.103365: step 1490, loss = 20.20 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 12:14:25.100615: step 1491, loss = 20.11 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 12:14:30.279691: step 1492, loss = 20.36 (12.4 examples/sec; 5.179 sec/batch)
2016-04-30 12:14:36.024559: step 1493, loss = 20.21 (11.1 examples/sec; 5.745 sec/batch)
2016-04-30 12:14:40.894296: step 1494, loss = 20.06 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 12:14:46.120985: step 1495, loss = 20.22 (12.2 examples/sec; 5.227 sec/batch)
2016-04-30 12:14:51.282605: step 1496, loss = 20.19 (12.4 examples/sec; 5.162 sec/batch)
2016-04-30 12:14:56.341589: step 1497, loss = 20.29 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 12:15:01.733203: step 1498, loss = 20.12 (11.9 examples/sec; 5.392 sec/batch)
2016-04-30 12:15:07.483306: step 1499, loss = 20.10 (11.1 examples/sec; 5.750 sec/batch)
2016-04-30 12:15:12.885003: step 1500, loss = 19.84 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 12:15:24.911004: step 1501, loss = 20.14 (12.5 examples/sec; 5.119 sec/batch)
2016-04-30 12:15:30.289169: step 1502, loss = 20.08 (11.9 examples/sec; 5.378 sec/batch)
2016-04-30 12:15:35.517675: step 1503, loss = 19.98 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 12:15:41.133374: step 1504, loss = 20.03 (11.4 examples/sec; 5.616 sec/batch)
2016-04-30 12:15:46.316113: step 1505, loss = 20.16 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 12:15:51.624892: step 1506, loss = 20.02 (12.1 examples/sec; 5.309 sec/batch)
2016-04-30 12:15:56.872418: step 1507, loss = 19.96 (12.2 examples/sec; 5.247 sec/batch)
2016-04-30 12:16:02.038110: step 1508, loss = 20.13 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 12:16:07.251525: step 1509, loss = 19.88 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 12:16:13.263616: step 1510, loss = 19.94 (10.6 examples/sec; 6.012 sec/batch)
2016-04-30 12:16:25.325959: step 1511, loss = 19.70 (12.7 examples/sec; 5.056 sec/batch)
2016-04-30 12:16:30.570211: step 1512, loss = 19.97 (12.2 examples/sec; 5.244 sec/batch)
2016-04-30 12:16:35.875557: step 1513, loss = 19.85 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 12:16:40.951063: step 1514, loss = 19.83 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 12:16:46.966145: step 1515, loss = 19.85 (10.6 examples/sec; 6.015 sec/batch)
2016-04-30 12:16:52.340473: step 1516, loss = 19.86 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 12:16:57.540443: step 1517, loss = 19.86 (12.3 examples/sec; 5.200 sec/batch)
2016-04-30 12:17:03.033731: step 1518, loss = 20.00 (11.7 examples/sec; 5.493 sec/batch)
2016-04-30 12:17:07.932483: step 1519, loss = 19.86 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 12:17:13.166323: step 1520, loss = 19.72 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 12:17:25.949074: step 1521, loss = 19.84 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 12:17:31.177668: step 1522, loss = 19.80 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 12:17:36.537607: step 1523, loss = 19.82 (11.9 examples/sec; 5.360 sec/batch)
2016-04-30 12:17:41.842869: step 1524, loss = 19.64 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 12:17:47.096058: step 1525, loss = 19.71 (12.2 examples/sec; 5.253 sec/batch)
2016-04-30 12:17:52.967123: step 1526, loss = 19.78 (10.9 examples/sec; 5.871 sec/batch)
2016-04-30 12:17:58.515579: step 1527, loss = 19.86 (11.5 examples/sec; 5.548 sec/batch)
2016-04-30 12:18:04.387223: step 1528, loss = 19.70 (10.9 examples/sec; 5.872 sec/batch)
2016-04-30 12:18:09.975860: step 1529, loss = 19.63 (11.5 examples/sec; 5.589 sec/batch)
2016-04-30 12:18:16.130929: step 1530, loss = 19.66 (10.4 examples/sec; 6.155 sec/batch)
2016-04-30 12:18:29.333946: step 1531, loss = 19.82 (10.8 examples/sec; 5.924 sec/batch)
2016-04-30 12:18:34.886732: step 1532, loss = 19.66 (11.5 examples/sec; 5.550 sec/batch)
2016-04-30 12:18:40.247343: step 1533, loss = 19.57 (11.9 examples/sec; 5.361 sec/batch)
2016-04-30 12:18:45.870191: step 1534, loss = 19.66 (11.4 examples/sec; 5.623 sec/batch)
2016-04-30 12:18:51.323836: step 1535, loss = 19.42 (11.7 examples/sec; 5.454 sec/batch)
2016-04-30 12:18:56.554873: step 1536, loss = 19.49 (12.2 examples/sec; 5.231 sec/batch)
2016-04-30 12:19:02.704254: step 1537, loss = 19.50 (10.4 examples/sec; 6.149 sec/batch)
2016-04-30 12:19:07.754750: step 1538, loss = 19.40 (12.7 examples/sec; 5.050 sec/batch)
2016-04-30 12:19:12.990409: step 1539, loss = 19.47 (12.2 examples/sec; 5.236 sec/batch)
2016-04-30 12:19:18.402890: step 1540, loss = 19.53 (11.8 examples/sec; 5.412 sec/batch)
2016-04-30 12:19:31.138081: step 1541, loss = 19.49 (11.3 examples/sec; 5.647 sec/batch)
2016-04-30 12:19:36.563804: step 1542, loss = 19.49 (11.8 examples/sec; 5.426 sec/batch)
2016-04-30 12:19:41.621139: step 1543, loss = 19.55 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 12:19:46.746126: step 1544, loss = 19.47 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 12:19:52.113197: step 1545, loss = 19.45 (11.9 examples/sec; 5.367 sec/batch)
2016-04-30 12:19:57.333302: step 1546, loss = 19.39 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 12:20:02.400388: step 1547, loss = 19.36 (12.6 examples/sec; 5.067 sec/batch)
2016-04-30 12:20:08.075958: step 1548, loss = 19.38 (11.3 examples/sec; 5.675 sec/batch)
2016-04-30 12:20:13.538506: step 1549, loss = 19.37 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 12:20:18.847655: step 1550, loss = 19.40 (12.1 examples/sec; 5.309 sec/batch)
2016-04-30 12:20:30.714923: step 1551, loss = 19.41 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 12:20:36.137092: step 1552, loss = 19.27 (11.8 examples/sec; 5.422 sec/batch)
2016-04-30 12:20:41.670926: step 1553, loss = 19.39 (11.6 examples/sec; 5.534 sec/batch)
2016-04-30 12:20:46.684736: step 1554, loss = 19.43 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 12:20:52.071454: step 1555, loss = 19.31 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 12:20:57.431706: step 1556, loss = 19.18 (11.9 examples/sec; 5.360 sec/batch)
2016-04-30 12:21:02.600965: step 1557, loss = 19.27 (12.4 examples/sec; 5.169 sec/batch)
2016-04-30 12:21:07.653476: step 1558, loss = 19.31 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 12:21:13.593682: step 1559, loss = 19.18 (10.8 examples/sec; 5.940 sec/batch)
2016-04-30 12:21:18.989475: step 1560, loss = 19.11 (11.9 examples/sec; 5.396 sec/batch)
2016-04-30 12:21:30.796494: step 1561, loss = 19.27 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 12:21:36.009722: step 1562, loss = 19.08 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 12:21:41.115985: step 1563, loss = 19.11 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 12:21:47.136991: step 1564, loss = 19.12 (10.6 examples/sec; 6.021 sec/batch)
2016-04-30 12:21:52.451069: step 1565, loss = 19.19 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 12:21:57.687585: step 1566, loss = 19.27 (12.2 examples/sec; 5.236 sec/batch)
2016-04-30 12:22:03.024518: step 1567, loss = 18.98 (12.0 examples/sec; 5.337 sec/batch)
2016-04-30 12:22:07.879385: step 1568, loss = 19.03 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 12:22:13.328016: step 1569, loss = 19.18 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 12:22:19.368340: step 1570, loss = 19.10 (10.6 examples/sec; 6.040 sec/batch)
2016-04-30 12:22:31.424574: step 1571, loss = 18.98 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 12:22:36.667189: step 1572, loss = 19.20 (12.2 examples/sec; 5.243 sec/batch)
2016-04-30 12:22:41.996418: step 1573, loss = 18.98 (12.0 examples/sec; 5.329 sec/batch)
2016-04-30 12:22:47.054750: step 1574, loss = 18.94 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 12:22:52.806753: step 1575, loss = 19.09 (11.1 examples/sec; 5.752 sec/batch)
2016-04-30 12:22:58.528846: step 1576, loss = 19.08 (11.2 examples/sec; 5.722 sec/batch)
2016-04-30 12:23:04.977862: step 1577, loss = 18.88 (9.9 examples/sec; 6.449 sec/batch)
2016-04-30 12:23:10.427936: step 1578, loss = 18.88 (11.7 examples/sec; 5.450 sec/batch)
2016-04-30 12:23:15.919011: step 1579, loss = 18.97 (11.7 examples/sec; 5.491 sec/batch)
2016-04-30 12:23:21.804347: step 1580, loss = 18.86 (10.9 examples/sec; 5.885 sec/batch)
2016-04-30 12:23:34.207342: step 1581, loss = 18.79 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 12:23:39.555834: step 1582, loss = 19.03 (12.0 examples/sec; 5.348 sec/batch)
2016-04-30 12:23:45.041063: step 1583, loss = 18.92 (11.7 examples/sec; 5.485 sec/batch)
2016-04-30 12:23:50.091881: step 1584, loss = 19.24 (12.7 examples/sec; 5.051 sec/batch)
2016-04-30 12:23:56.069318: step 1585, loss = 18.80 (10.7 examples/sec; 5.977 sec/batch)
2016-04-30 12:24:01.638346: step 1586, loss = 18.86 (11.5 examples/sec; 5.569 sec/batch)
2016-04-30 12:24:07.180028: step 1587, loss = 18.89 (11.5 examples/sec; 5.542 sec/batch)
2016-04-30 12:24:13.236380: step 1588, loss = 18.88 (10.6 examples/sec; 6.056 sec/batch)
2016-04-30 12:24:18.653463: step 1589, loss = 18.77 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 12:24:24.173083: step 1590, loss = 18.83 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 12:24:36.854536: step 1591, loss = 18.73 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 12:24:42.031278: step 1592, loss = 18.86 (12.4 examples/sec; 5.177 sec/batch)
2016-04-30 12:24:46.945528: step 1593, loss = 18.91 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 12:24:52.110918: step 1594, loss = 18.84 (12.4 examples/sec; 5.165 sec/batch)
2016-04-30 12:24:57.286424: step 1595, loss = 18.69 (12.4 examples/sec; 5.175 sec/batch)
2016-04-30 12:25:03.154964: step 1596, loss = 18.56 (10.9 examples/sec; 5.868 sec/batch)
2016-04-30 12:25:08.124484: step 1597, loss = 18.68 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 12:25:13.468143: step 1598, loss = 18.63 (12.0 examples/sec; 5.344 sec/batch)
2016-04-30 12:25:18.781584: step 1599, loss = 18.77 (12.0 examples/sec; 5.313 sec/batch)
2016-04-30 12:25:24.377042: step 1600, loss = 18.60 (11.4 examples/sec; 5.595 sec/batch)
2016-04-30 12:25:36.852194: step 1601, loss = 18.69 (11.5 examples/sec; 5.570 sec/batch)
2016-04-30 12:25:42.034714: step 1602, loss = 18.58 (12.3 examples/sec; 5.182 sec/batch)
2016-04-30 12:25:47.100808: step 1603, loss = 18.58 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 12:25:52.296890: step 1604, loss = 18.48 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 12:25:57.568325: step 1605, loss = 18.41 (12.1 examples/sec; 5.271 sec/batch)
2016-04-30 12:26:03.076898: step 1606, loss = 18.51 (11.6 examples/sec; 5.508 sec/batch)
2016-04-30 12:26:08.939735: step 1607, loss = 18.55 (10.9 examples/sec; 5.863 sec/batch)
2016-04-30 12:26:13.985388: step 1608, loss = 18.49 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 12:26:19.185411: step 1609, loss = 18.47 (12.3 examples/sec; 5.200 sec/batch)
2016-04-30 12:26:24.588480: step 1610, loss = 18.52 (11.8 examples/sec; 5.403 sec/batch)
2016-04-30 12:26:36.601702: step 1611, loss = 18.19 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 12:26:42.447911: step 1612, loss = 18.50 (10.9 examples/sec; 5.846 sec/batch)
2016-04-30 12:26:47.491173: step 1613, loss = 18.51 (12.7 examples/sec; 5.043 sec/batch)
2016-04-30 12:26:52.504451: step 1614, loss = 18.46 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 12:26:57.601615: step 1615, loss = 18.43 (12.6 examples/sec; 5.097 sec/batch)
2016-04-30 12:27:02.911346: step 1616, loss = 18.36 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 12:27:07.797269: step 1617, loss = 18.45 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 12:27:13.494739: step 1618, loss = 18.45 (11.2 examples/sec; 5.697 sec/batch)
2016-04-30 12:27:18.812334: step 1619, loss = 18.21 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 12:27:24.199856: step 1620, loss = 18.47 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 12:27:37.071726: step 1621, loss = 18.35 (11.5 examples/sec; 5.556 sec/batch)
2016-04-30 12:27:43.144189: step 1622, loss = 18.35 (10.5 examples/sec; 6.072 sec/batch)
2016-04-30 12:27:49.190589: step 1623, loss = 18.29 (10.6 examples/sec; 6.046 sec/batch)
2016-04-30 12:27:54.544401: step 1624, loss = 18.14 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 12:27:59.965118: step 1625, loss = 18.46 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 12:28:05.229390: step 1626, loss = 18.31 (12.2 examples/sec; 5.264 sec/batch)
2016-04-30 12:28:10.562336: step 1627, loss = 18.30 (12.0 examples/sec; 5.333 sec/batch)
2016-04-30 12:28:15.896568: step 1628, loss = 18.22 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 12:28:21.866185: step 1629, loss = 18.16 (10.7 examples/sec; 5.970 sec/batch)
2016-04-30 12:28:27.296459: step 1630, loss = 18.17 (11.8 examples/sec; 5.430 sec/batch)
2016-04-30 12:28:39.669846: step 1631, loss = 18.23 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 12:28:44.872604: step 1632, loss = 18.34 (12.3 examples/sec; 5.203 sec/batch)
2016-04-30 12:28:50.092886: step 1633, loss = 18.08 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 12:28:55.842266: step 1634, loss = 18.19 (11.1 examples/sec; 5.749 sec/batch)
2016-04-30 12:29:01.452905: step 1635, loss = 18.02 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 12:29:06.915717: step 1636, loss = 18.00 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 12:29:12.321122: step 1637, loss = 17.93 (11.8 examples/sec; 5.405 sec/batch)
2016-04-30 12:29:17.390441: step 1638, loss = 18.16 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 12:29:22.525206: step 1639, loss = 18.09 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 12:29:28.287898: step 1640, loss = 18.11 (11.1 examples/sec; 5.763 sec/batch)
2016-04-30 12:29:40.196101: step 1641, loss = 18.25 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 12:29:45.645339: step 1642, loss = 18.10 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 12:29:50.997806: step 1643, loss = 17.94 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 12:29:56.152662: step 1644, loss = 17.91 (12.4 examples/sec; 5.155 sec/batch)
2016-04-30 12:30:02.233074: step 1645, loss = 18.14 (10.5 examples/sec; 6.080 sec/batch)
2016-04-30 12:30:07.485038: step 1646, loss = 18.05 (12.2 examples/sec; 5.252 sec/batch)
2016-04-30 12:30:12.659458: step 1647, loss = 18.01 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 12:30:17.861697: step 1648, loss = 17.98 (12.3 examples/sec; 5.202 sec/batch)
2016-04-30 12:30:22.730000: step 1649, loss = 17.94 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 12:30:28.106436: step 1650, loss = 17.90 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 12:30:40.824430: step 1651, loss = 18.13 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 12:30:46.144463: step 1652, loss = 17.78 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 12:30:51.460456: step 1653, loss = 17.91 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 12:30:56.640333: step 1654, loss = 17.91 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 12:31:01.989393: step 1655, loss = 17.93 (12.0 examples/sec; 5.349 sec/batch)
2016-04-30 12:31:07.773502: step 1656, loss = 17.84 (11.1 examples/sec; 5.784 sec/batch)
2016-04-30 12:31:12.931241: step 1657, loss = 17.80 (12.4 examples/sec; 5.158 sec/batch)
2016-04-30 12:31:18.041374: step 1658, loss = 17.94 (12.5 examples/sec; 5.110 sec/batch)
2016-04-30 12:31:23.016417: step 1659, loss = 17.86 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 12:31:28.165569: step 1660, loss = 17.82 (12.4 examples/sec; 5.149 sec/batch)
2016-04-30 12:31:40.851433: step 1661, loss = 17.88 (11.2 examples/sec; 5.695 sec/batch)
2016-04-30 12:31:46.302473: step 1662, loss = 17.66 (11.7 examples/sec; 5.451 sec/batch)
2016-04-30 12:31:51.578676: step 1663, loss = 17.76 (12.1 examples/sec; 5.276 sec/batch)
2016-04-30 12:31:56.721030: step 1664, loss = 17.86 (12.4 examples/sec; 5.142 sec/batch)
2016-04-30 12:32:02.105612: step 1665, loss = 17.64 (11.9 examples/sec; 5.384 sec/batch)
2016-04-30 12:32:07.480467: step 1666, loss = 17.77 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 12:32:13.211438: step 1667, loss = 17.77 (11.2 examples/sec; 5.731 sec/batch)
2016-04-30 12:32:18.451054: step 1668, loss = 17.75 (12.2 examples/sec; 5.240 sec/batch)
2016-04-30 12:32:23.666343: step 1669, loss = 17.73 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 12:32:28.813755: step 1670, loss = 17.75 (12.4 examples/sec; 5.147 sec/batch)
2016-04-30 12:32:40.829374: step 1671, loss = 17.47 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 12:32:46.780747: step 1672, loss = 17.66 (10.8 examples/sec; 5.951 sec/batch)
2016-04-30 12:32:52.296214: step 1673, loss = 17.64 (11.6 examples/sec; 5.515 sec/batch)
2016-04-30 12:32:57.502946: step 1674, loss = 17.51 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 12:33:02.997965: step 1675, loss = 17.74 (11.6 examples/sec; 5.495 sec/batch)
2016-04-30 12:33:08.015729: step 1676, loss = 17.74 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 12:33:13.377433: step 1677, loss = 17.65 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 12:33:19.478987: step 1678, loss = 17.62 (10.5 examples/sec; 6.101 sec/batch)
2016-04-30 12:33:25.270230: step 1679, loss = 17.55 (11.1 examples/sec; 5.791 sec/batch)
2016-04-30 12:33:31.423561: step 1680, loss = 17.46 (10.4 examples/sec; 6.153 sec/batch)
2016-04-30 12:33:43.703568: step 1681, loss = 17.52 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 12:33:49.738052: step 1682, loss = 17.49 (10.6 examples/sec; 6.034 sec/batch)
2016-04-30 12:33:55.083786: step 1683, loss = 17.62 (12.0 examples/sec; 5.346 sec/batch)
2016-04-30 12:34:00.519620: step 1684, loss = 17.56 (11.8 examples/sec; 5.436 sec/batch)
2016-04-30 12:34:05.607871: step 1685, loss = 17.55 (12.6 examples/sec; 5.088 sec/batch)
2016-04-30 12:34:10.971605: step 1686, loss = 17.47 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 12:34:16.323647: step 1687, loss = 17.29 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 12:34:22.162290: step 1688, loss = 17.44 (11.0 examples/sec; 5.839 sec/batch)
2016-04-30 12:34:27.286350: step 1689, loss = 17.39 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 12:34:32.218330: step 1690, loss = 17.37 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 12:34:44.747919: step 1691, loss = 17.32 (12.3 examples/sec; 5.202 sec/batch)
2016-04-30 12:34:49.726445: step 1692, loss = 17.53 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 12:34:55.404986: step 1693, loss = 17.42 (11.3 examples/sec; 5.678 sec/batch)
2016-04-30 12:35:00.755972: step 1694, loss = 17.14 (12.0 examples/sec; 5.351 sec/batch)
2016-04-30 12:35:05.647216: step 1695, loss = 17.24 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 12:35:10.880506: step 1696, loss = 17.45 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 12:35:16.037101: step 1697, loss = 17.36 (12.4 examples/sec; 5.156 sec/batch)
2016-04-30 12:35:21.180289: step 1698, loss = 17.29 (12.4 examples/sec; 5.143 sec/batch)
2016-04-30 12:35:26.158962: step 1699, loss = 17.20 (12.9 examples/sec; 4.979 sec/batch)
2016-04-30 12:35:31.973881: step 1700, loss = 17.44 (11.0 examples/sec; 5.815 sec/batch)
2016-04-30 12:35:43.959418: step 1701, loss = 17.36 (13.5 examples/sec; 4.757 sec/batch)
2016-04-30 12:35:49.170940: step 1702, loss = 17.23 (12.3 examples/sec; 5.211 sec/batch)
2016-04-30 12:35:54.381134: step 1703, loss = 17.24 (12.3 examples/sec; 5.210 sec/batch)
2016-04-30 12:35:59.356692: step 1704, loss = 17.33 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 12:36:05.403935: step 1705, loss = 17.14 (10.6 examples/sec; 6.047 sec/batch)
2016-04-30 12:36:10.651489: step 1706, loss = 17.36 (12.2 examples/sec; 5.247 sec/batch)
2016-04-30 12:36:15.827987: step 1707, loss = 17.15 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 12:36:21.041581: step 1708, loss = 17.22 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 12:36:26.034152: step 1709, loss = 16.94 (12.8 examples/sec; 4.992 sec/batch)
2016-04-30 12:36:31.233214: step 1710, loss = 17.14 (12.3 examples/sec; 5.199 sec/batch)
2016-04-30 12:36:44.047331: step 1711, loss = 17.29 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 12:36:49.349812: step 1712, loss = 17.15 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 12:36:54.760824: step 1713, loss = 17.19 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 12:36:59.792716: step 1714, loss = 17.13 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 12:37:04.936446: step 1715, loss = 17.12 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 12:37:10.811236: step 1716, loss = 16.97 (10.9 examples/sec; 5.875 sec/batch)
2016-04-30 12:37:16.049933: step 1717, loss = 16.96 (12.2 examples/sec; 5.239 sec/batch)
2016-04-30 12:37:21.327184: step 1718, loss = 16.91 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 12:37:26.229314: step 1719, loss = 17.19 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 12:37:31.459811: step 1720, loss = 17.24 (12.2 examples/sec; 5.230 sec/batch)
2016-04-30 12:37:44.066634: step 1721, loss = 17.17 (11.6 examples/sec; 5.517 sec/batch)
2016-04-30 12:37:49.335794: step 1722, loss = 17.12 (12.1 examples/sec; 5.269 sec/batch)
2016-04-30 12:37:54.567663: step 1723, loss = 17.04 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 12:37:59.838463: step 1724, loss = 16.91 (12.1 examples/sec; 5.271 sec/batch)
2016-04-30 12:38:05.098324: step 1725, loss = 16.89 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 12:38:10.379397: step 1726, loss = 16.73 (12.1 examples/sec; 5.281 sec/batch)
2016-04-30 12:38:16.092393: step 1727, loss = 16.92 (11.2 examples/sec; 5.713 sec/batch)
2016-04-30 12:38:21.313821: step 1728, loss = 16.91 (12.3 examples/sec; 5.221 sec/batch)
2016-04-30 12:38:26.298055: step 1729, loss = 16.72 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 12:38:31.863237: step 1730, loss = 17.25 (11.5 examples/sec; 5.565 sec/batch)
2016-04-30 12:38:45.433894: step 1731, loss = 16.92 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 12:38:50.939936: step 1732, loss = 16.92 (11.6 examples/sec; 5.506 sec/batch)
2016-04-30 12:38:55.966010: step 1733, loss = 17.10 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 12:39:01.610280: step 1734, loss = 16.80 (11.3 examples/sec; 5.644 sec/batch)
2016-04-30 12:39:06.967664: step 1735, loss = 16.98 (11.9 examples/sec; 5.357 sec/batch)
2016-04-30 12:39:12.200067: step 1736, loss = 17.00 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 12:39:17.299523: step 1737, loss = 16.91 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 12:39:23.092146: step 1738, loss = 16.82 (11.0 examples/sec; 5.793 sec/batch)
2016-04-30 12:39:28.423322: step 1739, loss = 16.96 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 12:39:33.824560: step 1740, loss = 16.99 (11.8 examples/sec; 5.401 sec/batch)
2016-04-30 12:39:45.816261: step 1741, loss = 16.87 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 12:39:50.974739: step 1742, loss = 16.82 (12.4 examples/sec; 5.158 sec/batch)
2016-04-30 12:39:56.592051: step 1743, loss = 16.98 (11.4 examples/sec; 5.617 sec/batch)
2016-04-30 12:40:01.957566: step 1744, loss = 16.79 (11.9 examples/sec; 5.365 sec/batch)
2016-04-30 12:40:07.154213: step 1745, loss = 16.84 (12.3 examples/sec; 5.197 sec/batch)
2016-04-30 12:40:12.736685: step 1746, loss = 16.63 (11.5 examples/sec; 5.582 sec/batch)
2016-04-30 12:40:17.835385: step 1747, loss = 16.82 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 12:40:22.893999: step 1748, loss = 16.81 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 12:40:28.831229: step 1749, loss = 16.61 (10.8 examples/sec; 5.937 sec/batch)
2016-04-30 12:40:34.076458: step 1750, loss = 16.69 (12.2 examples/sec; 5.245 sec/batch)
2016-04-30 12:40:45.936170: step 1751, loss = 16.63 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 12:40:51.074733: step 1752, loss = 16.75 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 12:40:56.070700: step 1753, loss = 16.74 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 12:41:02.006833: step 1754, loss = 16.55 (10.8 examples/sec; 5.936 sec/batch)
2016-04-30 12:41:07.344612: step 1755, loss = 16.64 (12.0 examples/sec; 5.338 sec/batch)
2016-04-30 12:41:12.580886: step 1756, loss = 16.69 (12.2 examples/sec; 5.236 sec/batch)
2016-04-30 12:41:17.485747: step 1757, loss = 16.63 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 12:41:22.658532: step 1758, loss = 16.50 (12.4 examples/sec; 5.173 sec/batch)
2016-04-30 12:41:27.707010: step 1759, loss = 16.62 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 12:41:33.436182: step 1760, loss = 16.48 (11.2 examples/sec; 5.729 sec/batch)
2016-04-30 12:41:45.451081: step 1761, loss = 16.63 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 12:41:50.425385: step 1762, loss = 16.52 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 12:41:55.652738: step 1763, loss = 16.43 (12.2 examples/sec; 5.227 sec/batch)
2016-04-30 12:42:01.005472: step 1764, loss = 16.31 (12.0 examples/sec; 5.353 sec/batch)
2016-04-30 12:42:06.888963: step 1765, loss = 16.47 (10.9 examples/sec; 5.883 sec/batch)
2016-04-30 12:42:11.718558: step 1766, loss = 16.55 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 12:42:16.796521: step 1767, loss = 16.55 (12.6 examples/sec; 5.078 sec/batch)
2016-04-30 12:42:21.933037: step 1768, loss = 16.52 (12.5 examples/sec; 5.136 sec/batch)
2016-04-30 12:42:26.911177: step 1769, loss = 16.25 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 12:42:32.019455: step 1770, loss = 16.37 (12.5 examples/sec; 5.108 sec/batch)
2016-04-30 12:42:44.455924: step 1771, loss = 16.45 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 12:42:49.677244: step 1772, loss = 16.36 (12.3 examples/sec; 5.221 sec/batch)
2016-04-30 12:42:55.001018: step 1773, loss = 16.42 (12.0 examples/sec; 5.324 sec/batch)
2016-04-30 12:43:00.354361: step 1774, loss = 16.34 (12.0 examples/sec; 5.353 sec/batch)
2016-04-30 12:43:05.633092: step 1775, loss = 16.49 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 12:43:11.447376: step 1776, loss = 16.40 (11.0 examples/sec; 5.814 sec/batch)
2016-04-30 12:43:16.653785: step 1777, loss = 16.39 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 12:43:21.895680: step 1778, loss = 16.42 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 12:43:27.078309: step 1779, loss = 16.29 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 12:43:32.078112: step 1780, loss = 16.22 (12.8 examples/sec; 5.000 sec/batch)
2016-04-30 12:43:44.651354: step 1781, loss = 16.46 (11.8 examples/sec; 5.406 sec/batch)
2016-04-30 12:43:49.876452: step 1782, loss = 16.33 (12.3 examples/sec; 5.223 sec/batch)
2016-04-30 12:43:55.069396: step 1783, loss = 16.43 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 12:44:00.366529: step 1784, loss = 16.31 (12.1 examples/sec; 5.297 sec/batch)
2016-04-30 12:44:05.348358: step 1785, loss = 16.27 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 12:44:10.770121: step 1786, loss = 16.25 (11.8 examples/sec; 5.422 sec/batch)
2016-04-30 12:44:16.590554: step 1787, loss = 16.25 (11.0 examples/sec; 5.820 sec/batch)
2016-04-30 12:44:21.932424: step 1788, loss = 16.19 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 12:44:27.050314: step 1789, loss = 16.20 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 12:44:32.010401: step 1790, loss = 16.32 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 12:44:45.733086: step 1791, loss = 16.14 (11.1 examples/sec; 5.757 sec/batch)
2016-04-30 12:44:51.850551: step 1792, loss = 16.01 (10.5 examples/sec; 6.117 sec/batch)
2016-04-30 12:44:57.267186: step 1793, loss = 16.08 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 12:45:02.465156: step 1794, loss = 16.31 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 12:45:07.681780: step 1795, loss = 16.16 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 12:45:12.970705: step 1796, loss = 16.15 (12.1 examples/sec; 5.289 sec/batch)
2016-04-30 12:45:18.342736: step 1797, loss = 16.00 (11.9 examples/sec; 5.372 sec/batch)
2016-04-30 12:45:24.244245: step 1798, loss = 16.06 (10.8 examples/sec; 5.901 sec/batch)
2016-04-30 12:45:29.254342: step 1799, loss = 16.10 (12.8 examples/sec; 5.010 sec/batch)
2016-04-30 12:45:34.645657: step 1800, loss = 16.06 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 12:45:46.615442: step 1801, loss = 16.16 (12.5 examples/sec; 5.141 sec/batch)
2016-04-30 12:45:51.896298: step 1802, loss = 15.97 (12.1 examples/sec; 5.281 sec/batch)
2016-04-30 12:45:57.688386: step 1803, loss = 15.96 (11.0 examples/sec; 5.792 sec/batch)
2016-04-30 12:46:02.872457: step 1804, loss = 16.02 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 12:46:08.217944: step 1805, loss = 16.20 (12.0 examples/sec; 5.345 sec/batch)
2016-04-30 12:46:13.957591: step 1806, loss = 15.78 (11.2 examples/sec; 5.740 sec/batch)
2016-04-30 12:46:19.549271: step 1807, loss = 15.88 (11.4 examples/sec; 5.592 sec/batch)
2016-04-30 12:46:24.829481: step 1808, loss = 15.90 (12.1 examples/sec; 5.280 sec/batch)
2016-04-30 12:46:30.625225: step 1809, loss = 16.10 (11.0 examples/sec; 5.796 sec/batch)
2016-04-30 12:46:35.649397: step 1810, loss = 16.12 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 12:46:47.648415: step 1811, loss = 15.86 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 12:46:52.769009: step 1812, loss = 15.80 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 12:46:57.912440: step 1813, loss = 15.93 (12.4 examples/sec; 5.143 sec/batch)
2016-04-30 12:47:03.903959: step 1814, loss = 15.99 (10.7 examples/sec; 5.991 sec/batch)
2016-04-30 12:47:08.994938: step 1815, loss = 16.02 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 12:47:14.170743: step 1816, loss = 15.82 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 12:47:19.324020: step 1817, loss = 15.86 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 12:47:24.529196: step 1818, loss = 16.03 (12.3 examples/sec; 5.205 sec/batch)
2016-04-30 12:47:29.766126: step 1819, loss = 15.90 (12.2 examples/sec; 5.237 sec/batch)
2016-04-30 12:47:35.551347: step 1820, loss = 15.80 (11.1 examples/sec; 5.785 sec/batch)
2016-04-30 12:47:47.468132: step 1821, loss = 15.97 (13.5 examples/sec; 4.726 sec/batch)
2016-04-30 12:47:52.482065: step 1822, loss = 15.81 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 12:47:57.633188: step 1823, loss = 15.74 (12.4 examples/sec; 5.151 sec/batch)
2016-04-30 12:48:02.766259: step 1824, loss = 15.54 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 12:48:08.580477: step 1825, loss = 15.76 (11.0 examples/sec; 5.814 sec/batch)
2016-04-30 12:48:13.758490: step 1826, loss = 15.70 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 12:48:18.998242: step 1827, loss = 15.67 (12.2 examples/sec; 5.240 sec/batch)
2016-04-30 12:48:24.019580: step 1828, loss = 15.97 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 12:48:29.103390: step 1829, loss = 15.66 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 12:48:34.432462: step 1830, loss = 15.83 (12.0 examples/sec; 5.329 sec/batch)
2016-04-30 12:48:48.909624: step 1831, loss = 15.56 (10.4 examples/sec; 6.176 sec/batch)
2016-04-30 12:48:54.018564: step 1832, loss = 15.96 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 12:48:59.429846: step 1833, loss = 15.65 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 12:49:04.887111: step 1834, loss = 15.60 (11.7 examples/sec; 5.457 sec/batch)
2016-04-30 12:49:10.730696: step 1835, loss = 15.53 (11.0 examples/sec; 5.844 sec/batch)
2016-04-30 12:49:16.782176: step 1836, loss = 15.71 (10.6 examples/sec; 6.051 sec/batch)
2016-04-30 12:49:22.320419: step 1837, loss = 15.69 (11.6 examples/sec; 5.538 sec/batch)
2016-04-30 12:49:27.647096: step 1838, loss = 15.66 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 12:49:32.782166: step 1839, loss = 15.62 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 12:49:37.959054: step 1840, loss = 15.59 (12.4 examples/sec; 5.177 sec/batch)
2016-04-30 12:49:50.631888: step 1841, loss = 15.61 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 12:49:56.157964: step 1842, loss = 15.71 (11.6 examples/sec; 5.526 sec/batch)
2016-04-30 12:50:01.649878: step 1843, loss = 15.47 (11.7 examples/sec; 5.492 sec/batch)
2016-04-30 12:50:06.951976: step 1844, loss = 15.59 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 12:50:12.338968: step 1845, loss = 15.45 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 12:50:17.358896: step 1846, loss = 15.52 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 12:50:23.357431: step 1847, loss = 15.56 (10.7 examples/sec; 5.998 sec/batch)
2016-04-30 12:50:28.703383: step 1848, loss = 15.42 (12.0 examples/sec; 5.346 sec/batch)
2016-04-30 12:50:33.980451: step 1849, loss = 15.58 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 12:50:39.204498: step 1850, loss = 15.63 (12.3 examples/sec; 5.224 sec/batch)
2016-04-30 12:50:51.200065: step 1851, loss = 15.57 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 12:50:56.860520: step 1852, loss = 15.50 (11.3 examples/sec; 5.660 sec/batch)
2016-04-30 12:51:02.356708: step 1853, loss = 15.35 (11.6 examples/sec; 5.496 sec/batch)
2016-04-30 12:51:07.675256: step 1854, loss = 15.53 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 12:51:12.850645: step 1855, loss = 15.49 (12.4 examples/sec; 5.175 sec/batch)
2016-04-30 12:51:18.040071: step 1856, loss = 15.59 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 12:51:23.184929: step 1857, loss = 15.35 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 12:51:29.126257: step 1858, loss = 15.44 (10.8 examples/sec; 5.941 sec/batch)
2016-04-30 12:51:34.271018: step 1859, loss = 15.33 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 12:51:39.449455: step 1860, loss = 15.38 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 12:51:51.698562: step 1861, loss = 15.45 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 12:51:56.681437: step 1862, loss = 15.38 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 12:52:02.780512: step 1863, loss = 15.34 (10.5 examples/sec; 6.099 sec/batch)
2016-04-30 12:52:07.847096: step 1864, loss = 15.23 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 12:52:13.007772: step 1865, loss = 15.31 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 12:52:18.160068: step 1866, loss = 15.26 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 12:52:23.103393: step 1867, loss = 15.42 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 12:52:28.345543: step 1868, loss = 15.29 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 12:52:34.257784: step 1869, loss = 15.08 (10.8 examples/sec; 5.912 sec/batch)
2016-04-30 12:52:39.399204: step 1870, loss = 15.20 (12.4 examples/sec; 5.141 sec/batch)
2016-04-30 12:52:51.351614: step 1871, loss = 15.07 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 12:52:56.365147: step 1872, loss = 15.25 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 12:53:01.737487: step 1873, loss = 15.06 (11.9 examples/sec; 5.372 sec/batch)
2016-04-30 12:53:07.386457: step 1874, loss = 15.34 (11.3 examples/sec; 5.649 sec/batch)
2016-04-30 12:53:12.640523: step 1875, loss = 15.21 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 12:53:17.687119: step 1876, loss = 15.26 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 12:53:22.885652: step 1877, loss = 15.06 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 12:53:28.036461: step 1878, loss = 15.17 (12.4 examples/sec; 5.151 sec/batch)
2016-04-30 12:53:33.037194: step 1879, loss = 15.27 (12.8 examples/sec; 5.001 sec/batch)
2016-04-30 12:53:38.709636: step 1880, loss = 15.01 (11.3 examples/sec; 5.672 sec/batch)
2016-04-30 12:53:50.712464: step 1881, loss = 15.28 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 12:53:55.953475: step 1882, loss = 14.95 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 12:54:01.425194: step 1883, loss = 15.10 (11.7 examples/sec; 5.472 sec/batch)
2016-04-30 12:54:07.260933: step 1884, loss = 14.99 (11.0 examples/sec; 5.836 sec/batch)
2016-04-30 12:54:13.660676: step 1885, loss = 15.01 (10.0 examples/sec; 6.400 sec/batch)
2016-04-30 12:54:19.934411: step 1886, loss = 15.06 (10.2 examples/sec; 6.274 sec/batch)
2016-04-30 12:54:25.388462: step 1887, loss = 15.25 (11.7 examples/sec; 5.454 sec/batch)
2016-04-30 12:54:30.673948: step 1888, loss = 15.15 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 12:54:35.707451: step 1889, loss = 15.21 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 12:54:40.961718: step 1890, loss = 15.07 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 12:54:53.685289: step 1891, loss = 15.12 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 12:54:59.286957: step 1892, loss = 14.90 (11.4 examples/sec; 5.602 sec/batch)
2016-04-30 12:55:04.679572: step 1893, loss = 14.90 (11.9 examples/sec; 5.393 sec/batch)
2016-04-30 12:55:10.011147: step 1894, loss = 15.05 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 12:55:15.033863: step 1895, loss = 15.05 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 12:55:21.024515: step 1896, loss = 15.03 (10.7 examples/sec; 5.991 sec/batch)
2016-04-30 12:55:26.198912: step 1897, loss = 15.08 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 12:55:31.484666: step 1898, loss = 15.05 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 12:55:36.914214: step 1899, loss = 14.98 (11.8 examples/sec; 5.429 sec/batch)
2016-04-30 12:55:42.181915: step 1900, loss = 14.84 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 12:55:54.952731: step 1901, loss = 14.90 (12.4 examples/sec; 5.172 sec/batch)
2016-04-30 12:56:00.082470: step 1902, loss = 15.03 (12.5 examples/sec; 5.130 sec/batch)
2016-04-30 12:56:05.316436: step 1903, loss = 14.79 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 12:56:10.618493: step 1904, loss = 14.87 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 12:56:15.987810: step 1905, loss = 14.95 (11.9 examples/sec; 5.369 sec/batch)
2016-04-30 12:56:21.236642: step 1906, loss = 14.89 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 12:56:26.977811: step 1907, loss = 14.94 (11.1 examples/sec; 5.741 sec/batch)
2016-04-30 12:56:32.153559: step 1908, loss = 14.88 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 12:56:37.264242: step 1909, loss = 14.95 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 12:56:42.258824: step 1910, loss = 14.91 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 12:56:54.021831: step 1911, loss = 14.69 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 12:56:59.890102: step 1912, loss = 14.69 (10.9 examples/sec; 5.868 sec/batch)
2016-04-30 12:57:05.053709: step 1913, loss = 14.83 (12.4 examples/sec; 5.164 sec/batch)
2016-04-30 12:57:10.538801: step 1914, loss = 14.83 (11.7 examples/sec; 5.485 sec/batch)
2016-04-30 12:57:15.855363: step 1915, loss = 14.75 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 12:57:20.799044: step 1916, loss = 14.85 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 12:57:26.018077: step 1917, loss = 14.78 (12.3 examples/sec; 5.219 sec/batch)
2016-04-30 12:57:31.960658: step 1918, loss = 14.84 (10.8 examples/sec; 5.942 sec/batch)
2016-04-30 12:57:37.186693: step 1919, loss = 14.66 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 12:57:42.343764: step 1920, loss = 14.72 (12.4 examples/sec; 5.157 sec/batch)
2016-04-30 12:57:54.117581: step 1921, loss = 14.98 (13.3 examples/sec; 4.820 sec/batch)
2016-04-30 12:57:59.444600: step 1922, loss = 14.71 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 12:58:05.287161: step 1923, loss = 14.66 (11.0 examples/sec; 5.842 sec/batch)
2016-04-30 12:58:10.606826: step 1924, loss = 14.74 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 12:58:15.861687: step 1925, loss = 14.78 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 12:58:20.827072: step 1926, loss = 14.61 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 12:58:26.317866: step 1927, loss = 14.60 (11.7 examples/sec; 5.491 sec/batch)
2016-04-30 12:58:31.778361: step 1928, loss = 14.46 (11.7 examples/sec; 5.460 sec/batch)
2016-04-30 12:58:37.847555: step 1929, loss = 14.54 (10.5 examples/sec; 6.069 sec/batch)
2016-04-30 12:58:43.404293: step 1930, loss = 14.51 (11.5 examples/sec; 5.557 sec/batch)
2016-04-30 12:58:55.797605: step 1931, loss = 14.65 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 12:59:01.300474: step 1932, loss = 14.56 (11.6 examples/sec; 5.503 sec/batch)
2016-04-30 12:59:06.828482: step 1933, loss = 14.53 (11.6 examples/sec; 5.528 sec/batch)
2016-04-30 12:59:12.723297: step 1934, loss = 14.63 (10.9 examples/sec; 5.895 sec/batch)
2016-04-30 12:59:18.248817: step 1935, loss = 14.57 (11.6 examples/sec; 5.525 sec/batch)
2016-04-30 12:59:24.328248: step 1936, loss = 14.44 (10.5 examples/sec; 6.079 sec/batch)
2016-04-30 12:59:29.557218: step 1937, loss = 14.61 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 12:59:35.093579: step 1938, loss = 14.37 (11.6 examples/sec; 5.536 sec/batch)
2016-04-30 12:59:41.068028: step 1939, loss = 14.55 (10.7 examples/sec; 5.974 sec/batch)
2016-04-30 12:59:46.528469: step 1940, loss = 14.68 (11.7 examples/sec; 5.460 sec/batch)
2016-04-30 12:59:58.867820: step 1941, loss = 14.60 (12.4 examples/sec; 5.164 sec/batch)
2016-04-30 13:00:04.399452: step 1942, loss = 14.60 (11.6 examples/sec; 5.532 sec/batch)
2016-04-30 13:00:09.682741: step 1943, loss = 14.52 (12.1 examples/sec; 5.283 sec/batch)
2016-04-30 13:00:15.467077: step 1944, loss = 14.33 (11.1 examples/sec; 5.784 sec/batch)
2016-04-30 13:00:20.542632: step 1945, loss = 14.46 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 13:00:25.808304: step 1946, loss = 14.44 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 13:00:30.953851: step 1947, loss = 14.34 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 13:00:36.140548: step 1948, loss = 14.43 (12.3 examples/sec; 5.187 sec/batch)
2016-04-30 13:00:41.446660: step 1949, loss = 14.48 (12.1 examples/sec; 5.306 sec/batch)
2016-04-30 13:00:47.441324: step 1950, loss = 14.43 (10.7 examples/sec; 5.995 sec/batch)
2016-04-30 13:00:59.595481: step 1951, loss = 14.37 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 13:01:05.126772: step 1952, loss = 14.44 (11.6 examples/sec; 5.531 sec/batch)
2016-04-30 13:01:10.237493: step 1953, loss = 14.29 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 13:01:15.188780: step 1954, loss = 14.39 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 13:01:20.702786: step 1955, loss = 14.36 (11.6 examples/sec; 5.514 sec/batch)
2016-04-30 13:01:25.850768: step 1956, loss = 14.38 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 13:01:31.072487: step 1957, loss = 14.19 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 13:01:36.181409: step 1958, loss = 14.13 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 13:01:41.320408: step 1959, loss = 14.36 (12.5 examples/sec; 5.139 sec/batch)
2016-04-30 13:01:46.423424: step 1960, loss = 14.29 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 13:01:59.071517: step 1961, loss = 14.27 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 13:02:04.886503: step 1962, loss = 14.36 (11.0 examples/sec; 5.815 sec/batch)
2016-04-30 13:02:10.227872: step 1963, loss = 14.29 (12.0 examples/sec; 5.341 sec/batch)
2016-04-30 13:02:15.615308: step 1964, loss = 14.30 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 13:02:20.602095: step 1965, loss = 14.12 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 13:02:26.511082: step 1966, loss = 14.34 (10.8 examples/sec; 5.909 sec/batch)
2016-04-30 13:02:31.697557: step 1967, loss = 14.17 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 13:02:37.018550: step 1968, loss = 14.10 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 13:02:42.015506: step 1969, loss = 14.15 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 13:02:47.200082: step 1970, loss = 14.09 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 13:02:59.711696: step 1971, loss = 14.03 (12.0 examples/sec; 5.328 sec/batch)
2016-04-30 13:03:05.011272: step 1972, loss = 14.16 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 13:03:10.227538: step 1973, loss = 14.04 (12.3 examples/sec; 5.216 sec/batch)
2016-04-30 13:03:15.700446: step 1974, loss = 14.23 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 13:03:20.577450: step 1975, loss = 14.19 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 13:03:25.908452: step 1976, loss = 14.23 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 13:03:31.640041: step 1977, loss = 14.03 (11.2 examples/sec; 5.731 sec/batch)
2016-04-30 13:03:36.954682: step 1978, loss = 14.02 (12.0 examples/sec; 5.315 sec/batch)
2016-04-30 13:03:41.843028: step 1979, loss = 14.29 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 13:03:47.137610: step 1980, loss = 14.04 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 13:03:59.200648: step 1981, loss = 14.07 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 13:04:05.018787: step 1982, loss = 14.17 (11.0 examples/sec; 5.818 sec/batch)
2016-04-30 13:04:10.346307: step 1983, loss = 14.00 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 13:04:15.763732: step 1984, loss = 14.07 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 13:04:20.701970: step 1985, loss = 14.04 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 13:04:25.887418: step 1986, loss = 13.98 (12.3 examples/sec; 5.185 sec/batch)
2016-04-30 13:04:31.164053: step 1987, loss = 14.00 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 13:04:36.054500: step 1988, loss = 14.18 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 13:04:41.955248: step 1989, loss = 13.94 (10.8 examples/sec; 5.901 sec/batch)
2016-04-30 13:04:47.235001: step 1990, loss = 14.04 (12.1 examples/sec; 5.280 sec/batch)
2016-04-30 13:04:59.082569: step 1991, loss = 14.15 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 13:05:04.393045: step 1992, loss = 14.07 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 13:05:09.553821: step 1993, loss = 13.80 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 13:05:15.289236: step 1994, loss = 13.91 (11.2 examples/sec; 5.735 sec/batch)
2016-04-30 13:05:21.015070: step 1995, loss = 14.17 (11.2 examples/sec; 5.726 sec/batch)
2016-04-30 13:05:27.282764: step 1996, loss = 13.82 (10.2 examples/sec; 6.268 sec/batch)
2016-04-30 13:05:32.590061: step 1997, loss = 13.94 (12.1 examples/sec; 5.307 sec/batch)
2016-04-30 13:05:38.064714: step 1998, loss = 14.03 (11.7 examples/sec; 5.475 sec/batch)
2016-04-30 13:05:44.134037: step 1999, loss = 13.78 (10.5 examples/sec; 6.069 sec/batch)
2016-04-30 13:05:49.333560: step 2000, loss = 13.85 (12.3 examples/sec; 5.199 sec/batch)
2016-04-30 13:06:01.574243: step 2001, loss = 13.93 (12.1 examples/sec; 5.274 sec/batch)
2016-04-30 13:06:06.982822: step 2002, loss = 14.10 (11.8 examples/sec; 5.408 sec/batch)
2016-04-30 13:06:12.023426: step 2003, loss = 13.86 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 13:06:17.971921: step 2004, loss = 13.95 (10.8 examples/sec; 5.948 sec/batch)
2016-04-30 13:06:23.054168: step 2005, loss = 13.91 (12.6 examples/sec; 5.082 sec/batch)
2016-04-30 13:06:28.280250: step 2006, loss = 13.81 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 13:06:33.682345: step 2007, loss = 13.90 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 13:06:38.905360: step 2008, loss = 13.80 (12.3 examples/sec; 5.223 sec/batch)
2016-04-30 13:06:44.218792: step 2009, loss = 13.63 (12.0 examples/sec; 5.313 sec/batch)
2016-04-30 13:06:50.272783: step 2010, loss = 13.66 (10.6 examples/sec; 6.054 sec/batch)
2016-04-30 13:07:02.398330: step 2011, loss = 13.84 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 13:07:07.531966: step 2012, loss = 13.86 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 13:07:12.749037: step 2013, loss = 13.77 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 13:07:17.804235: step 2014, loss = 13.70 (12.7 examples/sec; 5.055 sec/batch)
2016-04-30 13:07:23.607086: step 2015, loss = 13.61 (11.0 examples/sec; 5.803 sec/batch)
2016-04-30 13:07:28.998114: step 2016, loss = 13.89 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 13:07:34.399180: step 2017, loss = 13.84 (11.8 examples/sec; 5.401 sec/batch)
2016-04-30 13:07:39.597409: step 2018, loss = 13.66 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 13:07:44.607063: step 2019, loss = 13.87 (12.8 examples/sec; 5.010 sec/batch)
2016-04-30 13:07:49.850575: step 2020, loss = 13.67 (12.2 examples/sec; 5.243 sec/batch)
2016-04-30 13:08:02.510938: step 2021, loss = 13.75 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 13:08:07.755126: step 2022, loss = 13.87 (12.2 examples/sec; 5.244 sec/batch)
2016-04-30 13:08:13.101953: step 2023, loss = 13.72 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 13:08:18.202275: step 2024, loss = 13.61 (12.5 examples/sec; 5.100 sec/batch)
2016-04-30 13:08:23.459700: step 2025, loss = 13.74 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 13:08:29.453060: step 2026, loss = 13.59 (10.7 examples/sec; 5.993 sec/batch)
2016-04-30 13:08:34.757833: step 2027, loss = 13.71 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 13:08:40.019583: step 2028, loss = 13.71 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 13:08:45.055788: step 2029, loss = 13.71 (12.7 examples/sec; 5.036 sec/batch)
2016-04-30 13:08:50.335521: step 2030, loss = 13.55 (12.1 examples/sec; 5.280 sec/batch)
2016-04-30 13:09:03.178960: step 2031, loss = 13.57 (11.1 examples/sec; 5.779 sec/batch)
2016-04-30 13:09:08.226881: step 2032, loss = 13.52 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 13:09:13.416463: step 2033, loss = 13.51 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 13:09:18.712441: step 2034, loss = 13.42 (12.1 examples/sec; 5.296 sec/batch)
2016-04-30 13:09:23.587890: step 2035, loss = 13.50 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 13:09:29.173326: step 2036, loss = 13.59 (11.5 examples/sec; 5.585 sec/batch)
2016-04-30 13:09:35.036719: step 2037, loss = 13.44 (10.9 examples/sec; 5.863 sec/batch)
2016-04-30 13:09:40.316024: step 2038, loss = 13.65 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 13:09:45.450660: step 2039, loss = 13.54 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 13:09:50.604443: step 2040, loss = 13.63 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 13:10:02.755739: step 2041, loss = 13.45 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 13:10:08.550725: step 2042, loss = 13.64 (11.0 examples/sec; 5.795 sec/batch)
2016-04-30 13:10:13.883056: step 2043, loss = 13.64 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 13:10:19.168462: step 2044, loss = 13.50 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 13:10:24.181819: step 2045, loss = 13.30 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 13:10:29.543462: step 2046, loss = 13.54 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 13:10:35.352336: step 2047, loss = 13.41 (11.0 examples/sec; 5.809 sec/batch)
2016-04-30 13:10:42.143116: step 2048, loss = 13.34 (9.4 examples/sec; 6.791 sec/batch)
2016-04-30 13:10:47.466289: step 2049, loss = 13.44 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 13:10:52.748302: step 2050, loss = 13.57 (12.1 examples/sec; 5.282 sec/batch)
2016-04-30 13:11:05.088953: step 2051, loss = 13.44 (12.1 examples/sec; 5.300 sec/batch)
2016-04-30 13:11:10.646070: step 2052, loss = 13.48 (11.5 examples/sec; 5.557 sec/batch)
2016-04-30 13:11:16.555485: step 2053, loss = 13.45 (10.8 examples/sec; 5.909 sec/batch)
2016-04-30 13:11:21.873717: step 2054, loss = 13.40 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 13:11:26.939950: step 2055, loss = 13.27 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 13:11:32.298080: step 2056, loss = 13.59 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 13:11:37.620114: step 2057, loss = 13.43 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 13:11:43.163775: step 2058, loss = 13.26 (11.5 examples/sec; 5.544 sec/batch)
2016-04-30 13:11:49.036496: step 2059, loss = 13.42 (10.9 examples/sec; 5.873 sec/batch)
2016-04-30 13:11:54.135184: step 2060, loss = 13.25 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 13:12:06.612134: step 2061, loss = 13.33 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 13:12:11.845447: step 2062, loss = 13.26 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 13:12:17.197037: step 2063, loss = 13.38 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 13:12:23.078565: step 2064, loss = 13.22 (10.9 examples/sec; 5.881 sec/batch)
2016-04-30 13:12:28.389944: step 2065, loss = 13.13 (12.0 examples/sec; 5.311 sec/batch)
2016-04-30 13:12:33.678325: step 2066, loss = 13.44 (12.1 examples/sec; 5.288 sec/batch)
2016-04-30 13:12:38.944409: step 2067, loss = 13.17 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 13:12:44.174779: step 2068, loss = 13.26 (12.2 examples/sec; 5.230 sec/batch)
2016-04-30 13:12:49.435305: step 2069, loss = 13.25 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 13:12:55.287454: step 2070, loss = 13.24 (10.9 examples/sec; 5.852 sec/batch)
2016-04-30 13:13:07.527948: step 2071, loss = 13.24 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 13:13:12.782489: step 2072, loss = 13.33 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 13:13:17.733166: step 2073, loss = 13.28 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 13:13:22.848890: step 2074, loss = 13.14 (12.5 examples/sec; 5.116 sec/batch)
2016-04-30 13:13:28.924315: step 2075, loss = 13.31 (10.5 examples/sec; 6.075 sec/batch)
2016-04-30 13:13:34.133207: step 2076, loss = 13.31 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 13:13:39.119207: step 2077, loss = 13.20 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 13:13:44.401439: step 2078, loss = 13.22 (12.1 examples/sec; 5.282 sec/batch)
2016-04-30 13:13:49.706680: step 2079, loss = 13.33 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 13:13:54.881491: step 2080, loss = 13.36 (12.4 examples/sec; 5.175 sec/batch)
2016-04-30 13:14:07.902471: step 2081, loss = 13.12 (12.2 examples/sec; 5.248 sec/batch)
2016-04-30 13:14:13.275795: step 2082, loss = 13.13 (11.9 examples/sec; 5.373 sec/batch)
2016-04-30 13:14:18.338761: step 2083, loss = 13.12 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 13:14:23.352488: step 2084, loss = 13.08 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 13:14:28.617385: step 2085, loss = 12.92 (12.2 examples/sec; 5.265 sec/batch)
2016-04-30 13:14:34.493427: step 2086, loss = 13.15 (10.9 examples/sec; 5.876 sec/batch)
2016-04-30 13:14:39.743968: step 2087, loss = 13.24 (12.2 examples/sec; 5.250 sec/batch)
2016-04-30 13:14:44.690053: step 2088, loss = 13.01 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 13:14:49.989279: step 2089, loss = 12.96 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 13:14:55.284675: step 2090, loss = 13.01 (12.1 examples/sec; 5.295 sec/batch)
2016-04-30 13:15:08.375567: step 2091, loss = 13.01 (10.9 examples/sec; 5.853 sec/batch)
2016-04-30 13:15:13.706442: step 2092, loss = 12.81 (12.0 examples/sec; 5.330 sec/batch)
2016-04-30 13:15:19.094030: step 2093, loss = 13.07 (11.9 examples/sec; 5.387 sec/batch)
2016-04-30 13:15:24.329080: step 2094, loss = 12.85 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 13:15:29.530863: step 2095, loss = 12.99 (12.3 examples/sec; 5.202 sec/batch)
2016-04-30 13:15:35.073058: step 2096, loss = 12.76 (11.5 examples/sec; 5.542 sec/batch)
2016-04-30 13:15:40.956099: step 2097, loss = 12.93 (10.9 examples/sec; 5.883 sec/batch)
2016-04-30 13:15:46.365982: step 2098, loss = 13.03 (11.8 examples/sec; 5.410 sec/batch)
2016-04-30 13:15:51.522273: step 2099, loss = 12.98 (12.4 examples/sec; 5.156 sec/batch)
2016-04-30 13:15:56.798241: step 2100, loss = 13.03 (12.1 examples/sec; 5.276 sec/batch)
2016-04-30 13:16:09.111072: step 2101, loss = 12.79 (13.2 examples/sec; 4.841 sec/batch)
2016-04-30 13:16:15.058837: step 2102, loss = 12.81 (10.8 examples/sec; 5.948 sec/batch)
2016-04-30 13:16:20.449657: step 2103, loss = 12.84 (11.9 examples/sec; 5.391 sec/batch)
2016-04-30 13:16:25.712251: step 2104, loss = 12.84 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 13:16:31.555658: step 2105, loss = 12.88 (11.0 examples/sec; 5.843 sec/batch)
2016-04-30 13:16:37.678592: step 2106, loss = 13.05 (10.5 examples/sec; 6.123 sec/batch)
2016-04-30 13:16:43.974991: step 2107, loss = 12.99 (10.2 examples/sec; 6.296 sec/batch)
2016-04-30 13:16:49.603627: step 2108, loss = 13.06 (11.4 examples/sec; 5.629 sec/batch)
2016-04-30 13:16:55.173221: step 2109, loss = 12.82 (11.5 examples/sec; 5.570 sec/batch)
2016-04-30 13:17:00.816376: step 2110, loss = 12.80 (11.3 examples/sec; 5.643 sec/batch)
2016-04-30 13:17:13.368832: step 2111, loss = 12.90 (12.1 examples/sec; 5.295 sec/batch)
2016-04-30 13:17:19.396664: step 2112, loss = 12.78 (10.6 examples/sec; 6.028 sec/batch)
2016-04-30 13:17:24.575902: step 2113, loss = 12.86 (12.4 examples/sec; 5.179 sec/batch)
2016-04-30 13:17:29.692848: step 2114, loss = 12.66 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 13:17:35.220460: step 2115, loss = 12.83 (11.6 examples/sec; 5.528 sec/batch)
2016-04-30 13:17:40.525415: step 2116, loss = 12.84 (12.1 examples/sec; 5.305 sec/batch)
2016-04-30 13:17:45.666462: step 2117, loss = 12.74 (12.4 examples/sec; 5.141 sec/batch)
2016-04-30 13:17:51.412327: step 2118, loss = 12.75 (11.1 examples/sec; 5.746 sec/batch)
2016-04-30 13:17:56.566663: step 2119, loss = 12.60 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 13:18:02.012071: step 2120, loss = 12.79 (11.8 examples/sec; 5.445 sec/batch)
2016-04-30 13:18:14.576367: step 2121, loss = 12.64 (12.9 examples/sec; 4.979 sec/batch)
2016-04-30 13:18:19.912495: step 2122, loss = 12.80 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 13:18:25.821857: step 2123, loss = 12.66 (10.8 examples/sec; 5.909 sec/batch)
2016-04-30 13:18:31.232892: step 2124, loss = 12.67 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 13:18:36.288037: step 2125, loss = 12.84 (12.7 examples/sec; 5.055 sec/batch)
2016-04-30 13:18:41.521579: step 2126, loss = 12.73 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 13:18:46.836861: step 2127, loss = 12.72 (12.0 examples/sec; 5.315 sec/batch)
2016-04-30 13:18:52.095535: step 2128, loss = 12.70 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 13:18:57.917702: step 2129, loss = 12.72 (11.0 examples/sec; 5.822 sec/batch)
2016-04-30 13:19:03.120464: step 2130, loss = 12.65 (12.3 examples/sec; 5.203 sec/batch)
2016-04-30 13:19:15.146228: step 2131, loss = 12.68 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 13:19:20.422898: step 2132, loss = 12.66 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 13:19:25.755250: step 2133, loss = 12.70 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 13:19:31.555624: step 2134, loss = 12.57 (11.0 examples/sec; 5.800 sec/batch)
2016-04-30 13:19:36.964658: step 2135, loss = 12.65 (11.8 examples/sec; 5.409 sec/batch)
2016-04-30 13:19:41.896457: step 2136, loss = 12.67 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 13:19:47.219488: step 2137, loss = 12.66 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 13:19:52.506336: step 2138, loss = 12.53 (12.1 examples/sec; 5.287 sec/batch)
2016-04-30 13:19:57.712918: step 2139, loss = 12.62 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 13:20:03.468973: step 2140, loss = 12.44 (11.1 examples/sec; 5.756 sec/batch)
2016-04-30 13:20:15.422066: step 2141, loss = 12.31 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 13:20:20.751812: step 2142, loss = 12.49 (12.0 examples/sec; 5.330 sec/batch)
2016-04-30 13:20:26.006014: step 2143, loss = 12.44 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 13:20:31.314892: step 2144, loss = 12.62 (12.1 examples/sec; 5.309 sec/batch)
2016-04-30 13:20:37.237193: step 2145, loss = 12.55 (10.8 examples/sec; 5.922 sec/batch)
2016-04-30 13:20:42.180049: step 2146, loss = 12.57 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 13:20:47.492482: step 2147, loss = 12.43 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 13:20:53.168810: step 2148, loss = 12.58 (11.3 examples/sec; 5.676 sec/batch)
2016-04-30 13:20:59.094917: step 2149, loss = 12.50 (10.8 examples/sec; 5.926 sec/batch)
2016-04-30 13:21:05.113480: step 2150, loss = 12.36 (10.6 examples/sec; 6.018 sec/batch)
2016-04-30 13:21:18.060778: step 2151, loss = 12.48 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 13:21:23.255141: step 2152, loss = 12.51 (12.3 examples/sec; 5.194 sec/batch)
2016-04-30 13:21:28.772678: step 2153, loss = 12.50 (11.6 examples/sec; 5.517 sec/batch)
2016-04-30 13:21:34.203963: step 2154, loss = 12.45 (11.8 examples/sec; 5.431 sec/batch)
2016-04-30 13:21:39.227767: step 2155, loss = 12.44 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 13:21:45.300853: step 2156, loss = 12.44 (10.5 examples/sec; 6.073 sec/batch)
2016-04-30 13:21:50.726231: step 2157, loss = 12.33 (11.8 examples/sec; 5.425 sec/batch)
2016-04-30 13:21:56.084453: step 2158, loss = 12.41 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 13:22:01.725181: step 2159, loss = 12.50 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 13:22:06.941974: step 2160, loss = 12.31 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 13:22:20.112433: step 2161, loss = 12.56 (12.1 examples/sec; 5.307 sec/batch)
2016-04-30 13:22:25.313516: step 2162, loss = 12.42 (12.3 examples/sec; 5.201 sec/batch)
2016-04-30 13:22:30.441914: step 2163, loss = 12.52 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 13:22:35.724526: step 2164, loss = 12.31 (12.1 examples/sec; 5.283 sec/batch)
2016-04-30 13:22:40.981915: step 2165, loss = 12.38 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 13:22:46.354141: step 2166, loss = 12.19 (11.9 examples/sec; 5.372 sec/batch)
2016-04-30 13:22:51.860130: step 2167, loss = 12.38 (11.6 examples/sec; 5.506 sec/batch)
2016-04-30 13:22:56.930824: step 2168, loss = 12.39 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 13:23:02.520960: step 2169, loss = 12.36 (11.4 examples/sec; 5.590 sec/batch)
2016-04-30 13:23:07.843643: step 2170, loss = 12.40 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 13:23:20.467950: step 2171, loss = 12.39 (11.4 examples/sec; 5.633 sec/batch)
2016-04-30 13:23:25.842134: step 2172, loss = 12.30 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 13:23:31.131935: step 2173, loss = 12.44 (12.1 examples/sec; 5.290 sec/batch)
2016-04-30 13:23:36.142940: step 2174, loss = 12.31 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 13:23:41.351709: step 2175, loss = 12.23 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 13:23:46.593795: step 2176, loss = 12.16 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 13:23:53.097383: step 2177, loss = 12.06 (9.8 examples/sec; 6.503 sec/batch)
2016-04-30 13:23:58.742027: step 2178, loss = 12.07 (11.3 examples/sec; 5.645 sec/batch)
2016-04-30 13:24:04.367982: step 2179, loss = 12.28 (11.4 examples/sec; 5.626 sec/batch)
2016-04-30 13:24:10.237933: step 2180, loss = 12.10 (10.9 examples/sec; 5.870 sec/batch)
2016-04-30 13:24:23.020145: step 2181, loss = 12.26 (12.3 examples/sec; 5.191 sec/batch)
2016-04-30 13:24:29.028543: step 2182, loss = 12.26 (10.7 examples/sec; 6.008 sec/batch)
2016-04-30 13:24:34.609526: step 2183, loss = 12.18 (11.5 examples/sec; 5.581 sec/batch)
2016-04-30 13:24:39.847941: step 2184, loss = 12.19 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 13:24:44.942018: step 2185, loss = 12.27 (12.6 examples/sec; 5.094 sec/batch)
2016-04-30 13:24:50.289521: step 2186, loss = 12.18 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 13:24:55.536173: step 2187, loss = 12.20 (12.2 examples/sec; 5.247 sec/batch)
2016-04-30 13:25:01.511712: step 2188, loss = 12.01 (10.7 examples/sec; 5.975 sec/batch)
2016-04-30 13:25:06.458365: step 2189, loss = 12.16 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 13:25:11.490459: step 2190, loss = 12.21 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 13:25:23.782590: step 2191, loss = 12.09 (11.7 examples/sec; 5.470 sec/batch)
2016-04-30 13:25:29.835668: step 2192, loss = 12.15 (10.6 examples/sec; 6.053 sec/batch)
2016-04-30 13:25:37.008031: step 2193, loss = 12.13 (8.9 examples/sec; 7.172 sec/batch)
2016-04-30 13:25:42.027317: step 2194, loss = 12.18 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 13:25:47.501888: step 2195, loss = 11.91 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 13:25:52.720489: step 2196, loss = 12.02 (12.3 examples/sec; 5.219 sec/batch)
2016-04-30 13:25:58.191139: step 2197, loss = 12.02 (11.7 examples/sec; 5.471 sec/batch)
2016-04-30 13:26:03.647832: step 2198, loss = 11.98 (11.7 examples/sec; 5.457 sec/batch)
2016-04-30 13:26:09.694489: step 2199, loss = 11.88 (10.6 examples/sec; 6.047 sec/batch)
2016-04-30 13:26:15.016485: step 2200, loss = 12.03 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 13:26:27.123864: step 2201, loss = 12.08 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 13:26:32.596701: step 2202, loss = 12.20 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 13:26:38.452467: step 2203, loss = 11.94 (10.9 examples/sec; 5.856 sec/batch)
2016-04-30 13:26:43.941837: step 2204, loss = 12.21 (11.7 examples/sec; 5.489 sec/batch)
2016-04-30 13:26:49.273223: step 2205, loss = 12.07 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 13:26:54.330149: step 2206, loss = 11.91 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 13:26:59.741309: step 2207, loss = 11.86 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 13:27:05.176029: step 2208, loss = 11.96 (11.8 examples/sec; 5.435 sec/batch)
2016-04-30 13:27:10.873928: step 2209, loss = 12.09 (11.2 examples/sec; 5.698 sec/batch)
2016-04-30 13:27:16.318917: step 2210, loss = 11.96 (11.8 examples/sec; 5.445 sec/batch)
2016-04-30 13:27:28.696505: step 2211, loss = 12.01 (12.3 examples/sec; 5.188 sec/batch)
2016-04-30 13:27:33.740515: step 2212, loss = 11.84 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 13:27:38.952712: step 2213, loss = 12.01 (12.3 examples/sec; 5.212 sec/batch)
2016-04-30 13:27:45.044967: step 2214, loss = 11.98 (10.5 examples/sec; 6.092 sec/batch)
2016-04-30 13:27:50.350656: step 2215, loss = 11.95 (12.1 examples/sec; 5.306 sec/batch)
2016-04-30 13:27:55.650106: step 2216, loss = 11.79 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 13:28:01.184515: step 2217, loss = 11.94 (11.6 examples/sec; 5.534 sec/batch)
2016-04-30 13:28:06.298024: step 2218, loss = 11.92 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 13:28:11.483921: step 2219, loss = 11.94 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 13:28:17.380051: step 2220, loss = 11.91 (10.9 examples/sec; 5.896 sec/batch)
2016-04-30 13:28:30.015563: step 2221, loss = 11.88 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 13:28:35.646842: step 2222, loss = 12.01 (11.4 examples/sec; 5.631 sec/batch)
2016-04-30 13:28:41.116715: step 2223, loss = 11.83 (11.7 examples/sec; 5.470 sec/batch)
2016-04-30 13:28:46.620868: step 2224, loss = 11.84 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 13:28:52.542803: step 2225, loss = 11.94 (10.8 examples/sec; 5.922 sec/batch)
2016-04-30 13:28:57.609915: step 2226, loss = 11.69 (12.6 examples/sec; 5.067 sec/batch)
2016-04-30 13:29:02.941615: step 2227, loss = 11.76 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 13:29:08.182904: step 2228, loss = 11.78 (12.2 examples/sec; 5.241 sec/batch)
2016-04-30 13:29:13.376165: step 2229, loss = 11.65 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 13:29:18.693276: step 2230, loss = 11.93 (12.0 examples/sec; 5.317 sec/batch)
2016-04-30 13:29:31.435132: step 2231, loss = 11.98 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 13:29:36.322044: step 2232, loss = 11.79 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 13:29:42.220963: step 2233, loss = 11.54 (10.8 examples/sec; 5.899 sec/batch)
2016-04-30 13:29:47.844643: step 2234, loss = 11.71 (11.4 examples/sec; 5.624 sec/batch)
2016-04-30 13:29:53.218223: step 2235, loss = 11.62 (11.9 examples/sec; 5.373 sec/batch)
2016-04-30 13:29:59.124561: step 2236, loss = 11.69 (10.8 examples/sec; 5.906 sec/batch)
2016-04-30 13:30:04.631977: step 2237, loss = 11.67 (11.6 examples/sec; 5.507 sec/batch)
2016-04-30 13:30:09.818392: step 2238, loss = 11.56 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 13:30:15.032577: step 2239, loss = 11.75 (12.3 examples/sec; 5.214 sec/batch)
2016-04-30 13:30:20.284987: step 2240, loss = 11.77 (12.2 examples/sec; 5.252 sec/batch)
2016-04-30 13:30:33.463323: step 2241, loss = 11.70 (10.5 examples/sec; 6.106 sec/batch)
2016-04-30 13:30:39.834678: step 2242, loss = 11.65 (10.0 examples/sec; 6.371 sec/batch)
2016-04-30 13:30:45.342336: step 2243, loss = 11.70 (11.6 examples/sec; 5.508 sec/batch)
2016-04-30 13:30:50.942406: step 2244, loss = 11.60 (11.4 examples/sec; 5.600 sec/batch)
2016-04-30 13:30:56.530761: step 2245, loss = 11.55 (11.5 examples/sec; 5.588 sec/batch)
2016-04-30 13:31:02.771550: step 2246, loss = 11.61 (10.3 examples/sec; 6.241 sec/batch)
2016-04-30 13:31:08.206958: step 2247, loss = 11.62 (11.8 examples/sec; 5.435 sec/batch)
2016-04-30 13:31:13.427974: step 2248, loss = 11.78 (12.3 examples/sec; 5.221 sec/batch)
2016-04-30 13:31:18.747092: step 2249, loss = 11.67 (12.0 examples/sec; 5.319 sec/batch)
2016-04-30 13:31:24.360226: step 2250, loss = 11.54 (11.4 examples/sec; 5.613 sec/batch)
2016-04-30 13:31:37.677654: step 2251, loss = 11.57 (10.9 examples/sec; 5.860 sec/batch)
2016-04-30 13:31:42.727650: step 2252, loss = 11.77 (12.7 examples/sec; 5.050 sec/batch)
2016-04-30 13:31:48.061254: step 2253, loss = 11.62 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 13:31:53.447270: step 2254, loss = 11.47 (11.9 examples/sec; 5.386 sec/batch)
2016-04-30 13:31:58.945154: step 2255, loss = 11.60 (11.6 examples/sec; 5.498 sec/batch)
2016-04-30 13:32:04.725205: step 2256, loss = 11.54 (11.1 examples/sec; 5.780 sec/batch)
2016-04-30 13:32:10.748514: step 2257, loss = 11.55 (10.6 examples/sec; 6.023 sec/batch)
2016-04-30 13:32:15.895466: step 2258, loss = 11.57 (12.4 examples/sec; 5.147 sec/batch)
2016-04-30 13:32:21.024184: step 2259, loss = 11.58 (12.5 examples/sec; 5.129 sec/batch)
2016-04-30 13:32:26.439419: step 2260, loss = 11.63 (11.8 examples/sec; 5.415 sec/batch)
2016-04-30 13:32:38.578432: step 2261, loss = 11.44 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 13:32:44.420895: step 2262, loss = 11.54 (11.0 examples/sec; 5.842 sec/batch)
2016-04-30 13:32:49.864342: step 2263, loss = 11.48 (11.8 examples/sec; 5.443 sec/batch)
2016-04-30 13:32:55.033476: step 2264, loss = 11.41 (12.4 examples/sec; 5.169 sec/batch)
2016-04-30 13:33:00.198464: step 2265, loss = 11.56 (12.4 examples/sec; 5.165 sec/batch)
2016-04-30 13:33:05.754604: step 2266, loss = 11.55 (11.5 examples/sec; 5.556 sec/batch)
2016-04-30 13:33:11.077209: step 2267, loss = 11.50 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 13:33:17.014810: step 2268, loss = 11.59 (10.8 examples/sec; 5.938 sec/batch)
2016-04-30 13:33:22.427970: step 2269, loss = 11.48 (11.8 examples/sec; 5.413 sec/batch)
2016-04-30 13:33:27.486595: step 2270, loss = 11.46 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 13:33:39.827164: step 2271, loss = 11.51 (12.5 examples/sec; 5.100 sec/batch)
2016-04-30 13:33:45.001418: step 2272, loss = 11.38 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 13:33:50.865276: step 2273, loss = 11.52 (10.9 examples/sec; 5.864 sec/batch)
2016-04-30 13:33:56.104343: step 2274, loss = 11.38 (12.2 examples/sec; 5.239 sec/batch)
2016-04-30 13:34:01.808252: step 2275, loss = 11.53 (11.2 examples/sec; 5.704 sec/batch)
2016-04-30 13:34:06.956935: step 2276, loss = 11.62 (12.4 examples/sec; 5.149 sec/batch)
2016-04-30 13:34:12.183141: step 2277, loss = 11.34 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 13:34:17.568341: step 2278, loss = 11.28 (11.9 examples/sec; 5.385 sec/batch)
2016-04-30 13:34:23.334339: step 2279, loss = 11.40 (11.1 examples/sec; 5.766 sec/batch)
2016-04-30 13:34:28.724911: step 2280, loss = 11.32 (11.9 examples/sec; 5.390 sec/batch)
2016-04-30 13:34:40.923874: step 2281, loss = 11.32 (12.4 examples/sec; 5.162 sec/batch)
2016-04-30 13:34:46.134326: step 2282, loss = 11.25 (12.3 examples/sec; 5.210 sec/batch)
2016-04-30 13:34:51.287296: step 2283, loss = 11.38 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 13:34:57.243090: step 2284, loss = 11.26 (10.7 examples/sec; 5.956 sec/batch)
2016-04-30 13:35:02.767843: step 2285, loss = 11.35 (11.6 examples/sec; 5.525 sec/batch)
2016-04-30 13:35:08.171174: step 2286, loss = 11.39 (11.8 examples/sec; 5.403 sec/batch)
2016-04-30 13:35:13.237852: step 2287, loss = 11.44 (12.6 examples/sec; 5.067 sec/batch)
2016-04-30 13:35:18.607217: step 2288, loss = 11.36 (11.9 examples/sec; 5.369 sec/batch)
2016-04-30 13:35:24.290164: step 2289, loss = 11.23 (11.3 examples/sec; 5.683 sec/batch)
2016-04-30 13:35:31.261138: step 2290, loss = 11.25 (9.2 examples/sec; 6.971 sec/batch)
2016-04-30 13:35:43.906375: step 2291, loss = 11.37 (11.6 examples/sec; 5.529 sec/batch)
2016-04-30 13:35:49.286290: step 2292, loss = 11.19 (11.9 examples/sec; 5.380 sec/batch)
2016-04-30 13:35:54.661428: step 2293, loss = 11.38 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 13:35:59.985472: step 2294, loss = 11.26 (12.0 examples/sec; 5.324 sec/batch)
2016-04-30 13:36:06.102548: step 2295, loss = 11.33 (10.5 examples/sec; 6.117 sec/batch)
2016-04-30 13:36:11.456460: step 2296, loss = 11.30 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 13:36:16.694719: step 2297, loss = 11.23 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 13:36:21.948460: step 2298, loss = 11.20 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 13:36:27.130268: step 2299, loss = 11.32 (12.4 examples/sec; 5.182 sec/batch)
2016-04-30 13:36:32.451408: step 2300, loss = 11.21 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 13:36:45.252624: step 2301, loss = 11.40 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 13:36:50.607784: step 2302, loss = 11.29 (12.0 examples/sec; 5.355 sec/batch)
2016-04-30 13:36:56.009483: step 2303, loss = 11.25 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 13:37:01.590538: step 2304, loss = 11.12 (11.5 examples/sec; 5.581 sec/batch)
2016-04-30 13:37:06.623761: step 2305, loss = 10.93 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 13:37:12.434124: step 2306, loss = 11.15 (11.0 examples/sec; 5.810 sec/batch)
2016-04-30 13:37:17.676012: step 2307, loss = 11.18 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 13:37:22.942296: step 2308, loss = 11.10 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 13:37:28.244469: step 2309, loss = 11.10 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 13:37:33.114134: step 2310, loss = 11.06 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 13:37:45.922875: step 2311, loss = 11.06 (11.6 examples/sec; 5.495 sec/batch)
2016-04-30 13:37:51.002328: step 2312, loss = 11.08 (12.6 examples/sec; 5.079 sec/batch)
2016-04-30 13:37:56.482723: step 2313, loss = 11.10 (11.7 examples/sec; 5.480 sec/batch)
2016-04-30 13:38:02.033114: step 2314, loss = 11.10 (11.5 examples/sec; 5.550 sec/batch)
2016-04-30 13:38:07.433391: step 2315, loss = 11.10 (11.9 examples/sec; 5.400 sec/batch)
2016-04-30 13:38:12.445429: step 2316, loss = 11.09 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 13:38:18.316944: step 2317, loss = 10.99 (10.9 examples/sec; 5.871 sec/batch)
2016-04-30 13:38:23.505030: step 2318, loss = 11.00 (12.3 examples/sec; 5.188 sec/batch)
2016-04-30 13:38:28.802677: step 2319, loss = 11.07 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 13:38:33.753570: step 2320, loss = 11.05 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 13:38:46.042989: step 2321, loss = 10.96 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 13:38:51.727927: step 2322, loss = 11.04 (11.3 examples/sec; 5.685 sec/batch)
2016-04-30 13:38:56.967657: step 2323, loss = 11.04 (12.2 examples/sec; 5.240 sec/batch)
2016-04-30 13:39:02.359935: step 2324, loss = 10.99 (11.9 examples/sec; 5.392 sec/batch)
2016-04-30 13:39:07.566782: step 2325, loss = 11.06 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 13:39:12.565116: step 2326, loss = 11.07 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 13:39:17.784991: step 2327, loss = 11.11 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 13:39:23.831246: step 2328, loss = 11.03 (10.6 examples/sec; 6.046 sec/batch)
2016-04-30 13:39:29.153877: step 2329, loss = 10.98 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 13:39:34.585458: step 2330, loss = 10.92 (11.8 examples/sec; 5.431 sec/batch)
2016-04-30 13:39:46.578890: step 2331, loss = 10.78 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 13:39:51.588473: step 2332, loss = 11.08 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 13:39:57.445740: step 2333, loss = 10.85 (10.9 examples/sec; 5.857 sec/batch)
2016-04-30 13:40:02.658389: step 2334, loss = 11.00 (12.3 examples/sec; 5.213 sec/batch)
2016-04-30 13:40:07.948502: step 2335, loss = 10.65 (12.1 examples/sec; 5.290 sec/batch)
2016-04-30 13:40:13.155603: step 2336, loss = 10.89 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 13:40:18.160475: step 2337, loss = 10.97 (12.8 examples/sec; 5.005 sec/batch)
2016-04-30 13:40:23.368511: step 2338, loss = 11.11 (12.3 examples/sec; 5.208 sec/batch)
2016-04-30 13:40:29.924421: step 2339, loss = 10.91 (9.8 examples/sec; 6.556 sec/batch)
2016-04-30 13:40:36.279074: step 2340, loss = 10.94 (10.1 examples/sec; 6.355 sec/batch)
2016-04-30 13:40:48.508694: step 2341, loss = 10.85 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 13:40:53.840611: step 2342, loss = 10.76 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 13:40:59.920041: step 2343, loss = 11.04 (10.5 examples/sec; 6.079 sec/batch)
2016-04-30 13:41:05.666538: step 2344, loss = 12.05 (11.1 examples/sec; 5.746 sec/batch)
2016-04-30 13:41:11.003561: step 2345, loss = 11.06 (12.0 examples/sec; 5.337 sec/batch)
2016-04-30 13:41:16.301767: step 2346, loss = 11.71 (12.1 examples/sec; 5.298 sec/batch)
2016-04-30 13:41:21.313462: step 2347, loss = 11.59 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 13:41:26.711148: step 2348, loss = 12.97 (11.9 examples/sec; 5.398 sec/batch)
2016-04-30 13:41:32.686994: step 2349, loss = 14.03 (10.7 examples/sec; 5.976 sec/batch)
2016-04-30 13:41:37.970559: step 2350, loss = 11.58 (12.1 examples/sec; 5.283 sec/batch)
2016-04-30 13:41:50.214876: step 2351, loss = 11.03 (12.5 examples/sec; 5.140 sec/batch)
2016-04-30 13:41:55.465170: step 2352, loss = 11.17 (12.2 examples/sec; 5.250 sec/batch)
2016-04-30 13:42:00.640064: step 2353, loss = 11.06 (12.4 examples/sec; 5.175 sec/batch)
2016-04-30 13:42:06.380506: step 2354, loss = 10.81 (11.1 examples/sec; 5.740 sec/batch)
2016-04-30 13:42:11.608453: step 2355, loss = 10.80 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 13:42:17.048575: step 2356, loss = 10.69 (11.8 examples/sec; 5.440 sec/batch)
2016-04-30 13:42:22.219454: step 2357, loss = 11.31 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 13:42:27.232155: step 2358, loss = 11.04 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 13:42:32.470488: step 2359, loss = 10.98 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 13:42:38.182734: step 2360, loss = 11.02 (11.2 examples/sec; 5.712 sec/batch)
2016-04-30 13:42:50.139112: step 2361, loss = 14.13 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 13:42:55.393777: step 2362, loss = 13.24 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 13:43:00.483591: step 2363, loss = 16.57 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 13:43:05.751176: step 2364, loss = 13.56 (12.2 examples/sec; 5.267 sec/batch)
2016-04-30 13:43:11.694859: step 2365, loss = 11.57 (10.8 examples/sec; 5.944 sec/batch)
2016-04-30 13:43:17.052403: step 2366, loss = 11.67 (11.9 examples/sec; 5.357 sec/batch)
2016-04-30 13:43:22.307067: step 2367, loss = 11.69 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 13:43:27.329898: step 2368, loss = 11.09 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 13:43:32.550117: step 2369, loss = 10.75 (12.3 examples/sec; 5.220 sec/batch)
2016-04-30 13:43:37.882895: step 2370, loss = 11.56 (12.0 examples/sec; 5.333 sec/batch)
2016-04-30 13:43:50.369206: step 2371, loss = 11.71 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 13:43:55.746322: step 2372, loss = 12.46 (11.9 examples/sec; 5.377 sec/batch)
2016-04-30 13:44:01.476478: step 2373, loss = 15.11 (11.2 examples/sec; 5.730 sec/batch)
2016-04-30 13:44:06.410427: step 2374, loss = 15.03 (13.0 examples/sec; 4.934 sec/batch)
2016-04-30 13:44:11.597480: step 2375, loss = 26.66 (12.3 examples/sec; 5.187 sec/batch)
2016-04-30 13:44:17.544420: step 2376, loss = 13.06 (10.8 examples/sec; 5.947 sec/batch)
2016-04-30 13:44:22.790137: step 2377, loss = 10.96 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 13:44:27.885804: step 2378, loss = 11.31 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 13:44:33.033765: step 2379, loss = 10.95 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 13:44:38.365912: step 2380, loss = 11.50 (12.0 examples/sec; 5.332 sec/batch)
2016-04-30 13:44:51.018236: step 2381, loss = 11.11 (11.5 examples/sec; 5.578 sec/batch)
2016-04-30 13:44:56.062767: step 2382, loss = 11.13 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 13:45:01.222718: step 2383, loss = 11.42 (12.4 examples/sec; 5.160 sec/batch)
2016-04-30 13:45:06.002572: step 2384, loss = 10.94 (13.4 examples/sec; 4.780 sec/batch)
2016-04-30 13:45:10.998661: step 2385, loss = 11.66 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 13:45:15.651015: step 2386, loss = 12.90 (13.8 examples/sec; 4.652 sec/batch)
2016-04-30 13:45:20.699538: step 2387, loss = 14.05 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 13:45:26.239442: step 2388, loss = 11.97 (11.6 examples/sec; 5.540 sec/batch)
2016-04-30 13:45:31.111284: step 2389, loss = 14.14 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 13:45:35.887607: step 2390, loss = 15.15 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 13:45:47.520326: step 2391, loss = 17.54 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 13:45:52.345695: step 2392, loss = 41.61 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 13:45:57.606218: step 2393, loss = 16.13 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 13:46:02.669553: step 2394, loss = 10.30 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 13:46:07.611517: step 2395, loss = 10.33 (13.0 examples/sec; 4.942 sec/batch)
2016-04-30 13:46:13.216260: step 2396, loss = 10.45 (11.4 examples/sec; 5.605 sec/batch)
2016-04-30 13:46:17.857406: step 2397, loss = 10.56 (13.8 examples/sec; 4.641 sec/batch)
2016-04-30 13:46:22.838776: step 2398, loss = 10.59 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 13:46:27.584464: step 2399, loss = 10.63 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 13:46:32.925090: step 2400, loss = 10.48 (12.0 examples/sec; 5.341 sec/batch)
2016-04-30 13:46:44.464488: step 2401, loss = 10.31 (13.5 examples/sec; 4.757 sec/batch)
2016-04-30 13:46:49.327540: step 2402, loss = 10.38 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 13:46:53.983846: step 2403, loss = 10.29 (13.7 examples/sec; 4.656 sec/batch)
2016-04-30 13:46:59.133905: step 2404, loss = 10.25 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 13:47:04.875592: step 2405, loss = 10.30 (11.1 examples/sec; 5.742 sec/batch)
2016-04-30 13:47:09.489612: step 2406, loss = 10.53 (13.9 examples/sec; 4.614 sec/batch)
2016-04-30 13:47:14.479332: step 2407, loss = 10.41 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 13:47:19.211187: step 2408, loss = 10.35 (13.5 examples/sec; 4.732 sec/batch)
2016-04-30 13:47:23.888913: step 2409, loss = 10.25 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 13:47:28.845460: step 2410, loss = 10.31 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 13:47:40.879416: step 2411, loss = 10.45 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 13:47:45.578268: step 2412, loss = 10.37 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 13:47:50.488606: step 2413, loss = 10.53 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 13:47:55.449112: step 2414, loss = 10.36 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 13:47:59.994911: step 2415, loss = 10.14 (14.1 examples/sec; 4.546 sec/batch)
2016-04-30 13:48:05.373833: step 2416, loss = 10.32 (11.9 examples/sec; 5.379 sec/batch)
2016-04-30 13:48:10.803313: step 2417, loss = 10.34 (11.8 examples/sec; 5.429 sec/batch)
2016-04-30 13:48:15.407393: step 2418, loss = 10.43 (13.9 examples/sec; 4.604 sec/batch)
2016-04-30 13:48:20.301987: step 2419, loss = 10.37 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 13:48:25.214605: step 2420, loss = 10.38 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 13:48:36.364580: step 2421, loss = 10.27 (14.2 examples/sec; 4.522 sec/batch)
2016-04-30 13:48:41.837340: step 2422, loss = 10.26 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 13:48:46.657570: step 2423, loss = 10.24 (13.3 examples/sec; 4.820 sec/batch)
2016-04-30 13:48:51.480341: step 2424, loss = 10.46 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 13:48:56.327748: step 2425, loss = 10.19 (13.2 examples/sec; 4.847 sec/batch)
2016-04-30 13:49:01.551853: step 2426, loss = 10.20 (12.3 examples/sec; 5.224 sec/batch)
2016-04-30 13:49:06.167531: step 2427, loss = 10.20 (13.9 examples/sec; 4.616 sec/batch)
2016-04-30 13:49:11.075875: step 2428, loss = 10.27 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 13:49:16.496497: step 2429, loss = 10.29 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 13:49:21.271600: step 2430, loss = 10.11 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 13:49:32.597966: step 2431, loss = 10.31 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 13:49:37.511924: step 2432, loss = 10.32 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 13:49:42.348845: step 2433, loss = 10.15 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 13:49:47.804237: step 2434, loss = 10.17 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 13:49:52.930175: step 2435, loss = 10.10 (12.5 examples/sec; 5.126 sec/batch)
2016-04-30 13:49:57.577229: step 2436, loss = 10.06 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 13:50:02.516584: step 2437, loss = 10.18 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 13:50:07.359325: step 2438, loss = 10.21 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 13:50:11.946075: step 2439, loss = 10.16 (14.0 examples/sec; 4.587 sec/batch)
2016-04-30 13:50:16.875536: step 2440, loss = 10.28 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 13:50:28.785601: step 2441, loss = 10.14 (13.3 examples/sec; 4.810 sec/batch)
2016-04-30 13:50:33.448136: step 2442, loss = 10.02 (13.7 examples/sec; 4.662 sec/batch)
2016-04-30 13:50:38.529401: step 2443, loss = 10.26 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 13:50:43.423566: step 2444, loss = 10.19 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 13:50:48.340265: step 2445, loss = 10.22 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 13:50:53.951790: step 2446, loss = 10.37 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 13:50:58.869159: step 2447, loss = 10.05 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 13:51:03.860674: step 2448, loss = 10.20 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 13:51:08.561641: step 2449, loss = 10.07 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 13:51:13.263768: step 2450, loss = 10.05 (13.6 examples/sec; 4.702 sec/batch)
2016-04-30 13:51:24.395161: step 2451, loss = 10.27 (14.1 examples/sec; 4.539 sec/batch)
2016-04-30 13:51:30.095246: step 2452, loss = 10.04 (11.2 examples/sec; 5.700 sec/batch)
2016-04-30 13:51:35.232071: step 2453, loss = 10.13 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 13:51:40.014038: step 2454, loss = 9.99 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 13:51:45.023388: step 2455, loss = 10.13 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 13:51:50.125493: step 2456, loss = 10.15 (12.5 examples/sec; 5.102 sec/batch)
2016-04-30 13:51:54.917876: step 2457, loss = 10.04 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 13:52:00.329147: step 2458, loss = 9.88 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 13:52:05.355942: step 2459, loss = 10.05 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 13:52:10.010629: step 2460, loss = 10.00 (13.7 examples/sec; 4.655 sec/batch)
2016-04-30 13:52:21.169854: step 2461, loss = 10.02 (14.6 examples/sec; 4.381 sec/batch)
2016-04-30 13:52:26.118354: step 2462, loss = 10.10 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 13:52:31.138361: step 2463, loss = 9.99 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 13:52:36.437622: step 2464, loss = 10.05 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 13:52:41.518075: step 2465, loss = 9.91 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 13:52:46.376344: step 2466, loss = 9.86 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 13:52:51.046161: step 2467, loss = 9.89 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 13:52:55.900597: step 2468, loss = 9.90 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 13:53:00.783404: step 2469, loss = 10.00 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 13:53:06.326911: step 2470, loss = 9.87 (11.5 examples/sec; 5.543 sec/batch)
2016-04-30 13:53:17.527173: step 2471, loss = 9.94 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 13:53:22.286432: step 2472, loss = 9.99 (13.4 examples/sec; 4.759 sec/batch)
2016-04-30 13:53:27.102295: step 2473, loss = 9.91 (13.3 examples/sec; 4.816 sec/batch)
2016-04-30 13:53:32.170261: step 2474, loss = 9.89 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 13:53:36.874738: step 2475, loss = 9.96 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 13:53:42.343072: step 2476, loss = 9.84 (11.7 examples/sec; 5.468 sec/batch)
2016-04-30 13:53:47.460026: step 2477, loss = 9.98 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 13:53:52.133409: step 2478, loss = 10.09 (13.7 examples/sec; 4.673 sec/batch)
2016-04-30 13:53:56.928435: step 2479, loss = 9.69 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 13:54:01.988503: step 2480, loss = 9.84 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 13:54:13.997247: step 2481, loss = 9.80 (11.8 examples/sec; 5.437 sec/batch)
2016-04-30 13:54:18.604101: step 2482, loss = 9.93 (13.9 examples/sec; 4.607 sec/batch)
2016-04-30 13:54:23.547567: step 2483, loss = 9.86 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 13:54:28.713497: step 2484, loss = 9.83 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 13:54:33.310526: step 2485, loss = 9.86 (13.9 examples/sec; 4.597 sec/batch)
2016-04-30 13:54:38.539139: step 2486, loss = 9.75 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 13:54:44.100224: step 2487, loss = 9.81 (11.5 examples/sec; 5.561 sec/batch)
2016-04-30 13:54:49.028517: step 2488, loss = 9.71 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 13:54:53.981332: step 2489, loss = 9.78 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 13:54:59.075970: step 2490, loss = 9.86 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 13:55:11.043194: step 2491, loss = 9.87 (12.3 examples/sec; 5.212 sec/batch)
2016-04-30 13:55:15.818366: step 2492, loss = 9.96 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 13:55:21.372326: step 2493, loss = 9.70 (11.5 examples/sec; 5.554 sec/batch)
2016-04-30 13:55:26.342818: step 2494, loss = 9.81 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 13:55:31.204716: step 2495, loss = 9.67 (13.2 examples/sec; 4.862 sec/batch)
2016-04-30 13:55:36.102438: step 2496, loss = 9.91 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 13:55:41.033071: step 2497, loss = 9.79 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 13:55:45.948896: step 2498, loss = 9.81 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 13:55:51.458519: step 2499, loss = 9.76 (11.6 examples/sec; 5.510 sec/batch)
2016-04-30 13:55:56.355092: step 2500, loss = 9.89 (13.1 examples/sec; 4.896 sec/batch)
2016-04-30 13:56:08.119606: step 2501, loss = 9.85 (13.0 examples/sec; 4.918 sec/batch)
2016-04-30 13:56:12.844573: step 2502, loss = 9.60 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 13:56:17.932241: step 2503, loss = 9.81 (12.6 examples/sec; 5.088 sec/batch)
2016-04-30 13:56:23.082393: step 2504, loss = 9.61 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 13:56:27.772120: step 2505, loss = 9.69 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 13:56:32.608978: step 2506, loss = 9.58 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 13:56:37.488440: step 2507, loss = 9.91 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 13:56:42.256493: step 2508, loss = 9.98 (13.4 examples/sec; 4.768 sec/batch)
2016-04-30 13:56:47.225782: step 2509, loss = 10.11 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 13:56:52.019570: step 2510, loss = 10.26 (13.4 examples/sec; 4.794 sec/batch)
2016-04-30 13:57:04.230950: step 2511, loss = 9.88 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 13:57:09.003169: step 2512, loss = 9.80 (13.4 examples/sec; 4.772 sec/batch)
2016-04-30 13:57:13.841415: step 2513, loss = 9.98 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 13:57:18.500659: step 2514, loss = 10.40 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 13:57:23.645893: step 2515, loss = 13.09 (12.4 examples/sec; 5.145 sec/batch)
2016-04-30 13:57:28.515038: step 2516, loss = 11.28 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 13:57:33.590495: step 2517, loss = 11.55 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 13:57:38.465583: step 2518, loss = 11.48 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 13:57:43.579018: step 2519, loss = 10.98 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 13:57:48.501748: step 2520, loss = 10.27 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 13:57:59.866013: step 2521, loss = 10.30 (14.2 examples/sec; 4.495 sec/batch)
2016-04-30 13:58:05.974380: step 2522, loss = 11.58 (10.5 examples/sec; 6.108 sec/batch)
2016-04-30 13:58:10.940967: step 2523, loss = 15.04 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 13:58:15.710085: step 2524, loss = 13.23 (13.4 examples/sec; 4.769 sec/batch)
2016-04-30 13:58:20.714442: step 2525, loss = 12.04 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 13:58:25.649157: step 2526, loss = 10.29 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 13:58:30.377722: step 2527, loss = 10.02 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 13:58:35.931014: step 2528, loss = 9.69 (11.5 examples/sec; 5.553 sec/batch)
2016-04-30 13:58:40.940452: step 2529, loss = 9.63 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 13:58:45.823986: step 2530, loss = 10.41 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 13:58:57.281448: step 2531, loss = 15.01 (13.8 examples/sec; 4.624 sec/batch)
2016-04-30 13:59:02.594492: step 2532, loss = 23.76 (12.0 examples/sec; 5.313 sec/batch)
2016-04-30 13:59:07.751826: step 2533, loss = 28.10 (12.4 examples/sec; 5.157 sec/batch)
2016-04-30 13:59:12.858263: step 2534, loss = 14.14 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 13:59:17.593776: step 2535, loss = 9.86 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 13:59:22.637826: step 2536, loss = 9.70 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 13:59:27.288553: step 2537, loss = 9.60 (13.8 examples/sec; 4.651 sec/batch)
2016-04-30 13:59:32.225561: step 2538, loss = 10.04 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 13:59:36.883510: step 2539, loss = 9.65 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 13:59:42.336774: step 2540, loss = 9.69 (11.7 examples/sec; 5.453 sec/batch)
2016-04-30 13:59:53.919671: step 2541, loss = 9.71 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 13:59:58.886541: step 2542, loss = 9.64 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 14:00:03.944111: step 2543, loss = 9.72 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 14:00:08.823943: step 2544, loss = 9.88 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 14:00:14.465070: step 2545, loss = 9.82 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 14:00:19.304765: step 2546, loss = 9.85 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 14:00:24.318382: step 2547, loss = 9.80 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 14:00:29.318276: step 2548, loss = 9.71 (12.8 examples/sec; 5.000 sec/batch)
2016-04-30 14:00:34.255099: step 2549, loss = 9.82 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 14:00:39.273288: step 2550, loss = 9.78 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 14:00:51.256401: step 2551, loss = 9.88 (12.2 examples/sec; 5.259 sec/batch)
2016-04-30 14:00:56.280919: step 2552, loss = 9.66 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 14:01:01.296933: step 2553, loss = 9.61 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 14:01:06.149992: step 2554, loss = 9.47 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 14:01:11.046269: step 2555, loss = 10.15 (13.1 examples/sec; 4.896 sec/batch)
2016-04-30 14:01:15.784063: step 2556, loss = 18.16 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 14:01:21.444491: step 2557, loss = 16.11 (11.3 examples/sec; 5.660 sec/batch)
2016-04-30 14:01:26.488444: step 2558, loss = 19.50 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 14:01:31.541633: step 2559, loss = 18.10 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 14:01:36.431570: step 2560, loss = 12.68 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 14:01:47.910117: step 2561, loss = 10.03 (14.2 examples/sec; 4.518 sec/batch)
2016-04-30 14:01:52.980301: step 2562, loss = 9.90 (12.6 examples/sec; 5.070 sec/batch)
2016-04-30 14:01:57.864179: step 2563, loss = 9.78 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 14:02:02.831609: step 2564, loss = 11.86 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 14:02:07.767478: step 2565, loss = 30.10 (13.0 examples/sec; 4.936 sec/batch)
2016-04-30 14:02:12.432469: step 2566, loss = 45.27 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 14:02:17.349294: step 2567, loss = 18.21 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 14:02:22.049434: step 2568, loss = 11.51 (13.6 examples/sec; 4.700 sec/batch)
2016-04-30 14:02:27.574646: step 2569, loss = 10.21 (11.6 examples/sec; 5.525 sec/batch)
2016-04-30 14:02:32.560289: step 2570, loss = 9.31 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 14:02:43.715206: step 2571, loss = 10.00 (14.0 examples/sec; 4.566 sec/batch)
2016-04-30 14:02:48.446046: step 2572, loss = 9.13 (13.5 examples/sec; 4.731 sec/batch)
2016-04-30 14:02:53.312122: step 2573, loss = 9.65 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 14:02:57.911206: step 2574, loss = 10.00 (13.9 examples/sec; 4.599 sec/batch)
2016-04-30 14:03:03.424184: step 2575, loss = 9.39 (11.6 examples/sec; 5.513 sec/batch)
2016-04-30 14:03:08.273295: step 2576, loss = 9.42 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 14:03:12.836969: step 2577, loss = 9.66 (14.0 examples/sec; 4.564 sec/batch)
2016-04-30 14:03:17.716069: step 2578, loss = 9.52 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 14:03:22.479348: step 2579, loss = 9.64 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 14:03:27.248341: step 2580, loss = 9.72 (13.4 examples/sec; 4.769 sec/batch)
2016-04-30 14:03:39.101922: step 2581, loss = 9.34 (14.4 examples/sec; 4.444 sec/batch)
2016-04-30 14:03:44.187346: step 2582, loss = 9.20 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 14:03:49.068350: step 2583, loss = 9.93 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 14:03:53.819785: step 2584, loss = 10.12 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 14:03:58.706419: step 2585, loss = 9.37 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 14:04:03.674135: step 2586, loss = 9.78 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 14:04:09.276169: step 2587, loss = 9.19 (11.4 examples/sec; 5.602 sec/batch)
2016-04-30 14:04:14.256431: step 2588, loss = 9.55 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 14:04:18.929357: step 2589, loss = 9.23 (13.7 examples/sec; 4.673 sec/batch)
2016-04-30 14:04:23.766498: step 2590, loss = 9.79 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 14:04:35.139005: step 2591, loss = 9.63 (13.6 examples/sec; 4.698 sec/batch)
2016-04-30 14:04:40.443137: step 2592, loss = 9.53 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 14:04:45.232477: step 2593, loss = 9.75 (13.4 examples/sec; 4.789 sec/batch)
2016-04-30 14:04:50.212486: step 2594, loss = 9.63 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 14:04:54.943658: step 2595, loss = 9.69 (13.5 examples/sec; 4.731 sec/batch)
2016-04-30 14:04:59.846041: step 2596, loss = 9.57 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 14:05:05.092142: step 2597, loss = 9.97 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 14:05:09.857122: step 2598, loss = 10.29 (13.4 examples/sec; 4.765 sec/batch)
2016-04-30 14:05:15.429310: step 2599, loss = 9.96 (11.5 examples/sec; 5.572 sec/batch)
2016-04-30 14:05:20.401954: step 2600, loss = 9.87 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 14:05:31.775054: step 2601, loss = 9.37 (13.5 examples/sec; 4.739 sec/batch)
2016-04-30 14:05:36.784475: step 2602, loss = 10.08 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 14:05:42.185432: step 2603, loss = 9.84 (11.8 examples/sec; 5.401 sec/batch)
2016-04-30 14:05:47.567015: step 2604, loss = 9.94 (11.9 examples/sec; 5.381 sec/batch)
2016-04-30 14:05:52.387297: step 2605, loss = 9.98 (13.3 examples/sec; 4.820 sec/batch)
2016-04-30 14:05:57.224680: step 2606, loss = 11.75 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 14:06:02.282280: step 2607, loss = 55.68 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 14:06:06.911763: step 2608, loss = 35.98 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 14:06:11.766911: step 2609, loss = 21.78 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 14:06:16.453780: step 2610, loss = 12.46 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 14:06:28.757475: step 2611, loss = 12.04 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 14:06:33.475897: step 2612, loss = 9.23 (13.6 examples/sec; 4.717 sec/batch)
2016-04-30 14:06:38.678911: step 2613, loss = 10.43 (12.3 examples/sec; 5.203 sec/batch)
2016-04-30 14:06:43.506603: step 2614, loss = 9.12 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 14:06:48.350849: step 2615, loss = 9.05 (13.2 examples/sec; 4.844 sec/batch)
2016-04-30 14:06:54.090943: step 2616, loss = 9.15 (11.1 examples/sec; 5.740 sec/batch)
2016-04-30 14:06:59.068941: step 2617, loss = 9.19 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 14:07:03.715935: step 2618, loss = 9.05 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 14:07:08.711258: step 2619, loss = 9.08 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 14:07:13.434323: step 2620, loss = 9.27 (13.6 examples/sec; 4.723 sec/batch)
2016-04-30 14:07:24.911121: step 2621, loss = 9.06 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 14:07:29.726976: step 2622, loss = 9.20 (13.3 examples/sec; 4.816 sec/batch)
2016-04-30 14:07:34.854043: step 2623, loss = 9.15 (12.5 examples/sec; 5.127 sec/batch)
2016-04-30 14:07:39.603456: step 2624, loss = 9.23 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 14:07:44.801270: step 2625, loss = 9.08 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 14:07:49.722969: step 2626, loss = 8.96 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 14:07:54.262261: step 2627, loss = 8.94 (14.1 examples/sec; 4.539 sec/batch)
2016-04-30 14:07:59.705559: step 2628, loss = 9.02 (11.8 examples/sec; 5.443 sec/batch)
2016-04-30 14:08:04.653714: step 2629, loss = 8.81 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 14:08:09.514217: step 2630, loss = 9.04 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 14:08:20.822741: step 2631, loss = 8.74 (14.0 examples/sec; 4.576 sec/batch)
2016-04-30 14:08:25.690858: step 2632, loss = 8.99 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 14:08:30.898535: step 2633, loss = 8.83 (12.3 examples/sec; 5.208 sec/batch)
2016-04-30 14:08:35.950912: step 2634, loss = 9.14 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 14:08:40.810529: step 2635, loss = 8.98 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 14:08:45.743867: step 2636, loss = 8.89 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 14:08:50.760451: step 2637, loss = 8.83 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 14:08:55.620683: step 2638, loss = 8.96 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 14:09:00.224492: step 2639, loss = 8.79 (13.9 examples/sec; 4.604 sec/batch)
2016-04-30 14:09:05.864466: step 2640, loss = 8.89 (11.3 examples/sec; 5.640 sec/batch)
2016-04-30 14:09:17.051682: step 2641, loss = 8.77 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 14:09:21.722294: step 2642, loss = 8.86 (13.7 examples/sec; 4.671 sec/batch)
2016-04-30 14:09:26.659654: step 2643, loss = 9.01 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 14:09:31.198540: step 2644, loss = 9.00 (14.1 examples/sec; 4.539 sec/batch)
2016-04-30 14:09:36.650220: step 2645, loss = 8.84 (11.7 examples/sec; 5.452 sec/batch)
2016-04-30 14:09:41.542872: step 2646, loss = 8.88 (13.1 examples/sec; 4.893 sec/batch)
2016-04-30 14:09:46.245407: step 2647, loss = 8.71 (13.6 examples/sec; 4.702 sec/batch)
2016-04-30 14:09:51.160347: step 2648, loss = 8.83 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 14:09:56.090341: step 2649, loss = 8.91 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 14:10:01.050295: step 2650, loss = 8.88 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 14:10:12.725786: step 2651, loss = 8.70 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 14:10:17.734627: step 2652, loss = 8.91 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 14:10:22.378503: step 2653, loss = 8.61 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 14:10:27.228718: step 2654, loss = 8.49 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 14:10:32.224538: step 2655, loss = 8.81 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 14:10:36.948657: step 2656, loss = 8.69 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 14:10:42.470923: step 2657, loss = 8.77 (11.6 examples/sec; 5.522 sec/batch)
2016-04-30 14:10:47.278992: step 2658, loss = 8.74 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 14:10:52.026394: step 2659, loss = 8.72 (13.5 examples/sec; 4.747 sec/batch)
2016-04-30 14:10:56.912444: step 2660, loss = 8.84 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 14:11:08.484060: step 2661, loss = 8.76 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 14:11:13.223809: step 2662, loss = 8.79 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 14:11:18.505717: step 2663, loss = 8.71 (12.1 examples/sec; 5.282 sec/batch)
2016-04-30 14:11:23.409824: step 2664, loss = 8.78 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 14:11:28.033019: step 2665, loss = 8.68 (13.8 examples/sec; 4.623 sec/batch)
2016-04-30 14:11:32.969693: step 2666, loss = 8.79 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 14:11:38.060068: step 2667, loss = 8.80 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 14:11:42.771781: step 2668, loss = 8.71 (13.6 examples/sec; 4.712 sec/batch)
2016-04-30 14:11:48.155737: step 2669, loss = 8.70 (11.9 examples/sec; 5.384 sec/batch)
2016-04-30 14:11:53.048908: step 2670, loss = 8.58 (13.1 examples/sec; 4.893 sec/batch)
2016-04-30 14:12:04.607424: step 2671, loss = 8.77 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 14:12:09.456183: step 2672, loss = 8.80 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 14:12:14.407956: step 2673, loss = 8.59 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 14:12:19.089667: step 2674, loss = 8.74 (13.7 examples/sec; 4.682 sec/batch)
2016-04-30 14:12:24.506533: step 2675, loss = 8.60 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 14:12:29.463908: step 2676, loss = 8.72 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 14:12:34.164875: step 2677, loss = 8.66 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 14:12:39.272591: step 2678, loss = 8.70 (12.5 examples/sec; 5.108 sec/batch)
2016-04-30 14:12:44.305775: step 2679, loss = 8.60 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 14:12:49.012737: step 2680, loss = 8.62 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 14:13:01.159451: step 2681, loss = 8.60 (13.9 examples/sec; 4.598 sec/batch)
2016-04-30 14:13:06.163959: step 2682, loss = 8.59 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 14:13:11.141587: step 2683, loss = 8.64 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 14:13:15.864760: step 2684, loss = 8.66 (13.6 examples/sec; 4.723 sec/batch)
2016-04-30 14:13:20.692927: step 2685, loss = 8.52 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 14:13:25.555423: step 2686, loss = 8.67 (13.2 examples/sec; 4.862 sec/batch)
2016-04-30 14:13:30.812634: step 2687, loss = 8.61 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 14:13:35.715361: step 2688, loss = 8.55 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 14:13:40.573521: step 2689, loss = 8.52 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 14:13:45.444032: step 2690, loss = 8.74 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 14:13:56.637098: step 2691, loss = 8.54 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 14:14:02.219445: step 2692, loss = 8.50 (11.5 examples/sec; 5.582 sec/batch)
2016-04-30 14:14:06.945895: step 2693, loss = 8.48 (13.5 examples/sec; 4.726 sec/batch)
2016-04-30 14:14:11.778395: step 2694, loss = 8.48 (13.2 examples/sec; 4.832 sec/batch)
2016-04-30 14:14:16.621733: step 2695, loss = 8.58 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 14:14:21.506941: step 2696, loss = 8.66 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 14:14:26.281251: step 2697, loss = 8.63 (13.4 examples/sec; 4.774 sec/batch)
2016-04-30 14:14:30.968967: step 2698, loss = 8.54 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 14:14:36.445429: step 2699, loss = 8.38 (11.7 examples/sec; 5.476 sec/batch)
2016-04-30 14:14:41.452459: step 2700, loss = 8.65 (12.8 examples/sec; 5.007 sec/batch)
2016-04-30 14:14:54.163551: step 2701, loss = 8.54 (14.7 examples/sec; 4.356 sec/batch)
2016-04-30 14:14:59.001431: step 2702, loss = 8.58 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 14:15:03.770882: step 2703, loss = 8.49 (13.4 examples/sec; 4.769 sec/batch)
2016-04-30 14:15:09.680948: step 2704, loss = 8.38 (10.8 examples/sec; 5.910 sec/batch)
2016-04-30 14:15:15.455512: step 2705, loss = 8.49 (11.1 examples/sec; 5.774 sec/batch)
2016-04-30 14:15:20.569052: step 2706, loss = 8.48 (12.5 examples/sec; 5.112 sec/batch)
2016-04-30 14:15:25.293726: step 2707, loss = 8.57 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 14:15:30.239128: step 2708, loss = 8.45 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 14:15:35.341257: step 2709, loss = 8.69 (12.5 examples/sec; 5.102 sec/batch)
2016-04-30 14:15:40.860293: step 2710, loss = 8.57 (11.6 examples/sec; 5.519 sec/batch)
2016-04-30 14:15:52.356472: step 2711, loss = 8.45 (14.1 examples/sec; 4.542 sec/batch)
2016-04-30 14:15:57.398432: step 2712, loss = 8.35 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 14:16:02.692131: step 2713, loss = 8.42 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 14:16:07.269749: step 2714, loss = 8.42 (14.0 examples/sec; 4.578 sec/batch)
2016-04-30 14:16:12.883345: step 2715, loss = 8.35 (11.4 examples/sec; 5.614 sec/batch)
2016-04-30 14:16:17.789874: step 2716, loss = 8.34 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 14:16:22.594300: step 2717, loss = 8.43 (13.3 examples/sec; 4.804 sec/batch)
2016-04-30 14:16:27.337913: step 2718, loss = 8.31 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 14:16:32.107553: step 2719, loss = 8.67 (13.4 examples/sec; 4.770 sec/batch)
2016-04-30 14:16:36.907776: step 2720, loss = 8.47 (13.3 examples/sec; 4.800 sec/batch)
2016-04-30 14:16:48.739961: step 2721, loss = 8.22 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 14:16:53.960857: step 2722, loss = 8.36 (12.3 examples/sec; 5.221 sec/batch)
2016-04-30 14:16:59.152641: step 2723, loss = 8.49 (12.3 examples/sec; 5.192 sec/batch)
2016-04-30 14:17:03.989631: step 2724, loss = 8.43 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 14:17:09.179206: step 2725, loss = 8.36 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 14:17:14.126618: step 2726, loss = 8.22 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 14:17:19.289087: step 2727, loss = 8.39 (12.4 examples/sec; 5.162 sec/batch)
2016-04-30 14:17:24.179631: step 2728, loss = 8.35 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 14:17:29.160659: step 2729, loss = 8.27 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 14:17:33.688122: step 2730, loss = 8.42 (14.1 examples/sec; 4.527 sec/batch)
2016-04-30 14:17:45.152516: step 2731, loss = 8.46 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 14:17:50.241692: step 2732, loss = 8.41 (12.6 examples/sec; 5.089 sec/batch)
2016-04-30 14:17:55.290882: step 2733, loss = 8.41 (12.7 examples/sec; 5.049 sec/batch)
2016-04-30 14:18:00.012104: step 2734, loss = 8.33 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 14:18:05.083985: step 2735, loss = 8.24 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 14:18:09.890602: step 2736, loss = 8.31 (13.3 examples/sec; 4.807 sec/batch)
2016-04-30 14:18:14.821275: step 2737, loss = 8.46 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 14:18:19.567643: step 2738, loss = 8.34 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 14:18:24.978287: step 2739, loss = 8.12 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 14:18:29.888487: step 2740, loss = 8.30 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 14:18:41.383482: step 2741, loss = 8.25 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 14:18:46.089859: step 2742, loss = 8.23 (13.6 examples/sec; 4.706 sec/batch)
2016-04-30 14:18:50.955759: step 2743, loss = 8.33 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 14:18:55.631257: step 2744, loss = 8.32 (13.7 examples/sec; 4.675 sec/batch)
2016-04-30 14:19:01.085305: step 2745, loss = 8.38 (11.7 examples/sec; 5.454 sec/batch)
2016-04-30 14:19:05.986104: step 2746, loss = 8.18 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 14:19:10.892898: step 2747, loss = 8.19 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 14:19:15.571562: step 2748, loss = 8.26 (13.7 examples/sec; 4.679 sec/batch)
2016-04-30 14:19:20.471028: step 2749, loss = 8.34 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 14:19:25.113096: step 2750, loss = 8.18 (13.8 examples/sec; 4.642 sec/batch)
2016-04-30 14:19:36.728580: step 2751, loss = 8.16 (14.7 examples/sec; 4.365 sec/batch)
2016-04-30 14:19:41.680021: step 2752, loss = 8.14 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 14:19:46.288538: step 2753, loss = 8.34 (13.9 examples/sec; 4.608 sec/batch)
2016-04-30 14:19:51.202944: step 2754, loss = 8.19 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 14:19:56.196542: step 2755, loss = 8.09 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 14:20:01.093665: step 2756, loss = 8.15 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 14:20:06.574699: step 2757, loss = 8.17 (11.7 examples/sec; 5.481 sec/batch)
2016-04-30 14:20:11.356967: step 2758, loss = 8.13 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 14:20:16.172102: step 2759, loss = 8.22 (13.3 examples/sec; 4.815 sec/batch)
2016-04-30 14:20:21.066545: step 2760, loss = 8.16 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 14:20:32.279247: step 2761, loss = 8.17 (13.5 examples/sec; 4.747 sec/batch)
2016-04-30 14:20:37.511953: step 2762, loss = 8.18 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 14:20:42.496793: step 2763, loss = 8.10 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 14:20:47.388875: step 2764, loss = 8.18 (13.1 examples/sec; 4.892 sec/batch)
2016-04-30 14:20:52.138847: step 2765, loss = 8.11 (13.5 examples/sec; 4.750 sec/batch)
2016-04-30 14:20:56.993827: step 2766, loss = 8.05 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 14:21:02.287124: step 2767, loss = 8.18 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 14:21:06.906625: step 2768, loss = 8.30 (13.9 examples/sec; 4.619 sec/batch)
2016-04-30 14:21:12.337499: step 2769, loss = 8.30 (11.8 examples/sec; 5.431 sec/batch)
2016-04-30 14:21:17.435706: step 2770, loss = 8.05 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 14:21:28.303052: step 2771, loss = 8.13 (14.6 examples/sec; 4.397 sec/batch)
2016-04-30 14:21:33.227182: step 2772, loss = 8.17 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 14:21:37.894978: step 2773, loss = 8.21 (13.7 examples/sec; 4.668 sec/batch)
2016-04-30 14:21:43.090356: step 2774, loss = 8.12 (12.3 examples/sec; 5.195 sec/batch)
2016-04-30 14:21:47.965826: step 2775, loss = 8.19 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 14:21:53.030939: step 2776, loss = 8.17 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 14:21:57.588436: step 2777, loss = 8.18 (14.0 examples/sec; 4.557 sec/batch)
2016-04-30 14:22:02.619670: step 2778, loss = 7.93 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 14:22:07.096179: step 2779, loss = 8.14 (14.3 examples/sec; 4.476 sec/batch)
2016-04-30 14:22:12.083981: step 2780, loss = 8.08 (12.8 examples/sec; 4.988 sec/batch)
2016-04-30 14:22:23.936898: step 2781, loss = 8.01 (13.7 examples/sec; 4.664 sec/batch)
2016-04-30 14:22:28.724301: step 2782, loss = 7.98 (13.4 examples/sec; 4.787 sec/batch)
2016-04-30 14:22:33.432946: step 2783, loss = 8.07 (13.6 examples/sec; 4.709 sec/batch)
2016-04-30 14:22:38.332886: step 2784, loss = 8.21 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 14:22:42.843699: step 2785, loss = 8.01 (14.2 examples/sec; 4.511 sec/batch)
2016-04-30 14:22:48.297115: step 2786, loss = 7.97 (11.7 examples/sec; 5.453 sec/batch)
2016-04-30 14:22:53.074362: step 2787, loss = 7.99 (13.4 examples/sec; 4.777 sec/batch)
2016-04-30 14:22:57.630139: step 2788, loss = 7.92 (14.0 examples/sec; 4.556 sec/batch)
2016-04-30 14:23:02.652872: step 2789, loss = 7.77 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 14:23:07.251120: step 2790, loss = 8.19 (13.9 examples/sec; 4.598 sec/batch)
2016-04-30 14:23:18.647806: step 2791, loss = 8.18 (14.6 examples/sec; 4.369 sec/batch)
2016-04-30 14:23:24.263750: step 2792, loss = 8.02 (11.4 examples/sec; 5.616 sec/batch)
2016-04-30 14:23:29.292436: step 2793, loss = 7.93 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 14:23:33.797064: step 2794, loss = 8.01 (14.2 examples/sec; 4.505 sec/batch)
2016-04-30 14:23:38.867519: step 2795, loss = 8.11 (12.6 examples/sec; 5.070 sec/batch)
2016-04-30 14:23:43.509377: step 2796, loss = 7.99 (13.8 examples/sec; 4.642 sec/batch)
2016-04-30 14:23:48.379363: step 2797, loss = 8.02 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 14:23:53.678045: step 2798, loss = 7.87 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 14:23:58.671236: step 2799, loss = 8.05 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 14:24:03.714449: step 2800, loss = 7.81 (12.7 examples/sec; 5.043 sec/batch)
2016-04-30 14:24:16.302850: step 2801, loss = 8.02 (11.9 examples/sec; 5.381 sec/batch)
2016-04-30 14:24:21.599428: step 2802, loss = 7.81 (12.1 examples/sec; 5.296 sec/batch)
2016-04-30 14:24:27.118272: step 2803, loss = 7.83 (11.6 examples/sec; 5.519 sec/batch)
2016-04-30 14:24:32.160441: step 2804, loss = 7.84 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 14:24:36.936151: step 2805, loss = 7.91 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 14:24:41.941579: step 2806, loss = 8.02 (12.8 examples/sec; 5.005 sec/batch)
2016-04-30 14:24:46.838096: step 2807, loss = 8.09 (13.1 examples/sec; 4.896 sec/batch)
2016-04-30 14:24:51.611046: step 2808, loss = 8.02 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 14:24:56.489226: step 2809, loss = 7.90 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 14:25:01.923042: step 2810, loss = 7.82 (11.8 examples/sec; 5.434 sec/batch)
2016-04-30 14:25:13.222586: step 2811, loss = 7.98 (14.3 examples/sec; 4.483 sec/batch)
2016-04-30 14:25:17.927310: step 2812, loss = 7.87 (13.6 examples/sec; 4.705 sec/batch)
2016-04-30 14:25:22.583007: step 2813, loss = 8.02 (13.7 examples/sec; 4.656 sec/batch)
2016-04-30 14:25:27.406465: step 2814, loss = 7.82 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 14:25:32.440730: step 2815, loss = 7.85 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 14:25:37.458757: step 2816, loss = 7.92 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 14:25:42.643735: step 2817, loss = 7.92 (12.3 examples/sec; 5.185 sec/batch)
2016-04-30 14:25:47.493144: step 2818, loss = 7.91 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 14:25:52.450492: step 2819, loss = 7.82 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 14:25:57.108101: step 2820, loss = 7.82 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 14:26:08.839300: step 2821, loss = 7.87 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 14:26:13.370171: step 2822, loss = 7.83 (14.1 examples/sec; 4.531 sec/batch)
2016-04-30 14:26:18.173254: step 2823, loss = 7.82 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 14:26:22.986250: step 2824, loss = 7.86 (13.3 examples/sec; 4.813 sec/batch)
2016-04-30 14:26:28.030458: step 2825, loss = 7.77 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 14:26:32.892789: step 2826, loss = 7.80 (13.2 examples/sec; 4.862 sec/batch)
2016-04-30 14:26:37.497402: step 2827, loss = 7.78 (13.9 examples/sec; 4.605 sec/batch)
2016-04-30 14:26:42.799828: step 2828, loss = 7.84 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 14:26:47.623117: step 2829, loss = 7.94 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 14:26:52.139918: step 2830, loss = 7.89 (14.2 examples/sec; 4.517 sec/batch)
2016-04-30 14:27:03.404595: step 2831, loss = 7.83 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 14:27:08.161093: step 2832, loss = 7.85 (13.5 examples/sec; 4.756 sec/batch)
2016-04-30 14:27:13.291845: step 2833, loss = 7.77 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 14:27:18.103523: step 2834, loss = 7.76 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 14:27:22.940100: step 2835, loss = 7.75 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 14:27:27.465305: step 2836, loss = 7.84 (14.1 examples/sec; 4.525 sec/batch)
2016-04-30 14:27:32.326021: step 2837, loss = 7.83 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 14:27:37.059525: step 2838, loss = 7.88 (13.5 examples/sec; 4.733 sec/batch)
2016-04-30 14:27:42.160716: step 2839, loss = 7.87 (12.5 examples/sec; 5.101 sec/batch)
2016-04-30 14:27:47.872939: step 2840, loss = 7.73 (11.2 examples/sec; 5.712 sec/batch)
2016-04-30 14:27:58.707897: step 2841, loss = 7.75 (14.6 examples/sec; 4.391 sec/batch)
2016-04-30 14:28:03.615964: step 2842, loss = 7.81 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 14:28:08.528487: step 2843, loss = 7.71 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 14:28:13.092616: step 2844, loss = 7.57 (14.0 examples/sec; 4.564 sec/batch)
2016-04-30 14:28:18.435709: step 2845, loss = 7.70 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 14:28:23.375052: step 2846, loss = 7.76 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 14:28:27.979286: step 2847, loss = 7.78 (13.9 examples/sec; 4.604 sec/batch)
2016-04-30 14:28:32.800734: step 2848, loss = 7.67 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 14:28:37.367294: step 2849, loss = 7.80 (14.0 examples/sec; 4.566 sec/batch)
2016-04-30 14:28:42.228494: step 2850, loss = 7.69 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 14:28:53.784179: step 2851, loss = 7.42 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 14:28:58.405705: step 2852, loss = 7.74 (13.8 examples/sec; 4.621 sec/batch)
2016-04-30 14:29:03.303139: step 2853, loss = 7.61 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 14:29:08.267021: step 2854, loss = 7.77 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 14:29:12.984064: step 2855, loss = 7.68 (13.6 examples/sec; 4.717 sec/batch)
2016-04-30 14:29:17.697517: step 2856, loss = 7.70 (13.6 examples/sec; 4.713 sec/batch)
2016-04-30 14:29:22.230759: step 2857, loss = 7.63 (14.1 examples/sec; 4.533 sec/batch)
2016-04-30 14:29:27.650465: step 2858, loss = 7.67 (11.8 examples/sec; 5.420 sec/batch)
2016-04-30 14:29:32.436166: step 2859, loss = 7.62 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 14:29:37.187044: step 2860, loss = 7.76 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 14:29:48.189999: step 2861, loss = 7.75 (14.1 examples/sec; 4.547 sec/batch)
2016-04-30 14:29:53.152913: step 2862, loss = 7.50 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 14:29:58.411389: step 2863, loss = 7.69 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 14:30:03.646317: step 2864, loss = 7.48 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 14:30:08.492482: step 2865, loss = 7.53 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 14:30:13.065310: step 2866, loss = 7.61 (14.0 examples/sec; 4.573 sec/batch)
2016-04-30 14:30:17.908479: step 2867, loss = 7.65 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 14:30:22.526036: step 2868, loss = 7.69 (13.9 examples/sec; 4.617 sec/batch)
2016-04-30 14:30:27.447350: step 2869, loss = 7.78 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 14:30:32.979685: step 2870, loss = 7.70 (11.6 examples/sec; 5.532 sec/batch)
2016-04-30 14:30:44.411892: step 2871, loss = 7.80 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 14:30:49.427938: step 2872, loss = 7.62 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 14:30:54.369501: step 2873, loss = 7.51 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 14:30:59.279365: step 2874, loss = 7.60 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 14:31:04.763400: step 2875, loss = 7.51 (11.7 examples/sec; 5.484 sec/batch)
2016-04-30 14:31:09.541500: step 2876, loss = 7.65 (13.4 examples/sec; 4.778 sec/batch)
2016-04-30 14:31:14.679617: step 2877, loss = 7.55 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 14:31:19.406828: step 2878, loss = 7.66 (13.5 examples/sec; 4.727 sec/batch)
2016-04-30 14:31:24.453212: step 2879, loss = 7.72 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 14:31:29.384747: step 2880, loss = 7.53 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 14:31:40.648125: step 2881, loss = 7.40 (14.1 examples/sec; 4.545 sec/batch)
2016-04-30 14:31:45.442982: step 2882, loss = 7.50 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 14:31:50.546377: step 2883, loss = 7.67 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 14:31:55.248050: step 2884, loss = 7.58 (13.6 examples/sec; 4.702 sec/batch)
2016-04-30 14:32:00.034079: step 2885, loss = 7.48 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 14:32:05.039832: step 2886, loss = 7.62 (12.8 examples/sec; 5.006 sec/batch)
2016-04-30 14:32:10.290233: step 2887, loss = 7.45 (12.2 examples/sec; 5.250 sec/batch)
2016-04-30 14:32:15.262588: step 2888, loss = 7.44 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 14:32:20.131584: step 2889, loss = 7.42 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 14:32:24.806301: step 2890, loss = 7.61 (13.7 examples/sec; 4.675 sec/batch)
2016-04-30 14:32:36.059580: step 2891, loss = 7.51 (13.9 examples/sec; 4.598 sec/batch)
2016-04-30 14:32:40.871613: step 2892, loss = 7.58 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 14:32:46.284433: step 2893, loss = 7.35 (11.8 examples/sec; 5.413 sec/batch)
2016-04-30 14:32:51.184529: step 2894, loss = 7.53 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 14:32:56.212086: step 2895, loss = 7.59 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 14:33:01.048012: step 2896, loss = 7.41 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 14:33:05.991135: step 2897, loss = 7.66 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 14:33:10.734015: step 2898, loss = 7.50 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 14:33:16.182665: step 2899, loss = 7.37 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 14:33:21.073900: step 2900, loss = 7.40 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 14:33:32.210039: step 2901, loss = 7.38 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 14:33:37.179434: step 2902, loss = 7.46 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 14:33:42.040212: step 2903, loss = 7.34 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 14:33:46.688929: step 2904, loss = 7.38 (13.8 examples/sec; 4.649 sec/batch)
2016-04-30 14:33:52.095491: step 2905, loss = 7.31 (11.8 examples/sec; 5.406 sec/batch)
2016-04-30 14:33:56.990942: step 2906, loss = 7.42 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 14:34:01.985169: step 2907, loss = 7.40 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 14:34:06.806704: step 2908, loss = 7.42 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 14:34:11.914384: step 2909, loss = 7.47 (12.5 examples/sec; 5.108 sec/batch)
2016-04-30 14:34:16.606633: step 2910, loss = 7.25 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 14:34:28.211234: step 2911, loss = 7.46 (14.4 examples/sec; 4.448 sec/batch)
2016-04-30 14:34:32.874459: step 2912, loss = 7.53 (13.7 examples/sec; 4.663 sec/batch)
2016-04-30 14:34:37.709351: step 2913, loss = 7.33 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 14:34:42.635317: step 2914, loss = 7.55 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 14:34:47.518659: step 2915, loss = 7.41 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 14:34:52.399627: step 2916, loss = 7.32 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 14:34:58.032086: step 2917, loss = 7.48 (11.4 examples/sec; 5.632 sec/batch)
2016-04-30 14:35:03.052014: step 2918, loss = 7.40 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 14:35:07.521048: step 2919, loss = 7.42 (14.3 examples/sec; 4.469 sec/batch)
2016-04-30 14:35:12.396041: step 2920, loss = 7.43 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 14:35:23.896068: step 2921, loss = 7.21 (13.6 examples/sec; 4.719 sec/batch)
2016-04-30 14:35:29.507301: step 2922, loss = 7.34 (11.4 examples/sec; 5.611 sec/batch)
2016-04-30 14:35:34.210003: step 2923, loss = 7.37 (13.6 examples/sec; 4.703 sec/batch)
2016-04-30 14:35:39.113733: step 2924, loss = 7.38 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 14:35:43.619889: step 2925, loss = 7.32 (14.2 examples/sec; 4.506 sec/batch)
2016-04-30 14:35:48.501647: step 2926, loss = 7.47 (13.1 examples/sec; 4.882 sec/batch)
2016-04-30 14:35:53.392216: step 2927, loss = 7.28 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 14:35:57.955467: step 2928, loss = 7.37 (14.0 examples/sec; 4.563 sec/batch)
2016-04-30 14:36:03.584408: step 2929, loss = 7.19 (11.4 examples/sec; 5.629 sec/batch)
2016-04-30 14:36:08.458914: step 2930, loss = 7.35 (13.1 examples/sec; 4.874 sec/batch)
2016-04-30 14:36:19.793869: step 2931, loss = 7.10 (13.8 examples/sec; 4.651 sec/batch)
2016-04-30 14:36:24.822962: step 2932, loss = 7.41 (12.7 examples/sec; 5.028 sec/batch)
2016-04-30 14:36:29.767275: step 2933, loss = 7.29 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 14:36:35.183890: step 2934, loss = 7.44 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 14:36:39.933175: step 2935, loss = 7.26 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 14:36:44.791710: step 2936, loss = 7.02 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 14:36:49.470404: step 2937, loss = 7.23 (13.7 examples/sec; 4.679 sec/batch)
2016-04-30 14:36:54.328496: step 2938, loss = 7.18 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 14:36:58.877565: step 2939, loss = 7.36 (14.1 examples/sec; 4.549 sec/batch)
2016-04-30 14:37:03.805791: step 2940, loss = 7.18 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 14:37:15.809474: step 2941, loss = 7.32 (14.6 examples/sec; 4.380 sec/batch)
2016-04-30 14:37:20.802275: step 2942, loss = 7.26 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 14:37:25.456629: step 2943, loss = 7.20 (13.8 examples/sec; 4.654 sec/batch)
2016-04-30 14:37:30.383753: step 2944, loss = 7.12 (13.0 examples/sec; 4.927 sec/batch)
2016-04-30 14:37:35.298104: step 2945, loss = 7.29 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 14:37:40.410525: step 2946, loss = 7.25 (12.5 examples/sec; 5.112 sec/batch)
2016-04-30 14:37:45.213382: step 2947, loss = 7.16 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 14:37:50.243304: step 2948, loss = 7.35 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 14:37:54.981045: step 2949, loss = 7.22 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 14:37:59.836466: step 2950, loss = 7.15 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 14:38:11.443584: step 2951, loss = 7.26 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 14:38:16.568254: step 2952, loss = 7.04 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 14:38:21.516184: step 2953, loss = 7.31 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 14:38:26.277444: step 2954, loss = 7.23 (13.4 examples/sec; 4.761 sec/batch)
2016-04-30 14:38:30.965689: step 2955, loss = 7.23 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 14:38:36.025999: step 2956, loss = 7.17 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 14:38:40.653607: step 2957, loss = 7.26 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 14:38:45.993414: step 2958, loss = 7.29 (12.0 examples/sec; 5.340 sec/batch)
2016-04-30 14:38:50.898264: step 2959, loss = 6.93 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 14:38:55.632405: step 2960, loss = 7.21 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 14:39:06.997690: step 2961, loss = 7.15 (13.8 examples/sec; 4.621 sec/batch)
2016-04-30 14:39:11.793022: step 2962, loss = 7.18 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 14:39:16.500925: step 2963, loss = 7.20 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 14:39:21.932938: step 2964, loss = 7.15 (11.8 examples/sec; 5.432 sec/batch)
2016-04-30 14:39:27.113354: step 2965, loss = 7.05 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 14:39:32.065904: step 2966, loss = 6.95 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 14:39:36.847281: step 2967, loss = 7.14 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 14:39:41.922310: step 2968, loss = 7.29 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 14:39:46.606327: step 2969, loss = 6.91 (13.7 examples/sec; 4.684 sec/batch)
2016-04-30 14:39:51.987512: step 2970, loss = 7.08 (11.9 examples/sec; 5.381 sec/batch)
2016-04-30 14:40:03.454614: step 2971, loss = 7.04 (13.0 examples/sec; 4.942 sec/batch)
2016-04-30 14:40:08.289312: step 2972, loss = 7.10 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 14:40:12.853660: step 2973, loss = 7.03 (14.0 examples/sec; 4.564 sec/batch)
2016-04-30 14:40:17.707824: step 2974, loss = 7.14 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 14:40:22.344419: step 2975, loss = 7.19 (13.8 examples/sec; 4.637 sec/batch)
2016-04-30 14:40:27.804234: step 2976, loss = 7.03 (11.7 examples/sec; 5.460 sec/batch)
2016-04-30 14:40:32.584457: step 2977, loss = 7.00 (13.4 examples/sec; 4.780 sec/batch)
2016-04-30 14:40:37.378596: step 2978, loss = 7.13 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 14:40:42.360536: step 2979, loss = 6.96 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 14:40:47.002487: step 2980, loss = 6.86 (13.8 examples/sec; 4.642 sec/batch)
2016-04-30 14:40:58.545401: step 2981, loss = 7.03 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 14:41:03.689189: step 2982, loss = 7.09 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 14:41:08.559637: step 2983, loss = 7.11 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 14:41:13.289161: step 2984, loss = 6.95 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 14:41:18.282754: step 2985, loss = 7.07 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 14:41:23.020524: step 2986, loss = 7.09 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 14:41:27.721266: step 2987, loss = 7.16 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 14:41:33.033058: step 2988, loss = 7.04 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 14:41:37.654735: step 2989, loss = 6.90 (13.8 examples/sec; 4.622 sec/batch)
2016-04-30 14:41:42.586664: step 2990, loss = 6.99 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 14:41:53.911264: step 2991, loss = 6.91 (14.0 examples/sec; 4.571 sec/batch)
2016-04-30 14:41:58.563334: step 2992, loss = 6.86 (13.8 examples/sec; 4.652 sec/batch)
2016-04-30 14:42:04.227039: step 2993, loss = 7.16 (11.3 examples/sec; 5.664 sec/batch)
2016-04-30 14:42:09.174178: step 2994, loss = 7.01 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 14:42:13.788563: step 2995, loss = 6.96 (13.9 examples/sec; 4.614 sec/batch)
2016-04-30 14:42:18.616208: step 2996, loss = 7.08 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 14:42:23.400822: step 2997, loss = 6.89 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 14:42:28.113298: step 2998, loss = 6.89 (13.6 examples/sec; 4.712 sec/batch)
2016-04-30 14:42:33.136200: step 2999, loss = 6.85 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 14:42:38.385491: step 3000, loss = 7.06 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 14:42:49.322922: step 3001, loss = 6.99 (14.1 examples/sec; 4.547 sec/batch)
2016-04-30 14:42:54.476693: step 3002, loss = 6.99 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 14:42:59.167788: step 3003, loss = 6.95 (13.6 examples/sec; 4.691 sec/batch)
2016-04-30 14:43:04.306503: step 3004, loss = 7.02 (12.5 examples/sec; 5.139 sec/batch)
2016-04-30 14:43:09.866721: step 3005, loss = 7.09 (11.5 examples/sec; 5.560 sec/batch)
2016-04-30 14:43:14.856108: step 3006, loss = 6.84 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 14:43:19.715226: step 3007, loss = 6.99 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 14:43:24.688438: step 3008, loss = 6.97 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 14:43:29.672673: step 3009, loss = 7.05 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 14:43:34.119484: step 3010, loss = 6.90 (14.4 examples/sec; 4.447 sec/batch)
2016-04-30 14:43:45.953975: step 3011, loss = 6.85 (12.2 examples/sec; 5.267 sec/batch)
2016-04-30 14:43:51.013726: step 3012, loss = 6.87 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 14:43:55.687727: step 3013, loss = 7.06 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 14:44:00.816975: step 3014, loss = 7.00 (12.5 examples/sec; 5.129 sec/batch)
2016-04-30 14:44:05.871225: step 3015, loss = 7.04 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 14:44:10.721810: step 3016, loss = 6.93 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 14:44:16.153692: step 3017, loss = 6.67 (11.8 examples/sec; 5.432 sec/batch)
2016-04-30 14:44:21.156676: step 3018, loss = 7.12 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 14:44:25.937919: step 3019, loss = 6.82 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 14:44:30.763029: step 3020, loss = 6.92 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 14:44:41.963941: step 3021, loss = 7.13 (13.7 examples/sec; 4.657 sec/batch)
2016-04-30 14:44:46.663362: step 3022, loss = 6.84 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 14:44:52.098748: step 3023, loss = 6.78 (11.8 examples/sec; 5.435 sec/batch)
2016-04-30 14:44:57.057519: step 3024, loss = 6.85 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 14:45:01.928424: step 3025, loss = 6.88 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 14:45:06.901425: step 3026, loss = 6.78 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 14:45:11.878061: step 3027, loss = 6.95 (12.9 examples/sec; 4.977 sec/batch)
2016-04-30 14:45:16.453871: step 3028, loss = 6.84 (14.0 examples/sec; 4.576 sec/batch)
2016-04-30 14:45:22.018495: step 3029, loss = 6.87 (11.5 examples/sec; 5.565 sec/batch)
2016-04-30 14:45:26.990704: step 3030, loss = 6.89 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 14:45:38.057161: step 3031, loss = 6.81 (14.2 examples/sec; 4.516 sec/batch)
2016-04-30 14:45:42.793780: step 3032, loss = 6.94 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 14:45:47.667604: step 3033, loss = 6.94 (13.1 examples/sec; 4.874 sec/batch)
2016-04-30 14:45:52.280040: step 3034, loss = 6.73 (13.9 examples/sec; 4.612 sec/batch)
2016-04-30 14:45:57.670403: step 3035, loss = 6.72 (11.9 examples/sec; 5.390 sec/batch)
2016-04-30 14:46:02.693549: step 3036, loss = 6.70 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 14:46:07.264576: step 3037, loss = 6.84 (14.0 examples/sec; 4.571 sec/batch)
2016-04-30 14:46:12.380124: step 3038, loss = 6.86 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 14:46:17.209500: step 3039, loss = 6.69 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 14:46:21.912434: step 3040, loss = 6.84 (13.6 examples/sec; 4.703 sec/batch)
2016-04-30 14:46:33.544452: step 3041, loss = 6.73 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 14:46:38.434888: step 3042, loss = 6.61 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 14:46:42.938716: step 3043, loss = 6.94 (14.2 examples/sec; 4.504 sec/batch)
2016-04-30 14:46:47.673719: step 3044, loss = 6.79 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 14:46:52.232892: step 3045, loss = 6.62 (14.0 examples/sec; 4.559 sec/batch)
2016-04-30 14:46:57.206455: step 3046, loss = 6.79 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 14:47:03.067988: step 3047, loss = 6.81 (10.9 examples/sec; 5.861 sec/batch)
2016-04-30 14:47:07.978136: step 3048, loss = 6.79 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 14:47:13.376517: step 3049, loss = 6.75 (11.9 examples/sec; 5.398 sec/batch)
2016-04-30 14:47:18.424942: step 3050, loss = 6.61 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 14:47:29.717373: step 3051, loss = 6.78 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 14:47:35.274864: step 3052, loss = 6.88 (11.5 examples/sec; 5.557 sec/batch)
2016-04-30 14:47:40.100597: step 3053, loss = 6.99 (13.3 examples/sec; 4.826 sec/batch)
2016-04-30 14:47:45.209701: step 3054, loss = 6.68 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 14:47:50.084623: step 3055, loss = 6.90 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 14:47:54.902481: step 3056, loss = 6.65 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 14:47:59.698012: step 3057, loss = 6.69 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 14:48:04.548510: step 3058, loss = 6.73 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 14:48:09.994219: step 3059, loss = 6.79 (11.8 examples/sec; 5.446 sec/batch)
2016-04-30 14:48:14.823454: step 3060, loss = 6.75 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 14:48:25.693880: step 3061, loss = 6.75 (14.5 examples/sec; 4.422 sec/batch)
2016-04-30 14:48:30.550179: step 3062, loss = 6.70 (13.2 examples/sec; 4.856 sec/batch)
2016-04-30 14:48:35.389398: step 3063, loss = 6.82 (13.2 examples/sec; 4.839 sec/batch)
2016-04-30 14:48:40.606290: step 3064, loss = 6.62 (12.3 examples/sec; 5.217 sec/batch)
2016-04-30 14:48:45.523604: step 3065, loss = 6.71 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 14:48:50.484851: step 3066, loss = 6.59 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 14:48:55.189236: step 3067, loss = 6.53 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 14:49:00.138628: step 3068, loss = 6.81 (12.9 examples/sec; 4.949 sec/batch)
2016-04-30 14:49:05.025915: step 3069, loss = 6.73 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 14:49:09.891255: step 3070, loss = 6.70 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 14:49:21.761427: step 3071, loss = 6.68 (14.0 examples/sec; 4.577 sec/batch)
2016-04-30 14:49:26.714179: step 3072, loss = 6.65 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 14:49:31.398145: step 3073, loss = 6.74 (13.7 examples/sec; 4.684 sec/batch)
2016-04-30 14:49:36.378698: step 3074, loss = 6.72 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 14:49:41.338063: step 3075, loss = 6.72 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 14:49:46.596468: step 3076, loss = 6.72 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 14:49:51.312669: step 3077, loss = 6.65 (13.6 examples/sec; 4.716 sec/batch)
2016-04-30 14:49:55.964251: step 3078, loss = 6.67 (13.8 examples/sec; 4.651 sec/batch)
2016-04-30 14:50:01.014998: step 3079, loss = 6.76 (12.7 examples/sec; 5.051 sec/batch)
2016-04-30 14:50:05.872457: step 3080, loss = 6.69 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 14:50:16.620426: step 3081, loss = 6.54 (14.4 examples/sec; 4.454 sec/batch)
2016-04-30 14:50:22.072453: step 3082, loss = 6.39 (11.7 examples/sec; 5.452 sec/batch)
2016-04-30 14:50:27.097933: step 3083, loss = 6.60 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 14:50:31.696921: step 3084, loss = 6.79 (13.9 examples/sec; 4.599 sec/batch)
2016-04-30 14:50:36.625692: step 3085, loss = 6.70 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 14:50:41.485897: step 3086, loss = 6.71 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 14:50:46.097154: step 3087, loss = 6.67 (13.9 examples/sec; 4.611 sec/batch)
2016-04-30 14:50:51.685943: step 3088, loss = 6.72 (11.5 examples/sec; 5.589 sec/batch)
2016-04-30 14:50:56.534007: step 3089, loss = 6.58 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 14:51:01.550726: step 3090, loss = 6.76 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 14:51:12.652784: step 3091, loss = 6.58 (14.2 examples/sec; 4.493 sec/batch)
2016-04-30 14:51:17.576392: step 3092, loss = 6.69 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 14:51:22.276605: step 3093, loss = 6.71 (13.6 examples/sec; 4.700 sec/batch)
2016-04-30 14:51:27.704334: step 3094, loss = 6.62 (11.8 examples/sec; 5.428 sec/batch)
2016-04-30 14:51:32.505685: step 3095, loss = 6.44 (13.3 examples/sec; 4.801 sec/batch)
2016-04-30 14:51:37.136809: step 3096, loss = 6.57 (13.8 examples/sec; 4.631 sec/batch)
2016-04-30 14:51:42.160451: step 3097, loss = 6.65 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 14:51:46.859881: step 3098, loss = 6.63 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 14:51:51.899937: step 3099, loss = 6.44 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 14:51:57.073673: step 3100, loss = 6.66 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 14:52:08.831325: step 3101, loss = 6.45 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 14:52:13.597808: step 3102, loss = 6.56 (13.4 examples/sec; 4.766 sec/batch)
2016-04-30 14:52:18.409650: step 3103, loss = 6.51 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 14:52:22.996895: step 3104, loss = 6.47 (14.0 examples/sec; 4.587 sec/batch)
2016-04-30 14:52:27.849788: step 3105, loss = 6.49 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 14:52:33.144257: step 3106, loss = 6.50 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 14:52:37.732443: step 3107, loss = 6.53 (13.9 examples/sec; 4.588 sec/batch)
2016-04-30 14:52:42.812890: step 3108, loss = 6.52 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 14:52:47.693334: step 3109, loss = 6.51 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 14:52:52.219589: step 3110, loss = 6.55 (14.1 examples/sec; 4.526 sec/batch)
2016-04-30 14:53:03.901758: step 3111, loss = 6.70 (12.2 examples/sec; 5.256 sec/batch)
2016-04-30 14:53:08.780014: step 3112, loss = 6.62 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 14:53:13.471530: step 3113, loss = 6.60 (13.6 examples/sec; 4.691 sec/batch)
2016-04-30 14:53:18.265048: step 3114, loss = 6.50 (13.4 examples/sec; 4.793 sec/batch)
2016-04-30 14:53:22.985910: step 3115, loss = 6.64 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 14:53:28.026997: step 3116, loss = 6.61 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 14:53:32.935588: step 3117, loss = 6.57 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 14:53:37.976295: step 3118, loss = 6.63 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 14:53:42.711108: step 3119, loss = 6.54 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 14:53:47.561805: step 3120, loss = 6.49 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 14:53:58.670295: step 3121, loss = 6.27 (14.0 examples/sec; 4.581 sec/batch)
2016-04-30 14:54:03.766575: step 3122, loss = 6.40 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 14:54:08.625640: step 3123, loss = 6.43 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 14:54:13.679431: step 3124, loss = 6.43 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 14:54:18.654795: step 3125, loss = 6.55 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 14:54:23.619465: step 3126, loss = 6.42 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 14:54:28.253336: step 3127, loss = 6.55 (13.8 examples/sec; 4.634 sec/batch)
2016-04-30 14:54:33.118395: step 3128, loss = 6.43 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 14:54:37.725394: step 3129, loss = 6.25 (13.9 examples/sec; 4.607 sec/batch)
2016-04-30 14:54:43.157787: step 3130, loss = 6.45 (11.8 examples/sec; 5.432 sec/batch)
2016-04-30 14:54:54.272245: step 3131, loss = 6.22 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 14:54:58.931668: step 3132, loss = 6.29 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 14:55:04.052464: step 3133, loss = 6.48 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 14:55:08.916548: step 3134, loss = 6.41 (13.2 examples/sec; 4.864 sec/batch)
2016-04-30 14:55:13.656991: step 3135, loss = 6.27 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 14:55:19.021255: step 3136, loss = 6.37 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 14:55:23.864966: step 3137, loss = 6.28 (13.2 examples/sec; 4.844 sec/batch)
2016-04-30 14:55:28.398488: step 3138, loss = 6.46 (14.1 examples/sec; 4.533 sec/batch)
2016-04-30 14:55:33.215533: step 3139, loss = 6.39 (13.3 examples/sec; 4.817 sec/batch)
2016-04-30 14:55:37.893664: step 3140, loss = 6.30 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 14:55:49.751429: step 3141, loss = 6.49 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 14:55:54.624966: step 3142, loss = 6.30 (13.1 examples/sec; 4.873 sec/batch)
2016-04-30 14:55:59.644966: step 3143, loss = 6.31 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 14:56:04.871476: step 3144, loss = 6.40 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 14:56:09.715643: step 3145, loss = 6.29 (13.2 examples/sec; 4.844 sec/batch)
2016-04-30 14:56:14.507318: step 3146, loss = 6.43 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 14:56:19.205147: step 3147, loss = 6.12 (13.6 examples/sec; 4.698 sec/batch)
2016-04-30 14:56:24.742208: step 3148, loss = 6.33 (11.6 examples/sec; 5.537 sec/batch)
2016-04-30 14:56:29.883485: step 3149, loss = 6.38 (12.4 examples/sec; 5.141 sec/batch)
2016-04-30 14:56:34.785411: step 3150, loss = 6.28 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 14:56:46.052440: step 3151, loss = 6.31 (13.8 examples/sec; 4.640 sec/batch)
2016-04-30 14:56:50.970981: step 3152, loss = 6.21 (13.0 examples/sec; 4.918 sec/batch)
2016-04-30 14:56:56.264807: step 3153, loss = 6.26 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 14:57:01.473479: step 3154, loss = 6.46 (12.3 examples/sec; 5.209 sec/batch)
2016-04-30 14:57:06.458446: step 3155, loss = 6.24 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 14:57:11.162582: step 3156, loss = 6.17 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 14:57:15.894711: step 3157, loss = 6.21 (13.5 examples/sec; 4.732 sec/batch)
2016-04-30 14:57:20.911023: step 3158, loss = 6.22 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 14:57:25.578542: step 3159, loss = 6.40 (13.7 examples/sec; 4.667 sec/batch)
2016-04-30 14:57:30.943517: step 3160, loss = 6.26 (11.9 examples/sec; 5.365 sec/batch)
2016-04-30 14:57:42.147605: step 3161, loss = 6.12 (13.7 examples/sec; 4.661 sec/batch)
2016-04-30 14:57:46.781219: step 3162, loss = 6.31 (13.8 examples/sec; 4.634 sec/batch)
2016-04-30 14:57:51.716454: step 3163, loss = 6.27 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 14:57:56.452932: step 3164, loss = 6.25 (13.5 examples/sec; 4.736 sec/batch)
2016-04-30 14:58:02.242757: step 3165, loss = 6.38 (11.1 examples/sec; 5.790 sec/batch)
2016-04-30 14:58:07.184498: step 3166, loss = 6.16 (13.0 examples/sec; 4.942 sec/batch)
2016-04-30 14:58:12.146038: step 3167, loss = 6.31 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 14:58:16.928358: step 3168, loss = 6.29 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 14:58:21.972449: step 3169, loss = 6.28 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 14:58:26.817158: step 3170, loss = 6.43 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 14:58:38.800419: step 3171, loss = 6.39 (13.3 examples/sec; 4.809 sec/batch)
2016-04-30 14:58:43.424106: step 3172, loss = 6.27 (13.8 examples/sec; 4.624 sec/batch)
2016-04-30 14:58:48.428416: step 3173, loss = 6.36 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 14:58:53.279145: step 3174, loss = 6.16 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 14:58:58.065219: step 3175, loss = 6.15 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 14:59:03.030278: step 3176, loss = 6.26 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 14:59:08.067508: step 3177, loss = 6.12 (12.7 examples/sec; 5.037 sec/batch)
2016-04-30 14:59:12.857077: step 3178, loss = 6.22 (13.4 examples/sec; 4.789 sec/batch)
2016-04-30 14:59:17.727313: step 3179, loss = 6.20 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 14:59:22.490529: step 3180, loss = 6.26 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 14:59:33.898583: step 3181, loss = 6.19 (13.9 examples/sec; 4.600 sec/batch)
2016-04-30 14:59:38.944708: step 3182, loss = 6.29 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 14:59:44.014264: step 3183, loss = 6.18 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 14:59:48.778219: step 3184, loss = 6.19 (13.4 examples/sec; 4.764 sec/batch)
2016-04-30 14:59:53.513580: step 3185, loss = 6.13 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 14:59:58.107946: step 3186, loss = 6.15 (13.9 examples/sec; 4.594 sec/batch)
2016-04-30 15:00:03.191051: step 3187, loss = 6.18 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 15:00:07.939863: step 3188, loss = 6.09 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 15:00:13.217811: step 3189, loss = 6.19 (12.1 examples/sec; 5.278 sec/batch)
2016-04-30 15:00:18.069869: step 3190, loss = 6.08 (13.2 examples/sec; 4.852 sec/batch)
2016-04-30 15:00:29.156150: step 3191, loss = 6.20 (14.1 examples/sec; 4.523 sec/batch)
2016-04-30 15:00:33.883557: step 3192, loss = 6.13 (13.5 examples/sec; 4.727 sec/batch)
2016-04-30 15:00:38.991775: step 3193, loss = 6.14 (12.5 examples/sec; 5.108 sec/batch)
2016-04-30 15:00:43.673272: step 3194, loss = 6.33 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 15:00:49.405840: step 3195, loss = 6.14 (11.2 examples/sec; 5.732 sec/batch)
2016-04-30 15:00:54.297216: step 3196, loss = 6.21 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 15:00:58.772206: step 3197, loss = 6.18 (14.3 examples/sec; 4.475 sec/batch)
2016-04-30 15:01:03.890629: step 3198, loss = 6.24 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 15:01:08.817172: step 3199, loss = 6.05 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 15:01:13.446810: step 3200, loss = 6.29 (13.8 examples/sec; 4.630 sec/batch)
2016-04-30 15:01:25.284875: step 3201, loss = 6.19 (14.1 examples/sec; 4.524 sec/batch)
2016-04-30 15:01:30.373379: step 3202, loss = 6.13 (12.6 examples/sec; 5.088 sec/batch)
2016-04-30 15:01:34.982812: step 3203, loss = 6.31 (13.9 examples/sec; 4.609 sec/batch)
2016-04-30 15:01:39.823151: step 3204, loss = 5.95 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 15:01:44.948468: step 3205, loss = 6.12 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 15:01:49.615760: step 3206, loss = 5.98 (13.7 examples/sec; 4.667 sec/batch)
2016-04-30 15:01:55.078608: step 3207, loss = 6.15 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 15:02:00.015985: step 3208, loss = 6.00 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 15:02:05.149888: step 3209, loss = 6.00 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 15:02:10.026129: step 3210, loss = 6.07 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 15:02:21.493088: step 3211, loss = 6.13 (13.4 examples/sec; 4.759 sec/batch)
2016-04-30 15:02:27.012260: step 3212, loss = 6.06 (11.6 examples/sec; 5.519 sec/batch)
2016-04-30 15:02:31.703773: step 3213, loss = 6.05 (13.6 examples/sec; 4.691 sec/batch)
2016-04-30 15:02:36.553211: step 3214, loss = 6.07 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 15:02:41.129892: step 3215, loss = 6.11 (14.0 examples/sec; 4.577 sec/batch)
2016-04-30 15:02:45.920461: step 3216, loss = 6.09 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 15:02:50.829596: step 3217, loss = 6.31 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 15:02:55.453478: step 3218, loss = 6.01 (13.8 examples/sec; 4.624 sec/batch)
2016-04-30 15:03:01.046627: step 3219, loss = 6.06 (11.4 examples/sec; 5.593 sec/batch)
2016-04-30 15:03:06.059239: step 3220, loss = 5.97 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 15:03:17.044456: step 3221, loss = 5.98 (14.3 examples/sec; 4.478 sec/batch)
2016-04-30 15:03:21.923171: step 3222, loss = 5.93 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 15:03:27.019811: step 3223, loss = 5.94 (12.6 examples/sec; 5.097 sec/batch)
2016-04-30 15:03:32.380230: step 3224, loss = 6.07 (11.9 examples/sec; 5.360 sec/batch)
2016-04-30 15:03:37.243105: step 3225, loss = 6.00 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 15:03:42.213089: step 3226, loss = 5.97 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 15:03:46.994980: step 3227, loss = 6.03 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 15:03:51.993108: step 3228, loss = 6.09 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 15:03:56.906383: step 3229, loss = 5.91 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 15:04:01.989852: step 3230, loss = 6.02 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 15:04:15.287403: step 3231, loss = 6.05 (14.0 examples/sec; 4.582 sec/batch)
2016-04-30 15:04:20.089447: step 3232, loss = 5.87 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 15:04:25.015891: step 3233, loss = 5.98 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 15:04:30.086855: step 3234, loss = 5.95 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 15:04:34.844612: step 3235, loss = 6.05 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 15:04:40.317371: step 3236, loss = 6.02 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 15:04:45.263983: step 3237, loss = 6.02 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 15:04:49.938081: step 3238, loss = 5.98 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 15:04:54.991233: step 3239, loss = 5.96 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 15:04:59.872416: step 3240, loss = 5.97 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 15:05:11.983767: step 3241, loss = 6.01 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 15:05:16.678308: step 3242, loss = 5.93 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 15:05:21.661452: step 3243, loss = 5.96 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 15:05:26.467341: step 3244, loss = 5.99 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 15:05:31.297073: step 3245, loss = 6.10 (13.3 examples/sec; 4.830 sec/batch)
2016-04-30 15:05:36.257712: step 3246, loss = 5.92 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 15:05:40.904449: step 3247, loss = 5.97 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 15:05:46.272971: step 3248, loss = 6.29 (11.9 examples/sec; 5.368 sec/batch)
2016-04-30 15:05:51.299584: step 3249, loss = 5.96 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 15:05:55.928507: step 3250, loss = 5.83 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 15:06:07.369311: step 3251, loss = 5.73 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 15:06:12.244092: step 3252, loss = 6.00 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 15:06:17.596564: step 3253, loss = 5.72 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 15:06:22.550940: step 3254, loss = 6.11 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 15:06:27.832441: step 3255, loss = 6.03 (12.1 examples/sec; 5.281 sec/batch)
2016-04-30 15:06:32.712165: step 3256, loss = 5.70 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 15:06:37.370310: step 3257, loss = 5.92 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 15:06:42.242464: step 3258, loss = 5.95 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 15:06:47.174052: step 3259, loss = 5.90 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 15:06:52.545371: step 3260, loss = 5.91 (11.9 examples/sec; 5.371 sec/batch)
2016-04-30 15:07:03.948343: step 3261, loss = 5.92 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 15:07:08.746144: step 3262, loss = 6.05 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 15:07:13.414099: step 3263, loss = 6.13 (13.7 examples/sec; 4.668 sec/batch)
2016-04-30 15:07:18.292431: step 3264, loss = 5.97 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 15:07:23.506163: step 3265, loss = 5.88 (12.3 examples/sec; 5.214 sec/batch)
2016-04-30 15:07:28.255743: step 3266, loss = 5.87 (13.5 examples/sec; 4.750 sec/batch)
2016-04-30 15:07:33.024434: step 3267, loss = 5.73 (13.4 examples/sec; 4.769 sec/batch)
2016-04-30 15:07:37.773333: step 3268, loss = 5.78 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 15:07:42.756251: step 3269, loss = 6.07 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 15:07:47.549247: step 3270, loss = 5.83 (13.4 examples/sec; 4.793 sec/batch)
2016-04-30 15:07:59.273659: step 3271, loss = 5.74 (13.9 examples/sec; 4.611 sec/batch)
2016-04-30 15:08:04.367557: step 3272, loss = 5.99 (12.6 examples/sec; 5.094 sec/batch)
2016-04-30 15:08:09.398301: step 3273, loss = 6.00 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 15:08:14.057351: step 3274, loss = 5.85 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 15:08:19.147933: step 3275, loss = 5.86 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 15:08:24.204927: step 3276, loss = 5.82 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 15:08:29.623230: step 3277, loss = 5.87 (11.8 examples/sec; 5.418 sec/batch)
2016-04-30 15:08:34.653518: step 3278, loss = 5.92 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 15:08:39.751247: step 3279, loss = 5.87 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 15:08:44.544779: step 3280, loss = 5.79 (13.4 examples/sec; 4.793 sec/batch)
2016-04-30 15:08:55.389969: step 3281, loss = 5.76 (14.3 examples/sec; 4.473 sec/batch)
2016-04-30 15:09:01.168443: step 3282, loss = 5.85 (11.1 examples/sec; 5.778 sec/batch)
2016-04-30 15:09:06.141928: step 3283, loss = 5.89 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 15:09:10.891245: step 3284, loss = 5.77 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 15:09:15.799042: step 3285, loss = 5.83 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 15:09:20.719457: step 3286, loss = 5.81 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 15:09:25.475101: step 3287, loss = 5.85 (13.5 examples/sec; 4.756 sec/batch)
2016-04-30 15:09:30.403176: step 3288, loss = 5.79 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 15:09:35.603413: step 3289, loss = 5.90 (12.3 examples/sec; 5.200 sec/batch)
2016-04-30 15:09:40.445030: step 3290, loss = 5.73 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 15:09:51.649769: step 3291, loss = 5.93 (13.5 examples/sec; 4.755 sec/batch)
2016-04-30 15:09:56.330651: step 3292, loss = 5.83 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 15:10:01.473249: step 3293, loss = 5.86 (12.4 examples/sec; 5.143 sec/batch)
2016-04-30 15:10:06.811088: step 3294, loss = 5.67 (12.0 examples/sec; 5.338 sec/batch)
2016-04-30 15:10:11.869976: step 3295, loss = 5.80 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 15:10:16.886301: step 3296, loss = 5.78 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 15:10:21.990183: step 3297, loss = 5.80 (12.5 examples/sec; 5.104 sec/batch)
2016-04-30 15:10:26.947183: step 3298, loss = 5.94 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 15:10:31.612455: step 3299, loss = 5.79 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 15:10:36.510328: step 3300, loss = 5.89 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 15:10:48.296048: step 3301, loss = 5.84 (13.6 examples/sec; 4.703 sec/batch)
2016-04-30 15:10:52.859215: step 3302, loss = 5.64 (14.0 examples/sec; 4.563 sec/batch)
2016-04-30 15:10:57.757438: step 3303, loss = 5.66 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 15:11:02.855222: step 3304, loss = 5.76 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 15:11:07.586006: step 3305, loss = 5.86 (13.5 examples/sec; 4.731 sec/batch)
2016-04-30 15:11:13.149253: step 3306, loss = 5.67 (11.5 examples/sec; 5.563 sec/batch)
2016-04-30 15:11:18.007189: step 3307, loss = 5.72 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 15:11:22.697200: step 3308, loss = 5.71 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 15:11:27.702741: step 3309, loss = 5.72 (12.8 examples/sec; 5.005 sec/batch)
2016-04-30 15:11:32.494474: step 3310, loss = 5.64 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 15:11:43.723895: step 3311, loss = 5.82 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 15:11:49.040444: step 3312, loss = 5.83 (12.0 examples/sec; 5.316 sec/batch)
2016-04-30 15:11:53.831019: step 3313, loss = 5.62 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 15:11:58.300468: step 3314, loss = 5.82 (14.3 examples/sec; 4.469 sec/batch)
2016-04-30 15:12:03.359218: step 3315, loss = 5.89 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 15:12:07.953041: step 3316, loss = 5.72 (13.9 examples/sec; 4.594 sec/batch)
2016-04-30 15:12:12.819281: step 3317, loss = 5.64 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 15:12:17.837311: step 3318, loss = 5.54 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:12:23.092864: step 3319, loss = 5.80 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 15:12:27.986094: step 3320, loss = 5.79 (13.1 examples/sec; 4.893 sec/batch)
2016-04-30 15:12:39.279139: step 3321, loss = 5.74 (13.5 examples/sec; 4.745 sec/batch)
2016-04-30 15:12:44.041202: step 3322, loss = 5.70 (13.4 examples/sec; 4.762 sec/batch)
2016-04-30 15:12:48.886877: step 3323, loss = 5.81 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 15:12:54.195770: step 3324, loss = 5.48 (12.1 examples/sec; 5.309 sec/batch)
2016-04-30 15:12:58.944484: step 3325, loss = 5.77 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 15:13:04.178477: step 3326, loss = 5.80 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 15:13:09.167661: step 3327, loss = 5.65 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 15:13:13.710126: step 3328, loss = 5.63 (14.1 examples/sec; 4.542 sec/batch)
2016-04-30 15:13:18.600296: step 3329, loss = 5.63 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 15:13:23.270405: step 3330, loss = 5.77 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 15:13:35.193841: step 3331, loss = 5.54 (14.3 examples/sec; 4.478 sec/batch)
2016-04-30 15:13:40.321231: step 3332, loss = 5.68 (12.5 examples/sec; 5.127 sec/batch)
2016-04-30 15:13:45.245200: step 3333, loss = 5.68 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 15:13:50.056304: step 3334, loss = 5.68 (13.3 examples/sec; 4.811 sec/batch)
2016-04-30 15:13:55.066323: step 3335, loss = 5.68 (12.8 examples/sec; 5.010 sec/batch)
2016-04-30 15:14:00.549196: step 3336, loss = 5.91 (11.7 examples/sec; 5.483 sec/batch)
2016-04-30 15:14:05.307500: step 3337, loss = 5.67 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 15:14:10.275961: step 3338, loss = 5.62 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 15:14:15.440885: step 3339, loss = 5.52 (12.4 examples/sec; 5.165 sec/batch)
2016-04-30 15:14:19.976561: step 3340, loss = 5.69 (14.1 examples/sec; 4.536 sec/batch)
2016-04-30 15:14:31.780196: step 3341, loss = 5.82 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 15:14:36.906997: step 3342, loss = 5.64 (12.5 examples/sec; 5.127 sec/batch)
2016-04-30 15:14:41.897449: step 3343, loss = 5.56 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 15:14:46.827729: step 3344, loss = 5.48 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 15:14:51.882047: step 3345, loss = 5.62 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 15:14:56.842321: step 3346, loss = 5.69 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 15:15:01.805946: step 3347, loss = 5.42 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 15:15:07.191383: step 3348, loss = 5.66 (11.9 examples/sec; 5.385 sec/batch)
2016-04-30 15:15:12.313403: step 3349, loss = 5.62 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 15:15:17.064511: step 3350, loss = 5.60 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 15:15:28.545992: step 3351, loss = 5.66 (13.5 examples/sec; 4.736 sec/batch)
2016-04-30 15:15:33.527663: step 3352, loss = 5.34 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 15:15:38.867838: step 3353, loss = 5.62 (12.0 examples/sec; 5.340 sec/batch)
2016-04-30 15:15:43.548209: step 3354, loss = 5.77 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 15:15:48.424453: step 3355, loss = 5.55 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 15:15:53.123851: step 3356, loss = 5.44 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 15:15:57.973744: step 3357, loss = 5.43 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 15:16:02.907388: step 3358, loss = 5.63 (13.0 examples/sec; 4.934 sec/batch)
2016-04-30 15:16:07.653212: step 3359, loss = 5.70 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 15:16:13.100427: step 3360, loss = 5.71 (11.7 examples/sec; 5.447 sec/batch)
2016-04-30 15:16:24.428325: step 3361, loss = 5.69 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 15:16:29.011831: step 3362, loss = 5.57 (14.0 examples/sec; 4.583 sec/batch)
2016-04-30 15:16:33.925463: step 3363, loss = 5.41 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 15:16:38.912062: step 3364, loss = 5.61 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 15:16:43.987490: step 3365, loss = 5.42 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 15:16:48.895832: step 3366, loss = 5.50 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 15:16:53.506356: step 3367, loss = 5.42 (13.9 examples/sec; 4.610 sec/batch)
2016-04-30 15:16:58.201121: step 3368, loss = 5.62 (13.6 examples/sec; 4.695 sec/batch)
2016-04-30 15:17:03.302167: step 3369, loss = 5.47 (12.5 examples/sec; 5.101 sec/batch)
2016-04-30 15:17:08.087976: step 3370, loss = 5.58 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 15:17:20.001022: step 3371, loss = 5.48 (12.2 examples/sec; 5.250 sec/batch)
2016-04-30 15:17:25.019652: step 3372, loss = 5.58 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 15:17:29.856717: step 3373, loss = 5.53 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 15:17:34.690843: step 3374, loss = 5.58 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 15:17:39.810958: step 3375, loss = 5.52 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 15:17:44.586710: step 3376, loss = 5.61 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 15:17:49.860341: step 3377, loss = 5.44 (12.1 examples/sec; 5.274 sec/batch)
2016-04-30 15:17:54.737385: step 3378, loss = 5.56 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 15:17:59.568484: step 3379, loss = 5.41 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 15:18:04.593106: step 3380, loss = 5.36 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 15:18:16.012899: step 3381, loss = 5.41 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 15:18:21.030954: step 3382, loss = 5.53 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:18:26.192247: step 3383, loss = 5.58 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 15:18:31.040467: step 3384, loss = 5.52 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 15:18:35.708338: step 3385, loss = 5.46 (13.7 examples/sec; 4.668 sec/batch)
2016-04-30 15:18:40.589490: step 3386, loss = 5.47 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 15:18:45.710296: step 3387, loss = 5.48 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 15:18:50.280102: step 3388, loss = 5.40 (14.0 examples/sec; 4.570 sec/batch)
2016-04-30 15:18:55.736417: step 3389, loss = 5.57 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 15:19:00.886700: step 3390, loss = 5.45 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 15:19:12.039233: step 3391, loss = 5.46 (13.6 examples/sec; 4.695 sec/batch)
2016-04-30 15:19:16.683127: step 3392, loss = 5.51 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 15:19:21.590313: step 3393, loss = 5.55 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 15:19:26.158692: step 3394, loss = 5.47 (14.0 examples/sec; 4.568 sec/batch)
2016-04-30 15:19:31.539831: step 3395, loss = 5.46 (11.9 examples/sec; 5.381 sec/batch)
2016-04-30 15:19:36.529638: step 3396, loss = 5.46 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 15:19:41.399200: step 3397, loss = 5.47 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 15:19:46.446651: step 3398, loss = 5.45 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 15:19:51.385557: step 3399, loss = 5.53 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 15:19:56.048750: step 3400, loss = 5.36 (13.7 examples/sec; 4.663 sec/batch)
2016-04-30 15:20:07.894628: step 3401, loss = 5.53 (14.2 examples/sec; 4.491 sec/batch)
2016-04-30 15:20:12.824500: step 3402, loss = 5.31 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 15:20:17.484434: step 3403, loss = 5.38 (13.7 examples/sec; 4.660 sec/batch)
2016-04-30 15:20:22.634842: step 3404, loss = 5.53 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 15:20:27.536201: step 3405, loss = 5.54 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 15:20:32.199891: step 3406, loss = 5.36 (13.7 examples/sec; 4.664 sec/batch)
2016-04-30 15:20:37.703805: step 3407, loss = 5.48 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 15:20:42.612480: step 3408, loss = 5.40 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 15:20:47.257739: step 3409, loss = 5.43 (13.8 examples/sec; 4.645 sec/batch)
2016-04-30 15:20:52.092996: step 3410, loss = 5.50 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 15:21:03.459793: step 3411, loss = 5.44 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 15:21:08.919762: step 3412, loss = 5.42 (11.7 examples/sec; 5.460 sec/batch)
2016-04-30 15:21:13.556499: step 3413, loss = 5.25 (13.8 examples/sec; 4.637 sec/batch)
2016-04-30 15:21:18.387039: step 3414, loss = 5.36 (13.2 examples/sec; 4.830 sec/batch)
2016-04-30 15:21:22.901799: step 3415, loss = 5.42 (14.2 examples/sec; 4.515 sec/batch)
2016-04-30 15:21:27.808440: step 3416, loss = 5.38 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 15:21:32.613251: step 3417, loss = 5.44 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 15:21:37.570458: step 3418, loss = 5.37 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 15:21:43.018752: step 3419, loss = 5.43 (11.7 examples/sec; 5.448 sec/batch)
2016-04-30 15:21:47.956726: step 3420, loss = 5.44 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 15:21:59.036800: step 3421, loss = 5.42 (13.8 examples/sec; 4.635 sec/batch)
2016-04-30 15:22:03.926012: step 3422, loss = 5.45 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 15:22:08.447439: step 3423, loss = 5.44 (14.2 examples/sec; 4.521 sec/batch)
2016-04-30 15:22:13.743320: step 3424, loss = 5.25 (12.1 examples/sec; 5.296 sec/batch)
2016-04-30 15:22:18.583299: step 3425, loss = 5.41 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 15:22:23.198407: step 3426, loss = 5.28 (13.9 examples/sec; 4.615 sec/batch)
2016-04-30 15:22:28.081918: step 3427, loss = 5.39 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 15:22:32.983389: step 3428, loss = 5.41 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 15:22:37.636003: step 3429, loss = 5.40 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 15:22:42.468163: step 3430, loss = 5.43 (13.2 examples/sec; 4.832 sec/batch)
2016-04-30 15:22:54.323089: step 3431, loss = 5.31 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 15:22:58.920469: step 3432, loss = 5.39 (13.9 examples/sec; 4.597 sec/batch)
2016-04-30 15:23:04.321060: step 3433, loss = 5.22 (11.9 examples/sec; 5.401 sec/batch)
2016-04-30 15:23:09.242728: step 3434, loss = 5.26 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 15:23:13.952288: step 3435, loss = 5.34 (13.6 examples/sec; 4.709 sec/batch)
2016-04-30 15:23:19.618029: step 3436, loss = 5.37 (11.3 examples/sec; 5.666 sec/batch)
2016-04-30 15:23:24.636070: step 3437, loss = 5.38 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:23:29.361923: step 3438, loss = 5.24 (13.5 examples/sec; 4.726 sec/batch)
2016-04-30 15:23:34.210399: step 3439, loss = 5.47 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 15:23:38.944248: step 3440, loss = 5.28 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 15:23:50.698841: step 3441, loss = 5.30 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 15:23:56.021410: step 3442, loss = 5.37 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 15:24:01.160174: step 3443, loss = 5.40 (12.5 examples/sec; 5.139 sec/batch)
2016-04-30 15:24:06.089993: step 3444, loss = 5.32 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 15:24:11.694673: step 3445, loss = 5.38 (11.4 examples/sec; 5.605 sec/batch)
2016-04-30 15:24:16.956224: step 3446, loss = 5.17 (12.2 examples/sec; 5.261 sec/batch)
2016-04-30 15:24:22.014383: step 3447, loss = 5.40 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 15:24:27.523010: step 3448, loss = 5.44 (11.6 examples/sec; 5.509 sec/batch)
2016-04-30 15:24:32.245635: step 3449, loss = 5.29 (13.6 examples/sec; 4.723 sec/batch)
2016-04-30 15:24:37.335419: step 3450, loss = 5.20 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 15:24:48.693356: step 3451, loss = 5.15 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 15:24:53.341476: step 3452, loss = 5.31 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 15:24:58.780046: step 3453, loss = 5.38 (11.8 examples/sec; 5.438 sec/batch)
2016-04-30 15:25:03.753928: step 3454, loss = 5.30 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 15:25:08.428107: step 3455, loss = 5.27 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 15:25:13.230601: step 3456, loss = 5.25 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 15:25:18.208219: step 3457, loss = 5.23 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 15:25:22.834769: step 3458, loss = 5.13 (13.8 examples/sec; 4.626 sec/batch)
2016-04-30 15:25:27.864563: step 3459, loss = 5.12 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 15:25:33.259699: step 3460, loss = 5.31 (11.9 examples/sec; 5.395 sec/batch)
2016-04-30 15:25:44.133792: step 3461, loss = 5.30 (14.7 examples/sec; 4.361 sec/batch)
2016-04-30 15:25:49.095753: step 3462, loss = 5.27 (12.9 examples/sec; 4.962 sec/batch)
2016-04-30 15:25:54.279287: step 3463, loss = 5.16 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 15:25:58.931042: step 3464, loss = 5.33 (13.8 examples/sec; 4.652 sec/batch)
2016-04-30 15:26:04.424538: step 3465, loss = 5.19 (11.7 examples/sec; 5.493 sec/batch)
2016-04-30 15:26:09.245363: step 3466, loss = 5.11 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 15:26:13.984469: step 3467, loss = 5.37 (13.5 examples/sec; 4.739 sec/batch)
2016-04-30 15:26:18.995486: step 3468, loss = 5.32 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 15:26:23.840798: step 3469, loss = 5.25 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 15:26:28.609918: step 3470, loss = 5.26 (13.4 examples/sec; 4.769 sec/batch)
2016-04-30 15:26:40.384247: step 3471, loss = 5.18 (11.9 examples/sec; 5.370 sec/batch)
2016-04-30 15:26:45.425305: step 3472, loss = 5.26 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 15:26:50.126084: step 3473, loss = 5.10 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 15:26:55.101741: step 3474, loss = 5.09 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 15:26:59.799460: step 3475, loss = 5.24 (13.6 examples/sec; 4.698 sec/batch)
2016-04-30 15:27:04.879334: step 3476, loss = 5.08 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 15:27:10.474209: step 3477, loss = 5.31 (11.4 examples/sec; 5.595 sec/batch)
2016-04-30 15:27:15.399262: step 3478, loss = 5.00 (13.0 examples/sec; 4.925 sec/batch)
2016-04-30 15:27:20.049951: step 3479, loss = 5.10 (13.8 examples/sec; 4.651 sec/batch)
2016-04-30 15:27:25.013976: step 3480, loss = 5.13 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 15:27:36.179802: step 3481, loss = 4.97 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 15:27:40.960896: step 3482, loss = 5.13 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 15:27:46.456613: step 3483, loss = 5.06 (11.6 examples/sec; 5.496 sec/batch)
2016-04-30 15:27:51.371280: step 3484, loss = 5.17 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 15:27:56.157705: step 3485, loss = 5.03 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 15:28:01.184450: step 3486, loss = 5.25 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 15:28:05.986062: step 3487, loss = 5.07 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 15:28:10.714996: step 3488, loss = 5.08 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 15:28:16.104417: step 3489, loss = 5.10 (11.9 examples/sec; 5.389 sec/batch)
2016-04-30 15:28:20.832488: step 3490, loss = 5.06 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 15:28:31.923679: step 3491, loss = 5.19 (14.1 examples/sec; 4.551 sec/batch)
2016-04-30 15:28:36.912663: step 3492, loss = 5.30 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 15:28:41.811713: step 3493, loss = 5.13 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 15:28:46.580290: step 3494, loss = 4.98 (13.4 examples/sec; 4.768 sec/batch)
2016-04-30 15:28:51.997165: step 3495, loss = 5.26 (11.8 examples/sec; 5.417 sec/batch)
2016-04-30 15:28:56.881974: step 3496, loss = 5.08 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 15:29:01.718918: step 3497, loss = 5.06 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 15:29:06.522133: step 3498, loss = 5.21 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 15:29:11.336431: step 3499, loss = 5.19 (13.3 examples/sec; 4.814 sec/batch)
2016-04-30 15:29:16.151106: step 3500, loss = 5.07 (13.3 examples/sec; 4.815 sec/batch)
2016-04-30 15:29:27.779310: step 3501, loss = 5.09 (13.7 examples/sec; 4.672 sec/batch)
2016-04-30 15:29:32.644613: step 3502, loss = 5.06 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 15:29:37.662331: step 3503, loss = 4.99 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:29:42.638131: step 3504, loss = 4.96 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 15:29:47.375056: step 3505, loss = 5.05 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 15:29:52.262495: step 3506, loss = 5.01 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 15:29:57.574149: step 3507, loss = 5.26 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 15:30:02.483715: step 3508, loss = 5.22 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 15:30:07.348454: step 3509, loss = 4.86 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 15:30:12.150019: step 3510, loss = 5.21 (13.3 examples/sec; 4.801 sec/batch)
2016-04-30 15:30:23.046481: step 3511, loss = 5.17 (14.4 examples/sec; 4.457 sec/batch)
2016-04-30 15:30:28.563254: step 3512, loss = 5.16 (11.6 examples/sec; 5.516 sec/batch)
2016-04-30 15:30:33.396329: step 3513, loss = 5.00 (13.2 examples/sec; 4.833 sec/batch)
2016-04-30 15:30:37.941768: step 3514, loss = 5.02 (14.1 examples/sec; 4.545 sec/batch)
2016-04-30 15:30:42.832527: step 3515, loss = 4.98 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 15:30:47.772501: step 3516, loss = 5.02 (13.0 examples/sec; 4.940 sec/batch)
2016-04-30 15:30:52.691955: step 3517, loss = 5.04 (13.0 examples/sec; 4.918 sec/batch)
2016-04-30 15:30:57.647209: step 3518, loss = 5.12 (12.9 examples/sec; 4.955 sec/batch)
2016-04-30 15:31:03.239857: step 3519, loss = 5.12 (11.4 examples/sec; 5.593 sec/batch)
2016-04-30 15:31:07.957373: step 3520, loss = 5.13 (13.6 examples/sec; 4.717 sec/batch)
2016-04-30 15:31:19.251000: step 3521, loss = 5.15 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 15:31:24.157790: step 3522, loss = 5.16 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 15:31:28.786815: step 3523, loss = 5.09 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 15:31:34.050297: step 3524, loss = 4.86 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 15:31:38.856062: step 3525, loss = 5.06 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 15:31:43.641598: step 3526, loss = 4.99 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 15:31:48.693370: step 3527, loss = 5.06 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 15:31:53.383076: step 3528, loss = 4.94 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 15:31:58.313428: step 3529, loss = 4.83 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 15:32:03.372459: step 3530, loss = 5.02 (12.7 examples/sec; 5.059 sec/batch)
2016-04-30 15:32:15.189353: step 3531, loss = 5.07 (13.6 examples/sec; 4.711 sec/batch)
2016-04-30 15:32:19.849425: step 3532, loss = 4.97 (13.7 examples/sec; 4.660 sec/batch)
2016-04-30 15:32:24.795657: step 3533, loss = 4.99 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 15:32:29.520199: step 3534, loss = 5.12 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 15:32:34.381212: step 3535, loss = 4.95 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 15:32:39.222717: step 3536, loss = 4.97 (13.2 examples/sec; 4.841 sec/batch)
2016-04-30 15:32:44.551082: step 3537, loss = 4.90 (12.0 examples/sec; 5.328 sec/batch)
2016-04-30 15:32:49.432626: step 3538, loss = 4.90 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 15:32:54.683233: step 3539, loss = 4.91 (12.2 examples/sec; 5.251 sec/batch)
2016-04-30 15:32:59.375454: step 3540, loss = 5.06 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 15:33:10.907549: step 3541, loss = 4.97 (14.4 examples/sec; 4.457 sec/batch)
2016-04-30 15:33:16.456044: step 3542, loss = 4.96 (11.5 examples/sec; 5.548 sec/batch)
2016-04-30 15:33:21.475714: step 3543, loss = 4.77 (12.8 examples/sec; 5.020 sec/batch)
2016-04-30 15:33:26.484128: step 3544, loss = 5.14 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 15:33:31.615374: step 3545, loss = 5.01 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 15:33:36.478064: step 3546, loss = 4.99 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 15:33:41.451316: step 3547, loss = 4.99 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 15:33:46.839057: step 3548, loss = 5.12 (11.9 examples/sec; 5.388 sec/batch)
2016-04-30 15:33:52.313661: step 3549, loss = 5.02 (11.7 examples/sec; 5.475 sec/batch)
2016-04-30 15:33:57.555816: step 3550, loss = 4.90 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 15:34:09.135832: step 3551, loss = 4.97 (13.8 examples/sec; 4.637 sec/batch)
2016-04-30 15:34:13.868202: step 3552, loss = 5.06 (13.5 examples/sec; 4.732 sec/batch)
2016-04-30 15:34:19.226504: step 3553, loss = 4.80 (11.9 examples/sec; 5.358 sec/batch)
2016-04-30 15:34:24.331120: step 3554, loss = 4.79 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 15:34:28.962133: step 3555, loss = 4.93 (13.8 examples/sec; 4.631 sec/batch)
2016-04-30 15:34:33.879279: step 3556, loss = 5.03 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 15:34:38.560446: step 3557, loss = 5.17 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 15:34:43.475352: step 3558, loss = 4.73 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 15:34:48.339304: step 3559, loss = 5.00 (13.2 examples/sec; 4.864 sec/batch)
2016-04-30 15:34:53.749855: step 3560, loss = 4.97 (11.8 examples/sec; 5.410 sec/batch)
2016-04-30 15:35:05.306466: step 3561, loss = 4.90 (13.5 examples/sec; 4.757 sec/batch)
2016-04-30 15:35:10.384614: step 3562, loss = 4.86 (12.6 examples/sec; 5.078 sec/batch)
2016-04-30 15:35:15.220940: step 3563, loss = 4.91 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 15:35:20.048144: step 3564, loss = 4.64 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 15:35:25.652916: step 3565, loss = 5.02 (11.4 examples/sec; 5.605 sec/batch)
2016-04-30 15:35:30.557465: step 3566, loss = 4.92 (13.0 examples/sec; 4.904 sec/batch)
2016-04-30 15:35:35.249345: step 3567, loss = 4.71 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 15:35:40.537854: step 3568, loss = 4.86 (12.1 examples/sec; 5.288 sec/batch)
2016-04-30 15:35:45.497564: step 3569, loss = 4.87 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 15:35:50.204087: step 3570, loss = 4.93 (13.6 examples/sec; 4.706 sec/batch)
2016-04-30 15:36:02.499581: step 3571, loss = 4.76 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 15:36:07.583485: step 3572, loss = 4.83 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 15:36:12.625245: step 3573, loss = 5.05 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 15:36:17.420133: step 3574, loss = 4.73 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 15:36:22.338960: step 3575, loss = 4.96 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 15:36:27.324118: step 3576, loss = 4.83 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 15:36:32.457528: step 3577, loss = 4.87 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 15:36:37.560660: step 3578, loss = 5.03 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 15:36:42.658314: step 3579, loss = 5.09 (12.6 examples/sec; 5.098 sec/batch)
2016-04-30 15:36:47.437274: step 3580, loss = 4.77 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 15:36:58.966988: step 3581, loss = 4.99 (14.2 examples/sec; 4.508 sec/batch)
2016-04-30 15:37:04.862433: step 3582, loss = 4.98 (10.9 examples/sec; 5.895 sec/batch)
2016-04-30 15:37:09.930865: step 3583, loss = 4.93 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 15:37:14.890453: step 3584, loss = 5.00 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 15:37:19.925493: step 3585, loss = 4.81 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 15:37:24.943416: step 3586, loss = 4.94 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:37:29.865499: step 3587, loss = 4.80 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 15:37:34.725089: step 3588, loss = 4.76 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 15:37:40.143084: step 3589, loss = 4.77 (11.8 examples/sec; 5.418 sec/batch)
2016-04-30 15:37:44.828922: step 3590, loss = 4.85 (13.7 examples/sec; 4.686 sec/batch)
2016-04-30 15:37:56.087657: step 3591, loss = 4.98 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 15:38:01.290913: step 3592, loss = 4.95 (12.3 examples/sec; 5.203 sec/batch)
2016-04-30 15:38:06.011590: step 3593, loss = 4.75 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 15:38:11.412879: step 3594, loss = 4.80 (11.8 examples/sec; 5.401 sec/batch)
2016-04-30 15:38:16.409951: step 3595, loss = 4.89 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 15:38:21.144691: step 3596, loss = 4.89 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 15:38:25.813239: step 3597, loss = 4.86 (13.7 examples/sec; 4.668 sec/batch)
2016-04-30 15:38:30.648408: step 3598, loss = 4.87 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 15:38:35.214160: step 3599, loss = 4.71 (14.0 examples/sec; 4.566 sec/batch)
2016-04-30 15:38:40.238635: step 3600, loss = 4.75 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 15:38:52.161380: step 3601, loss = 4.83 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 15:38:56.862579: step 3602, loss = 4.95 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 15:39:01.764682: step 3603, loss = 4.88 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 15:39:06.678397: step 3604, loss = 4.78 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 15:39:11.533858: step 3605, loss = 4.72 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 15:39:17.085956: step 3606, loss = 4.79 (11.5 examples/sec; 5.552 sec/batch)
2016-04-30 15:39:22.206563: step 3607, loss = 4.83 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 15:39:27.012228: step 3608, loss = 4.89 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 15:39:31.814799: step 3609, loss = 4.61 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 15:39:36.846469: step 3610, loss = 4.81 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 15:39:47.795483: step 3611, loss = 4.65 (14.2 examples/sec; 4.508 sec/batch)
2016-04-30 15:39:53.131763: step 3612, loss = 4.76 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 15:39:58.155293: step 3613, loss = 4.97 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 15:40:03.179671: step 3614, loss = 4.65 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 15:40:08.028135: step 3615, loss = 4.66 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 15:40:13.187046: step 3616, loss = 4.84 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 15:40:17.959982: step 3617, loss = 5.00 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 15:40:23.273338: step 3618, loss = 4.75 (12.0 examples/sec; 5.313 sec/batch)
2016-04-30 15:40:28.224615: step 3619, loss = 4.76 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 15:40:33.051920: step 3620, loss = 4.86 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 15:40:44.388909: step 3621, loss = 4.84 (14.0 examples/sec; 4.571 sec/batch)
2016-04-30 15:40:49.349415: step 3622, loss = 4.88 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 15:40:54.166797: step 3623, loss = 4.65 (13.3 examples/sec; 4.817 sec/batch)
2016-04-30 15:40:59.530776: step 3624, loss = 4.74 (11.9 examples/sec; 5.364 sec/batch)
2016-04-30 15:41:04.788468: step 3625, loss = 4.87 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 15:41:09.867880: step 3626, loss = 4.64 (12.6 examples/sec; 5.079 sec/batch)
2016-04-30 15:41:14.634820: step 3627, loss = 4.75 (13.4 examples/sec; 4.767 sec/batch)
2016-04-30 15:41:19.542896: step 3628, loss = 4.62 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 15:41:24.737164: step 3629, loss = 4.53 (12.3 examples/sec; 5.194 sec/batch)
2016-04-30 15:41:29.889860: step 3630, loss = 4.54 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 15:41:41.101320: step 3631, loss = 4.72 (14.0 examples/sec; 4.561 sec/batch)
2016-04-30 15:41:47.109448: step 3632, loss = 4.68 (10.7 examples/sec; 6.008 sec/batch)
2016-04-30 15:41:52.948118: step 3633, loss = 4.57 (11.0 examples/sec; 5.839 sec/batch)
2016-04-30 15:41:57.918883: step 3634, loss = 4.62 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 15:42:03.553522: step 3635, loss = 4.78 (11.4 examples/sec; 5.635 sec/batch)
2016-04-30 15:42:08.241789: step 3636, loss = 4.80 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 15:42:13.308420: step 3637, loss = 4.87 (12.6 examples/sec; 5.067 sec/batch)
2016-04-30 15:42:18.051922: step 3638, loss = 4.79 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 15:42:23.106007: step 3639, loss = 4.75 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 15:42:27.969586: step 3640, loss = 4.73 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 15:42:40.040126: step 3641, loss = 4.63 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 15:42:44.788076: step 3642, loss = 4.83 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 15:42:49.789247: step 3643, loss = 4.65 (12.8 examples/sec; 5.001 sec/batch)
2016-04-30 15:42:54.683112: step 3644, loss = 4.63 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 15:42:59.545571: step 3645, loss = 4.57 (13.2 examples/sec; 4.862 sec/batch)
2016-04-30 15:43:04.543426: step 3646, loss = 4.80 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 15:43:10.133617: step 3647, loss = 4.67 (11.4 examples/sec; 5.590 sec/batch)
2016-04-30 15:43:14.780500: step 3648, loss = 4.78 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 15:43:19.723921: step 3649, loss = 4.63 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 15:43:24.827220: step 3650, loss = 4.66 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 15:43:35.811353: step 3651, loss = 4.79 (14.3 examples/sec; 4.487 sec/batch)
2016-04-30 15:43:41.410969: step 3652, loss = 4.61 (11.4 examples/sec; 5.600 sec/batch)
2016-04-30 15:43:46.262518: step 3653, loss = 4.53 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 15:43:51.247683: step 3654, loss = 4.71 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 15:43:56.060802: step 3655, loss = 4.69 (13.3 examples/sec; 4.813 sec/batch)
2016-04-30 15:44:01.271984: step 3656, loss = 4.71 (12.3 examples/sec; 5.211 sec/batch)
2016-04-30 15:44:06.298235: step 3657, loss = 4.62 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 15:44:11.249423: step 3658, loss = 4.64 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 15:44:16.796478: step 3659, loss = 4.67 (11.5 examples/sec; 5.547 sec/batch)
2016-04-30 15:44:22.010242: step 3660, loss = 4.71 (12.3 examples/sec; 5.214 sec/batch)
2016-04-30 15:44:33.358160: step 3661, loss = 4.64 (13.3 examples/sec; 4.799 sec/batch)
2016-04-30 15:44:38.048442: step 3662, loss = 4.70 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 15:44:43.148983: step 3663, loss = 4.66 (12.5 examples/sec; 5.100 sec/batch)
2016-04-30 15:44:48.673132: step 3664, loss = 4.64 (11.6 examples/sec; 5.524 sec/batch)
2016-04-30 15:44:53.278083: step 3665, loss = 4.53 (13.9 examples/sec; 4.605 sec/batch)
2016-04-30 15:44:58.157704: step 3666, loss = 4.60 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 15:45:02.974048: step 3667, loss = 4.59 (13.3 examples/sec; 4.816 sec/batch)
2016-04-30 15:45:07.855213: step 3668, loss = 4.76 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 15:45:12.991029: step 3669, loss = 4.55 (12.5 examples/sec; 5.136 sec/batch)
2016-04-30 15:45:17.782056: step 3670, loss = 4.79 (13.4 examples/sec; 4.791 sec/batch)
2016-04-30 15:45:29.915023: step 3671, loss = 4.71 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 15:45:34.908121: step 3672, loss = 4.59 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 15:45:40.008939: step 3673, loss = 4.67 (12.5 examples/sec; 5.101 sec/batch)
2016-04-30 15:45:44.784338: step 3674, loss = 4.68 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 15:45:49.807879: step 3675, loss = 4.63 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 15:45:55.231872: step 3676, loss = 4.61 (11.8 examples/sec; 5.424 sec/batch)
2016-04-30 15:46:00.039745: step 3677, loss = 4.55 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 15:46:05.047820: step 3678, loss = 4.69 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 15:46:10.171094: step 3679, loss = 4.43 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 15:46:15.022853: step 3680, loss = 4.51 (13.2 examples/sec; 4.852 sec/batch)
2016-04-30 15:46:26.644136: step 3681, loss = 4.59 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 15:46:31.607346: step 3682, loss = 4.55 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 15:46:36.636291: step 3683, loss = 4.61 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 15:46:41.505464: step 3684, loss = 4.72 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 15:46:46.480612: step 3685, loss = 4.63 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 15:46:51.463615: step 3686, loss = 4.55 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 15:46:56.117185: step 3687, loss = 4.50 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 15:47:01.817121: step 3688, loss = 4.65 (11.2 examples/sec; 5.700 sec/batch)
2016-04-30 15:47:07.081268: step 3689, loss = 4.80 (12.2 examples/sec; 5.264 sec/batch)
2016-04-30 15:47:12.282592: step 3690, loss = 4.67 (12.3 examples/sec; 5.201 sec/batch)
2016-04-30 15:47:23.727653: step 3691, loss = 4.49 (13.7 examples/sec; 4.679 sec/batch)
2016-04-30 15:47:28.842041: step 3692, loss = 4.59 (12.5 examples/sec; 5.114 sec/batch)
2016-04-30 15:47:34.372442: step 3693, loss = 4.81 (11.6 examples/sec; 5.530 sec/batch)
2016-04-30 15:47:39.300096: step 3694, loss = 4.54 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 15:47:44.358131: step 3695, loss = 4.57 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 15:47:49.529864: step 3696, loss = 4.43 (12.4 examples/sec; 5.172 sec/batch)
2016-04-30 15:47:54.532411: step 3697, loss = 4.55 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 15:47:59.177109: step 3698, loss = 4.41 (13.8 examples/sec; 4.645 sec/batch)
2016-04-30 15:48:05.003934: step 3699, loss = 4.66 (11.0 examples/sec; 5.827 sec/batch)
2016-04-30 15:48:10.052476: step 3700, loss = 4.51 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 15:48:21.304835: step 3701, loss = 4.41 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 15:48:26.392031: step 3702, loss = 4.59 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 15:48:31.369856: step 3703, loss = 4.45 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 15:48:36.300440: step 3704, loss = 4.56 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 15:48:41.627536: step 3705, loss = 4.60 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 15:48:46.541674: step 3706, loss = 4.59 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 15:48:51.559375: step 3707, loss = 4.65 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 15:48:56.157836: step 3708, loss = 4.45 (13.9 examples/sec; 4.598 sec/batch)
2016-04-30 15:49:01.099742: step 3709, loss = 4.62 (13.0 examples/sec; 4.942 sec/batch)
2016-04-30 15:49:05.803557: step 3710, loss = 4.39 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 15:49:17.330775: step 3711, loss = 4.66 (14.3 examples/sec; 4.479 sec/batch)
2016-04-30 15:49:22.329923: step 3712, loss = 4.65 (12.8 examples/sec; 4.999 sec/batch)
2016-04-30 15:49:26.938519: step 3713, loss = 4.50 (13.9 examples/sec; 4.609 sec/batch)
2016-04-30 15:49:31.797998: step 3714, loss = 4.52 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 15:49:36.504328: step 3715, loss = 4.47 (13.6 examples/sec; 4.706 sec/batch)
2016-04-30 15:49:41.275645: step 3716, loss = 4.52 (13.4 examples/sec; 4.771 sec/batch)
2016-04-30 15:49:46.804632: step 3717, loss = 4.56 (11.6 examples/sec; 5.529 sec/batch)
2016-04-30 15:49:51.626080: step 3718, loss = 4.55 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 15:49:56.220616: step 3719, loss = 4.51 (13.9 examples/sec; 4.594 sec/batch)
2016-04-30 15:50:01.384967: step 3720, loss = 4.55 (12.4 examples/sec; 5.164 sec/batch)
2016-04-30 15:50:12.551905: step 3721, loss = 4.53 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 15:50:17.723339: step 3722, loss = 4.72 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 15:50:22.523109: step 3723, loss = 4.53 (13.3 examples/sec; 4.800 sec/batch)
2016-04-30 15:50:27.382912: step 3724, loss = 4.53 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 15:50:32.048924: step 3725, loss = 4.38 (13.7 examples/sec; 4.666 sec/batch)
2016-04-30 15:50:37.202247: step 3726, loss = 4.55 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 15:50:41.973583: step 3727, loss = 4.54 (13.4 examples/sec; 4.771 sec/batch)
2016-04-30 15:50:46.889906: step 3728, loss = 4.34 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 15:50:52.315252: step 3729, loss = 4.43 (11.8 examples/sec; 5.425 sec/batch)
2016-04-30 15:50:57.059027: step 3730, loss = 4.29 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 15:51:08.481347: step 3731, loss = 4.35 (13.9 examples/sec; 4.603 sec/batch)
2016-04-30 15:51:13.265375: step 3732, loss = 4.66 (13.4 examples/sec; 4.784 sec/batch)
2016-04-30 15:51:18.126007: step 3733, loss = 4.55 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 15:51:23.523199: step 3734, loss = 4.46 (11.9 examples/sec; 5.397 sec/batch)
2016-04-30 15:51:28.395706: step 3735, loss = 4.48 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 15:51:32.950008: step 3736, loss = 4.51 (14.1 examples/sec; 4.554 sec/batch)
2016-04-30 15:51:37.922070: step 3737, loss = 4.51 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 15:51:42.836440: step 3738, loss = 4.48 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 15:51:47.565217: step 3739, loss = 4.59 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 15:51:52.516160: step 3740, loss = 4.31 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 15:52:04.458496: step 3741, loss = 4.52 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 15:52:09.475301: step 3742, loss = 4.45 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 15:52:14.164455: step 3743, loss = 4.54 (13.6 examples/sec; 4.689 sec/batch)
2016-04-30 15:52:19.043586: step 3744, loss = 4.56 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 15:52:23.707014: step 3745, loss = 4.50 (13.7 examples/sec; 4.663 sec/batch)
2016-04-30 15:52:29.207772: step 3746, loss = 4.50 (11.6 examples/sec; 5.501 sec/batch)
2016-04-30 15:52:34.165090: step 3747, loss = 4.45 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 15:52:38.857800: step 3748, loss = 4.40 (13.6 examples/sec; 4.693 sec/batch)
2016-04-30 15:52:43.762622: step 3749, loss = 4.45 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 15:52:48.737910: step 3750, loss = 4.45 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 15:52:59.901218: step 3751, loss = 4.61 (14.5 examples/sec; 4.399 sec/batch)
2016-04-30 15:53:05.535686: step 3752, loss = 4.33 (11.4 examples/sec; 5.634 sec/batch)
2016-04-30 15:53:10.507111: step 3753, loss = 4.34 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 15:53:15.334639: step 3754, loss = 4.50 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 15:53:20.257733: step 3755, loss = 4.42 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 15:53:25.329428: step 3756, loss = 4.46 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 15:53:30.075338: step 3757, loss = 4.52 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 15:53:35.515336: step 3758, loss = 4.35 (11.8 examples/sec; 5.440 sec/batch)
2016-04-30 15:53:40.386604: step 3759, loss = 4.38 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 15:53:45.509955: step 3760, loss = 4.35 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 15:53:56.616514: step 3761, loss = 4.22 (14.0 examples/sec; 4.561 sec/batch)
2016-04-30 15:54:01.636635: step 3762, loss = 4.49 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 15:54:06.658819: step 3763, loss = 4.33 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 15:54:11.734701: step 3764, loss = 4.32 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 15:54:16.603442: step 3765, loss = 4.35 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 15:54:21.291343: step 3766, loss = 4.46 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 15:54:26.160877: step 3767, loss = 4.25 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 15:54:31.242395: step 3768, loss = 4.56 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 15:54:36.131010: step 3769, loss = 4.21 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 15:54:41.511554: step 3770, loss = 4.30 (11.9 examples/sec; 5.380 sec/batch)
2016-04-30 15:54:52.968151: step 3771, loss = 4.51 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 15:54:57.872231: step 3772, loss = 4.38 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 15:55:02.740884: step 3773, loss = 4.28 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 15:55:07.754118: step 3774, loss = 4.55 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 15:55:12.952178: step 3775, loss = 4.28 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 15:55:17.977334: step 3776, loss = 4.39 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 15:55:22.975980: step 3777, loss = 4.45 (12.8 examples/sec; 4.999 sec/batch)
2016-04-30 15:55:27.943353: step 3778, loss = 4.43 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 15:55:32.733724: step 3779, loss = 4.48 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 15:55:37.636519: step 3780, loss = 4.33 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 15:55:49.570273: step 3781, loss = 4.17 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 15:55:54.614336: step 3782, loss = 4.16 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 15:55:59.295462: step 3783, loss = 4.43 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 15:56:04.327620: step 3784, loss = 4.25 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 15:56:09.062376: step 3785, loss = 4.30 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 15:56:13.952099: step 3786, loss = 4.40 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 15:56:19.200922: step 3787, loss = 4.50 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 15:56:24.108453: step 3788, loss = 4.37 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 15:56:28.975124: step 3789, loss = 4.37 (13.2 examples/sec; 4.867 sec/batch)
2016-04-30 15:56:33.924134: step 3790, loss = 4.33 (12.9 examples/sec; 4.949 sec/batch)
2016-04-30 15:56:45.387074: step 3791, loss = 4.38 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 15:56:50.231930: step 3792, loss = 4.36 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 15:56:55.799764: step 3793, loss = 4.38 (11.5 examples/sec; 5.568 sec/batch)
2016-04-30 15:57:00.984468: step 3794, loss = 4.33 (12.3 examples/sec; 5.185 sec/batch)
2016-04-30 15:57:05.736021: step 3795, loss = 4.31 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 15:57:10.708643: step 3796, loss = 4.32 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 15:57:15.722350: step 3797, loss = 4.44 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 15:57:20.462329: step 3798, loss = 4.28 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 15:57:26.232499: step 3799, loss = 4.38 (11.1 examples/sec; 5.770 sec/batch)
2016-04-30 15:57:31.191454: step 3800, loss = 4.22 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 15:57:42.434517: step 3801, loss = 4.39 (13.6 examples/sec; 4.691 sec/batch)
2016-04-30 15:57:47.164351: step 3802, loss = 4.38 (13.5 examples/sec; 4.730 sec/batch)
2016-04-30 15:57:52.136461: step 3803, loss = 4.19 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 15:57:56.757328: step 3804, loss = 4.27 (13.9 examples/sec; 4.621 sec/batch)
2016-04-30 15:58:02.422449: step 3805, loss = 4.27 (11.3 examples/sec; 5.665 sec/batch)
2016-04-30 15:58:07.450847: step 3806, loss = 4.30 (12.7 examples/sec; 5.028 sec/batch)
2016-04-30 15:58:12.380479: step 3807, loss = 4.28 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 15:58:17.283606: step 3808, loss = 4.33 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 15:58:22.244899: step 3809, loss = 4.41 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 15:58:26.871094: step 3810, loss = 4.34 (13.8 examples/sec; 4.626 sec/batch)
2016-04-30 15:58:38.808451: step 3811, loss = 4.29 (14.0 examples/sec; 4.581 sec/batch)
2016-04-30 15:58:43.778116: step 3812, loss = 4.25 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 15:58:48.454339: step 3813, loss = 4.38 (13.7 examples/sec; 4.676 sec/batch)
2016-04-30 15:58:53.187202: step 3814, loss = 4.34 (13.5 examples/sec; 4.733 sec/batch)
2016-04-30 15:58:58.131564: step 3815, loss = 4.30 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 15:59:02.874305: step 3816, loss = 4.09 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 15:59:08.451091: step 3817, loss = 4.27 (11.5 examples/sec; 5.577 sec/batch)
2016-04-30 15:59:13.366146: step 3818, loss = 4.16 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 15:59:17.903736: step 3819, loss = 4.24 (14.1 examples/sec; 4.538 sec/batch)
2016-04-30 15:59:22.815994: step 3820, loss = 4.23 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 15:59:34.351678: step 3821, loss = 4.29 (13.3 examples/sec; 4.800 sec/batch)
2016-04-30 15:59:39.984212: step 3822, loss = 4.34 (11.4 examples/sec; 5.632 sec/batch)
2016-04-30 15:59:44.700503: step 3823, loss = 4.22 (13.6 examples/sec; 4.716 sec/batch)
2016-04-30 15:59:49.678040: step 3824, loss = 4.38 (12.9 examples/sec; 4.977 sec/batch)
2016-04-30 15:59:54.266817: step 3825, loss = 4.19 (13.9 examples/sec; 4.589 sec/batch)
2016-04-30 15:59:59.176146: step 3826, loss = 4.27 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 16:00:04.251156: step 3827, loss = 4.35 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 16:00:08.960787: step 3828, loss = 4.40 (13.6 examples/sec; 4.710 sec/batch)
2016-04-30 16:00:14.273230: step 3829, loss = 4.27 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 16:00:19.235238: step 3830, loss = 4.29 (12.9 examples/sec; 4.962 sec/batch)
2016-04-30 16:00:30.758197: step 3831, loss = 4.26 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 16:00:35.497845: step 3832, loss = 4.13 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 16:00:40.370803: step 3833, loss = 4.19 (13.1 examples/sec; 4.873 sec/batch)
2016-04-30 16:00:45.964473: step 3834, loss = 4.22 (11.4 examples/sec; 5.594 sec/batch)
2016-04-30 16:00:50.785621: step 3835, loss = 4.46 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 16:00:56.034223: step 3836, loss = 4.08 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 16:01:01.176079: step 3837, loss = 4.26 (12.4 examples/sec; 5.142 sec/batch)
2016-04-30 16:01:05.907939: step 3838, loss = 4.16 (13.5 examples/sec; 4.732 sec/batch)
2016-04-30 16:01:11.011393: step 3839, loss = 4.25 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 16:01:16.065159: step 3840, loss = 4.30 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 16:01:27.497600: step 3841, loss = 4.29 (13.9 examples/sec; 4.596 sec/batch)
2016-04-30 16:01:32.305944: step 3842, loss = 4.17 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 16:01:37.276781: step 3843, loss = 4.10 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 16:01:41.910978: step 3844, loss = 4.33 (13.8 examples/sec; 4.634 sec/batch)
2016-04-30 16:01:46.837078: step 3845, loss = 4.27 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 16:01:52.188785: step 3846, loss = 4.31 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 16:01:56.833314: step 3847, loss = 4.49 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 16:02:01.877128: step 3848, loss = 4.27 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 16:02:06.962156: step 3849, loss = 4.26 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 16:02:12.016290: step 3850, loss = 4.24 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 16:02:24.232535: step 3851, loss = 4.20 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 16:02:29.138814: step 3852, loss = 4.11 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 16:02:34.070789: step 3853, loss = 4.22 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 16:02:38.741124: step 3854, loss = 4.15 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 16:02:43.569393: step 3855, loss = 4.02 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 16:02:48.373355: step 3856, loss = 4.36 (13.3 examples/sec; 4.804 sec/batch)
2016-04-30 16:02:53.117108: step 3857, loss = 4.06 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 16:02:58.556051: step 3858, loss = 4.29 (11.8 examples/sec; 5.439 sec/batch)
2016-04-30 16:03:03.815772: step 3859, loss = 4.06 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 16:03:08.438403: step 3860, loss = 4.32 (13.8 examples/sec; 4.623 sec/batch)
2016-04-30 16:03:20.386974: step 3861, loss = 4.18 (13.9 examples/sec; 4.597 sec/batch)
2016-04-30 16:03:25.319784: step 3862, loss = 4.29 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 16:03:30.735099: step 3863, loss = 4.12 (11.8 examples/sec; 5.415 sec/batch)
2016-04-30 16:03:35.576537: step 3864, loss = 4.29 (13.2 examples/sec; 4.841 sec/batch)
2016-04-30 16:03:40.652970: step 3865, loss = 4.03 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 16:03:45.676050: step 3866, loss = 4.17 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 16:03:50.596944: step 3867, loss = 4.29 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 16:03:55.610844: step 3868, loss = 4.07 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 16:04:00.606163: step 3869, loss = 4.24 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 16:04:06.160183: step 3870, loss = 4.09 (11.5 examples/sec; 5.554 sec/batch)
2016-04-30 16:04:17.389159: step 3871, loss = 4.15 (14.3 examples/sec; 4.481 sec/batch)
2016-04-30 16:04:22.348793: step 3872, loss = 4.19 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 16:04:27.093199: step 3873, loss = 4.11 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 16:04:32.150700: step 3874, loss = 4.20 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 16:04:37.545149: step 3875, loss = 4.13 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 16:04:42.679550: step 3876, loss = 4.20 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 16:04:47.556001: step 3877, loss = 4.12 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 16:04:52.594452: step 3878, loss = 4.04 (12.7 examples/sec; 5.038 sec/batch)
2016-04-30 16:04:57.258105: step 3879, loss = 4.25 (13.7 examples/sec; 4.664 sec/batch)
2016-04-30 16:05:02.439100: step 3880, loss = 4.11 (12.4 examples/sec; 5.181 sec/batch)
2016-04-30 16:05:14.028338: step 3881, loss = 4.01 (13.6 examples/sec; 4.695 sec/batch)
2016-04-30 16:05:19.010243: step 3882, loss = 4.32 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 16:05:23.667993: step 3883, loss = 4.00 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 16:05:28.646155: step 3884, loss = 4.15 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 16:05:33.333922: step 3885, loss = 4.09 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 16:05:38.239831: step 3886, loss = 4.08 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 16:05:43.707616: step 3887, loss = 4.18 (11.7 examples/sec; 5.468 sec/batch)
2016-04-30 16:05:48.338967: step 3888, loss = 3.96 (13.8 examples/sec; 4.631 sec/batch)
2016-04-30 16:05:53.157859: step 3889, loss = 4.08 (13.3 examples/sec; 4.819 sec/batch)
2016-04-30 16:05:58.086115: step 3890, loss = 3.95 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 16:06:09.395525: step 3891, loss = 4.12 (14.4 examples/sec; 4.454 sec/batch)
2016-04-30 16:06:15.015526: step 3892, loss = 4.12 (11.4 examples/sec; 5.620 sec/batch)
2016-04-30 16:06:19.862000: step 3893, loss = 4.09 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 16:06:24.916242: step 3894, loss = 3.97 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 16:06:29.647329: step 3895, loss = 4.07 (13.5 examples/sec; 4.731 sec/batch)
2016-04-30 16:06:34.641123: step 3896, loss = 4.15 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 16:06:39.537885: step 3897, loss = 4.04 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 16:06:44.412946: step 3898, loss = 4.14 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 16:06:49.935554: step 3899, loss = 3.90 (11.6 examples/sec; 5.523 sec/batch)
2016-04-30 16:06:54.896217: step 3900, loss = 3.98 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 16:07:06.028795: step 3901, loss = 4.26 (14.1 examples/sec; 4.538 sec/batch)
2016-04-30 16:07:11.345811: step 3902, loss = 3.93 (12.0 examples/sec; 5.317 sec/batch)
2016-04-30 16:07:16.466034: step 3903, loss = 4.10 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 16:07:21.912936: step 3904, loss = 4.00 (11.8 examples/sec; 5.447 sec/batch)
2016-04-30 16:07:26.590511: step 3905, loss = 4.14 (13.7 examples/sec; 4.677 sec/batch)
2016-04-30 16:07:31.557818: step 3906, loss = 4.14 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 16:07:36.487546: step 3907, loss = 4.07 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 16:07:41.446341: step 3908, loss = 4.12 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 16:07:46.354162: step 3909, loss = 3.89 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 16:07:51.128594: step 3910, loss = 3.87 (13.4 examples/sec; 4.774 sec/batch)
2016-04-30 16:08:03.411761: step 3911, loss = 4.08 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 16:08:08.142415: step 3912, loss = 4.11 (13.5 examples/sec; 4.731 sec/batch)
2016-04-30 16:08:13.382803: step 3913, loss = 4.13 (12.2 examples/sec; 5.240 sec/batch)
2016-04-30 16:08:18.000195: step 3914, loss = 4.16 (13.9 examples/sec; 4.617 sec/batch)
2016-04-30 16:08:22.829148: step 3915, loss = 4.12 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 16:08:28.349205: step 3916, loss = 4.10 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 16:08:32.915213: step 3917, loss = 4.16 (14.0 examples/sec; 4.566 sec/batch)
2016-04-30 16:08:37.823471: step 3918, loss = 4.03 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 16:08:42.635382: step 3919, loss = 3.92 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 16:08:47.405661: step 3920, loss = 4.13 (13.4 examples/sec; 4.770 sec/batch)
2016-04-30 16:08:59.350180: step 3921, loss = 3.98 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 16:09:04.391571: step 3922, loss = 3.93 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 16:09:09.075523: step 3923, loss = 4.03 (13.7 examples/sec; 4.684 sec/batch)
2016-04-30 16:09:13.969854: step 3924, loss = 3.99 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 16:09:18.998985: step 3925, loss = 4.07 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 16:09:23.628209: step 3926, loss = 3.98 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 16:09:28.531581: step 3927, loss = 3.98 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:09:33.659821: step 3928, loss = 4.06 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 16:09:38.402624: step 3929, loss = 4.10 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 16:09:43.426834: step 3930, loss = 3.87 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 16:09:54.451427: step 3931, loss = 4.24 (14.0 examples/sec; 4.561 sec/batch)
2016-04-30 16:09:59.340225: step 3932, loss = 4.11 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 16:10:05.232010: step 3933, loss = 4.08 (10.9 examples/sec; 5.892 sec/batch)
2016-04-30 16:10:10.287188: step 3934, loss = 3.97 (12.7 examples/sec; 5.055 sec/batch)
2016-04-30 16:10:15.232424: step 3935, loss = 4.03 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 16:10:20.439173: step 3936, loss = 3.99 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 16:10:25.422392: step 3937, loss = 3.97 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 16:10:30.264410: step 3938, loss = 3.93 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 16:10:35.060475: step 3939, loss = 4.03 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 16:10:40.509894: step 3940, loss = 4.02 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 16:10:51.968901: step 3941, loss = 4.03 (13.6 examples/sec; 4.709 sec/batch)
2016-04-30 16:10:56.802932: step 3942, loss = 3.96 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 16:11:01.907881: step 3943, loss = 3.96 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 16:11:06.824371: step 3944, loss = 4.02 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 16:11:12.050641: step 3945, loss = 4.01 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 16:11:16.994747: step 3946, loss = 3.84 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 16:11:22.035037: step 3947, loss = 4.09 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 16:11:26.789257: step 3948, loss = 3.80 (13.5 examples/sec; 4.754 sec/batch)
2016-04-30 16:11:31.776010: step 3949, loss = 4.01 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 16:11:36.656918: step 3950, loss = 3.92 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 16:11:48.543131: step 3951, loss = 3.85 (13.8 examples/sec; 4.636 sec/batch)
2016-04-30 16:11:53.481188: step 3952, loss = 3.92 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 16:11:58.510760: step 3953, loss = 3.74 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 16:12:03.799366: step 3954, loss = 3.90 (12.1 examples/sec; 5.289 sec/batch)
2016-04-30 16:12:08.482211: step 3955, loss = 4.13 (13.7 examples/sec; 4.683 sec/batch)
2016-04-30 16:12:13.554427: step 3956, loss = 3.79 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 16:12:19.042284: step 3957, loss = 4.05 (11.7 examples/sec; 5.488 sec/batch)
2016-04-30 16:12:24.096529: step 3958, loss = 3.96 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 16:12:29.033922: step 3959, loss = 3.96 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 16:12:34.051029: step 3960, loss = 3.94 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 16:12:45.415527: step 3961, loss = 4.14 (13.6 examples/sec; 4.718 sec/batch)
2016-04-30 16:12:50.820227: step 3962, loss = 3.93 (11.8 examples/sec; 5.405 sec/batch)
2016-04-30 16:12:55.709929: step 3963, loss = 4.05 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 16:13:00.499943: step 3964, loss = 4.15 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 16:13:05.294422: step 3965, loss = 4.03 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 16:13:10.192337: step 3966, loss = 3.91 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 16:13:15.115987: step 3967, loss = 4.18 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 16:13:20.135437: step 3968, loss = 3.80 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 16:13:25.693024: step 3969, loss = 3.94 (11.5 examples/sec; 5.557 sec/batch)
2016-04-30 16:13:30.556509: step 3970, loss = 3.94 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 16:13:41.789892: step 3971, loss = 4.22 (14.0 examples/sec; 4.556 sec/batch)
2016-04-30 16:13:46.882792: step 3972, loss = 3.79 (12.6 examples/sec; 5.093 sec/batch)
2016-04-30 16:13:51.571232: step 3973, loss = 4.23 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 16:13:56.800335: step 3974, loss = 3.93 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 16:14:02.170608: step 3975, loss = 3.96 (11.9 examples/sec; 5.370 sec/batch)
2016-04-30 16:14:07.223399: step 3976, loss = 3.92 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 16:14:12.041178: step 3977, loss = 4.08 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 16:14:17.072875: step 3978, loss = 4.00 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 16:14:22.130700: step 3979, loss = 3.94 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 16:14:26.868527: step 3980, loss = 3.94 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 16:14:38.983054: step 3981, loss = 4.00 (13.9 examples/sec; 4.605 sec/batch)
2016-04-30 16:14:43.920313: step 3982, loss = 3.97 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 16:14:48.678181: step 3983, loss = 3.81 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 16:14:53.276440: step 3984, loss = 4.12 (13.9 examples/sec; 4.598 sec/batch)
2016-04-30 16:14:58.295985: step 3985, loss = 3.87 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 16:15:03.703471: step 3986, loss = 3.78 (11.8 examples/sec; 5.407 sec/batch)
2016-04-30 16:15:08.409671: step 3987, loss = 3.84 (13.6 examples/sec; 4.706 sec/batch)
2016-04-30 16:15:13.715308: step 3988, loss = 3.94 (12.1 examples/sec; 5.306 sec/batch)
2016-04-30 16:15:18.389332: step 3989, loss = 3.95 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 16:15:23.420144: step 3990, loss = 3.79 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 16:15:35.402208: step 3991, loss = 3.91 (12.2 examples/sec; 5.244 sec/batch)
2016-04-30 16:15:40.466175: step 3992, loss = 3.82 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 16:15:45.227678: step 3993, loss = 3.83 (13.4 examples/sec; 4.761 sec/batch)
2016-04-30 16:15:50.125040: step 3994, loss = 4.02 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 16:15:54.972353: step 3995, loss = 3.80 (13.2 examples/sec; 4.847 sec/batch)
2016-04-30 16:15:59.822225: step 3996, loss = 3.67 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 16:16:04.927497: step 3997, loss = 3.97 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 16:16:10.702427: step 3998, loss = 3.82 (11.1 examples/sec; 5.775 sec/batch)
2016-04-30 16:16:15.395146: step 3999, loss = 3.95 (13.6 examples/sec; 4.693 sec/batch)
2016-04-30 16:16:20.353532: step 4000, loss = 4.06 (12.9 examples/sec; 4.958 sec/batch)
2016-04-30 16:16:31.883879: step 4001, loss = 3.81 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 16:16:36.826211: step 4002, loss = 3.83 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 16:16:42.019061: step 4003, loss = 3.83 (12.3 examples/sec; 5.193 sec/batch)
2016-04-30 16:16:47.069358: step 4004, loss = 4.00 (12.7 examples/sec; 5.050 sec/batch)
2016-04-30 16:16:52.108437: step 4005, loss = 3.98 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 16:16:56.939470: step 4006, loss = 3.93 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 16:17:02.247210: step 4007, loss = 3.80 (12.1 examples/sec; 5.308 sec/batch)
2016-04-30 16:17:07.235290: step 4008, loss = 3.89 (12.8 examples/sec; 4.988 sec/batch)
2016-04-30 16:17:11.978042: step 4009, loss = 3.71 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 16:17:17.311934: step 4010, loss = 3.76 (12.0 examples/sec; 5.334 sec/batch)
2016-04-30 16:17:28.422812: step 4011, loss = 3.96 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 16:17:33.075574: step 4012, loss = 3.76 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 16:17:38.311059: step 4013, loss = 3.82 (12.2 examples/sec; 5.235 sec/batch)
2016-04-30 16:17:43.566464: step 4014, loss = 3.96 (12.2 examples/sec; 5.255 sec/batch)
2016-04-30 16:17:49.207480: step 4015, loss = 3.97 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 16:17:54.099268: step 4016, loss = 3.72 (13.1 examples/sec; 4.892 sec/batch)
2016-04-30 16:17:59.318791: step 4017, loss = 3.78 (12.3 examples/sec; 5.219 sec/batch)
2016-04-30 16:18:04.563661: step 4018, loss = 3.85 (12.2 examples/sec; 5.245 sec/batch)
2016-04-30 16:18:09.113727: step 4019, loss = 3.69 (14.1 examples/sec; 4.550 sec/batch)
2016-04-30 16:18:14.127800: step 4020, loss = 3.78 (12.8 examples/sec; 5.014 sec/batch)
2016-04-30 16:18:26.047364: step 4021, loss = 3.81 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 16:18:31.096443: step 4022, loss = 3.88 (12.7 examples/sec; 5.049 sec/batch)
2016-04-30 16:18:36.002777: step 4023, loss = 3.89 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 16:18:41.055841: step 4024, loss = 3.63 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 16:18:45.892294: step 4025, loss = 3.94 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 16:18:50.685660: step 4026, loss = 3.88 (13.4 examples/sec; 4.793 sec/batch)
2016-04-30 16:18:56.192199: step 4027, loss = 3.67 (11.6 examples/sec; 5.506 sec/batch)
2016-04-30 16:19:01.473161: step 4028, loss = 3.83 (12.1 examples/sec; 5.281 sec/batch)
2016-04-30 16:19:06.311937: step 4029, loss = 3.82 (13.2 examples/sec; 4.839 sec/batch)
2016-04-30 16:19:11.370260: step 4030, loss = 3.89 (12.7 examples/sec; 5.058 sec/batch)
2016-04-30 16:19:22.611680: step 4031, loss = 3.82 (13.5 examples/sec; 4.727 sec/batch)
2016-04-30 16:19:28.086981: step 4032, loss = 3.81 (11.7 examples/sec; 5.475 sec/batch)
2016-04-30 16:19:32.994910: step 4033, loss = 3.86 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 16:19:38.125595: step 4034, loss = 3.77 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 16:19:43.071762: step 4035, loss = 3.81 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 16:19:47.785815: step 4036, loss = 3.73 (13.6 examples/sec; 4.714 sec/batch)
2016-04-30 16:19:52.684443: step 4037, loss = 3.83 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 16:19:57.375655: step 4038, loss = 3.85 (13.6 examples/sec; 4.691 sec/batch)
2016-04-30 16:20:03.117219: step 4039, loss = 3.74 (11.1 examples/sec; 5.741 sec/batch)
2016-04-30 16:20:08.146769: step 4040, loss = 3.81 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 16:20:19.581316: step 4041, loss = 3.58 (13.3 examples/sec; 4.819 sec/batch)
2016-04-30 16:20:24.202340: step 4042, loss = 3.93 (13.9 examples/sec; 4.621 sec/batch)
2016-04-30 16:20:29.091941: step 4043, loss = 3.81 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 16:20:34.612440: step 4044, loss = 3.76 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 16:20:39.282535: step 4045, loss = 3.76 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 16:20:44.269831: step 4046, loss = 3.74 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 16:20:49.182601: step 4047, loss = 3.81 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 16:20:53.930642: step 4048, loss = 3.86 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 16:20:58.697746: step 4049, loss = 3.88 (13.4 examples/sec; 4.767 sec/batch)
2016-04-30 16:21:03.988804: step 4050, loss = 3.83 (12.1 examples/sec; 5.291 sec/batch)
2016-04-30 16:21:15.663471: step 4051, loss = 3.89 (14.0 examples/sec; 4.585 sec/batch)
2016-04-30 16:21:20.901161: step 4052, loss = 3.68 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 16:21:25.919399: step 4053, loss = 3.79 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 16:21:30.802400: step 4054, loss = 3.68 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 16:21:35.639300: step 4055, loss = 3.81 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 16:21:41.312467: step 4056, loss = 3.49 (11.3 examples/sec; 5.673 sec/batch)
2016-04-30 16:21:46.281843: step 4057, loss = 3.81 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 16:21:51.264087: step 4058, loss = 3.85 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 16:21:56.259155: step 4059, loss = 3.71 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 16:22:01.493692: step 4060, loss = 3.85 (12.2 examples/sec; 5.234 sec/batch)
2016-04-30 16:22:13.417258: step 4061, loss = 3.87 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 16:22:18.267729: step 4062, loss = 3.77 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 16:22:23.155008: step 4063, loss = 3.80 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 16:22:28.175792: step 4064, loss = 3.72 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 16:22:32.940665: step 4065, loss = 3.68 (13.4 examples/sec; 4.765 sec/batch)
2016-04-30 16:22:37.979844: step 4066, loss = 3.70 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 16:22:42.693440: step 4067, loss = 3.75 (13.6 examples/sec; 4.714 sec/batch)
2016-04-30 16:22:48.119925: step 4068, loss = 3.81 (11.8 examples/sec; 5.426 sec/batch)
2016-04-30 16:22:53.229394: step 4069, loss = 3.59 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 16:22:57.992451: step 4070, loss = 3.73 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 16:23:09.694965: step 4071, loss = 3.69 (14.0 examples/sec; 4.570 sec/batch)
2016-04-30 16:23:14.525121: step 4072, loss = 3.73 (13.3 examples/sec; 4.830 sec/batch)
2016-04-30 16:23:19.992140: step 4073, loss = 3.82 (11.7 examples/sec; 5.467 sec/batch)
2016-04-30 16:23:24.640352: step 4074, loss = 3.82 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 16:23:29.551891: step 4075, loss = 3.61 (13.0 examples/sec; 4.911 sec/batch)
2016-04-30 16:23:34.595168: step 4076, loss = 3.64 (12.7 examples/sec; 5.043 sec/batch)
2016-04-30 16:23:39.357429: step 4077, loss = 3.75 (13.4 examples/sec; 4.762 sec/batch)
2016-04-30 16:23:44.360075: step 4078, loss = 3.78 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 16:23:49.630269: step 4079, loss = 3.87 (12.1 examples/sec; 5.270 sec/batch)
2016-04-30 16:23:54.412047: step 4080, loss = 3.69 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 16:24:06.073097: step 4081, loss = 3.85 (14.0 examples/sec; 4.562 sec/batch)
2016-04-30 16:24:12.241697: step 4082, loss = 3.74 (10.4 examples/sec; 6.168 sec/batch)
2016-04-30 16:24:17.691160: step 4083, loss = 3.66 (11.7 examples/sec; 5.449 sec/batch)
2016-04-30 16:24:23.353669: step 4084, loss = 3.75 (11.3 examples/sec; 5.662 sec/batch)
2016-04-30 16:24:28.236162: step 4085, loss = 3.58 (13.1 examples/sec; 4.882 sec/batch)
2016-04-30 16:24:33.045448: step 4086, loss = 3.66 (13.3 examples/sec; 4.809 sec/batch)
2016-04-30 16:24:38.050112: step 4087, loss = 3.67 (12.8 examples/sec; 5.005 sec/batch)
2016-04-30 16:24:42.983347: step 4088, loss = 3.84 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 16:24:47.748005: step 4089, loss = 3.92 (13.4 examples/sec; 4.765 sec/batch)
2016-04-30 16:24:52.816206: step 4090, loss = 3.63 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 16:25:04.830492: step 4091, loss = 3.70 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 16:25:09.405690: step 4092, loss = 3.72 (14.0 examples/sec; 4.575 sec/batch)
2016-04-30 16:25:14.273652: step 4093, loss = 3.72 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 16:25:19.128141: step 4094, loss = 3.51 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 16:25:23.796264: step 4095, loss = 3.59 (13.7 examples/sec; 4.668 sec/batch)
2016-04-30 16:25:29.505640: step 4096, loss = 3.68 (11.2 examples/sec; 5.709 sec/batch)
2016-04-30 16:25:34.384891: step 4097, loss = 3.44 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 16:25:38.953809: step 4098, loss = 3.58 (14.0 examples/sec; 4.569 sec/batch)
2016-04-30 16:25:43.865983: step 4099, loss = 3.64 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 16:25:48.512250: step 4100, loss = 3.78 (13.8 examples/sec; 4.646 sec/batch)
2016-04-30 16:25:59.898812: step 4101, loss = 3.61 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 16:26:05.528827: step 4102, loss = 3.60 (11.4 examples/sec; 5.630 sec/batch)
2016-04-30 16:26:10.567905: step 4103, loss = 3.69 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 16:26:15.306952: step 4104, loss = 3.64 (13.5 examples/sec; 4.739 sec/batch)
2016-04-30 16:26:20.255105: step 4105, loss = 3.57 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 16:26:25.368068: step 4106, loss = 3.79 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 16:26:30.116083: step 4107, loss = 3.58 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 16:26:35.526924: step 4108, loss = 3.50 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 16:26:40.540455: step 4109, loss = 3.55 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 16:26:45.151651: step 4110, loss = 3.69 (13.9 examples/sec; 4.611 sec/batch)
2016-04-30 16:26:56.465012: step 4111, loss = 3.78 (13.6 examples/sec; 4.700 sec/batch)
2016-04-30 16:27:01.664484: step 4112, loss = 3.62 (12.3 examples/sec; 5.199 sec/batch)
2016-04-30 16:27:06.293414: step 4113, loss = 3.66 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 16:27:11.806276: step 4114, loss = 3.62 (11.6 examples/sec; 5.513 sec/batch)
2016-04-30 16:27:16.892154: step 4115, loss = 3.59 (12.6 examples/sec; 5.086 sec/batch)
2016-04-30 16:27:21.572492: step 4116, loss = 3.49 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 16:27:26.451596: step 4117, loss = 3.48 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 16:27:31.278271: step 4118, loss = 3.69 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 16:27:36.036406: step 4119, loss = 3.53 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 16:27:41.564966: step 4120, loss = 3.70 (11.6 examples/sec; 5.528 sec/batch)
2016-04-30 16:27:53.060511: step 4121, loss = 3.76 (13.2 examples/sec; 4.864 sec/batch)
2016-04-30 16:27:57.818680: step 4122, loss = 3.79 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 16:28:02.809691: step 4123, loss = 3.72 (12.8 examples/sec; 4.991 sec/batch)
2016-04-30 16:28:08.106037: step 4124, loss = 3.60 (12.1 examples/sec; 5.296 sec/batch)
2016-04-30 16:28:13.042622: step 4125, loss = 3.76 (13.0 examples/sec; 4.936 sec/batch)
2016-04-30 16:28:18.299839: step 4126, loss = 3.46 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 16:28:23.259334: step 4127, loss = 3.78 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 16:28:28.095127: step 4128, loss = 3.57 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 16:28:32.839542: step 4129, loss = 3.53 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 16:28:37.766584: step 4130, loss = 3.77 (13.0 examples/sec; 4.927 sec/batch)
2016-04-30 16:28:49.680475: step 4131, loss = 3.55 (12.1 examples/sec; 5.276 sec/batch)
2016-04-30 16:28:54.311407: step 4132, loss = 3.45 (13.8 examples/sec; 4.630 sec/batch)
2016-04-30 16:28:59.151175: step 4133, loss = 3.71 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 16:29:04.257612: step 4134, loss = 3.48 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 16:29:08.844998: step 4135, loss = 3.68 (14.0 examples/sec; 4.587 sec/batch)
2016-04-30 16:29:13.906399: step 4136, loss = 3.60 (12.6 examples/sec; 5.061 sec/batch)
2016-04-30 16:29:18.482059: step 4137, loss = 3.59 (14.0 examples/sec; 4.576 sec/batch)
2016-04-30 16:29:23.865210: step 4138, loss = 3.63 (11.9 examples/sec; 5.383 sec/batch)
2016-04-30 16:29:28.758264: step 4139, loss = 3.63 (13.1 examples/sec; 4.893 sec/batch)
2016-04-30 16:29:33.385534: step 4140, loss = 3.63 (13.8 examples/sec; 4.627 sec/batch)
2016-04-30 16:29:45.090895: step 4141, loss = 3.62 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 16:29:50.233043: step 4142, loss = 3.76 (12.4 examples/sec; 5.142 sec/batch)
2016-04-30 16:29:55.674210: step 4143, loss = 3.69 (11.8 examples/sec; 5.441 sec/batch)
2016-04-30 16:30:00.520593: step 4144, loss = 3.58 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 16:30:05.531783: step 4145, loss = 3.65 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 16:30:10.530783: step 4146, loss = 3.54 (12.8 examples/sec; 4.999 sec/batch)
2016-04-30 16:30:15.254879: step 4147, loss = 3.55 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 16:30:20.213688: step 4148, loss = 3.63 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 16:30:25.183446: step 4149, loss = 3.61 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 16:30:30.497969: step 4150, loss = 3.37 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 16:30:41.850369: step 4151, loss = 3.56 (13.8 examples/sec; 4.626 sec/batch)
2016-04-30 16:30:46.770989: step 4152, loss = 3.59 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 16:30:51.567031: step 4153, loss = 3.60 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 16:30:56.340459: step 4154, loss = 3.61 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 16:31:01.924455: step 4155, loss = 3.31 (11.5 examples/sec; 5.584 sec/batch)
2016-04-30 16:31:06.573150: step 4156, loss = 3.43 (13.8 examples/sec; 4.649 sec/batch)
2016-04-30 16:31:11.450283: step 4157, loss = 3.56 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 16:31:16.418210: step 4158, loss = 3.56 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 16:31:21.153493: step 4159, loss = 3.42 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 16:31:26.134862: step 4160, loss = 3.58 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 16:31:38.139557: step 4161, loss = 3.71 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 16:31:42.920693: step 4162, loss = 3.62 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 16:31:47.704112: step 4163, loss = 3.57 (13.4 examples/sec; 4.783 sec/batch)
2016-04-30 16:31:52.663604: step 4164, loss = 3.66 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 16:31:57.323805: step 4165, loss = 3.69 (13.7 examples/sec; 4.660 sec/batch)
2016-04-30 16:32:02.494774: step 4166, loss = 3.40 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 16:32:07.996479: step 4167, loss = 3.55 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 16:32:12.800423: step 4168, loss = 3.43 (13.3 examples/sec; 4.804 sec/batch)
2016-04-30 16:32:17.899822: step 4169, loss = 3.67 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 16:32:22.855422: step 4170, loss = 3.61 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 16:32:34.142603: step 4171, loss = 3.44 (13.7 examples/sec; 4.655 sec/batch)
2016-04-30 16:32:39.296506: step 4172, loss = 3.42 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 16:32:44.326223: step 4173, loss = 3.62 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 16:32:49.330400: step 4174, loss = 3.63 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 16:32:54.034277: step 4175, loss = 3.47 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 16:32:59.030477: step 4176, loss = 3.53 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 16:33:04.015868: step 4177, loss = 3.70 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 16:33:08.838522: step 4178, loss = 3.34 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 16:33:14.436067: step 4179, loss = 3.36 (11.4 examples/sec; 5.597 sec/batch)
2016-04-30 16:33:19.403675: step 4180, loss = 3.54 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 16:33:30.633605: step 4181, loss = 3.43 (14.2 examples/sec; 4.513 sec/batch)
2016-04-30 16:33:35.633559: step 4182, loss = 3.62 (12.8 examples/sec; 5.000 sec/batch)
2016-04-30 16:33:40.495048: step 4183, loss = 3.62 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 16:33:46.034146: step 4184, loss = 3.42 (11.6 examples/sec; 5.539 sec/batch)
2016-04-30 16:33:50.858990: step 4185, loss = 3.53 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 16:33:55.836454: step 4186, loss = 3.45 (12.9 examples/sec; 4.977 sec/batch)
2016-04-30 16:34:00.723322: step 4187, loss = 3.46 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 16:34:05.738211: step 4188, loss = 3.57 (12.8 examples/sec; 5.015 sec/batch)
2016-04-30 16:34:10.649054: step 4189, loss = 3.38 (13.0 examples/sec; 4.911 sec/batch)
2016-04-30 16:34:15.353866: step 4190, loss = 3.57 (13.6 examples/sec; 4.705 sec/batch)
2016-04-30 16:34:27.269038: step 4191, loss = 3.47 (14.1 examples/sec; 4.531 sec/batch)
2016-04-30 16:34:32.225031: step 4192, loss = 3.35 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 16:34:36.978456: step 4193, loss = 3.63 (13.5 examples/sec; 4.753 sec/batch)
2016-04-30 16:34:41.774407: step 4194, loss = 3.50 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 16:34:46.801292: step 4195, loss = 3.45 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 16:34:52.169945: step 4196, loss = 3.63 (11.9 examples/sec; 5.369 sec/batch)
2016-04-30 16:34:56.943024: step 4197, loss = 3.41 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 16:35:02.254547: step 4198, loss = 3.37 (12.0 examples/sec; 5.311 sec/batch)
2016-04-30 16:35:07.207059: step 4199, loss = 3.54 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 16:35:11.969448: step 4200, loss = 3.51 (13.4 examples/sec; 4.762 sec/batch)
2016-04-30 16:35:23.809382: step 4201, loss = 3.29 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 16:35:28.678985: step 4202, loss = 3.52 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 16:35:33.390682: step 4203, loss = 3.57 (13.6 examples/sec; 4.712 sec/batch)
2016-04-30 16:35:38.287747: step 4204, loss = 3.44 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 16:35:43.223499: step 4205, loss = 3.49 (13.0 examples/sec; 4.936 sec/batch)
2016-04-30 16:35:47.961095: step 4206, loss = 3.56 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 16:35:52.955058: step 4207, loss = 3.34 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 16:35:58.391154: step 4208, loss = 3.44 (11.8 examples/sec; 5.436 sec/batch)
2016-04-30 16:36:03.257480: step 4209, loss = 3.56 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 16:36:08.235895: step 4210, loss = 3.44 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 16:36:19.661973: step 4211, loss = 3.33 (13.9 examples/sec; 4.615 sec/batch)
2016-04-30 16:36:24.296418: step 4212, loss = 3.36 (13.8 examples/sec; 4.634 sec/batch)
2016-04-30 16:36:29.817092: step 4213, loss = 3.32 (11.6 examples/sec; 5.521 sec/batch)
2016-04-30 16:36:34.747479: step 4214, loss = 3.36 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 16:36:39.377921: step 4215, loss = 3.50 (13.8 examples/sec; 4.630 sec/batch)
2016-04-30 16:36:44.499878: step 4216, loss = 3.33 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 16:36:49.403394: step 4217, loss = 3.42 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:36:53.967082: step 4218, loss = 3.51 (14.0 examples/sec; 4.564 sec/batch)
2016-04-30 16:36:59.110142: step 4219, loss = 3.51 (12.4 examples/sec; 5.143 sec/batch)
2016-04-30 16:37:04.737893: step 4220, loss = 3.41 (11.4 examples/sec; 5.628 sec/batch)
2016-04-30 16:37:16.455729: step 4221, loss = 3.51 (13.4 examples/sec; 4.793 sec/batch)
2016-04-30 16:37:21.240448: step 4222, loss = 3.48 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 16:37:26.243905: step 4223, loss = 3.62 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 16:37:30.988201: step 4224, loss = 3.40 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 16:37:36.324999: step 4225, loss = 3.37 (12.0 examples/sec; 5.337 sec/batch)
2016-04-30 16:37:41.228862: step 4226, loss = 3.29 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 16:37:46.073499: step 4227, loss = 3.42 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 16:37:51.019433: step 4228, loss = 3.49 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 16:37:55.998171: step 4229, loss = 3.40 (12.9 examples/sec; 4.979 sec/batch)
2016-04-30 16:38:00.867292: step 4230, loss = 3.64 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 16:38:12.643497: step 4231, loss = 3.37 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 16:38:17.705689: step 4232, loss = 3.36 (12.6 examples/sec; 5.062 sec/batch)
2016-04-30 16:38:22.812862: step 4233, loss = 3.23 (12.5 examples/sec; 5.107 sec/batch)
2016-04-30 16:38:27.499583: step 4234, loss = 3.45 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 16:38:32.497368: step 4235, loss = 3.34 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 16:38:37.475282: step 4236, loss = 3.41 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 16:38:42.708440: step 4237, loss = 3.34 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 16:38:47.847541: step 4238, loss = 3.48 (12.5 examples/sec; 5.139 sec/batch)
2016-04-30 16:38:52.864733: step 4239, loss = 3.52 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 16:38:57.566934: step 4240, loss = 3.56 (13.6 examples/sec; 4.702 sec/batch)
2016-04-30 16:39:08.914467: step 4241, loss = 3.43 (14.7 examples/sec; 4.354 sec/batch)
2016-04-30 16:39:14.256152: step 4242, loss = 3.50 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 16:39:19.057696: step 4243, loss = 3.44 (13.3 examples/sec; 4.801 sec/batch)
2016-04-30 16:39:23.782537: step 4244, loss = 3.55 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 16:39:28.668449: step 4245, loss = 3.33 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 16:39:33.263253: step 4246, loss = 3.32 (13.9 examples/sec; 4.595 sec/batch)
2016-04-30 16:39:38.330808: step 4247, loss = 3.36 (12.6 examples/sec; 5.067 sec/batch)
2016-04-30 16:39:43.379063: step 4248, loss = 3.38 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 16:39:48.673022: step 4249, loss = 3.41 (12.1 examples/sec; 5.294 sec/batch)
2016-04-30 16:39:53.593057: step 4250, loss = 3.41 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 16:40:05.146211: step 4251, loss = 3.31 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 16:40:10.181367: step 4252, loss = 3.50 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 16:40:14.819655: step 4253, loss = 3.46 (13.8 examples/sec; 4.638 sec/batch)
2016-04-30 16:40:19.919999: step 4254, loss = 3.34 (12.5 examples/sec; 5.100 sec/batch)
2016-04-30 16:40:24.837838: step 4255, loss = 3.35 (13.0 examples/sec; 4.918 sec/batch)
2016-04-30 16:40:29.798546: step 4256, loss = 3.44 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 16:40:34.634982: step 4257, loss = 3.43 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 16:40:39.281644: step 4258, loss = 3.17 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 16:40:44.093351: step 4259, loss = 3.50 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 16:40:48.737744: step 4260, loss = 3.31 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 16:41:00.836983: step 4261, loss = 3.36 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 16:41:05.746081: step 4262, loss = 3.46 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 16:41:10.761957: step 4263, loss = 3.32 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 16:41:15.553848: step 4264, loss = 3.47 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 16:41:20.480260: step 4265, loss = 3.53 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 16:41:25.655612: step 4266, loss = 3.36 (12.4 examples/sec; 5.175 sec/batch)
2016-04-30 16:41:30.702266: step 4267, loss = 3.37 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 16:41:35.737143: step 4268, loss = 3.46 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 16:41:40.684015: step 4269, loss = 3.46 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 16:41:45.468163: step 4270, loss = 3.29 (13.4 examples/sec; 4.784 sec/batch)
2016-04-30 16:41:56.655073: step 4271, loss = 3.35 (14.1 examples/sec; 4.538 sec/batch)
2016-04-30 16:42:02.191828: step 4272, loss = 3.40 (11.6 examples/sec; 5.537 sec/batch)
2016-04-30 16:42:06.901958: step 4273, loss = 3.36 (13.6 examples/sec; 4.710 sec/batch)
2016-04-30 16:42:11.838233: step 4274, loss = 3.53 (13.0 examples/sec; 4.936 sec/batch)
2016-04-30 16:42:16.918438: step 4275, loss = 3.31 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 16:42:21.496537: step 4276, loss = 3.22 (14.0 examples/sec; 4.578 sec/batch)
2016-04-30 16:42:26.433505: step 4277, loss = 3.48 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 16:42:31.222738: step 4278, loss = 3.36 (13.4 examples/sec; 4.789 sec/batch)
2016-04-30 16:42:36.479479: step 4279, loss = 3.54 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 16:42:41.385009: step 4280, loss = 3.27 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 16:42:52.530466: step 4281, loss = 3.36 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 16:42:57.264519: step 4282, loss = 3.43 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 16:43:02.315777: step 4283, loss = 3.35 (12.7 examples/sec; 5.051 sec/batch)
2016-04-30 16:43:07.712343: step 4284, loss = 3.16 (11.9 examples/sec; 5.396 sec/batch)
2016-04-30 16:43:12.344557: step 4285, loss = 3.38 (13.8 examples/sec; 4.632 sec/batch)
2016-04-30 16:43:17.273689: step 4286, loss = 3.36 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 16:43:22.236736: step 4287, loss = 3.25 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 16:43:27.035686: step 4288, loss = 3.26 (13.3 examples/sec; 4.799 sec/batch)
2016-04-30 16:43:31.915574: step 4289, loss = 3.43 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 16:43:36.613121: step 4290, loss = 3.51 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 16:43:48.453435: step 4291, loss = 3.27 (14.2 examples/sec; 4.497 sec/batch)
2016-04-30 16:43:53.422039: step 4292, loss = 3.18 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 16:43:58.198999: step 4293, loss = 3.16 (13.4 examples/sec; 4.777 sec/batch)
2016-04-30 16:44:03.272331: step 4294, loss = 3.23 (12.6 examples/sec; 5.073 sec/batch)
2016-04-30 16:44:08.262814: step 4295, loss = 3.31 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 16:44:13.606109: step 4296, loss = 3.28 (12.0 examples/sec; 5.343 sec/batch)
2016-04-30 16:44:18.386927: step 4297, loss = 3.26 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 16:44:23.255806: step 4298, loss = 3.26 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 16:44:28.096916: step 4299, loss = 3.33 (13.2 examples/sec; 4.841 sec/batch)
2016-04-30 16:44:32.891067: step 4300, loss = 3.47 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 16:44:44.729464: step 4301, loss = 3.25 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 16:44:49.696429: step 4302, loss = 3.28 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 16:44:54.379147: step 4303, loss = 3.35 (13.7 examples/sec; 4.683 sec/batch)
2016-04-30 16:44:59.281756: step 4304, loss = 3.50 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:45:04.145166: step 4305, loss = 3.48 (13.2 examples/sec; 4.863 sec/batch)
2016-04-30 16:45:08.847750: step 4306, loss = 3.29 (13.6 examples/sec; 4.702 sec/batch)
2016-04-30 16:45:13.817525: step 4307, loss = 3.43 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 16:45:18.985920: step 4308, loss = 3.46 (12.4 examples/sec; 5.168 sec/batch)
2016-04-30 16:45:23.925043: step 4309, loss = 3.19 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 16:45:29.131554: step 4310, loss = 3.32 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 16:45:40.553431: step 4311, loss = 3.30 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 16:45:45.330839: step 4312, loss = 3.33 (13.4 examples/sec; 4.777 sec/batch)
2016-04-30 16:45:50.821957: step 4313, loss = 3.38 (11.7 examples/sec; 5.491 sec/batch)
2016-04-30 16:45:55.756455: step 4314, loss = 3.54 (13.0 examples/sec; 4.934 sec/batch)
2016-04-30 16:46:00.686358: step 4315, loss = 3.35 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 16:46:05.584421: step 4316, loss = 3.40 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 16:46:10.536424: step 4317, loss = 3.35 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 16:46:15.221015: step 4318, loss = 3.51 (13.7 examples/sec; 4.685 sec/batch)
2016-04-30 16:46:20.267896: step 4319, loss = 3.78 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 16:46:25.623374: step 4320, loss = 5.02 (12.0 examples/sec; 5.355 sec/batch)
2016-04-30 16:46:36.775262: step 4321, loss = 4.93 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 16:46:41.714783: step 4322, loss = 4.38 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 16:46:46.821032: step 4323, loss = 3.54 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 16:46:51.428411: step 4324, loss = 3.48 (13.9 examples/sec; 4.607 sec/batch)
2016-04-30 16:46:56.948886: step 4325, loss = 3.47 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 16:47:02.119626: step 4326, loss = 3.62 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 16:47:06.973867: step 4327, loss = 4.78 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 16:47:11.965892: step 4328, loss = 5.64 (12.8 examples/sec; 4.992 sec/batch)
2016-04-30 16:47:16.931293: step 4329, loss = 5.10 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 16:47:21.728026: step 4330, loss = 6.44 (13.3 examples/sec; 4.797 sec/batch)
2016-04-30 16:47:33.355671: step 4331, loss = 5.09 (12.3 examples/sec; 5.205 sec/batch)
2016-04-30 16:47:38.360176: step 4332, loss = 4.28 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 16:47:42.933285: step 4333, loss = 3.37 (14.0 examples/sec; 4.573 sec/batch)
2016-04-30 16:47:47.913808: step 4334, loss = 4.07 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 16:47:52.960445: step 4335, loss = 12.70 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 16:47:57.818254: step 4336, loss = 11.97 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 16:48:03.376438: step 4337, loss = 9.26 (11.5 examples/sec; 5.558 sec/batch)
2016-04-30 16:48:08.063057: step 4338, loss = 6.89 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 16:48:13.704595: step 4339, loss = 3.93 (11.3 examples/sec; 5.641 sec/batch)
2016-04-30 16:48:18.585382: step 4340, loss = 4.17 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 16:48:29.836252: step 4341, loss = 3.52 (14.2 examples/sec; 4.496 sec/batch)
2016-04-30 16:48:35.155339: step 4342, loss = 3.33 (12.0 examples/sec; 5.319 sec/batch)
2016-04-30 16:48:40.042475: step 4343, loss = 4.05 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 16:48:44.945541: step 4344, loss = 7.52 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:48:50.023746: step 4345, loss = 8.97 (12.6 examples/sec; 5.078 sec/batch)
2016-04-30 16:48:54.792076: step 4346, loss = 17.55 (13.4 examples/sec; 4.768 sec/batch)
2016-04-30 16:48:59.686458: step 4347, loss = 14.89 (13.1 examples/sec; 4.894 sec/batch)
2016-04-30 16:49:04.725796: step 4348, loss = 7.65 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 16:49:09.881398: step 4349, loss = 6.64 (12.4 examples/sec; 5.156 sec/batch)
2016-04-30 16:49:14.864479: step 4350, loss = 5.57 (12.8 examples/sec; 4.983 sec/batch)
2016-04-30 16:49:26.004449: step 4351, loss = 4.79 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 16:49:30.782063: step 4352, loss = 4.67 (13.4 examples/sec; 4.778 sec/batch)
2016-04-30 16:49:35.685122: step 4353, loss = 5.52 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:49:40.833143: step 4354, loss = 3.91 (12.4 examples/sec; 5.148 sec/batch)
2016-04-30 16:49:45.834905: step 4355, loss = 5.30 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 16:49:50.999656: step 4356, loss = 5.73 (12.4 examples/sec; 5.165 sec/batch)
2016-04-30 16:49:55.943391: step 4357, loss = 7.14 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 16:50:00.906614: step 4358, loss = 6.06 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 16:50:06.027224: step 4359, loss = 14.43 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 16:50:10.989169: step 4360, loss = 20.07 (12.9 examples/sec; 4.962 sec/batch)
2016-04-30 16:50:22.896439: step 4361, loss = 17.64 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 16:50:27.654574: step 4362, loss = 10.61 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 16:50:32.599440: step 4363, loss = 7.73 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 16:50:37.387149: step 4364, loss = 4.82 (13.4 examples/sec; 4.788 sec/batch)
2016-04-30 16:50:42.026635: step 4365, loss = 4.33 (13.8 examples/sec; 4.639 sec/batch)
2016-04-30 16:50:47.608150: step 4366, loss = 4.16 (11.5 examples/sec; 5.581 sec/batch)
2016-04-30 16:50:52.280741: step 4367, loss = 3.74 (13.7 examples/sec; 4.673 sec/batch)
2016-04-30 16:50:57.042066: step 4368, loss = 3.84 (13.4 examples/sec; 4.761 sec/batch)
2016-04-30 16:51:02.230972: step 4369, loss = 4.37 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 16:51:06.905408: step 4370, loss = 4.33 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 16:51:17.880126: step 4371, loss = 9.92 (14.0 examples/sec; 4.558 sec/batch)
2016-04-30 16:51:23.246165: step 4372, loss = 8.23 (11.9 examples/sec; 5.366 sec/batch)
2016-04-30 16:51:27.924447: step 4373, loss = 11.43 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 16:51:32.692782: step 4374, loss = 23.91 (13.4 examples/sec; 4.768 sec/batch)
2016-04-30 16:51:37.589457: step 4375, loss = 23.35 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 16:51:42.326535: step 4376, loss = 15.02 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 16:51:47.341348: step 4377, loss = 10.07 (12.8 examples/sec; 5.015 sec/batch)
2016-04-30 16:51:52.013415: step 4378, loss = 8.85 (13.7 examples/sec; 4.672 sec/batch)
2016-04-30 16:51:57.444426: step 4379, loss = 9.46 (11.8 examples/sec; 5.431 sec/batch)
2016-04-30 16:52:02.478213: step 4380, loss = 10.04 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 16:52:13.668612: step 4381, loss = 8.15 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 16:52:18.739809: step 4382, loss = 12.50 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 16:52:23.867643: step 4383, loss = 36.46 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 16:52:29.204677: step 4384, loss = 62.11 (12.0 examples/sec; 5.337 sec/batch)
2016-04-30 16:52:33.898585: step 4385, loss = 13.08 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 16:52:38.829184: step 4386, loss = 9.79 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 16:52:43.960343: step 4387, loss = 11.27 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 16:52:48.485253: step 4388, loss = 17.11 (14.1 examples/sec; 4.525 sec/batch)
2016-04-30 16:52:53.343971: step 4389, loss = 21.44 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 16:52:57.922130: step 4390, loss = 17.65 (14.0 examples/sec; 4.578 sec/batch)
2016-04-30 16:53:10.173142: step 4391, loss = 7.39 (13.8 examples/sec; 4.649 sec/batch)
2016-04-30 16:53:15.058479: step 4392, loss = 20.70 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 16:53:19.980043: step 4393, loss = 29.18 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 16:53:24.695094: step 4394, loss = 25.66 (13.6 examples/sec; 4.715 sec/batch)
2016-04-30 16:53:29.636119: step 4395, loss = 14.83 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 16:53:35.161296: step 4396, loss = 23.43 (11.6 examples/sec; 5.525 sec/batch)
2016-04-30 16:53:39.850815: step 4397, loss = 19.99 (13.6 examples/sec; 4.689 sec/batch)
2016-04-30 16:53:44.877160: step 4398, loss = 27.19 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 16:53:50.015376: step 4399, loss = 33.90 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 16:53:54.868183: step 4400, loss = 31.50 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 16:54:08.684034: step 4401, loss = 23.14 (11.4 examples/sec; 5.603 sec/batch)
2016-04-30 16:54:13.604266: step 4402, loss = 23.73 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 16:54:18.210850: step 4403, loss = 16.99 (13.9 examples/sec; 4.606 sec/batch)
2016-04-30 16:54:23.334926: step 4404, loss = 7.19 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 16:54:28.063214: step 4405, loss = 8.42 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 16:54:33.045175: step 4406, loss = 10.84 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 16:54:38.337927: step 4407, loss = 25.17 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 16:54:43.256828: step 4408, loss = 39.34 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 16:54:48.160745: step 4409, loss = 38.21 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 16:54:53.262799: step 4410, loss = 38.89 (12.5 examples/sec; 5.102 sec/batch)
2016-04-30 16:55:05.174748: step 4411, loss = 29.83 (12.6 examples/sec; 5.074 sec/batch)
2016-04-30 16:55:09.905007: step 4412, loss = 33.99 (13.5 examples/sec; 4.730 sec/batch)
2016-04-30 16:55:15.479974: step 4413, loss = 18.26 (11.5 examples/sec; 5.575 sec/batch)
2016-04-30 16:55:20.458068: step 4414, loss = 12.13 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 16:55:25.217452: step 4415, loss = 35.41 (13.4 examples/sec; 4.759 sec/batch)
2016-04-30 16:55:30.138622: step 4416, loss = 35.68 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 16:55:35.269508: step 4417, loss = 64.67 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 16:55:39.991339: step 4418, loss = 50.54 (13.6 examples/sec; 4.722 sec/batch)
2016-04-30 16:55:45.379147: step 4419, loss = 35.77 (11.9 examples/sec; 5.388 sec/batch)
2016-04-30 16:55:50.444373: step 4420, loss = 34.54 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 16:56:02.078846: step 4421, loss = 39.88 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 16:56:07.110801: step 4422, loss = 56.23 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 16:56:12.051555: step 4423, loss = 56.98 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 16:56:17.698854: step 4424, loss = 69.84 (11.3 examples/sec; 5.647 sec/batch)
2016-04-30 16:56:22.734197: step 4425, loss = 36.64 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 16:56:27.513190: step 4426, loss = 25.90 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 16:56:32.537171: step 4427, loss = 25.13 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 16:56:37.860251: step 4428, loss = 25.17 (12.0 examples/sec; 5.323 sec/batch)
2016-04-30 16:56:42.553881: step 4429, loss = 69.81 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 16:56:47.783044: step 4430, loss = 37.42 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 16:57:00.054941: step 4431, loss = 60.69 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 16:57:05.428338: step 4432, loss = 141.79 (11.9 examples/sec; 5.373 sec/batch)
2016-04-30 16:57:10.162910: step 4433, loss = 114.20 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 16:57:15.228327: step 4434, loss = 33.10 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 16:57:20.166336: step 4435, loss = 19.19 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 16:57:25.960627: step 4436, loss = 55.55 (11.0 examples/sec; 5.794 sec/batch)
2016-04-30 16:57:32.019160: step 4437, loss = 91.36 (10.6 examples/sec; 6.058 sec/batch)
2016-04-30 16:57:36.853084: step 4438, loss = 603.19 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 16:57:41.819107: step 4439, loss = 38.32 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 16:57:46.836013: step 4440, loss = 4.69 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 16:57:58.240558: step 4441, loss = 4.23 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 16:58:03.417445: step 4442, loss = 3.95 (12.4 examples/sec; 5.177 sec/batch)
2016-04-30 16:58:08.351026: step 4443, loss = 3.46 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 16:58:13.007284: step 4444, loss = 3.34 (13.7 examples/sec; 4.656 sec/batch)
2016-04-30 16:58:18.040234: step 4445, loss = 3.20 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 16:58:23.024864: step 4446, loss = 3.36 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 16:58:27.708400: step 4447, loss = 3.39 (13.7 examples/sec; 4.683 sec/batch)
2016-04-30 16:58:33.248673: step 4448, loss = 3.42 (11.6 examples/sec; 5.540 sec/batch)
2016-04-30 16:58:38.296705: step 4449, loss = 3.36 (12.7 examples/sec; 5.048 sec/batch)
2016-04-30 16:58:42.995362: step 4450, loss = 3.39 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 16:58:54.312555: step 4451, loss = 3.20 (14.2 examples/sec; 4.496 sec/batch)
2016-04-30 16:58:59.210086: step 4452, loss = 3.50 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 16:59:04.735893: step 4453, loss = 3.25 (11.6 examples/sec; 5.526 sec/batch)
2016-04-30 16:59:09.414116: step 4454, loss = 3.31 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 16:59:14.403907: step 4455, loss = 3.48 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 16:59:19.061305: step 4456, loss = 3.46 (13.7 examples/sec; 4.657 sec/batch)
2016-04-30 16:59:23.944023: step 4457, loss = 3.28 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 16:59:28.842332: step 4458, loss = 3.30 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 16:59:33.605588: step 4459, loss = 3.35 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 16:59:39.170038: step 4460, loss = 3.35 (11.5 examples/sec; 5.564 sec/batch)
2016-04-30 16:59:50.520343: step 4461, loss = 3.07 (13.2 examples/sec; 4.864 sec/batch)
2016-04-30 16:59:55.301589: step 4462, loss = 3.37 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 17:00:00.103455: step 4463, loss = 3.30 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 17:00:05.370104: step 4464, loss = 3.29 (12.2 examples/sec; 5.267 sec/batch)
2016-04-30 17:00:10.923472: step 4465, loss = 3.69 (11.5 examples/sec; 5.553 sec/batch)
2016-04-30 17:00:15.761938: step 4466, loss = 3.59 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 17:00:20.739927: step 4467, loss = 3.26 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 17:00:25.716479: step 4468, loss = 3.40 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 17:00:30.508092: step 4469, loss = 3.23 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 17:00:35.517450: step 4470, loss = 3.45 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 17:00:47.665188: step 4471, loss = 3.56 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 17:00:52.368739: step 4472, loss = 3.30 (13.6 examples/sec; 4.703 sec/batch)
2016-04-30 17:00:57.207411: step 4473, loss = 3.18 (13.2 examples/sec; 4.839 sec/batch)
2016-04-30 17:01:02.359065: step 4474, loss = 3.37 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 17:01:06.969480: step 4475, loss = 2.99 (13.9 examples/sec; 4.610 sec/batch)
2016-04-30 17:01:12.185127: step 4476, loss = 3.35 (12.3 examples/sec; 5.216 sec/batch)
2016-04-30 17:01:17.640414: step 4477, loss = 3.18 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 17:01:22.479223: step 4478, loss = 3.41 (13.2 examples/sec; 4.839 sec/batch)
2016-04-30 17:01:27.303941: step 4479, loss = 3.24 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 17:01:32.223789: step 4480, loss = 3.29 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 17:01:43.249240: step 4481, loss = 3.37 (13.9 examples/sec; 4.604 sec/batch)
2016-04-30 17:01:48.701510: step 4482, loss = 3.50 (11.7 examples/sec; 5.452 sec/batch)
2016-04-30 17:01:53.655272: step 4483, loss = 3.27 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 17:01:58.480233: step 4484, loss = 3.32 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 17:02:03.498637: step 4485, loss = 3.32 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 17:02:08.427715: step 4486, loss = 3.28 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 17:02:12.987727: step 4487, loss = 3.37 (14.0 examples/sec; 4.560 sec/batch)
2016-04-30 17:02:17.830471: step 4488, loss = 3.44 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 17:02:23.262970: step 4489, loss = 3.24 (11.8 examples/sec; 5.432 sec/batch)
2016-04-30 17:02:28.062496: step 4490, loss = 3.20 (13.3 examples/sec; 4.799 sec/batch)
2016-04-30 17:02:39.507737: step 4491, loss = 3.31 (14.6 examples/sec; 4.377 sec/batch)
2016-04-30 17:02:44.460485: step 4492, loss = 3.41 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 17:02:49.143988: step 4493, loss = 3.40 (13.7 examples/sec; 4.683 sec/batch)
2016-04-30 17:02:54.617912: step 4494, loss = 3.56 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 17:02:59.491920: step 4495, loss = 3.37 (13.1 examples/sec; 4.874 sec/batch)
2016-04-30 17:03:04.428793: step 4496, loss = 3.26 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 17:03:09.312464: step 4497, loss = 3.38 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 17:03:14.212434: step 4498, loss = 3.41 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 17:03:18.809072: step 4499, loss = 3.72 (13.9 examples/sec; 4.597 sec/batch)
2016-04-30 17:03:23.845253: step 4500, loss = 3.66 (12.7 examples/sec; 5.036 sec/batch)
2016-04-30 17:03:35.780115: step 4501, loss = 3.11 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 17:03:40.867653: step 4502, loss = 3.52 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 17:03:45.780116: step 4503, loss = 3.41 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 17:03:50.802797: step 4504, loss = 3.36 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 17:03:55.798192: step 4505, loss = 3.13 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 17:04:01.196507: step 4506, loss = 3.35 (11.9 examples/sec; 5.398 sec/batch)
2016-04-30 17:04:06.006055: step 4507, loss = 3.18 (13.3 examples/sec; 4.809 sec/batch)
2016-04-30 17:04:11.055473: step 4508, loss = 3.43 (12.7 examples/sec; 5.049 sec/batch)
2016-04-30 17:04:15.668848: step 4509, loss = 3.50 (13.9 examples/sec; 4.613 sec/batch)
2016-04-30 17:04:20.441561: step 4510, loss = 3.48 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 17:04:31.801443: step 4511, loss = 3.21 (13.5 examples/sec; 4.741 sec/batch)
2016-04-30 17:04:37.143613: step 4512, loss = 3.36 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 17:04:42.133775: step 4513, loss = 3.35 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 17:04:46.987388: step 4514, loss = 3.37 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 17:04:51.772729: step 4515, loss = 3.49 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 17:04:56.891465: step 4516, loss = 3.35 (12.5 examples/sec; 5.119 sec/batch)
2016-04-30 17:05:01.951888: step 4517, loss = 3.53 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 17:05:07.260474: step 4518, loss = 3.26 (12.1 examples/sec; 5.308 sec/batch)
2016-04-30 17:05:12.419820: step 4519, loss = 3.04 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 17:05:17.482847: step 4520, loss = 3.41 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 17:05:28.646230: step 4521, loss = 3.16 (14.0 examples/sec; 4.585 sec/batch)
2016-04-30 17:05:33.289621: step 4522, loss = 3.53 (13.8 examples/sec; 4.643 sec/batch)
2016-04-30 17:05:38.384054: step 4523, loss = 3.48 (12.6 examples/sec; 5.094 sec/batch)
2016-04-30 17:05:43.189313: step 4524, loss = 3.40 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 17:05:48.183303: step 4525, loss = 3.26 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 17:05:53.070512: step 4526, loss = 3.36 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 17:05:57.856085: step 4527, loss = 3.37 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 17:06:02.892430: step 4528, loss = 3.11 (12.7 examples/sec; 5.036 sec/batch)
2016-04-30 17:06:07.741420: step 4529, loss = 3.44 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 17:06:12.974771: step 4530, loss = 3.41 (12.2 examples/sec; 5.233 sec/batch)
2016-04-30 17:06:24.262176: step 4531, loss = 3.42 (14.0 examples/sec; 4.576 sec/batch)
2016-04-30 17:06:29.043315: step 4532, loss = 3.36 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 17:06:33.674702: step 4533, loss = 3.74 (13.8 examples/sec; 4.631 sec/batch)
2016-04-30 17:06:38.546402: step 4534, loss = 3.42 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 17:06:43.040820: step 4535, loss = 3.54 (14.2 examples/sec; 4.494 sec/batch)
2016-04-30 17:06:48.505485: step 4536, loss = 3.22 (11.7 examples/sec; 5.465 sec/batch)
2016-04-30 17:06:53.529056: step 4537, loss = 3.49 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 17:06:58.332547: step 4538, loss = 3.29 (13.3 examples/sec; 4.803 sec/batch)
2016-04-30 17:07:03.403368: step 4539, loss = 3.42 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 17:07:08.516462: step 4540, loss = 3.20 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 17:07:20.207572: step 4541, loss = 3.40 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 17:07:25.111054: step 4542, loss = 3.19 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 17:07:29.922156: step 4543, loss = 3.64 (13.3 examples/sec; 4.811 sec/batch)
2016-04-30 17:07:34.812879: step 4544, loss = 3.31 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 17:07:39.839404: step 4545, loss = 2.90 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 17:07:44.923245: step 4546, loss = 3.19 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 17:07:49.911647: step 4547, loss = 3.44 (12.8 examples/sec; 4.988 sec/batch)
2016-04-30 17:07:55.045754: step 4548, loss = 3.22 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 17:07:59.978559: step 4549, loss = 3.66 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 17:08:04.864340: step 4550, loss = 3.41 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 17:08:15.841949: step 4551, loss = 3.23 (14.2 examples/sec; 4.523 sec/batch)
2016-04-30 17:08:20.867767: step 4552, loss = 3.32 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 17:08:26.353577: step 4553, loss = 3.51 (11.7 examples/sec; 5.486 sec/batch)
2016-04-30 17:08:31.106186: step 4554, loss = 3.40 (13.5 examples/sec; 4.753 sec/batch)
2016-04-30 17:08:36.037960: step 4555, loss = 3.09 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 17:08:40.937883: step 4556, loss = 3.35 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 17:08:45.645291: step 4557, loss = 3.35 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 17:08:50.574686: step 4558, loss = 3.64 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 17:08:55.269590: step 4559, loss = 3.13 (13.6 examples/sec; 4.695 sec/batch)
2016-04-30 17:09:01.013917: step 4560, loss = 3.36 (11.1 examples/sec; 5.744 sec/batch)
2016-04-30 17:09:12.716611: step 4561, loss = 3.49 (13.6 examples/sec; 4.715 sec/batch)
2016-04-30 17:09:17.632631: step 4562, loss = 3.29 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 17:09:22.147632: step 4563, loss = 3.59 (14.2 examples/sec; 4.515 sec/batch)
2016-04-30 17:09:27.079401: step 4564, loss = 3.20 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 17:09:32.563150: step 4565, loss = 3.16 (11.7 examples/sec; 5.484 sec/batch)
2016-04-30 17:09:37.201079: step 4566, loss = 3.44 (13.8 examples/sec; 4.638 sec/batch)
2016-04-30 17:09:42.050466: step 4567, loss = 3.31 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 17:09:46.839089: step 4568, loss = 3.12 (13.4 examples/sec; 4.789 sec/batch)
2016-04-30 17:09:51.805743: step 4569, loss = 3.29 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 17:09:56.696694: step 4570, loss = 3.26 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 17:10:08.617944: step 4571, loss = 2.98 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 17:10:13.314044: step 4572, loss = 3.44 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 17:10:18.303773: step 4573, loss = 3.43 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 17:10:23.280029: step 4574, loss = 3.18 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 17:10:27.914623: step 4575, loss = 3.06 (13.8 examples/sec; 4.635 sec/batch)
2016-04-30 17:10:32.918112: step 4576, loss = 3.46 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 17:10:38.374607: step 4577, loss = 3.17 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 17:10:42.944427: step 4578, loss = 3.10 (14.0 examples/sec; 4.570 sec/batch)
2016-04-30 17:10:48.171014: step 4579, loss = 3.32 (12.2 examples/sec; 5.227 sec/batch)
2016-04-30 17:10:53.285740: step 4580, loss = 3.16 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 17:11:04.593422: step 4581, loss = 3.39 (13.7 examples/sec; 4.661 sec/batch)
2016-04-30 17:11:09.892327: step 4582, loss = 3.33 (12.1 examples/sec; 5.299 sec/batch)
2016-04-30 17:11:14.864543: step 4583, loss = 3.47 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 17:11:19.658604: step 4584, loss = 3.20 (13.4 examples/sec; 4.794 sec/batch)
2016-04-30 17:11:24.378219: step 4585, loss = 3.31 (13.6 examples/sec; 4.720 sec/batch)
2016-04-30 17:11:29.204472: step 4586, loss = 2.96 (13.3 examples/sec; 4.826 sec/batch)
2016-04-30 17:11:33.873701: step 4587, loss = 3.11 (13.7 examples/sec; 4.669 sec/batch)
2016-04-30 17:11:38.706240: step 4588, loss = 3.16 (13.2 examples/sec; 4.832 sec/batch)
2016-04-30 17:11:44.128221: step 4589, loss = 3.31 (11.8 examples/sec; 5.422 sec/batch)
2016-04-30 17:11:48.785070: step 4590, loss = 2.98 (13.7 examples/sec; 4.657 sec/batch)
2016-04-30 17:12:00.190858: step 4591, loss = 3.33 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 17:12:05.285928: step 4592, loss = 3.11 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 17:12:09.875362: step 4593, loss = 2.94 (13.9 examples/sec; 4.589 sec/batch)
2016-04-30 17:12:15.384944: step 4594, loss = 3.11 (11.6 examples/sec; 5.509 sec/batch)
2016-04-30 17:12:20.467904: step 4595, loss = 3.00 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 17:12:25.160436: step 4596, loss = 3.05 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 17:12:30.207770: step 4597, loss = 3.13 (12.7 examples/sec; 5.047 sec/batch)
2016-04-30 17:12:35.158704: step 4598, loss = 3.20 (12.9 examples/sec; 4.951 sec/batch)
2016-04-30 17:12:39.831079: step 4599, loss = 3.17 (13.7 examples/sec; 4.672 sec/batch)
2016-04-30 17:12:44.750787: step 4600, loss = 2.99 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 17:12:56.735118: step 4601, loss = 3.12 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 17:13:02.028503: step 4602, loss = 3.12 (12.1 examples/sec; 5.293 sec/batch)
2016-04-30 17:13:06.931938: step 4603, loss = 3.20 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 17:13:11.945441: step 4604, loss = 3.13 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 17:13:16.750848: step 4605, loss = 3.01 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 17:13:22.081447: step 4606, loss = 2.99 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 17:13:26.984262: step 4607, loss = 3.13 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 17:13:31.763246: step 4608, loss = 3.26 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 17:13:36.594579: step 4609, loss = 3.08 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 17:13:41.563212: step 4610, loss = 3.04 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 17:13:52.710566: step 4611, loss = 3.11 (13.8 examples/sec; 4.627 sec/batch)
2016-04-30 17:13:58.011272: step 4612, loss = 3.17 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 17:14:03.158556: step 4613, loss = 3.23 (12.4 examples/sec; 5.147 sec/batch)
2016-04-30 17:14:08.279593: step 4614, loss = 3.04 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 17:14:13.053849: step 4615, loss = 3.08 (13.4 examples/sec; 4.774 sec/batch)
2016-04-30 17:14:18.048816: step 4616, loss = 3.06 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 17:14:23.113784: step 4617, loss = 3.10 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 17:14:28.241675: step 4618, loss = 3.06 (12.5 examples/sec; 5.128 sec/batch)
2016-04-30 17:14:33.143118: step 4619, loss = 3.04 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 17:14:38.063527: step 4620, loss = 3.21 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 17:14:49.392447: step 4621, loss = 3.20 (13.9 examples/sec; 4.592 sec/batch)
2016-04-30 17:14:54.581125: step 4622, loss = 3.06 (12.3 examples/sec; 5.189 sec/batch)
2016-04-30 17:15:00.141724: step 4623, loss = 3.03 (11.5 examples/sec; 5.561 sec/batch)
2016-04-30 17:15:05.206049: step 4624, loss = 2.90 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 17:15:10.147414: step 4625, loss = 2.93 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 17:15:15.070472: step 4626, loss = 3.06 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 17:15:20.336329: step 4627, loss = 2.93 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 17:15:25.370680: step 4628, loss = 3.08 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 17:15:30.198997: step 4629, loss = 2.89 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 17:15:35.615236: step 4630, loss = 3.36 (11.8 examples/sec; 5.416 sec/batch)
2016-04-30 17:15:46.849423: step 4631, loss = 3.13 (13.8 examples/sec; 4.640 sec/batch)
2016-04-30 17:15:51.481061: step 4632, loss = 2.91 (13.8 examples/sec; 4.632 sec/batch)
2016-04-30 17:15:56.440204: step 4633, loss = 3.03 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 17:16:01.573362: step 4634, loss = 2.96 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 17:16:07.029782: step 4635, loss = 3.09 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 17:16:12.031948: step 4636, loss = 2.89 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 17:16:17.003655: step 4637, loss = 3.08 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 17:16:21.728828: step 4638, loss = 3.00 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 17:16:26.603220: step 4639, loss = 3.16 (13.1 examples/sec; 4.874 sec/batch)
2016-04-30 17:16:31.500468: step 4640, loss = 3.17 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 17:16:43.043945: step 4641, loss = 2.85 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 17:16:48.370787: step 4642, loss = 3.07 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 17:16:53.467575: step 4643, loss = 3.05 (12.6 examples/sec; 5.097 sec/batch)
2016-04-30 17:16:59.287592: step 4644, loss = 3.19 (11.0 examples/sec; 5.820 sec/batch)
2016-04-30 17:17:04.566949: step 4645, loss = 3.35 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 17:17:09.455695: step 4646, loss = 3.19 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 17:17:14.868669: step 4647, loss = 3.05 (11.8 examples/sec; 5.413 sec/batch)
2016-04-30 17:17:19.688228: step 4648, loss = 3.39 (13.3 examples/sec; 4.819 sec/batch)
2016-04-30 17:17:24.531546: step 4649, loss = 3.36 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 17:17:29.423805: step 4650, loss = 3.03 (13.1 examples/sec; 4.892 sec/batch)
2016-04-30 17:17:40.539495: step 4651, loss = 3.21 (14.2 examples/sec; 4.495 sec/batch)
2016-04-30 17:17:45.933404: step 4652, loss = 3.60 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 17:17:50.858484: step 4653, loss = 3.04 (13.0 examples/sec; 4.925 sec/batch)
2016-04-30 17:17:55.544924: step 4654, loss = 3.05 (13.7 examples/sec; 4.686 sec/batch)
2016-04-30 17:18:00.743536: step 4655, loss = 3.20 (12.3 examples/sec; 5.199 sec/batch)
2016-04-30 17:18:05.897477: step 4656, loss = 3.29 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 17:18:10.673194: step 4657, loss = 3.33 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 17:18:15.746780: step 4658, loss = 3.67 (12.6 examples/sec; 5.074 sec/batch)
2016-04-30 17:18:21.520440: step 4659, loss = 3.40 (11.1 examples/sec; 5.774 sec/batch)
2016-04-30 17:18:26.706284: step 4660, loss = 3.34 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 17:18:37.646592: step 4661, loss = 3.73 (14.0 examples/sec; 4.570 sec/batch)
2016-04-30 17:18:42.712304: step 4662, loss = 3.23 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 17:18:47.621066: step 4663, loss = 3.36 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 17:18:52.882647: step 4664, loss = 3.48 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 17:18:57.721748: step 4665, loss = 3.74 (13.2 examples/sec; 4.839 sec/batch)
2016-04-30 17:19:02.720222: step 4666, loss = 3.43 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 17:19:07.464289: step 4667, loss = 3.59 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 17:19:12.439908: step 4668, loss = 4.15 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 17:19:17.431539: step 4669, loss = 3.59 (12.8 examples/sec; 4.992 sec/batch)
2016-04-30 17:19:22.140889: step 4670, loss = 4.40 (13.6 examples/sec; 4.709 sec/batch)
2016-04-30 17:19:34.149732: step 4671, loss = 3.51 (14.3 examples/sec; 4.490 sec/batch)
2016-04-30 17:19:38.971088: step 4672, loss = 3.39 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 17:19:43.861760: step 4673, loss = 3.42 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 17:19:48.691443: step 4674, loss = 3.17 (13.3 examples/sec; 4.830 sec/batch)
2016-04-30 17:19:53.507807: step 4675, loss = 3.30 (13.3 examples/sec; 4.816 sec/batch)
2016-04-30 17:19:58.612907: step 4676, loss = 3.63 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 17:20:03.594780: step 4677, loss = 3.81 (12.8 examples/sec; 4.982 sec/batch)
2016-04-30 17:20:08.672873: step 4678, loss = 4.31 (12.6 examples/sec; 5.078 sec/batch)
2016-04-30 17:20:13.350865: step 4679, loss = 3.69 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 17:20:18.196082: step 4680, loss = 4.02 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 17:20:29.862080: step 4681, loss = 3.40 (12.6 examples/sec; 5.079 sec/batch)
2016-04-30 17:20:34.655838: step 4682, loss = 3.48 (13.4 examples/sec; 4.794 sec/batch)
2016-04-30 17:20:39.596454: step 4683, loss = 3.76 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 17:20:44.429426: step 4684, loss = 3.46 (13.2 examples/sec; 4.833 sec/batch)
2016-04-30 17:20:49.031841: step 4685, loss = 3.79 (13.9 examples/sec; 4.602 sec/batch)
2016-04-30 17:20:53.945967: step 4686, loss = 4.50 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 17:20:58.703596: step 4687, loss = 4.74 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 17:21:04.257041: step 4688, loss = 5.43 (11.5 examples/sec; 5.553 sec/batch)
2016-04-30 17:21:09.159952: step 4689, loss = 6.84 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 17:21:14.040444: step 4690, loss = 4.55 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 17:21:24.938876: step 4691, loss = 4.06 (14.8 examples/sec; 4.331 sec/batch)
2016-04-30 17:21:29.842614: step 4692, loss = 5.04 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 17:21:34.453029: step 4693, loss = 5.83 (13.9 examples/sec; 4.610 sec/batch)
2016-04-30 17:21:39.910156: step 4694, loss = 6.05 (11.7 examples/sec; 5.457 sec/batch)
2016-04-30 17:21:45.064332: step 4695, loss = 6.34 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 17:21:49.641256: step 4696, loss = 6.84 (14.0 examples/sec; 4.577 sec/batch)
2016-04-30 17:21:54.594598: step 4697, loss = 5.68 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 17:21:59.518472: step 4698, loss = 5.62 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 17:22:04.483632: step 4699, loss = 5.14 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 17:22:09.905106: step 4700, loss = 7.26 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 17:22:21.277327: step 4701, loss = 7.80 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 17:22:26.283383: step 4702, loss = 8.49 (12.8 examples/sec; 5.006 sec/batch)
2016-04-30 17:22:31.065688: step 4703, loss = 6.84 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 17:22:36.239929: step 4704, loss = 7.71 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 17:22:41.486177: step 4705, loss = 7.80 (12.2 examples/sec; 5.246 sec/batch)
2016-04-30 17:22:46.546930: step 4706, loss = 8.22 (12.6 examples/sec; 5.061 sec/batch)
2016-04-30 17:22:51.461452: step 4707, loss = 8.83 (13.0 examples/sec; 4.914 sec/batch)
2016-04-30 17:22:56.713165: step 4708, loss = 9.94 (12.2 examples/sec; 5.252 sec/batch)
2016-04-30 17:23:01.816548: step 4709, loss = 8.87 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 17:23:06.999423: step 4710, loss = 4.57 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 17:23:18.987052: step 4711, loss = 6.10 (12.2 examples/sec; 5.253 sec/batch)
2016-04-30 17:23:24.067632: step 4712, loss = 9.16 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 17:23:28.633954: step 4713, loss = 8.36 (14.0 examples/sec; 4.566 sec/batch)
2016-04-30 17:23:33.470806: step 4714, loss = 10.13 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 17:23:38.545895: step 4715, loss = 6.38 (12.6 examples/sec; 5.075 sec/batch)
2016-04-30 17:23:43.127493: step 4716, loss = 5.65 (14.0 examples/sec; 4.582 sec/batch)
2016-04-30 17:23:48.773032: step 4717, loss = 8.79 (11.3 examples/sec; 5.645 sec/batch)
2016-04-30 17:23:53.888495: step 4718, loss = 10.34 (12.5 examples/sec; 5.115 sec/batch)
2016-04-30 17:23:58.627884: step 4719, loss = 14.57 (13.5 examples/sec; 4.739 sec/batch)
2016-04-30 17:24:03.558708: step 4720, loss = 13.66 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 17:24:16.560130: step 4721, loss = 11.87 (11.4 examples/sec; 5.628 sec/batch)
2016-04-30 17:24:22.370604: step 4722, loss = 10.58 (11.0 examples/sec; 5.810 sec/batch)
2016-04-30 17:24:27.632018: step 4723, loss = 10.56 (12.2 examples/sec; 5.261 sec/batch)
2016-04-30 17:24:33.104626: step 4724, loss = 11.18 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 17:24:38.109001: step 4725, loss = 12.47 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 17:24:42.779096: step 4726, loss = 8.10 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 17:24:48.007511: step 4727, loss = 14.73 (12.2 examples/sec; 5.228 sec/batch)
2016-04-30 17:24:52.860972: step 4728, loss = 17.60 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 17:24:58.230038: step 4729, loss = 13.57 (11.9 examples/sec; 5.369 sec/batch)
2016-04-30 17:25:03.720135: step 4730, loss = 12.35 (11.7 examples/sec; 5.490 sec/batch)
2016-04-30 17:25:15.138511: step 4731, loss = 11.29 (13.5 examples/sec; 4.727 sec/batch)
2016-04-30 17:25:19.885401: step 4732, loss = 13.02 (13.5 examples/sec; 4.747 sec/batch)
2016-04-30 17:25:24.791048: step 4733, loss = 10.83 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 17:25:30.312858: step 4734, loss = 7.20 (11.6 examples/sec; 5.522 sec/batch)
2016-04-30 17:25:35.429502: step 4735, loss = 6.71 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 17:25:40.188663: step 4736, loss = 7.99 (13.4 examples/sec; 4.759 sec/batch)
2016-04-30 17:25:45.109442: step 4737, loss = 7.05 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 17:25:50.078600: step 4738, loss = 10.96 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 17:25:55.112043: step 4739, loss = 11.28 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 17:26:00.744937: step 4740, loss = 15.76 (11.4 examples/sec; 5.633 sec/batch)
2016-04-30 17:26:12.322155: step 4741, loss = 13.82 (13.6 examples/sec; 4.710 sec/batch)
2016-04-30 17:26:17.242857: step 4742, loss = 10.83 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 17:26:21.980204: step 4743, loss = 11.06 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 17:26:27.020397: step 4744, loss = 12.77 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 17:26:31.633400: step 4745, loss = 15.99 (13.9 examples/sec; 4.613 sec/batch)
2016-04-30 17:26:37.219151: step 4746, loss = 18.34 (11.5 examples/sec; 5.586 sec/batch)
2016-04-30 17:26:42.141261: step 4747, loss = 15.21 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 17:26:47.644920: step 4748, loss = 17.40 (11.6 examples/sec; 5.504 sec/batch)
2016-04-30 17:26:52.760783: step 4749, loss = 12.56 (12.5 examples/sec; 5.116 sec/batch)
2016-04-30 17:26:57.845976: step 4750, loss = 12.41 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 17:27:10.017212: step 4751, loss = 14.29 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 17:27:15.039273: step 4752, loss = 21.64 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 17:27:19.831536: step 4753, loss = 14.13 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 17:27:24.730591: step 4754, loss = 10.84 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 17:27:29.654938: step 4755, loss = 14.49 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 17:27:34.255449: step 4756, loss = 18.49 (13.9 examples/sec; 4.600 sec/batch)
2016-04-30 17:27:39.717169: step 4757, loss = 14.03 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 17:27:44.720084: step 4758, loss = 14.83 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 17:27:49.456946: step 4759, loss = 23.21 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 17:27:54.376395: step 4760, loss = 21.54 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 17:28:06.103788: step 4761, loss = 8.45 (13.3 examples/sec; 4.809 sec/batch)
2016-04-30 17:28:10.660690: step 4762, loss = 17.28 (14.1 examples/sec; 4.551 sec/batch)
2016-04-30 17:28:16.095985: step 4763, loss = 22.78 (11.8 examples/sec; 5.435 sec/batch)
2016-04-30 17:28:21.050677: step 4764, loss = 19.61 (12.9 examples/sec; 4.955 sec/batch)
2016-04-30 17:28:26.110245: step 4765, loss = 18.50 (12.6 examples/sec; 5.059 sec/batch)
2016-04-30 17:28:30.906657: step 4766, loss = 15.65 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 17:28:36.163309: step 4767, loss = 15.32 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 17:28:40.973572: step 4768, loss = 11.28 (13.3 examples/sec; 4.810 sec/batch)
2016-04-30 17:28:46.421325: step 4769, loss = 12.84 (11.7 examples/sec; 5.448 sec/batch)
2016-04-30 17:28:51.443746: step 4770, loss = 24.78 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 17:29:03.165481: step 4771, loss = 22.05 (12.8 examples/sec; 5.016 sec/batch)
2016-04-30 17:29:08.249488: step 4772, loss = 30.87 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 17:29:13.048000: step 4773, loss = 19.76 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 17:29:18.651797: step 4774, loss = 29.18 (11.4 examples/sec; 5.604 sec/batch)
2016-04-30 17:29:23.660133: step 4775, loss = 13.78 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 17:29:28.313479: step 4776, loss = 15.12 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 17:29:33.213630: step 4777, loss = 27.50 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 17:29:37.946564: step 4778, loss = 21.46 (13.5 examples/sec; 4.733 sec/batch)
2016-04-30 17:29:42.911608: step 4779, loss = 24.37 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 17:29:47.965487: step 4780, loss = 33.03 (12.7 examples/sec; 5.054 sec/batch)
2016-04-30 17:30:00.256126: step 4781, loss = 28.07 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 17:30:05.215314: step 4782, loss = 28.66 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 17:30:10.193636: step 4783, loss = 24.77 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 17:30:15.354724: step 4784, loss = 25.12 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 17:30:20.058266: step 4785, loss = 29.80 (13.6 examples/sec; 4.703 sec/batch)
2016-04-30 17:30:25.576477: step 4786, loss = 15.80 (11.6 examples/sec; 5.518 sec/batch)
2016-04-30 17:30:30.533730: step 4787, loss = 11.64 (12.9 examples/sec; 4.957 sec/batch)
2016-04-30 17:30:35.548504: step 4788, loss = 20.23 (12.8 examples/sec; 5.015 sec/batch)
2016-04-30 17:30:40.434581: step 4789, loss = 33.39 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 17:30:45.539976: step 4790, loss = 35.84 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 17:30:57.503063: step 4791, loss = 22.71 (11.9 examples/sec; 5.382 sec/batch)
2016-04-30 17:31:02.630293: step 4792, loss = 29.82 (12.5 examples/sec; 5.127 sec/batch)
2016-04-30 17:31:07.475788: step 4793, loss = 24.27 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 17:31:12.656407: step 4794, loss = 24.97 (12.4 examples/sec; 5.181 sec/batch)
2016-04-30 17:31:17.728098: step 4795, loss = 25.32 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 17:31:22.417007: step 4796, loss = 45.65 (13.6 examples/sec; 4.689 sec/batch)
2016-04-30 17:31:27.428542: step 4797, loss = 27.87 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 17:31:32.884468: step 4798, loss = 22.39 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 17:31:37.622507: step 4799, loss = 34.68 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 17:31:42.534543: step 4800, loss = 38.15 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 17:31:53.981210: step 4801, loss = 22.15 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 17:31:58.507711: step 4802, loss = 12.28 (14.1 examples/sec; 4.526 sec/batch)
2016-04-30 17:32:04.235146: step 4803, loss = 27.42 (11.2 examples/sec; 5.727 sec/batch)
2016-04-30 17:32:09.330666: step 4804, loss = 29.34 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 17:32:14.383937: step 4805, loss = 36.02 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 17:32:19.154055: step 4806, loss = 33.86 (13.4 examples/sec; 4.770 sec/batch)
2016-04-30 17:32:24.023099: step 4807, loss = 27.70 (13.1 examples/sec; 4.869 sec/batch)
2016-04-30 17:32:28.699004: step 4808, loss = 28.80 (13.7 examples/sec; 4.676 sec/batch)
2016-04-30 17:32:33.576984: step 4809, loss = 31.37 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 17:32:38.939705: step 4810, loss = 36.12 (11.9 examples/sec; 5.363 sec/batch)
2016-04-30 17:32:50.493852: step 4811, loss = 21.81 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 17:32:55.268527: step 4812, loss = 37.66 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 17:33:00.293744: step 4813, loss = 43.75 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 17:33:05.211846: step 4814, loss = 32.32 (13.0 examples/sec; 4.918 sec/batch)
2016-04-30 17:33:10.524217: step 4815, loss = 20.54 (12.0 examples/sec; 5.312 sec/batch)
2016-04-30 17:33:15.609086: step 4816, loss = 33.04 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 17:33:20.478761: step 4817, loss = 34.98 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 17:33:25.272574: step 4818, loss = 36.48 (13.4 examples/sec; 4.794 sec/batch)
2016-04-30 17:33:30.234432: step 4819, loss = 25.26 (12.9 examples/sec; 4.962 sec/batch)
2016-04-30 17:33:35.136478: step 4820, loss = 40.18 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 17:33:47.040555: step 4821, loss = 42.92 (14.0 examples/sec; 4.560 sec/batch)
2016-04-30 17:33:51.916289: step 4822, loss = 58.74 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 17:33:57.032596: step 4823, loss = 17.01 (12.5 examples/sec; 5.116 sec/batch)
2016-04-30 17:34:02.021411: step 4824, loss = 30.38 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 17:34:07.005375: step 4825, loss = 26.95 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 17:34:12.143770: step 4826, loss = 41.28 (12.5 examples/sec; 5.138 sec/batch)
2016-04-30 17:34:17.599328: step 4827, loss = 64.80 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 17:34:22.555853: step 4828, loss = 51.77 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 17:34:27.768178: step 4829, loss = 61.78 (12.3 examples/sec; 5.212 sec/batch)
2016-04-30 17:34:32.752887: step 4830, loss = 34.51 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 17:34:43.784473: step 4831, loss = 43.96 (14.0 examples/sec; 4.565 sec/batch)
2016-04-30 17:34:49.302265: step 4832, loss = 44.39 (11.6 examples/sec; 5.518 sec/batch)
2016-04-30 17:34:54.152509: step 4833, loss = 37.39 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 17:34:58.876577: step 4834, loss = 48.59 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 17:35:03.916416: step 4835, loss = 36.42 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 17:35:08.962249: step 4836, loss = 47.70 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 17:35:13.538070: step 4837, loss = 63.38 (14.0 examples/sec; 4.576 sec/batch)
2016-04-30 17:35:18.486085: step 4838, loss = 70.21 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 17:35:23.879813: step 4839, loss = 40.19 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 17:35:28.392605: step 4840, loss = 38.39 (14.2 examples/sec; 4.513 sec/batch)
2016-04-30 17:35:39.479958: step 4841, loss = 74.39 (14.0 examples/sec; 4.575 sec/batch)
2016-04-30 17:35:44.307545: step 4842, loss = 47.33 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 17:35:48.989745: step 4843, loss = 37.13 (13.7 examples/sec; 4.682 sec/batch)
2016-04-30 17:35:54.457476: step 4844, loss = 52.05 (11.7 examples/sec; 5.468 sec/batch)
2016-04-30 17:35:59.587310: step 4845, loss = 77.60 (12.5 examples/sec; 5.130 sec/batch)
2016-04-30 17:36:04.437161: step 4846, loss = 79.06 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 17:36:09.218425: step 4847, loss = 13.45 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 17:36:14.059656: step 4848, loss = 74.20 (13.2 examples/sec; 4.841 sec/batch)
2016-04-30 17:36:18.779281: step 4849, loss = 48.62 (13.6 examples/sec; 4.720 sec/batch)
2016-04-30 17:36:23.923826: step 4850, loss = 46.84 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 17:36:35.817429: step 4851, loss = 38.54 (13.4 examples/sec; 4.777 sec/batch)
2016-04-30 17:36:40.699039: step 4852, loss = 47.22 (13.1 examples/sec; 4.882 sec/batch)
2016-04-30 17:36:45.794829: step 4853, loss = 75.99 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 17:36:51.324442: step 4854, loss = 64.91 (11.6 examples/sec; 5.530 sec/batch)
2016-04-30 17:36:56.236938: step 4855, loss = 51.48 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 17:37:01.599585: step 4856, loss = 36.06 (11.9 examples/sec; 5.363 sec/batch)
2016-04-30 17:37:06.509889: step 4857, loss = 68.53 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 17:37:11.565389: step 4858, loss = 87.56 (12.7 examples/sec; 5.055 sec/batch)
2016-04-30 17:37:16.392484: step 4859, loss = 42.87 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 17:37:21.395749: step 4860, loss = 50.52 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 17:37:33.535620: step 4861, loss = 51.57 (12.0 examples/sec; 5.314 sec/batch)
2016-04-30 17:37:38.431024: step 4862, loss = 80.49 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 17:37:43.253145: step 4863, loss = 76.60 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 17:37:48.397693: step 4864, loss = 42.82 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 17:37:53.175899: step 4865, loss = 57.95 (13.4 examples/sec; 4.778 sec/batch)
2016-04-30 17:37:58.121244: step 4866, loss = 84.51 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 17:38:03.319200: step 4867, loss = 56.50 (12.3 examples/sec; 5.198 sec/batch)
2016-04-30 17:38:08.872466: step 4868, loss = 89.28 (11.5 examples/sec; 5.553 sec/batch)
2016-04-30 17:38:13.733779: step 4869, loss = 114.41 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 17:38:18.513050: step 4870, loss = 31.15 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 17:38:30.036036: step 4871, loss = 28.44 (13.7 examples/sec; 4.655 sec/batch)
2016-04-30 17:38:34.699750: step 4872, loss = 70.81 (13.7 examples/sec; 4.664 sec/batch)
2016-04-30 17:38:40.138137: step 4873, loss = 131.91 (11.8 examples/sec; 5.438 sec/batch)
2016-04-30 17:38:45.116209: step 4874, loss = 82.43 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 17:38:49.706904: step 4875, loss = 63.08 (13.9 examples/sec; 4.591 sec/batch)
2016-04-30 17:38:54.622307: step 4876, loss = 109.02 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 17:38:59.499091: step 4877, loss = 79.42 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 17:39:04.433811: step 4878, loss = 24.80 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 17:39:09.381674: step 4879, loss = 91.83 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 17:39:14.724139: step 4880, loss = 96.50 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 17:39:25.788016: step 4881, loss = 71.28 (14.1 examples/sec; 4.538 sec/batch)
2016-04-30 17:39:30.652166: step 4882, loss = 68.21 (13.2 examples/sec; 4.864 sec/batch)
2016-04-30 17:39:35.722895: step 4883, loss = 89.58 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 17:39:40.479267: step 4884, loss = 84.79 (13.5 examples/sec; 4.756 sec/batch)
2016-04-30 17:39:45.892872: step 4885, loss = 115.14 (11.8 examples/sec; 5.414 sec/batch)
2016-04-30 17:39:50.799394: step 4886, loss = 117.56 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 17:39:55.562287: step 4887, loss = 49.67 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 17:40:00.791140: step 4888, loss = 72.80 (12.2 examples/sec; 5.229 sec/batch)
2016-04-30 17:40:05.907917: step 4889, loss = 79.36 (12.5 examples/sec; 5.117 sec/batch)
2016-04-30 17:40:10.585316: step 4890, loss = 134.22 (13.7 examples/sec; 4.677 sec/batch)
2016-04-30 17:40:22.734467: step 4891, loss = 150.57 (12.9 examples/sec; 4.958 sec/batch)
2016-04-30 17:40:27.780246: step 4892, loss = 98.48 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 17:40:32.631963: step 4893, loss = 24.59 (13.2 examples/sec; 4.852 sec/batch)
2016-04-30 17:40:37.238645: step 4894, loss = 90.19 (13.9 examples/sec; 4.607 sec/batch)
2016-04-30 17:40:42.350190: step 4895, loss = 123.54 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 17:40:47.251522: step 4896, loss = 104.80 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 17:40:52.529021: step 4897, loss = 98.65 (12.1 examples/sec; 5.277 sec/batch)
2016-04-30 17:40:57.481673: step 4898, loss = 64.30 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 17:41:02.634086: step 4899, loss = 106.22 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 17:41:07.370783: step 4900, loss = 146.84 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 17:41:18.628193: step 4901, loss = 141.99 (13.6 examples/sec; 4.716 sec/batch)
2016-04-30 17:41:23.551109: step 4902, loss = 112.72 (13.0 examples/sec; 4.923 sec/batch)
2016-04-30 17:41:28.671478: step 4903, loss = 83.21 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 17:41:33.584426: step 4904, loss = 105.86 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 17:41:38.455280: step 4905, loss = 88.32 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 17:41:43.145500: step 4906, loss = 107.53 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 17:41:48.360302: step 4907, loss = 146.17 (12.3 examples/sec; 5.215 sec/batch)
2016-04-30 17:41:53.187820: step 4908, loss = 79.82 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 17:41:58.455864: step 4909, loss = 65.82 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 17:42:03.607995: step 4910, loss = 147.49 (12.4 examples/sec; 5.152 sec/batch)
2016-04-30 17:42:15.368437: step 4911, loss = 102.75 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 17:42:20.164057: step 4912, loss = 71.33 (13.3 examples/sec; 4.796 sec/batch)
2016-04-30 17:42:25.085911: step 4913, loss = 105.51 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 17:42:30.673513: step 4914, loss = 155.64 (11.5 examples/sec; 5.587 sec/batch)
2016-04-30 17:42:35.533803: step 4915, loss = 176.26 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 17:42:40.256508: step 4916, loss = 59.75 (13.6 examples/sec; 4.723 sec/batch)
2016-04-30 17:42:45.302582: step 4917, loss = 80.83 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 17:42:50.301759: step 4918, loss = 162.15 (12.8 examples/sec; 4.999 sec/batch)
2016-04-30 17:42:55.144468: step 4919, loss = 107.60 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 17:43:00.148607: step 4920, loss = 122.61 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 17:43:12.437783: step 4921, loss = 130.46 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 17:43:17.533411: step 4922, loss = 112.93 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 17:43:22.217367: step 4923, loss = 116.64 (13.7 examples/sec; 4.684 sec/batch)
2016-04-30 17:43:27.239388: step 4924, loss = 101.03 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 17:43:31.887217: step 4925, loss = 101.93 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 17:43:37.323230: step 4926, loss = 184.95 (11.8 examples/sec; 5.436 sec/batch)
2016-04-30 17:43:42.286458: step 4927, loss = 188.02 (12.9 examples/sec; 4.963 sec/batch)
2016-04-30 17:43:46.915139: step 4928, loss = 78.06 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 17:43:51.763397: step 4929, loss = 155.84 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 17:43:56.731854: step 4930, loss = 207.00 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 17:44:07.985315: step 4931, loss = 251.63 (14.1 examples/sec; 4.531 sec/batch)
2016-04-30 17:44:13.593702: step 4932, loss = 168.63 (11.4 examples/sec; 5.608 sec/batch)
2016-04-30 17:44:18.565342: step 4933, loss = 97.10 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 17:44:23.396745: step 4934, loss = 112.09 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 17:44:28.223273: step 4935, loss = 123.62 (13.3 examples/sec; 4.826 sec/batch)
2016-04-30 17:44:33.173807: step 4936, loss = 130.53 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 17:44:38.027220: step 4937, loss = 256.59 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 17:44:43.473790: step 4938, loss = 103.31 (11.8 examples/sec; 5.446 sec/batch)
2016-04-30 17:44:48.519075: step 4939, loss = 177.60 (12.7 examples/sec; 5.045 sec/batch)
2016-04-30 17:44:53.500391: step 4940, loss = 170.55 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 17:45:04.924487: step 4941, loss = 149.12 (13.4 examples/sec; 4.765 sec/batch)
2016-04-30 17:45:09.900448: step 4942, loss = 163.56 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 17:45:15.220524: step 4943, loss = 124.92 (12.0 examples/sec; 5.320 sec/batch)
2016-04-30 17:45:20.139178: step 4944, loss = 268.91 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 17:45:25.263818: step 4945, loss = 240.99 (12.5 examples/sec; 5.125 sec/batch)
2016-04-30 17:45:30.382064: step 4946, loss = 66.16 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 17:45:34.981092: step 4947, loss = 186.91 (13.9 examples/sec; 4.599 sec/batch)
2016-04-30 17:45:39.831345: step 4948, loss = 213.15 (13.2 examples/sec; 4.850 sec/batch)
2016-04-30 17:45:44.810652: step 4949, loss = 158.78 (12.9 examples/sec; 4.979 sec/batch)
2016-04-30 17:45:49.952152: step 4950, loss = 88.60 (12.4 examples/sec; 5.141 sec/batch)
2016-04-30 17:46:01.528463: step 4951, loss = 232.91 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 17:46:06.424750: step 4952, loss = 233.08 (13.1 examples/sec; 4.896 sec/batch)
2016-04-30 17:46:11.611375: step 4953, loss = 178.01 (12.3 examples/sec; 5.187 sec/batch)
2016-04-30 17:46:16.353946: step 4954, loss = 201.53 (13.5 examples/sec; 4.742 sec/batch)
2016-04-30 17:46:21.772554: step 4955, loss = 191.66 (11.8 examples/sec; 5.419 sec/batch)
2016-04-30 17:46:26.807035: step 4956, loss = 373.01 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 17:46:31.544844: step 4957, loss = 238.69 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 17:46:36.575849: step 4958, loss = 175.66 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 17:46:41.523455: step 4959, loss = 128.85 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 17:46:46.247242: step 4960, loss = 221.87 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 17:46:58.166721: step 4961, loss = 308.29 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 17:47:03.303296: step 4962, loss = 227.95 (12.5 examples/sec; 5.136 sec/batch)
2016-04-30 17:47:07.989918: step 4963, loss = 296.35 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 17:47:12.991039: step 4964, loss = 238.57 (12.8 examples/sec; 5.001 sec/batch)
2016-04-30 17:47:17.945220: step 4965, loss = 113.42 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 17:47:22.779782: step 4966, loss = 208.96 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 17:47:28.290536: step 4967, loss = 295.66 (11.6 examples/sec; 5.511 sec/batch)
2016-04-30 17:47:33.373393: step 4968, loss = 270.33 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 17:47:38.259989: step 4969, loss = 296.44 (13.1 examples/sec; 4.887 sec/batch)
2016-04-30 17:47:43.179192: step 4970, loss = 251.40 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 17:47:54.505268: step 4971, loss = 392.59 (13.4 examples/sec; 4.761 sec/batch)
2016-04-30 17:47:59.256823: step 4972, loss = 267.16 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 17:48:04.730687: step 4973, loss = 144.89 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 17:48:09.859631: step 4974, loss = 293.77 (12.5 examples/sec; 5.129 sec/batch)
2016-04-30 17:48:15.997003: step 4975, loss = 322.55 (10.4 examples/sec; 6.137 sec/batch)
2016-04-30 17:48:21.328570: step 4976, loss = 336.90 (12.0 examples/sec; 5.331 sec/batch)
2016-04-30 17:48:26.219671: step 4977, loss = 215.38 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 17:48:31.283382: step 4978, loss = 304.76 (12.6 examples/sec; 5.064 sec/batch)
2016-04-30 17:48:36.786743: step 4979, loss = 203.17 (11.6 examples/sec; 5.503 sec/batch)
2016-04-30 17:48:42.044707: step 4980, loss = 228.53 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 17:48:53.083018: step 4981, loss = 294.51 (14.1 examples/sec; 4.543 sec/batch)
2016-04-30 17:48:58.064175: step 4982, loss = 314.76 (12.8 examples/sec; 4.981 sec/batch)
2016-04-30 17:49:03.258512: step 4983, loss = 422.60 (12.3 examples/sec; 5.194 sec/batch)
2016-04-30 17:49:08.825922: step 4984, loss = 299.71 (11.5 examples/sec; 5.567 sec/batch)
2016-04-30 17:49:13.441138: step 4985, loss = 265.18 (13.9 examples/sec; 4.615 sec/batch)
2016-04-30 17:49:18.439002: step 4986, loss = 414.59 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 17:49:23.224519: step 4987, loss = 398.35 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 17:49:28.049599: step 4988, loss = 319.44 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 17:49:33.024816: step 4989, loss = 311.43 (12.9 examples/sec; 4.975 sec/batch)
2016-04-30 17:49:37.837651: step 4990, loss = 205.14 (13.3 examples/sec; 4.813 sec/batch)
2016-04-30 17:49:49.539286: step 4991, loss = 427.39 (14.5 examples/sec; 4.405 sec/batch)
2016-04-30 17:49:54.361624: step 4992, loss = 437.48 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 17:49:58.894308: step 4993, loss = 318.14 (14.1 examples/sec; 4.533 sec/batch)
2016-04-30 17:50:03.805896: step 4994, loss = 294.11 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 17:50:08.685742: step 4995, loss = 356.94 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 17:50:13.724644: step 4996, loss = 318.68 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 17:50:18.633431: step 4997, loss = 347.66 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 17:50:23.693813: step 4998, loss = 517.16 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 17:50:28.448221: step 4999, loss = 376.86 (13.5 examples/sec; 4.754 sec/batch)
2016-04-30 17:50:33.296619: step 5000, loss = 331.06 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 17:50:45.876922: step 5001, loss = 229.00 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 17:50:50.831574: step 5002, loss = 414.59 (12.9 examples/sec; 4.955 sec/batch)
2016-04-30 17:50:55.549432: step 5003, loss = 671.46 (13.6 examples/sec; 4.718 sec/batch)
2016-04-30 17:51:00.733061: step 5004, loss = 183.42 (12.3 examples/sec; 5.184 sec/batch)
2016-04-30 17:51:05.794433: step 5005, loss = 238.29 (12.6 examples/sec; 5.061 sec/batch)
2016-04-30 17:51:10.618827: step 5006, loss = 648.82 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 17:51:15.657477: step 5007, loss = 489.75 (12.7 examples/sec; 5.039 sec/batch)
2016-04-30 17:51:21.333232: step 5008, loss = 339.40 (11.3 examples/sec; 5.676 sec/batch)
2016-04-30 17:51:26.221663: step 5009, loss = 383.83 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 17:51:31.194655: step 5010, loss = 783.58 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 17:51:42.457332: step 5011, loss = 400.59 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 17:51:47.200929: step 5012, loss = 296.32 (13.5 examples/sec; 4.743 sec/batch)
2016-04-30 17:51:52.710987: step 5013, loss = 398.97 (11.6 examples/sec; 5.510 sec/batch)
2016-04-30 17:51:57.819830: step 5014, loss = 612.60 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 17:52:03.175234: step 5015, loss = 451.69 (12.0 examples/sec; 5.355 sec/batch)
2016-04-30 17:52:07.762317: step 5016, loss = 298.25 (14.0 examples/sec; 4.587 sec/batch)
2016-04-30 17:52:12.798025: step 5017, loss = 254.54 (12.7 examples/sec; 5.036 sec/batch)
2016-04-30 17:52:17.871267: step 5018, loss = 301.31 (12.6 examples/sec; 5.073 sec/batch)
2016-04-30 17:52:22.607504: step 5019, loss = 487.72 (13.5 examples/sec; 4.736 sec/batch)
2016-04-30 17:52:28.014624: step 5020, loss = 574.22 (11.8 examples/sec; 5.407 sec/batch)
2016-04-30 17:52:39.505458: step 5021, loss = 632.33 (12.9 examples/sec; 4.955 sec/batch)
2016-04-30 17:52:44.775374: step 5022, loss = 450.47 (12.1 examples/sec; 5.270 sec/batch)
2016-04-30 17:52:49.946146: step 5023, loss = 343.81 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 17:52:55.033014: step 5024, loss = 616.91 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 17:53:00.818230: step 5025, loss = 677.98 (11.1 examples/sec; 5.785 sec/batch)
2016-04-30 17:53:05.807235: step 5026, loss = 603.71 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 17:53:10.833829: step 5027, loss = 333.68 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 17:53:15.910490: step 5028, loss = 428.18 (12.6 examples/sec; 5.077 sec/batch)
2016-04-30 17:53:20.981229: step 5029, loss = 714.56 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 17:53:25.911672: step 5030, loss = 653.82 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 17:53:37.731376: step 5031, loss = 635.63 (14.4 examples/sec; 4.454 sec/batch)
2016-04-30 17:53:42.613390: step 5032, loss = 540.66 (13.1 examples/sec; 4.882 sec/batch)
2016-04-30 17:53:47.513410: step 5033, loss = 814.13 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 17:53:52.355476: step 5034, loss = 392.40 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 17:53:57.314706: step 5035, loss = 676.63 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 17:54:02.304768: step 5036, loss = 702.95 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 17:54:07.725234: step 5037, loss = 823.90 (11.8 examples/sec; 5.420 sec/batch)
2016-04-30 17:54:13.060480: step 5038, loss = 538.07 (12.0 examples/sec; 5.335 sec/batch)
2016-04-30 17:54:18.309553: step 5039, loss = 472.92 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 17:54:23.175379: step 5040, loss = 768.07 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 17:54:34.438848: step 5041, loss = 708.65 (14.2 examples/sec; 4.521 sec/batch)
2016-04-30 17:54:39.860904: step 5042, loss = 207.20 (11.8 examples/sec; 5.422 sec/batch)
2016-04-30 17:54:44.861590: step 5043, loss = 461.42 (12.8 examples/sec; 5.001 sec/batch)
2016-04-30 17:54:49.633226: step 5044, loss = 659.08 (13.4 examples/sec; 4.771 sec/batch)
2016-04-30 17:54:54.783119: step 5045, loss = 959.42 (12.4 examples/sec; 5.150 sec/batch)
2016-04-30 17:54:59.752240: step 5046, loss = 534.60 (12.9 examples/sec; 4.969 sec/batch)
2016-04-30 17:55:04.580342: step 5047, loss = 580.20 (13.3 examples/sec; 4.828 sec/batch)
2016-04-30 17:55:09.927145: step 5048, loss = 600.24 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 17:55:15.112167: step 5049, loss = 612.47 (12.3 examples/sec; 5.185 sec/batch)
2016-04-30 17:55:19.867321: step 5050, loss = 509.16 (13.5 examples/sec; 4.755 sec/batch)
2016-04-30 17:55:31.610801: step 5051, loss = 688.35 (14.1 examples/sec; 4.536 sec/batch)
2016-04-30 17:55:36.706439: step 5052, loss = 776.36 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 17:55:41.632930: step 5053, loss = 842.61 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 17:55:47.978494: step 5054, loss = 361.71 (10.1 examples/sec; 6.345 sec/batch)
2016-04-30 17:55:52.736907: step 5055, loss = 373.88 (13.5 examples/sec; 4.758 sec/batch)
2016-04-30 17:55:57.812911: step 5056, loss = 797.42 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 17:56:03.135229: step 5057, loss = 1423.53 (12.0 examples/sec; 5.322 sec/batch)
2016-04-30 17:56:07.828341: step 5058, loss = 940.12 (13.6 examples/sec; 4.693 sec/batch)
2016-04-30 17:56:12.915411: step 5059, loss = 520.21 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 17:56:18.330792: step 5060, loss = 539.81 (11.8 examples/sec; 5.415 sec/batch)
2016-04-30 17:56:30.088458: step 5061, loss = 1076.65 (13.5 examples/sec; 4.738 sec/batch)
2016-04-30 17:56:34.865699: step 5062, loss = 927.95 (13.4 examples/sec; 4.777 sec/batch)
2016-04-30 17:56:39.887841: step 5063, loss = 674.22 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 17:56:44.790712: step 5064, loss = 611.24 (13.1 examples/sec; 4.903 sec/batch)
2016-04-30 17:56:50.056490: step 5065, loss = 772.66 (12.2 examples/sec; 5.266 sec/batch)
2016-04-30 17:56:54.907118: step 5066, loss = 985.50 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 17:56:59.742211: step 5067, loss = 731.75 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 17:57:04.621387: step 5068, loss = 694.21 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 17:57:09.586114: step 5069, loss = 939.70 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 17:57:14.226043: step 5070, loss = 1084.57 (13.8 examples/sec; 4.640 sec/batch)
2016-04-30 17:57:25.823084: step 5071, loss = 964.29 (12.5 examples/sec; 5.134 sec/batch)
2016-04-30 17:57:30.905968: step 5072, loss = 707.32 (12.6 examples/sec; 5.083 sec/batch)
2016-04-30 17:57:36.185734: step 5073, loss = 585.24 (12.1 examples/sec; 5.280 sec/batch)
2016-04-30 17:57:40.848230: step 5074, loss = 969.87 (13.7 examples/sec; 4.662 sec/batch)
2016-04-30 17:57:45.838744: step 5075, loss = 1749.90 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 17:57:50.805688: step 5076, loss = 697.11 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 17:57:56.097080: step 5077, loss = 309.10 (12.1 examples/sec; 5.291 sec/batch)
2016-04-30 17:58:01.193499: step 5078, loss = 469.27 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 17:58:06.119901: step 5079, loss = 1060.59 (13.0 examples/sec; 4.926 sec/batch)
2016-04-30 17:58:11.036039: step 5080, loss = 1575.93 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 17:58:22.391942: step 5081, loss = 1079.15 (14.2 examples/sec; 4.495 sec/batch)
2016-04-30 17:58:27.834508: step 5082, loss = 707.98 (11.8 examples/sec; 5.442 sec/batch)
2016-04-30 17:58:32.717647: step 5083, loss = 1375.50 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 17:58:37.404940: step 5084, loss = 488.84 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 17:58:42.371420: step 5085, loss = 887.90 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 17:58:47.126041: step 5086, loss = 925.75 (13.5 examples/sec; 4.755 sec/batch)
2016-04-30 17:58:52.309058: step 5087, loss = 1332.62 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 17:58:57.342841: step 5088, loss = 1937.75 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 17:59:03.070668: step 5089, loss = 1680.18 (11.2 examples/sec; 5.728 sec/batch)
2016-04-30 17:59:08.008498: step 5090, loss = 773.89 (13.0 examples/sec; 4.938 sec/batch)
2016-04-30 17:59:19.548418: step 5091, loss = 1116.53 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 17:59:24.383130: step 5092, loss = 997.34 (13.2 examples/sec; 4.835 sec/batch)
2016-04-30 17:59:28.932815: step 5093, loss = 965.87 (14.1 examples/sec; 4.550 sec/batch)
2016-04-30 17:59:34.242553: step 5094, loss = 1082.94 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 17:59:39.174703: step 5095, loss = 1244.24 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 17:59:43.948034: step 5096, loss = 1398.56 (13.4 examples/sec; 4.773 sec/batch)
2016-04-30 17:59:48.790066: step 5097, loss = 915.29 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 17:59:54.053225: step 5098, loss = 2356.05 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 17:59:58.627939: step 5099, loss = 525.43 (14.0 examples/sec; 4.575 sec/batch)
2016-04-30 18:00:03.885818: step 5100, loss = 798.35 (12.2 examples/sec; 5.258 sec/batch)
2016-04-30 18:00:15.925795: step 5101, loss = 1769.09 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 18:00:20.762083: step 5102, loss = 1493.50 (13.2 examples/sec; 4.836 sec/batch)
2016-04-30 18:00:25.741226: step 5103, loss = 1549.09 (12.9 examples/sec; 4.979 sec/batch)
2016-04-30 18:00:31.095093: step 5104, loss = 1463.52 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 18:00:36.104251: step 5105, loss = 1605.91 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 18:00:41.257442: step 5106, loss = 1889.77 (12.4 examples/sec; 5.153 sec/batch)
2016-04-30 18:00:46.261424: step 5107, loss = 928.11 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 18:00:51.287936: step 5108, loss = 391.59 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 18:00:56.048254: step 5109, loss = 1344.18 (13.4 examples/sec; 4.760 sec/batch)
2016-04-30 18:01:01.133864: step 5110, loss = 1531.13 (12.6 examples/sec; 5.086 sec/batch)
2016-04-30 18:01:13.224822: step 5111, loss = 1987.41 (11.9 examples/sec; 5.375 sec/batch)
2016-04-30 18:01:18.419016: step 5112, loss = 1880.39 (12.3 examples/sec; 5.194 sec/batch)
2016-04-30 18:01:23.145703: step 5113, loss = 1456.62 (13.5 examples/sec; 4.727 sec/batch)
2016-04-30 18:01:28.025616: step 5114, loss = 1134.84 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 18:01:33.033884: step 5115, loss = 1474.33 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 18:01:37.646671: step 5116, loss = 996.11 (13.9 examples/sec; 4.613 sec/batch)
2016-04-30 18:01:42.553878: step 5117, loss = 1565.00 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 18:01:48.027141: step 5118, loss = 2587.26 (11.7 examples/sec; 5.473 sec/batch)
2016-04-30 18:01:52.733254: step 5119, loss = 1490.08 (13.6 examples/sec; 4.706 sec/batch)
2016-04-30 18:01:57.698214: step 5120, loss = 1712.80 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 18:02:09.404960: step 5121, loss = 2132.34 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 18:02:14.301713: step 5122, loss = 1601.19 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 18:02:19.773423: step 5123, loss = 1035.80 (11.7 examples/sec; 5.472 sec/batch)
2016-04-30 18:02:25.033794: step 5124, loss = 2757.61 (12.2 examples/sec; 5.260 sec/batch)
2016-04-30 18:02:30.036927: step 5125, loss = 2029.17 (12.8 examples/sec; 5.003 sec/batch)
2016-04-30 18:02:35.022973: step 5126, loss = 1417.58 (12.8 examples/sec; 4.986 sec/batch)
2016-04-30 18:02:40.098859: step 5127, loss = 2185.57 (12.6 examples/sec; 5.076 sec/batch)
2016-04-30 18:02:45.167773: step 5128, loss = 2005.60 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 18:02:49.931224: step 5129, loss = 2035.03 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 18:02:55.474925: step 5130, loss = 3048.33 (11.5 examples/sec; 5.544 sec/batch)
2016-04-30 18:03:07.233783: step 5131, loss = 1418.85 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 18:03:12.344198: step 5132, loss = 1504.53 (12.5 examples/sec; 5.110 sec/batch)
2016-04-30 18:03:17.038094: step 5133, loss = 1210.39 (13.6 examples/sec; 4.694 sec/batch)
2016-04-30 18:03:22.246372: step 5134, loss = 1618.71 (12.3 examples/sec; 5.208 sec/batch)
2016-04-30 18:03:27.713529: step 5135, loss = 2456.96 (11.7 examples/sec; 5.467 sec/batch)
2016-04-30 18:03:32.340915: step 5136, loss = 4048.90 (13.8 examples/sec; 4.627 sec/batch)
2016-04-30 18:03:37.242908: step 5137, loss = 1653.93 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 18:03:42.170428: step 5138, loss = 1860.22 (13.0 examples/sec; 4.927 sec/batch)
2016-04-30 18:03:46.898146: step 5139, loss = 999.71 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 18:03:51.982589: step 5140, loss = 2010.00 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 18:04:04.251006: step 5141, loss = 2954.78 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 18:04:09.408104: step 5142, loss = 2862.12 (12.4 examples/sec; 5.157 sec/batch)
2016-04-30 18:04:14.096618: step 5143, loss = 2422.03 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 18:04:18.981299: step 5144, loss = 2775.11 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 18:04:23.990000: step 5145, loss = 2786.92 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 18:04:28.688891: step 5146, loss = 1928.79 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 18:04:34.165529: step 5147, loss = 1482.20 (11.7 examples/sec; 5.477 sec/batch)
2016-04-30 18:04:39.218089: step 5148, loss = 2793.55 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 18:04:43.877846: step 5149, loss = 3525.23 (13.7 examples/sec; 4.660 sec/batch)
2016-04-30 18:04:48.805435: step 5150, loss = 3638.90 (13.0 examples/sec; 4.927 sec/batch)
2016-04-30 18:05:00.456263: step 5151, loss = 1823.77 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 18:05:06.040333: step 5152, loss = 3922.39 (11.5 examples/sec; 5.584 sec/batch)
2016-04-30 18:05:10.707618: step 5153, loss = 3786.48 (13.7 examples/sec; 4.667 sec/batch)
2016-04-30 18:05:15.715404: step 5154, loss = 2052.78 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 18:05:20.624549: step 5155, loss = 1031.85 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 18:05:25.321061: step 5156, loss = 1458.55 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 18:05:30.487413: step 5157, loss = 4542.10 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 18:05:35.276434: step 5158, loss = 3981.46 (13.4 examples/sec; 4.789 sec/batch)
2016-04-30 18:05:40.612522: step 5159, loss = 3534.35 (12.0 examples/sec; 5.336 sec/batch)
2016-04-30 18:05:45.509139: step 5160, loss = 3232.19 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 18:05:56.984754: step 5161, loss = 3975.32 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 18:06:01.837845: step 5162, loss = 1799.03 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 18:06:06.782098: step 5163, loss = 2813.20 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 18:06:12.325262: step 5164, loss = 3618.66 (11.5 examples/sec; 5.543 sec/batch)
2016-04-30 18:06:17.048631: step 5165, loss = 2736.19 (13.5 examples/sec; 4.723 sec/batch)
2016-04-30 18:06:21.856964: step 5166, loss = 4909.47 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 18:06:26.703349: step 5167, loss = 1904.25 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 18:06:31.490456: step 5168, loss = 3696.46 (13.4 examples/sec; 4.787 sec/batch)
2016-04-30 18:06:36.511599: step 5169, loss = 2800.59 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 18:06:41.221281: step 5170, loss = 3265.10 (13.6 examples/sec; 4.710 sec/batch)
2016-04-30 18:06:53.091551: step 5171, loss = 2958.12 (14.0 examples/sec; 4.561 sec/batch)
2016-04-30 18:06:58.227082: step 5172, loss = 4227.17 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 18:07:03.495507: step 5173, loss = 3608.60 (12.1 examples/sec; 5.268 sec/batch)
2016-04-30 18:07:08.086270: step 5174, loss = 4277.54 (13.9 examples/sec; 4.591 sec/batch)
2016-04-30 18:07:13.089927: step 5175, loss = 3046.36 (12.8 examples/sec; 5.004 sec/batch)
2016-04-30 18:07:18.442250: step 5176, loss = 4399.50 (12.0 examples/sec; 5.352 sec/batch)
2016-04-30 18:07:23.303298: step 5177, loss = 4374.66 (13.2 examples/sec; 4.861 sec/batch)
2016-04-30 18:07:28.529016: step 5178, loss = 1812.89 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 18:07:33.659947: step 5179, loss = 4119.65 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 18:07:38.389948: step 5180, loss = 3423.71 (13.5 examples/sec; 4.730 sec/batch)
2016-04-30 18:07:50.543481: step 5181, loss = 4277.12 (12.0 examples/sec; 5.317 sec/batch)
2016-04-30 18:07:55.491995: step 5182, loss = 4310.21 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 18:08:00.707834: step 5183, loss = 3758.13 (12.3 examples/sec; 5.216 sec/batch)
2016-04-30 18:08:05.400529: step 5184, loss = 3094.30 (13.6 examples/sec; 4.693 sec/batch)
2016-04-30 18:08:10.266266: step 5185, loss = 2878.47 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 18:08:15.458813: step 5186, loss = 2409.69 (12.3 examples/sec; 5.192 sec/batch)
2016-04-30 18:08:20.053083: step 5187, loss = 4348.04 (13.9 examples/sec; 4.594 sec/batch)
2016-04-30 18:08:25.558491: step 5188, loss = 5938.69 (11.6 examples/sec; 5.505 sec/batch)
2016-04-30 18:08:30.643858: step 5189, loss = 4064.14 (12.6 examples/sec; 5.085 sec/batch)
2016-04-30 18:08:35.272299: step 5190, loss = 3136.34 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 18:08:46.726245: step 5191, loss = 6507.12 (14.2 examples/sec; 4.514 sec/batch)
2016-04-30 18:08:51.673511: step 5192, loss = 6479.69 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 18:08:57.272538: step 5193, loss = 2102.29 (11.4 examples/sec; 5.599 sec/batch)
2016-04-30 18:09:02.126943: step 5194, loss = 6892.68 (13.2 examples/sec; 4.854 sec/batch)
2016-04-30 18:09:07.061413: step 5195, loss = 4162.64 (13.0 examples/sec; 4.934 sec/batch)
2016-04-30 18:09:12.009018: step 5196, loss = 3118.02 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 18:09:16.897039: step 5197, loss = 3675.66 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 18:09:21.863257: step 5198, loss = 3922.77 (12.9 examples/sec; 4.966 sec/batch)
2016-04-30 18:09:26.735464: step 5199, loss = 6267.69 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 18:09:31.931582: step 5200, loss = 7406.54 (12.3 examples/sec; 5.196 sec/batch)
2016-04-30 18:09:43.305505: step 5201, loss = 7374.38 (13.9 examples/sec; 4.605 sec/batch)
2016-04-30 18:09:48.224166: step 5202, loss = 3993.62 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 18:09:52.972444: step 5203, loss = 3747.97 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 18:09:57.946742: step 5204, loss = 5415.80 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 18:10:03.683081: step 5205, loss = 6402.81 (11.2 examples/sec; 5.736 sec/batch)
2016-04-30 18:10:08.279644: step 5206, loss = 9442.45 (13.9 examples/sec; 4.596 sec/batch)
2016-04-30 18:10:13.194857: step 5207, loss = 4838.26 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 18:10:18.282398: step 5208, loss = 4878.46 (12.6 examples/sec; 5.087 sec/batch)
2016-04-30 18:10:23.017805: step 5209, loss = 3758.16 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 18:10:28.098476: step 5210, loss = 4651.81 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 18:10:39.859712: step 5211, loss = 7850.87 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 18:10:44.584859: step 5212, loss = 4542.59 (13.5 examples/sec; 4.725 sec/batch)
2016-04-30 18:10:49.474154: step 5213, loss = 5573.23 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 18:10:54.364207: step 5214, loss = 7979.10 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 18:10:58.944815: step 5215, loss = 6857.39 (14.0 examples/sec; 4.581 sec/batch)
2016-04-30 18:11:04.068827: step 5216, loss = 5278.09 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 18:11:09.468942: step 5217, loss = 5575.95 (11.9 examples/sec; 5.400 sec/batch)
2016-04-30 18:11:14.055070: step 5218, loss = 11360.85 (14.0 examples/sec; 4.586 sec/batch)
2016-04-30 18:11:18.902311: step 5219, loss = 7914.47 (13.2 examples/sec; 4.847 sec/batch)
2016-04-30 18:11:23.601873: step 5220, loss = 3243.89 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 18:11:34.455545: step 5221, loss = 4084.83 (14.5 examples/sec; 4.425 sec/batch)
2016-04-30 18:11:39.604812: step 5222, loss = 9947.00 (12.4 examples/sec; 5.149 sec/batch)
2016-04-30 18:11:44.366269: step 5223, loss = 13263.85 (13.4 examples/sec; 4.761 sec/batch)
2016-04-30 18:11:49.263233: step 5224, loss = 4577.59 (13.1 examples/sec; 4.897 sec/batch)
2016-04-30 18:11:54.316538: step 5225, loss = 7183.60 (12.7 examples/sec; 5.053 sec/batch)
2016-04-30 18:11:59.060880: step 5226, loss = 6565.65 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 18:12:04.059693: step 5227, loss = 11224.83 (12.8 examples/sec; 4.999 sec/batch)
2016-04-30 18:12:08.799596: step 5228, loss = 5919.19 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 18:12:13.879393: step 5229, loss = 5381.57 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 18:12:18.677896: step 5230, loss = 8449.27 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 18:12:29.664433: step 5231, loss = 11424.24 (13.8 examples/sec; 4.649 sec/batch)
2016-04-30 18:12:34.416335: step 5232, loss = 11932.86 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 18:12:39.336493: step 5233, loss = 1920.79 (13.0 examples/sec; 4.920 sec/batch)
2016-04-30 18:12:43.991472: step 5234, loss = 8923.63 (13.7 examples/sec; 4.655 sec/batch)
2016-04-30 18:12:50.148623: step 5235, loss = 7329.39 (10.4 examples/sec; 6.157 sec/batch)
2016-04-30 18:12:55.331500: step 5236, loss = 10285.29 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 18:13:00.834076: step 5237, loss = 11379.40 (11.6 examples/sec; 5.502 sec/batch)
2016-04-30 18:13:05.732119: step 5238, loss = 12403.18 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 18:13:10.597752: step 5239, loss = 5005.85 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 18:13:15.551414: step 5240, loss = 5268.46 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 18:13:27.374483: step 5241, loss = 8336.90 (13.6 examples/sec; 4.712 sec/batch)
2016-04-30 18:13:32.076004: step 5242, loss = 14068.83 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 18:13:37.015026: step 5243, loss = 8722.64 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 18:13:41.886395: step 5244, loss = 12070.72 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 18:13:46.715079: step 5245, loss = 6439.71 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 18:13:52.406104: step 5246, loss = 15369.29 (11.2 examples/sec; 5.691 sec/batch)
2016-04-30 18:13:57.446096: step 5247, loss = 19525.36 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 18:14:02.418954: step 5248, loss = 7181.38 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 18:14:07.599499: step 5249, loss = 7625.06 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 18:14:12.831855: step 5250, loss = 8586.53 (12.2 examples/sec; 5.232 sec/batch)
2016-04-30 18:14:23.813346: step 5251, loss = 8775.15 (14.2 examples/sec; 4.520 sec/batch)
2016-04-30 18:14:29.176432: step 5252, loss = 13605.91 (11.9 examples/sec; 5.363 sec/batch)
2016-04-30 18:14:34.057975: step 5253, loss = 14075.27 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 18:14:38.855374: step 5254, loss = 11375.98 (13.3 examples/sec; 4.797 sec/batch)
2016-04-30 18:14:43.692160: step 5255, loss = 9909.94 (13.2 examples/sec; 4.837 sec/batch)
2016-04-30 18:14:48.698122: step 5256, loss = 9193.50 (12.8 examples/sec; 5.006 sec/batch)
2016-04-30 18:14:53.419395: step 5257, loss = 13198.88 (13.6 examples/sec; 4.721 sec/batch)
2016-04-30 18:14:58.797712: step 5258, loss = 10836.37 (11.9 examples/sec; 5.378 sec/batch)
2016-04-30 18:15:03.896942: step 5259, loss = 12415.03 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 18:15:08.602150: step 5260, loss = 10233.24 (13.6 examples/sec; 4.705 sec/batch)
2016-04-30 18:15:20.727729: step 5261, loss = 19499.56 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 18:15:25.642435: step 5262, loss = 16602.74 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 18:15:31.143035: step 5263, loss = 8837.40 (11.6 examples/sec; 5.501 sec/batch)
2016-04-30 18:15:36.049455: step 5264, loss = 11837.36 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 18:15:40.709917: step 5265, loss = 17265.24 (13.7 examples/sec; 4.660 sec/batch)
2016-04-30 18:15:45.639301: step 5266, loss = 19010.42 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 18:15:50.551319: step 5267, loss = 12003.76 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 18:15:55.424054: step 5268, loss = 5419.38 (13.1 examples/sec; 4.873 sec/batch)
2016-04-30 18:16:00.686267: step 5269, loss = 10730.35 (12.2 examples/sec; 5.262 sec/batch)
2016-04-30 18:16:06.335910: step 5270, loss = 22724.27 (11.3 examples/sec; 5.650 sec/batch)
2016-04-30 18:16:17.397362: step 5271, loss = 12604.56 (14.0 examples/sec; 4.570 sec/batch)
2016-04-30 18:16:22.301486: step 5272, loss = 7148.04 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 18:16:27.211746: step 5273, loss = 16276.42 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 18:16:32.123650: step 5274, loss = 10683.89 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 18:16:37.548459: step 5275, loss = 11355.55 (11.8 examples/sec; 5.425 sec/batch)
2016-04-30 18:16:42.585071: step 5276, loss = 17985.48 (12.7 examples/sec; 5.036 sec/batch)
2016-04-30 18:16:47.201908: step 5277, loss = 23106.90 (13.9 examples/sec; 4.617 sec/batch)
2016-04-30 18:16:52.201806: step 5278, loss = 6536.36 (12.8 examples/sec; 5.000 sec/batch)
2016-04-30 18:16:57.115055: step 5279, loss = 10286.77 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 18:17:02.074631: step 5280, loss = 13834.41 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 18:17:13.889406: step 5281, loss = 28899.08 (12.0 examples/sec; 5.342 sec/batch)
2016-04-30 18:17:18.843127: step 5282, loss = 6614.91 (12.9 examples/sec; 4.954 sec/batch)
2016-04-30 18:17:23.643687: step 5283, loss = 10593.69 (13.3 examples/sec; 4.800 sec/batch)
2016-04-30 18:17:28.436013: step 5284, loss = 22449.16 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 18:17:33.466789: step 5285, loss = 15688.33 (12.7 examples/sec; 5.031 sec/batch)
2016-04-30 18:17:38.094505: step 5286, loss = 12389.19 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 18:17:43.587508: step 5287, loss = 16905.24 (11.7 examples/sec; 5.493 sec/batch)
2016-04-30 18:17:48.696137: step 5288, loss = 28118.66 (12.5 examples/sec; 5.109 sec/batch)
2016-04-30 18:17:53.478225: step 5289, loss = 19712.81 (13.4 examples/sec; 4.782 sec/batch)
2016-04-30 18:17:58.488954: step 5290, loss = 14292.95 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 18:18:10.256491: step 5291, loss = 16647.60 (13.6 examples/sec; 4.699 sec/batch)
2016-04-30 18:18:15.767111: step 5292, loss = 16151.22 (11.6 examples/sec; 5.511 sec/batch)
2016-04-30 18:18:21.129108: step 5293, loss = 17219.94 (11.9 examples/sec; 5.362 sec/batch)
2016-04-30 18:18:25.837703: step 5294, loss = 13337.01 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 18:18:30.722446: step 5295, loss = 22980.15 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 18:18:35.435471: step 5296, loss = 15287.53 (13.6 examples/sec; 4.713 sec/batch)
2016-04-30 18:18:40.542607: step 5297, loss = 11133.30 (12.5 examples/sec; 5.107 sec/batch)
2016-04-30 18:18:45.552031: step 5298, loss = 17495.45 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 18:18:51.015162: step 5299, loss = 21037.26 (11.7 examples/sec; 5.463 sec/batch)
2016-04-30 18:18:55.761930: step 5300, loss = 21691.42 (13.5 examples/sec; 4.747 sec/batch)
2016-04-30 18:19:07.337268: step 5301, loss = 12828.40 (14.2 examples/sec; 4.516 sec/batch)
2016-04-30 18:19:12.405266: step 5302, loss = 8601.16 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 18:19:17.214304: step 5303, loss = 22956.07 (13.3 examples/sec; 4.809 sec/batch)
2016-04-30 18:19:22.846119: step 5304, loss = 26601.17 (11.4 examples/sec; 5.632 sec/batch)
2016-04-30 18:19:27.764841: step 5305, loss = 23512.40 (13.0 examples/sec; 4.919 sec/batch)
2016-04-30 18:19:32.439476: step 5306, loss = 21299.41 (13.7 examples/sec; 4.675 sec/batch)
2016-04-30 18:19:37.383101: step 5307, loss = 18113.48 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 18:19:42.347821: step 5308, loss = 24785.41 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 18:19:47.089975: step 5309, loss = 11931.77 (13.5 examples/sec; 4.742 sec/batch)
2016-04-30 18:19:52.196430: step 5310, loss = 31279.40 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 18:20:04.495923: step 5311, loss = 30835.12 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 18:20:09.395192: step 5312, loss = 30164.92 (13.1 examples/sec; 4.899 sec/batch)
2016-04-30 18:20:14.158682: step 5313, loss = 29413.62 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 18:20:19.079537: step 5314, loss = 21392.10 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 18:20:23.700540: step 5315, loss = 21727.41 (13.9 examples/sec; 4.621 sec/batch)
2016-04-30 18:20:29.099365: step 5316, loss = 19691.69 (11.9 examples/sec; 5.399 sec/batch)
2016-04-30 18:20:34.008714: step 5317, loss = 30930.42 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 18:20:38.608452: step 5318, loss = 24298.12 (13.9 examples/sec; 4.600 sec/batch)
2016-04-30 18:20:43.452946: step 5319, loss = 29076.25 (13.2 examples/sec; 4.844 sec/batch)
2016-04-30 18:20:48.426856: step 5320, loss = 27864.85 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 18:20:59.531360: step 5321, loss = 13079.77 (14.7 examples/sec; 4.361 sec/batch)
2016-04-30 18:21:04.974247: step 5322, loss = 29083.03 (11.8 examples/sec; 5.443 sec/batch)
2016-04-30 18:21:10.030942: step 5323, loss = 37162.23 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 18:21:14.655350: step 5324, loss = 34483.72 (13.8 examples/sec; 4.624 sec/batch)
2016-04-30 18:21:19.529078: step 5325, loss = 11895.08 (13.1 examples/sec; 4.874 sec/batch)
2016-04-30 18:21:24.436338: step 5326, loss = 23841.35 (13.0 examples/sec; 4.907 sec/batch)
2016-04-30 18:21:29.161937: step 5327, loss = 33285.94 (13.5 examples/sec; 4.726 sec/batch)
2016-04-30 18:21:34.564005: step 5328, loss = 24839.39 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 18:21:39.334444: step 5329, loss = 25081.60 (13.4 examples/sec; 4.770 sec/batch)
2016-04-30 18:21:43.972230: step 5330, loss = 46446.46 (13.8 examples/sec; 4.638 sec/batch)
2016-04-30 18:21:55.182918: step 5331, loss = 33950.75 (13.5 examples/sec; 4.723 sec/batch)
2016-04-30 18:21:59.917731: step 5332, loss = 17492.95 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 18:22:04.954953: step 5333, loss = 45396.80 (12.7 examples/sec; 5.037 sec/batch)
2016-04-30 18:22:10.475167: step 5334, loss = 47107.57 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 18:22:15.504779: step 5335, loss = 26113.35 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 18:22:20.116436: step 5336, loss = 39431.39 (13.9 examples/sec; 4.612 sec/batch)
2016-04-30 18:22:25.000921: step 5337, loss = 24072.36 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 18:22:29.794608: step 5338, loss = 20250.58 (13.4 examples/sec; 4.794 sec/batch)
2016-04-30 18:22:34.889230: step 5339, loss = 49083.76 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 18:22:40.151932: step 5340, loss = 46014.42 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 18:22:51.704239: step 5341, loss = 34047.89 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 18:22:56.369022: step 5342, loss = 20975.10 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 18:23:01.923308: step 5343, loss = 43239.22 (11.5 examples/sec; 5.554 sec/batch)
2016-04-30 18:23:07.092458: step 5344, loss = 50458.09 (12.4 examples/sec; 5.169 sec/batch)
2016-04-30 18:23:12.094536: step 5345, loss = 33419.77 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 18:23:17.496714: step 5346, loss = 24061.39 (11.8 examples/sec; 5.402 sec/batch)
2016-04-30 18:23:22.389813: step 5347, loss = 50869.07 (13.1 examples/sec; 4.893 sec/batch)
2016-04-30 18:23:27.415537: step 5348, loss = 52271.65 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 18:23:32.103903: step 5349, loss = 36663.53 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 18:23:37.046621: step 5350, loss = 21550.30 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 18:23:48.945841: step 5351, loss = 33015.34 (11.8 examples/sec; 5.403 sec/batch)
2016-04-30 18:23:53.674805: step 5352, loss = 51877.79 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 18:23:58.574679: step 5353, loss = 62577.66 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 18:24:03.620853: step 5354, loss = 32557.49 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 18:24:08.669564: step 5355, loss = 47454.43 (12.7 examples/sec; 5.049 sec/batch)
2016-04-30 18:24:15.224905: step 5356, loss = 39875.24 (9.8 examples/sec; 6.555 sec/batch)
2016-04-30 18:24:21.239102: step 5357, loss = 33135.77 (10.6 examples/sec; 6.014 sec/batch)
2016-04-30 18:24:26.043873: step 5358, loss = 59590.12 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 18:24:31.118777: step 5359, loss = 50380.21 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 18:24:35.985130: step 5360, loss = 47418.43 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 18:24:47.611976: step 5361, loss = 29108.41 (13.9 examples/sec; 4.591 sec/batch)
2016-04-30 18:24:53.086281: step 5362, loss = 49547.41 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 18:24:57.954044: step 5363, loss = 70086.94 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 18:25:02.977840: step 5364, loss = 64238.42 (12.7 examples/sec; 5.024 sec/batch)
2016-04-30 18:25:07.835234: step 5365, loss = 20543.08 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 18:25:12.677717: step 5366, loss = 51383.23 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 18:25:17.279753: step 5367, loss = 72900.06 (13.9 examples/sec; 4.602 sec/batch)
2016-04-30 18:25:22.450502: step 5368, loss = 36467.57 (12.4 examples/sec; 5.171 sec/batch)
2016-04-30 18:25:27.972971: step 5369, loss = 33288.00 (11.6 examples/sec; 5.522 sec/batch)
2016-04-30 18:25:32.881066: step 5370, loss = 56474.53 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 18:25:44.031139: step 5371, loss = 52712.77 (14.2 examples/sec; 4.497 sec/batch)
2016-04-30 18:25:49.060638: step 5372, loss = 44327.34 (12.7 examples/sec; 5.029 sec/batch)
2016-04-30 18:25:53.816093: step 5373, loss = 54896.45 (13.5 examples/sec; 4.755 sec/batch)
2016-04-30 18:25:59.162851: step 5374, loss = 40663.68 (12.0 examples/sec; 5.347 sec/batch)
2016-04-30 18:26:04.173986: step 5375, loss = 48950.19 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 18:26:08.675965: step 5376, loss = 89075.45 (14.2 examples/sec; 4.502 sec/batch)
2016-04-30 18:26:13.593424: step 5377, loss = 57598.62 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 18:26:18.638819: step 5378, loss = 72142.53 (12.7 examples/sec; 5.045 sec/batch)
2016-04-30 18:26:23.402194: step 5379, loss = 38670.13 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 18:26:28.308031: step 5380, loss = 33447.71 (13.0 examples/sec; 4.906 sec/batch)
2016-04-30 18:26:39.944620: step 5381, loss = 58459.29 (14.0 examples/sec; 4.583 sec/batch)
2016-04-30 18:26:44.825879: step 5382, loss = 97540.27 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 18:26:49.630590: step 5383, loss = 77182.27 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 18:26:54.501581: step 5384, loss = 39412.43 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 18:26:59.083860: step 5385, loss = 52821.88 (14.0 examples/sec; 4.582 sec/batch)
2016-04-30 18:27:04.674497: step 5386, loss = 101232.60 (11.4 examples/sec; 5.591 sec/batch)
2016-04-30 18:27:09.633192: step 5387, loss = 63379.01 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 18:27:14.290716: step 5388, loss = 61096.50 (13.7 examples/sec; 4.657 sec/batch)
2016-04-30 18:27:19.245439: step 5389, loss = 71132.32 (12.9 examples/sec; 4.955 sec/batch)
2016-04-30 18:27:23.904990: step 5390, loss = 33210.65 (13.7 examples/sec; 4.659 sec/batch)
2016-04-30 18:27:34.828653: step 5391, loss = 65994.37 (14.2 examples/sec; 4.516 sec/batch)
2016-04-30 18:27:40.284833: step 5392, loss = 82968.01 (11.7 examples/sec; 5.456 sec/batch)
2016-04-30 18:27:45.020994: step 5393, loss = 105189.10 (13.5 examples/sec; 4.736 sec/batch)
2016-04-30 18:27:49.691570: step 5394, loss = 91435.46 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 18:27:54.599314: step 5395, loss = 55881.16 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 18:27:59.375386: step 5396, loss = 90579.70 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 18:28:04.529681: step 5397, loss = 89865.02 (12.4 examples/sec; 5.154 sec/batch)
2016-04-30 18:28:09.701534: step 5398, loss = 43195.16 (12.4 examples/sec; 5.172 sec/batch)
2016-04-30 18:28:14.984476: step 5399, loss = 79597.52 (12.1 examples/sec; 5.283 sec/batch)
2016-04-30 18:28:19.733752: step 5400, loss = 70497.67 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 18:28:31.121152: step 5401, loss = 106274.83 (13.3 examples/sec; 4.827 sec/batch)
2016-04-30 18:28:35.844518: step 5402, loss = 74280.19 (13.5 examples/sec; 4.723 sec/batch)
2016-04-30 18:28:40.786299: step 5403, loss = 102871.16 (13.0 examples/sec; 4.942 sec/batch)
2016-04-30 18:28:46.295343: step 5404, loss = 94780.35 (11.6 examples/sec; 5.509 sec/batch)
2016-04-30 18:28:51.501475: step 5405, loss = 82483.85 (12.3 examples/sec; 5.206 sec/batch)
2016-04-30 18:28:56.148854: step 5406, loss = 42280.06 (13.8 examples/sec; 4.647 sec/batch)
2016-04-30 18:29:01.450536: step 5407, loss = 46434.26 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 18:29:06.429115: step 5408, loss = 109020.77 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 18:29:11.471410: step 5409, loss = 118650.82 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 18:29:17.070780: step 5410, loss = 109590.18 (11.4 examples/sec; 5.599 sec/batch)
2016-04-30 18:29:28.595565: step 5411, loss = 80710.91 (12.8 examples/sec; 5.011 sec/batch)
2016-04-30 18:29:33.667404: step 5412, loss = 50627.43 (12.6 examples/sec; 5.072 sec/batch)
2016-04-30 18:29:38.407592: step 5413, loss = 66956.53 (13.5 examples/sec; 4.740 sec/batch)
2016-04-30 18:29:43.397564: step 5414, loss = 83020.82 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 18:29:48.231670: step 5415, loss = 94379.26 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 18:29:53.683560: step 5416, loss = 84776.15 (11.7 examples/sec; 5.452 sec/batch)
2016-04-30 18:29:58.529403: step 5417, loss = 121690.06 (13.2 examples/sec; 4.846 sec/batch)
2016-04-30 18:30:03.477144: step 5418, loss = 81220.52 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 18:30:08.703038: step 5419, loss = 81087.41 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 18:30:13.636391: step 5420, loss = 123384.98 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 18:30:25.901165: step 5421, loss = 83286.34 (12.0 examples/sec; 5.333 sec/batch)
2016-04-30 18:30:30.874940: step 5422, loss = 61115.88 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 18:30:35.609476: step 5423, loss = 89918.03 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 18:30:40.621286: step 5424, loss = 141402.95 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 18:30:45.684515: step 5425, loss = 65597.40 (12.6 examples/sec; 5.063 sec/batch)
2016-04-30 18:30:50.460226: step 5426, loss = 99729.75 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 18:30:56.086845: step 5427, loss = 151016.58 (11.4 examples/sec; 5.627 sec/batch)
2016-04-30 18:31:01.469364: step 5428, loss = 106876.16 (11.9 examples/sec; 5.382 sec/batch)
2016-04-30 18:31:06.604865: step 5429, loss = 97773.84 (12.5 examples/sec; 5.135 sec/batch)
2016-04-30 18:31:11.368424: step 5430, loss = 75043.66 (13.4 examples/sec; 4.763 sec/batch)
2016-04-30 18:31:22.806066: step 5431, loss = 144670.14 (14.2 examples/sec; 4.515 sec/batch)
2016-04-30 18:31:27.983933: step 5432, loss = 136095.47 (12.4 examples/sec; 5.178 sec/batch)
2016-04-30 18:31:32.850270: step 5433, loss = 173409.73 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 18:31:37.732901: step 5434, loss = 88754.28 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 18:31:42.813498: step 5435, loss = 116818.80 (12.6 examples/sec; 5.081 sec/batch)
2016-04-30 18:31:47.441093: step 5436, loss = 139902.09 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 18:31:52.439523: step 5437, loss = 97682.34 (12.8 examples/sec; 4.998 sec/batch)
2016-04-30 18:31:57.424222: step 5438, loss = 78336.76 (12.8 examples/sec; 4.985 sec/batch)
2016-04-30 18:32:03.162192: step 5439, loss = 167882.59 (11.2 examples/sec; 5.738 sec/batch)
2016-04-30 18:32:07.897700: step 5440, loss = 90879.27 (13.5 examples/sec; 4.735 sec/batch)
2016-04-30 18:32:19.244338: step 5441, loss = 99382.95 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 18:32:24.224354: step 5442, loss = 151619.94 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 18:32:28.921104: step 5443, loss = 226559.78 (13.6 examples/sec; 4.697 sec/batch)
2016-04-30 18:32:34.329143: step 5444, loss = 163771.06 (11.8 examples/sec; 5.408 sec/batch)
2016-04-30 18:32:39.279527: step 5445, loss = 58725.46 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 18:32:44.149562: step 5446, loss = 131133.80 (13.1 examples/sec; 4.870 sec/batch)
2016-04-30 18:32:49.036127: step 5447, loss = 165840.28 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 18:32:53.697694: step 5448, loss = 103054.21 (13.7 examples/sec; 4.661 sec/batch)
2016-04-30 18:32:58.797176: step 5449, loss = 170428.22 (12.6 examples/sec; 5.099 sec/batch)
2016-04-30 18:33:03.910367: step 5450, loss = 137029.20 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 18:33:15.844879: step 5451, loss = 165977.75 (13.3 examples/sec; 4.798 sec/batch)
2016-04-30 18:33:20.581576: step 5452, loss = 146716.88 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 18:33:25.464461: step 5453, loss = 39207.59 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 18:33:30.483716: step 5454, loss = 80936.28 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 18:33:35.533332: step 5455, loss = 193587.02 (12.7 examples/sec; 5.050 sec/batch)
2016-04-30 18:33:40.977553: step 5456, loss = 271057.00 (11.8 examples/sec; 5.444 sec/batch)
2016-04-30 18:33:46.009566: step 5457, loss = 133518.88 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 18:33:50.894968: step 5458, loss = 129599.54 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 18:33:55.796833: step 5459, loss = 258816.78 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 18:34:00.824823: step 5460, loss = 155588.62 (12.7 examples/sec; 5.028 sec/batch)
2016-04-30 18:34:12.714085: step 5461, loss = 108249.51 (13.3 examples/sec; 4.826 sec/batch)
2016-04-30 18:34:17.670495: step 5462, loss = 99986.90 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 18:34:22.550475: step 5463, loss = 240047.56 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 18:34:27.503538: step 5464, loss = 121685.42 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 18:34:32.371881: step 5465, loss = 147807.17 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 18:34:37.236895: step 5466, loss = 211049.44 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 18:34:41.911157: step 5467, loss = 202507.84 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 18:34:47.152887: step 5468, loss = 196665.64 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 18:34:52.095747: step 5469, loss = 212485.42 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 18:34:56.963160: step 5470, loss = 192500.05 (13.1 examples/sec; 4.867 sec/batch)
2016-04-30 18:35:08.163305: step 5471, loss = 162357.45 (14.2 examples/sec; 4.505 sec/batch)
2016-04-30 18:35:13.389685: step 5472, loss = 197734.52 (12.2 examples/sec; 5.226 sec/batch)
2016-04-30 18:35:18.181395: step 5473, loss = 200659.92 (13.4 examples/sec; 4.792 sec/batch)
2016-04-30 18:35:23.499109: step 5474, loss = 363344.72 (12.0 examples/sec; 5.318 sec/batch)
2016-04-30 18:35:28.427579: step 5475, loss = 243183.91 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 18:35:33.156157: step 5476, loss = 101660.27 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 18:35:38.044586: step 5477, loss = 107820.85 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 18:35:42.945125: step 5478, loss = 157600.45 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 18:35:47.800688: step 5479, loss = 267627.00 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 18:35:53.370700: step 5480, loss = 294445.78 (11.5 examples/sec; 5.570 sec/batch)
2016-04-30 18:36:05.015003: step 5481, loss = 212800.88 (12.4 examples/sec; 5.176 sec/batch)
2016-04-30 18:36:10.007133: step 5482, loss = 200926.84 (12.8 examples/sec; 4.992 sec/batch)
2016-04-30 18:36:14.744415: step 5483, loss = 161996.69 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 18:36:19.714866: step 5484, loss = 213709.66 (12.9 examples/sec; 4.970 sec/batch)
2016-04-30 18:36:25.155486: step 5485, loss = 283346.94 (11.8 examples/sec; 5.441 sec/batch)
2016-04-30 18:36:29.915224: step 5486, loss = 154620.97 (13.4 examples/sec; 4.760 sec/batch)
2016-04-30 18:36:34.950619: step 5487, loss = 170239.34 (12.7 examples/sec; 5.035 sec/batch)
2016-04-30 18:36:39.882653: step 5488, loss = 257206.62 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 18:36:44.395044: step 5489, loss = 239062.89 (14.2 examples/sec; 4.512 sec/batch)
2016-04-30 18:36:49.310601: step 5490, loss = 140265.64 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 18:37:01.354161: step 5491, loss = 291181.62 (11.7 examples/sec; 5.474 sec/batch)
2016-04-30 18:37:06.120069: step 5492, loss = 225114.91 (13.4 examples/sec; 4.765 sec/batch)
2016-04-30 18:37:10.978804: step 5493, loss = 117467.40 (13.2 examples/sec; 4.859 sec/batch)
2016-04-30 18:37:15.869601: step 5494, loss = 220739.20 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 18:37:20.895437: step 5495, loss = 158611.34 (12.7 examples/sec; 5.026 sec/batch)
2016-04-30 18:37:25.744133: step 5496, loss = 279260.94 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 18:37:30.912030: step 5497, loss = 256714.92 (12.4 examples/sec; 5.168 sec/batch)
2016-04-30 18:37:36.149784: step 5498, loss = 348841.66 (12.2 examples/sec; 5.238 sec/batch)
2016-04-30 18:37:40.877737: step 5499, loss = 318442.06 (13.5 examples/sec; 4.728 sec/batch)
2016-04-30 18:37:45.923761: step 5500, loss = 167068.84 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 18:37:57.555398: step 5501, loss = 231759.52 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 18:38:02.601351: step 5502, loss = 243160.84 (12.7 examples/sec; 5.046 sec/batch)
2016-04-30 18:38:08.305517: step 5503, loss = 293326.12 (11.2 examples/sec; 5.704 sec/batch)
2016-04-30 18:38:13.405336: step 5504, loss = 156242.36 (12.6 examples/sec; 5.095 sec/batch)
2016-04-30 18:38:18.600071: step 5505, loss = 283362.38 (12.3 examples/sec; 5.195 sec/batch)
2016-04-30 18:38:23.271334: step 5506, loss = 327862.16 (13.7 examples/sec; 4.671 sec/batch)
2016-04-30 18:38:28.202132: step 5507, loss = 294087.97 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 18:38:32.812246: step 5508, loss = 174631.23 (13.9 examples/sec; 4.610 sec/batch)
2016-04-30 18:38:38.171036: step 5509, loss = 234396.05 (11.9 examples/sec; 5.359 sec/batch)
2016-04-30 18:38:43.099631: step 5510, loss = 431982.16 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 18:38:54.641760: step 5511, loss = 345680.66 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 18:38:59.261317: step 5512, loss = 177895.30 (13.9 examples/sec; 4.619 sec/batch)
2016-04-30 18:39:04.234332: step 5513, loss = 179590.36 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 18:39:08.823685: step 5514, loss = 372680.81 (13.9 examples/sec; 4.589 sec/batch)
2016-04-30 18:39:14.318659: step 5515, loss = 385483.44 (11.6 examples/sec; 5.495 sec/batch)
2016-04-30 18:39:19.190733: step 5516, loss = 476311.44 (13.1 examples/sec; 4.872 sec/batch)
2016-04-30 18:39:23.809836: step 5517, loss = 149537.30 (13.9 examples/sec; 4.619 sec/batch)
2016-04-30 18:39:28.664914: step 5518, loss = 264344.97 (13.2 examples/sec; 4.855 sec/batch)
2016-04-30 18:39:33.682419: step 5519, loss = 371847.94 (12.8 examples/sec; 5.017 sec/batch)
2016-04-30 18:39:38.356440: step 5520, loss = 290492.88 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 18:39:50.277036: step 5521, loss = 441152.38 (14.3 examples/sec; 4.462 sec/batch)
2016-04-30 18:39:55.493299: step 5522, loss = 527408.25 (12.3 examples/sec; 5.216 sec/batch)
2016-04-30 18:40:00.163595: step 5523, loss = 244535.86 (13.7 examples/sec; 4.670 sec/batch)
2016-04-30 18:40:05.449172: step 5524, loss = 292327.25 (12.1 examples/sec; 5.285 sec/batch)
2016-04-30 18:40:10.427616: step 5525, loss = 480939.50 (12.9 examples/sec; 4.978 sec/batch)
2016-04-30 18:40:15.089917: step 5526, loss = 168787.48 (13.7 examples/sec; 4.662 sec/batch)
2016-04-30 18:40:20.631221: step 5527, loss = 219378.88 (11.5 examples/sec; 5.541 sec/batch)
2016-04-30 18:40:25.596418: step 5528, loss = 268111.06 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 18:40:30.573573: step 5529, loss = 324294.25 (12.9 examples/sec; 4.977 sec/batch)
2016-04-30 18:40:35.274905: step 5530, loss = 537260.81 (13.6 examples/sec; 4.701 sec/batch)
2016-04-30 18:40:46.553274: step 5531, loss = 519647.03 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 18:40:51.964940: step 5532, loss = 379245.31 (11.8 examples/sec; 5.412 sec/batch)
2016-04-30 18:40:56.645234: step 5533, loss = 239714.16 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 18:41:01.737177: step 5534, loss = 374770.09 (12.6 examples/sec; 5.092 sec/batch)
2016-04-30 18:41:06.770544: step 5535, loss = 343938.00 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 18:41:11.779197: step 5536, loss = 488739.69 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 18:41:16.871761: step 5537, loss = 638397.19 (12.6 examples/sec; 5.092 sec/batch)
2016-04-30 18:41:22.392086: step 5538, loss = 375655.62 (11.6 examples/sec; 5.520 sec/batch)
2016-04-30 18:41:27.220422: step 5539, loss = 447877.00 (13.3 examples/sec; 4.823 sec/batch)
2016-04-30 18:41:32.044233: step 5540, loss = 454875.56 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 18:41:43.411089: step 5541, loss = 453975.50 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 18:41:48.491025: step 5542, loss = 288276.50 (12.6 examples/sec; 5.080 sec/batch)
2016-04-30 18:41:53.434443: step 5543, loss = 370194.59 (12.9 examples/sec; 4.943 sec/batch)
2016-04-30 18:41:59.395472: step 5544, loss = 532110.56 (10.7 examples/sec; 5.961 sec/batch)
2016-04-30 18:42:04.416794: step 5545, loss = 536984.62 (12.7 examples/sec; 5.021 sec/batch)
2016-04-30 18:42:09.218460: step 5546, loss = 304968.53 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 18:42:13.997548: step 5547, loss = 239287.64 (13.4 examples/sec; 4.779 sec/batch)
2016-04-30 18:42:19.050083: step 5548, loss = 630841.94 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 18:42:23.591178: step 5549, loss = 705861.75 (14.1 examples/sec; 4.541 sec/batch)
2016-04-30 18:42:28.951013: step 5550, loss = 275938.28 (11.9 examples/sec; 5.360 sec/batch)
2016-04-30 18:42:40.245420: step 5551, loss = 259229.33 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 18:42:44.900204: step 5552, loss = 479591.44 (13.7 examples/sec; 4.655 sec/batch)
2016-04-30 18:42:49.740075: step 5553, loss = 679321.44 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 18:42:54.592838: step 5554, loss = 659576.00 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 18:42:59.232844: step 5555, loss = 356116.75 (13.8 examples/sec; 4.640 sec/batch)
2016-04-30 18:43:04.695190: step 5556, loss = 691789.19 (11.7 examples/sec; 5.462 sec/batch)
2016-04-30 18:43:09.429551: step 5557, loss = 590415.50 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 18:43:14.064299: step 5558, loss = 615665.56 (13.8 examples/sec; 4.635 sec/batch)
2016-04-30 18:43:18.947116: step 5559, loss = 291281.88 (13.1 examples/sec; 4.883 sec/batch)
2016-04-30 18:43:23.563093: step 5560, loss = 576076.19 (13.9 examples/sec; 4.616 sec/batch)
2016-04-30 18:43:35.691531: step 5561, loss = 776913.06 (12.8 examples/sec; 5.013 sec/batch)
2016-04-30 18:43:40.534026: step 5562, loss = 724688.25 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 18:43:45.237752: step 5563, loss = 297217.91 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 18:43:50.061619: step 5564, loss = 797734.19 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 18:43:55.205730: step 5565, loss = 517579.28 (12.4 examples/sec; 5.144 sec/batch)
2016-04-30 18:43:59.893935: step 5566, loss = 745322.50 (13.7 examples/sec; 4.688 sec/batch)
2016-04-30 18:44:04.858468: step 5567, loss = 701228.81 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 18:44:10.279223: step 5568, loss = 671860.88 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 18:44:14.931849: step 5569, loss = 672599.06 (13.8 examples/sec; 4.653 sec/batch)
2016-04-30 18:44:19.698187: step 5570, loss = 400392.16 (13.4 examples/sec; 4.766 sec/batch)
2016-04-30 18:44:31.528912: step 5571, loss = 576024.88 (13.7 examples/sec; 4.671 sec/batch)
2016-04-30 18:44:36.286895: step 5572, loss = 860020.44 (13.5 examples/sec; 4.757 sec/batch)
2016-04-30 18:44:41.707707: step 5573, loss = 329876.88 (11.8 examples/sec; 5.421 sec/batch)
2016-04-30 18:44:46.687674: step 5574, loss = 576973.94 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 18:44:51.609167: step 5575, loss = 711973.69 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 18:44:56.290364: step 5576, loss = 411214.84 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 18:45:01.553941: step 5577, loss = 702231.88 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 18:45:06.289594: step 5578, loss = 986719.00 (13.5 examples/sec; 4.736 sec/batch)
2016-04-30 18:45:11.340983: step 5579, loss = 1205424.12 (12.7 examples/sec; 5.051 sec/batch)
2016-04-30 18:45:16.730434: step 5580, loss = 390324.19 (11.9 examples/sec; 5.389 sec/batch)
2016-04-30 18:45:28.177642: step 5581, loss = 715644.94 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 18:45:32.889940: step 5582, loss = 955514.12 (13.6 examples/sec; 4.712 sec/batch)
2016-04-30 18:45:37.721207: step 5583, loss = 828875.56 (13.2 examples/sec; 4.831 sec/batch)
2016-04-30 18:45:42.521990: step 5584, loss = 498020.09 (13.3 examples/sec; 4.801 sec/batch)
2016-04-30 18:45:47.825972: step 5585, loss = 849647.75 (12.1 examples/sec; 5.304 sec/batch)
2016-04-30 18:45:52.843898: step 5586, loss = 879609.44 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 18:45:57.800346: step 5587, loss = 822156.31 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 18:46:02.702844: step 5588, loss = 781382.31 (13.1 examples/sec; 4.902 sec/batch)
2016-04-30 18:46:07.682888: step 5589, loss = 929591.88 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 18:46:12.803141: step 5590, loss = 668787.69 (12.5 examples/sec; 5.120 sec/batch)
2016-04-30 18:46:24.359743: step 5591, loss = 1201667.38 (13.9 examples/sec; 4.610 sec/batch)
2016-04-30 18:46:29.193453: step 5592, loss = 647303.19 (13.2 examples/sec; 4.834 sec/batch)
2016-04-30 18:46:34.330307: step 5593, loss = 704432.00 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 18:46:38.968217: step 5594, loss = 1009375.50 (13.8 examples/sec; 4.638 sec/batch)
2016-04-30 18:46:43.755573: step 5595, loss = 964901.50 (13.4 examples/sec; 4.787 sec/batch)
2016-04-30 18:46:48.656336: step 5596, loss = 1191587.00 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 18:46:53.963420: step 5597, loss = 1156126.50 (12.1 examples/sec; 5.307 sec/batch)
2016-04-30 18:46:58.893223: step 5598, loss = 533536.38 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 18:47:03.825297: step 5599, loss = 589673.12 (13.0 examples/sec; 4.932 sec/batch)
2016-04-30 18:47:08.778552: step 5600, loss = 559532.19 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 18:47:19.994702: step 5601, loss = 1316739.25 (14.0 examples/sec; 4.560 sec/batch)
2016-04-30 18:47:25.449779: step 5602, loss = 921218.00 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 18:47:30.220567: step 5603, loss = 1495160.50 (13.4 examples/sec; 4.771 sec/batch)
2016-04-30 18:47:35.032085: step 5604, loss = 574344.62 (13.3 examples/sec; 4.811 sec/batch)
2016-04-30 18:47:40.280912: step 5605, loss = 1081624.75 (12.2 examples/sec; 5.249 sec/batch)
2016-04-30 18:47:44.988814: step 5606, loss = 1539290.25 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 18:47:49.941969: step 5607, loss = 1052770.75 (12.9 examples/sec; 4.953 sec/batch)
2016-04-30 18:47:54.660641: step 5608, loss = 610770.69 (13.6 examples/sec; 4.719 sec/batch)
2016-04-30 18:47:59.962189: step 5609, loss = 1341787.25 (12.1 examples/sec; 5.301 sec/batch)
2016-04-30 18:48:05.032855: step 5610, loss = 1417600.75 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 18:48:16.774727: step 5611, loss = 819305.75 (13.5 examples/sec; 4.745 sec/batch)
2016-04-30 18:48:21.595496: step 5612, loss = 797074.75 (13.3 examples/sec; 4.821 sec/batch)
2016-04-30 18:48:26.477126: step 5613, loss = 940378.75 (13.1 examples/sec; 4.882 sec/batch)
2016-04-30 18:48:32.112403: step 5614, loss = 1840036.50 (11.4 examples/sec; 5.635 sec/batch)
2016-04-30 18:48:37.422955: step 5615, loss = 1065989.75 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 18:48:42.168560: step 5616, loss = 483812.31 (13.5 examples/sec; 4.746 sec/batch)
2016-04-30 18:48:47.230102: step 5617, loss = 1583932.00 (12.6 examples/sec; 5.061 sec/batch)
2016-04-30 18:48:52.280335: step 5618, loss = 1047302.62 (12.7 examples/sec; 5.050 sec/batch)
2016-04-30 18:48:56.970737: step 5619, loss = 1546125.00 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 18:49:01.917086: step 5620, loss = 1057812.00 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 18:49:14.326649: step 5621, loss = 1144737.50 (11.9 examples/sec; 5.374 sec/batch)
2016-04-30 18:49:19.306466: step 5622, loss = 884227.88 (12.9 examples/sec; 4.980 sec/batch)
2016-04-30 18:49:24.084136: step 5623, loss = 926743.62 (13.9 examples/sec; 4.595 sec/batch)
2016-04-30 18:49:29.015664: step 5624, loss = 1322426.25 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 18:49:33.860542: step 5625, loss = 1455250.75 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 18:49:39.085508: step 5626, loss = 896435.00 (12.2 examples/sec; 5.225 sec/batch)
2016-04-30 18:49:43.830677: step 5627, loss = 783532.50 (13.5 examples/sec; 4.745 sec/batch)
2016-04-30 18:49:48.637070: step 5628, loss = 1028273.44 (13.3 examples/sec; 4.806 sec/batch)
2016-04-30 18:49:53.301967: step 5629, loss = 2166986.75 (13.7 examples/sec; 4.665 sec/batch)
2016-04-30 18:49:58.475168: step 5630, loss = 2608605.00 (12.4 examples/sec; 5.173 sec/batch)
2016-04-30 18:50:09.762284: step 5631, loss = 1132295.25 (13.7 examples/sec; 4.673 sec/batch)
2016-04-30 18:50:14.928100: step 5632, loss = 779950.31 (12.4 examples/sec; 5.166 sec/batch)
2016-04-30 18:50:19.774939: step 5633, loss = 1669057.25 (13.2 examples/sec; 4.847 sec/batch)
2016-04-30 18:50:24.742172: step 5634, loss = 1149330.62 (12.9 examples/sec; 4.967 sec/batch)
2016-04-30 18:50:29.416260: step 5635, loss = 2158338.75 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 18:50:34.357705: step 5636, loss = 1443224.75 (13.0 examples/sec; 4.941 sec/batch)
2016-04-30 18:50:39.050338: step 5637, loss = 1765303.50 (13.6 examples/sec; 4.693 sec/batch)
2016-04-30 18:50:44.387386: step 5638, loss = 1344847.38 (12.0 examples/sec; 5.337 sec/batch)
2016-04-30 18:50:49.429359: step 5639, loss = 1707042.25 (12.7 examples/sec; 5.042 sec/batch)
2016-04-30 18:50:54.086178: step 5640, loss = 992605.12 (13.7 examples/sec; 4.657 sec/batch)
2016-04-30 18:51:05.536234: step 5641, loss = 2181153.25 (13.8 examples/sec; 4.650 sec/batch)
2016-04-30 18:51:10.355029: step 5642, loss = 1336314.12 (13.3 examples/sec; 4.819 sec/batch)
2016-04-30 18:51:15.265431: step 5643, loss = 1722457.50 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 18:51:20.737473: step 5644, loss = 1805139.62 (11.7 examples/sec; 5.472 sec/batch)
2016-04-30 18:51:25.523858: step 5645, loss = 2636552.00 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 18:51:30.447492: step 5646, loss = 1479174.62 (13.0 examples/sec; 4.924 sec/batch)
2016-04-30 18:51:35.271537: step 5647, loss = 1155845.50 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 18:51:40.058090: step 5648, loss = 2545675.75 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 18:51:44.934009: step 5649, loss = 1992429.62 (13.1 examples/sec; 4.876 sec/batch)
2016-04-30 18:51:50.457777: step 5650, loss = 1712484.25 (11.6 examples/sec; 5.524 sec/batch)
2016-04-30 18:52:02.262542: step 5651, loss = 2024671.25 (13.4 examples/sec; 4.764 sec/batch)
2016-04-30 18:52:07.614067: step 5652, loss = 1576921.62 (12.0 examples/sec; 5.351 sec/batch)
2016-04-30 18:52:12.221636: step 5653, loss = 892691.00 (13.9 examples/sec; 4.607 sec/batch)
2016-04-30 18:52:17.243564: step 5654, loss = 2664095.25 (12.7 examples/sec; 5.022 sec/batch)
2016-04-30 18:52:22.750943: step 5655, loss = 2407497.75 (11.6 examples/sec; 5.507 sec/batch)
2016-04-30 18:52:27.843102: step 5656, loss = 2148513.75 (12.6 examples/sec; 5.092 sec/batch)
2016-04-30 18:52:32.443524: step 5657, loss = 1839409.00 (13.9 examples/sec; 4.600 sec/batch)
2016-04-30 18:52:37.495635: step 5658, loss = 3019715.50 (12.7 examples/sec; 5.052 sec/batch)
2016-04-30 18:52:42.360480: step 5659, loss = 3226815.25 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 18:52:47.273825: step 5660, loss = 1600139.00 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 18:52:59.353309: step 5661, loss = 641152.12 (12.0 examples/sec; 5.355 sec/batch)
2016-04-30 18:53:04.288764: step 5662, loss = 1233947.12 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 18:53:08.963540: step 5663, loss = 2384474.25 (13.7 examples/sec; 4.675 sec/batch)
2016-04-30 18:53:13.868456: step 5664, loss = 4357058.50 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 18:53:18.719511: step 5665, loss = 3037759.25 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 18:53:23.564742: step 5666, loss = 1521754.50 (13.2 examples/sec; 4.845 sec/batch)
2016-04-30 18:53:29.130530: step 5667, loss = 2751856.25 (11.5 examples/sec; 5.566 sec/batch)
2016-04-30 18:53:34.074220: step 5668, loss = 2325863.50 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 18:53:39.030306: step 5669, loss = 851359.75 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 18:53:43.976963: step 5670, loss = 1890408.75 (12.9 examples/sec; 4.947 sec/batch)
2016-04-30 18:53:55.305098: step 5671, loss = 2647671.00 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 18:53:59.952968: step 5672, loss = 4879370.50 (13.8 examples/sec; 4.648 sec/batch)
2016-04-30 18:54:05.498592: step 5673, loss = 2743907.25 (11.5 examples/sec; 5.546 sec/batch)
2016-04-30 18:54:10.684493: step 5674, loss = 1673054.75 (12.3 examples/sec; 5.186 sec/batch)
2016-04-30 18:54:15.435363: step 5675, loss = 2182974.75 (13.5 examples/sec; 4.751 sec/batch)
2016-04-30 18:54:20.308391: step 5676, loss = 3435389.00 (13.1 examples/sec; 4.873 sec/batch)
2016-04-30 18:54:25.241498: step 5677, loss = 1636885.38 (13.0 examples/sec; 4.933 sec/batch)
2016-04-30 18:54:29.872113: step 5678, loss = 2265817.50 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 18:54:35.440302: step 5679, loss = 3512022.00 (11.5 examples/sec; 5.568 sec/batch)
2016-04-30 18:54:40.325500: step 5680, loss = 2846528.00 (13.1 examples/sec; 4.885 sec/batch)
2016-04-30 18:54:51.395497: step 5681, loss = 2557807.75 (13.8 examples/sec; 4.644 sec/batch)
2016-04-30 18:54:56.316391: step 5682, loss = 3368595.00 (13.0 examples/sec; 4.921 sec/batch)
2016-04-30 18:55:01.595910: step 5683, loss = 3229779.25 (12.1 examples/sec; 5.279 sec/batch)
2016-04-30 18:55:06.525278: step 5684, loss = 2136329.00 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 18:55:11.764178: step 5685, loss = 3089940.25 (12.2 examples/sec; 5.239 sec/batch)
2016-04-30 18:55:16.623880: step 5686, loss = 3600195.00 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 18:55:21.453689: step 5687, loss = 3234300.00 (13.3 examples/sec; 4.830 sec/batch)
2016-04-30 18:55:26.209341: step 5688, loss = 2997971.75 (13.5 examples/sec; 4.756 sec/batch)
2016-04-30 18:55:31.080083: step 5689, loss = 4701110.50 (13.1 examples/sec; 4.871 sec/batch)
2016-04-30 18:55:35.832600: step 5690, loss = 2006568.50 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 18:55:47.457923: step 5691, loss = 4076155.25 (14.2 examples/sec; 4.509 sec/batch)
2016-04-30 18:55:53.041095: step 5692, loss = 3623910.50 (11.5 examples/sec; 5.583 sec/batch)
2016-04-30 18:55:58.059728: step 5693, loss = 2335683.25 (12.8 examples/sec; 5.019 sec/batch)
2016-04-30 18:56:02.878092: step 5694, loss = 2595809.75 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 18:56:08.009896: step 5695, loss = 3471697.25 (12.5 examples/sec; 5.132 sec/batch)
2016-04-30 18:56:13.182661: step 5696, loss = 5641645.50 (12.4 examples/sec; 5.173 sec/batch)
2016-04-30 18:56:18.092016: step 5697, loss = 2356949.50 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 18:56:22.901634: step 5698, loss = 1066059.50 (13.3 examples/sec; 4.810 sec/batch)
2016-04-30 18:56:27.869729: step 5699, loss = 2326053.00 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 18:56:32.388869: step 5700, loss = 4556582.00 (14.2 examples/sec; 4.519 sec/batch)
2016-04-30 18:56:43.664454: step 5701, loss = 5738313.00 (13.4 examples/sec; 4.776 sec/batch)
2016-04-30 18:56:48.959554: step 5702, loss = 2496246.75 (12.1 examples/sec; 5.295 sec/batch)
2016-04-30 18:56:53.776452: step 5703, loss = 3316821.00 (13.3 examples/sec; 4.817 sec/batch)
2016-04-30 18:56:58.704652: step 5704, loss = 2636549.75 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 18:57:03.590391: step 5705, loss = 3271610.00 (13.1 examples/sec; 4.886 sec/batch)
2016-04-30 18:57:08.583254: step 5706, loss = 6894302.00 (12.8 examples/sec; 4.993 sec/batch)
2016-04-30 18:57:13.487408: step 5707, loss = 5174602.50 (13.1 examples/sec; 4.904 sec/batch)
2016-04-30 18:57:18.317689: step 5708, loss = 4546293.50 (13.3 examples/sec; 4.830 sec/batch)
2016-04-30 18:57:23.812949: step 5709, loss = 2591185.00 (11.6 examples/sec; 5.495 sec/batch)
2016-04-30 18:57:28.744071: step 5710, loss = 3512002.25 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 18:57:40.245544: step 5711, loss = 6555332.50 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 18:57:44.850374: step 5712, loss = 3838201.75 (13.9 examples/sec; 4.605 sec/batch)
2016-04-30 18:57:49.815205: step 5713, loss = 1047458.75 (12.9 examples/sec; 4.965 sec/batch)
2016-04-30 18:57:55.265080: step 5714, loss = 4362025.50 (11.7 examples/sec; 5.450 sec/batch)
2016-04-30 18:58:00.180633: step 5715, loss = 3956562.25 (13.0 examples/sec; 4.915 sec/batch)
2016-04-30 18:58:05.153595: step 5716, loss = 7447652.50 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 18:58:10.129663: step 5717, loss = 4263814.00 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 18:58:14.801449: step 5718, loss = 3793458.25 (13.7 examples/sec; 4.672 sec/batch)
2016-04-30 18:58:19.630139: step 5719, loss = 5610280.00 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 18:58:24.603016: step 5720, loss = 3510396.75 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 18:58:37.013051: step 5721, loss = 3127288.75 (12.9 examples/sec; 4.968 sec/batch)
2016-04-30 18:58:41.624803: step 5722, loss = 3091638.25 (13.9 examples/sec; 4.611 sec/batch)
2016-04-30 18:58:46.674096: step 5723, loss = 5648318.00 (12.7 examples/sec; 5.049 sec/batch)
2016-04-30 18:58:51.619704: step 5724, loss = 4129753.75 (12.9 examples/sec; 4.946 sec/batch)
2016-04-30 18:58:56.593295: step 5725, loss = 4958101.50 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 18:59:02.213309: step 5726, loss = 10059749.00 (11.4 examples/sec; 5.620 sec/batch)
2016-04-30 18:59:07.344480: step 5727, loss = 3374600.25 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 18:59:12.119161: step 5728, loss = 4554674.50 (13.4 examples/sec; 4.775 sec/batch)
2016-04-30 18:59:17.205601: step 5729, loss = 5625043.50 (12.6 examples/sec; 5.086 sec/batch)
2016-04-30 18:59:22.327678: step 5730, loss = 9046545.00 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 18:59:34.081702: step 5731, loss = 4041065.00 (12.2 examples/sec; 5.256 sec/batch)
2016-04-30 18:59:38.716456: step 5732, loss = 3179895.75 (13.8 examples/sec; 4.634 sec/batch)
2016-04-30 18:59:43.705275: step 5733, loss = 7084467.50 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 18:59:48.562357: step 5734, loss = 5807299.50 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 18:59:53.307462: step 5735, loss = 7374340.00 (13.5 examples/sec; 4.745 sec/batch)
2016-04-30 18:59:58.364448: step 5736, loss = 5105053.50 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 19:00:03.241013: step 5737, loss = 6496119.50 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 19:00:08.902166: step 5738, loss = 3702505.75 (11.3 examples/sec; 5.661 sec/batch)
2016-04-30 19:00:14.024754: step 5739, loss = 5586401.50 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 19:00:18.985059: step 5740, loss = 7758659.00 (12.9 examples/sec; 4.960 sec/batch)
2016-04-30 19:00:30.139510: step 5741, loss = 8383020.50 (14.3 examples/sec; 4.477 sec/batch)
2016-04-30 19:00:35.005655: step 5742, loss = 5576973.00 (13.2 examples/sec; 4.866 sec/batch)
2016-04-30 19:00:40.291578: step 5743, loss = 3839897.25 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 19:00:44.981634: step 5744, loss = 7729730.50 (13.6 examples/sec; 4.690 sec/batch)
2016-04-30 19:00:49.860681: step 5745, loss = 8341821.50 (13.1 examples/sec; 4.879 sec/batch)
2016-04-30 19:00:54.983029: step 5746, loss = 6005226.00 (12.5 examples/sec; 5.122 sec/batch)
2016-04-30 19:00:59.806927: step 5747, loss = 4591918.50 (13.3 examples/sec; 4.824 sec/batch)
2016-04-30 19:01:04.804489: step 5748, loss = 12087446.00 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 19:01:09.528364: step 5749, loss = 7290481.00 (13.5 examples/sec; 4.724 sec/batch)
2016-04-30 19:01:14.843200: step 5750, loss = 6125561.50 (12.0 examples/sec; 5.315 sec/batch)
2016-04-30 19:01:26.188155: step 5751, loss = 7352697.00 (13.7 examples/sec; 4.675 sec/batch)
2016-04-30 19:01:31.048148: step 5752, loss = 5875536.00 (13.2 examples/sec; 4.860 sec/batch)
2016-04-30 19:01:35.620991: step 5753, loss = 10520797.00 (14.0 examples/sec; 4.573 sec/batch)
2016-04-30 19:01:40.703550: step 5754, loss = 7383192.00 (12.6 examples/sec; 5.082 sec/batch)
2016-04-30 19:01:46.179757: step 5755, loss = 2424841.00 (11.7 examples/sec; 5.476 sec/batch)
2016-04-30 19:01:50.929257: step 5756, loss = 5915482.50 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 19:01:55.952239: step 5757, loss = 7032267.50 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 19:02:01.238214: step 5758, loss = 10889687.00 (12.1 examples/sec; 5.286 sec/batch)
2016-04-30 19:02:05.945095: step 5759, loss = 10560757.00 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 19:02:11.051039: step 5760, loss = 5368778.00 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 19:02:22.787983: step 5761, loss = 8934046.00 (13.4 examples/sec; 4.778 sec/batch)
2016-04-30 19:02:27.532348: step 5762, loss = 7713216.50 (13.5 examples/sec; 4.744 sec/batch)
2016-04-30 19:02:32.352666: step 5763, loss = 10374319.00 (13.3 examples/sec; 4.820 sec/batch)
2016-04-30 19:02:37.308326: step 5764, loss = 9213046.00 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 19:02:42.110524: step 5765, loss = 7010854.50 (13.3 examples/sec; 4.802 sec/batch)
2016-04-30 19:02:47.027103: step 5766, loss = 12119502.00 (13.0 examples/sec; 4.916 sec/batch)
2016-04-30 19:02:52.563660: step 5767, loss = 8026130.50 (11.6 examples/sec; 5.536 sec/batch)
2016-04-30 19:02:57.293072: step 5768, loss = 12787913.00 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 19:03:02.298590: step 5769, loss = 8980926.00 (12.8 examples/sec; 5.005 sec/batch)
2016-04-30 19:03:07.235626: step 5770, loss = 8750732.00 (13.0 examples/sec; 4.937 sec/batch)
2016-04-30 19:03:18.529004: step 5771, loss = 6514410.00 (13.6 examples/sec; 4.695 sec/batch)
2016-04-30 19:03:23.983927: step 5772, loss = 8342401.50 (11.7 examples/sec; 5.455 sec/batch)
2016-04-30 19:03:29.002096: step 5773, loss = 13204304.00 (12.8 examples/sec; 5.018 sec/batch)
2016-04-30 19:03:33.890101: step 5774, loss = 16343015.00 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 19:03:38.597410: step 5775, loss = 13530525.00 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 19:03:43.568334: step 5776, loss = 6416445.00 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 19:03:48.446376: step 5777, loss = 10489950.00 (13.1 examples/sec; 4.878 sec/batch)
2016-04-30 19:03:53.354878: step 5778, loss = 12152754.00 (13.0 examples/sec; 4.908 sec/batch)
2016-04-30 19:03:58.790646: step 5779, loss = 7894073.50 (11.8 examples/sec; 5.436 sec/batch)
2016-04-30 19:04:03.539213: step 5780, loss = 7100590.50 (13.5 examples/sec; 4.748 sec/batch)
2016-04-30 19:04:14.749851: step 5781, loss = 13388941.00 (14.2 examples/sec; 4.493 sec/batch)
2016-04-30 19:04:19.637864: step 5782, loss = 22185178.00 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 19:04:24.266453: step 5783, loss = 10293019.00 (13.8 examples/sec; 4.629 sec/batch)
2016-04-30 19:04:29.593093: step 5784, loss = 5629698.50 (12.0 examples/sec; 5.327 sec/batch)
2016-04-30 19:04:34.431567: step 5785, loss = 6686286.00 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 19:04:39.006926: step 5786, loss = 17066688.00 (14.0 examples/sec; 4.575 sec/batch)
2016-04-30 19:04:44.067848: step 5787, loss = 12859003.00 (12.6 examples/sec; 5.061 sec/batch)
2016-04-30 19:04:48.919722: step 5788, loss = 4967270.50 (13.2 examples/sec; 4.852 sec/batch)
2016-04-30 19:04:53.555797: step 5789, loss = 10672112.00 (13.8 examples/sec; 4.636 sec/batch)
2016-04-30 19:04:58.615754: step 5790, loss = 19462916.00 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 19:05:10.987686: step 5791, loss = 13514994.00 (13.2 examples/sec; 4.848 sec/batch)
2016-04-30 19:05:15.736456: step 5792, loss = 14044381.00 (13.5 examples/sec; 4.749 sec/batch)
2016-04-30 19:05:20.780989: step 5793, loss = 14989845.00 (12.7 examples/sec; 5.044 sec/batch)
2016-04-30 19:05:25.776009: step 5794, loss = 10304838.00 (12.8 examples/sec; 4.995 sec/batch)
2016-04-30 19:05:30.565975: step 5795, loss = 5907043.50 (13.4 examples/sec; 4.790 sec/batch)
2016-04-30 19:05:36.187887: step 5796, loss = 9884491.00 (11.4 examples/sec; 5.622 sec/batch)
2016-04-30 19:05:41.056306: step 5797, loss = 19351752.00 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 19:05:45.956116: step 5798, loss = 18271888.00 (13.1 examples/sec; 4.900 sec/batch)
2016-04-30 19:05:51.034796: step 5799, loss = 5875396.50 (12.6 examples/sec; 5.079 sec/batch)
2016-04-30 19:05:55.993903: step 5800, loss = 21018836.00 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 19:06:08.307311: step 5801, loss = 4639640.00 (11.8 examples/sec; 5.411 sec/batch)
2016-04-30 19:06:13.412961: step 5802, loss = 7783503.50 (12.5 examples/sec; 5.106 sec/batch)
2016-04-30 19:06:18.301761: step 5803, loss = 22384066.00 (13.1 examples/sec; 4.889 sec/batch)
2016-04-30 19:06:23.263839: step 5804, loss = 17751248.00 (12.9 examples/sec; 4.962 sec/batch)
2016-04-30 19:06:28.148011: step 5805, loss = 23491056.00 (13.1 examples/sec; 4.884 sec/batch)
2016-04-30 19:06:32.770849: step 5806, loss = 17261368.00 (13.8 examples/sec; 4.623 sec/batch)
2016-04-30 19:06:37.760236: step 5807, loss = 14582285.00 (12.8 examples/sec; 4.989 sec/batch)
2016-04-30 19:06:43.172539: step 5808, loss = 13074673.00 (11.8 examples/sec; 5.412 sec/batch)
2016-04-30 19:06:47.743321: step 5809, loss = 5272153.50 (14.0 examples/sec; 4.571 sec/batch)
2016-04-30 19:06:52.808441: step 5810, loss = 21834736.00 (12.6 examples/sec; 5.065 sec/batch)
2016-04-30 19:07:04.166626: step 5811, loss = 23097268.00 (13.1 examples/sec; 4.895 sec/batch)
2016-04-30 19:07:08.827818: step 5812, loss = 21193920.00 (13.7 examples/sec; 4.661 sec/batch)
2016-04-30 19:07:14.353080: step 5813, loss = 12519265.00 (11.6 examples/sec; 5.525 sec/batch)
2016-04-30 19:07:19.329672: step 5814, loss = 18750936.00 (12.9 examples/sec; 4.976 sec/batch)
2016-04-30 19:07:24.010230: step 5815, loss = 17836188.00 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 19:07:29.123548: step 5816, loss = 14290209.00 (12.5 examples/sec; 5.113 sec/batch)
2016-04-30 19:07:33.878524: step 5817, loss = 19115634.00 (13.5 examples/sec; 4.755 sec/batch)
2016-04-30 19:07:38.678694: step 5818, loss = 13134597.00 (13.3 examples/sec; 4.800 sec/batch)
2016-04-30 19:07:43.622978: step 5819, loss = 11038981.00 (12.9 examples/sec; 4.944 sec/batch)
2016-04-30 19:07:49.016119: step 5820, loss = 21206704.00 (11.9 examples/sec; 5.393 sec/batch)
2016-04-30 19:08:00.214267: step 5821, loss = 28278946.00 (14.9 examples/sec; 4.296 sec/batch)
2016-04-30 19:08:05.166814: step 5822, loss = 13957732.00 (12.9 examples/sec; 4.952 sec/batch)
2016-04-30 19:08:10.153698: step 5823, loss = 10024075.00 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 19:08:14.976047: step 5824, loss = 14032942.00 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 19:08:20.277866: step 5825, loss = 15034290.00 (12.1 examples/sec; 5.302 sec/batch)
2016-04-30 19:08:25.279317: step 5826, loss = 16278678.00 (12.8 examples/sec; 5.001 sec/batch)
2016-04-30 19:08:30.064449: step 5827, loss = 21795330.00 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 19:08:34.941053: step 5828, loss = 27252762.00 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 19:08:39.462856: step 5829, loss = 28431652.00 (14.2 examples/sec; 4.522 sec/batch)
2016-04-30 19:08:44.200073: step 5830, loss = 17376912.00 (13.5 examples/sec; 4.737 sec/batch)
2016-04-30 19:08:56.126271: step 5831, loss = 21239764.00 (11.7 examples/sec; 5.454 sec/batch)
2016-04-30 19:09:01.429450: step 5832, loss = 23936996.00 (12.1 examples/sec; 5.303 sec/batch)
2016-04-30 19:09:06.049789: step 5833, loss = 13850864.00 (13.9 examples/sec; 4.620 sec/batch)
2016-04-30 19:09:10.939712: step 5834, loss = 18166348.00 (13.1 examples/sec; 4.890 sec/batch)
2016-04-30 19:09:15.753468: step 5835, loss = 14510162.00 (13.3 examples/sec; 4.814 sec/batch)
2016-04-30 19:09:20.427186: step 5836, loss = 16519967.00 (13.7 examples/sec; 4.674 sec/batch)
2016-04-30 19:09:25.784432: step 5837, loss = 22658858.00 (11.9 examples/sec; 5.357 sec/batch)
2016-04-30 19:09:30.569183: step 5838, loss = 25322166.00 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 19:09:35.255459: step 5839, loss = 18840124.00 (13.7 examples/sec; 4.686 sec/batch)
2016-04-30 19:09:40.143358: step 5840, loss = 8181948.00 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 19:09:51.232624: step 5841, loss = 10535237.00 (14.2 examples/sec; 4.512 sec/batch)
2016-04-30 19:09:56.242803: step 5842, loss = 33648212.00 (12.8 examples/sec; 5.010 sec/batch)
2016-04-30 19:10:01.950443: step 5843, loss = 36192376.00 (11.2 examples/sec; 5.708 sec/batch)
2016-04-30 19:10:06.984166: step 5844, loss = 22993000.00 (12.7 examples/sec; 5.034 sec/batch)
2016-04-30 19:10:11.840597: step 5845, loss = 13170869.00 (13.2 examples/sec; 4.856 sec/batch)
2016-04-30 19:10:16.870521: step 5846, loss = 30072812.00 (12.7 examples/sec; 5.030 sec/batch)
2016-04-30 19:10:21.555622: step 5847, loss = 37493156.00 (13.7 examples/sec; 4.685 sec/batch)
2016-04-30 19:10:26.407163: step 5848, loss = 23088476.00 (13.2 examples/sec; 4.851 sec/batch)
2016-04-30 19:10:32.057533: step 5849, loss = 8413659.00 (11.3 examples/sec; 5.650 sec/batch)
2016-04-30 19:10:36.955983: step 5850, loss = 17161604.00 (13.1 examples/sec; 4.898 sec/batch)
2016-04-30 19:10:47.966131: step 5851, loss = 30265404.00 (13.9 examples/sec; 4.592 sec/batch)
2016-04-30 19:10:52.939291: step 5852, loss = 25179594.00 (12.9 examples/sec; 4.973 sec/batch)
2016-04-30 19:10:57.751575: step 5853, loss = 32984288.00 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 19:11:02.747907: step 5854, loss = 25131968.00 (12.8 examples/sec; 4.996 sec/batch)
2016-04-30 19:11:08.271125: step 5855, loss = 19707194.00 (11.6 examples/sec; 5.523 sec/batch)
2016-04-30 19:11:13.100010: step 5856, loss = 19604506.00 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 19:11:17.745506: step 5857, loss = 28454372.00 (13.8 examples/sec; 4.645 sec/batch)
2016-04-30 19:11:22.671023: step 5858, loss = 17651028.00 (13.0 examples/sec; 4.925 sec/batch)
2016-04-30 19:11:27.233065: step 5859, loss = 31205948.00 (14.0 examples/sec; 4.562 sec/batch)
2016-04-30 19:11:32.113375: step 5860, loss = 26278752.00 (13.1 examples/sec; 4.880 sec/batch)
2016-04-30 19:11:43.783159: step 5861, loss = 37430040.00 (13.7 examples/sec; 4.687 sec/batch)
2016-04-30 19:11:48.449890: step 5862, loss = 16511297.00 (13.7 examples/sec; 4.667 sec/batch)
2016-04-30 19:11:53.351216: step 5863, loss = 28527380.00 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 19:11:58.189562: step 5864, loss = 31929992.00 (13.2 examples/sec; 4.838 sec/batch)
2016-04-30 19:12:02.944103: step 5865, loss = 40626644.00 (13.5 examples/sec; 4.754 sec/batch)
2016-04-30 19:12:07.889228: step 5866, loss = 18717916.00 (12.9 examples/sec; 4.945 sec/batch)
2016-04-30 19:12:13.422419: step 5867, loss = 33288922.00 (11.6 examples/sec; 5.533 sec/batch)
2016-04-30 19:12:18.207839: step 5868, loss = 47673760.00 (13.4 examples/sec; 4.785 sec/batch)
2016-04-30 19:12:23.135969: step 5869, loss = 21239836.00 (13.0 examples/sec; 4.928 sec/batch)
2016-04-30 19:12:28.144086: step 5870, loss = 13744415.00 (12.8 examples/sec; 5.008 sec/batch)
2016-04-30 19:12:39.416662: step 5871, loss = 39553324.00 (14.4 examples/sec; 4.451 sec/batch)
2016-04-30 19:12:44.770390: step 5872, loss = 54613092.00 (12.0 examples/sec; 5.354 sec/batch)
2016-04-30 19:12:49.742957: step 5873, loss = 27529902.00 (12.9 examples/sec; 4.972 sec/batch)
2016-04-30 19:12:54.400895: step 5874, loss = 41678496.00 (13.7 examples/sec; 4.658 sec/batch)
2016-04-30 19:12:59.130623: step 5875, loss = 33653644.00 (13.5 examples/sec; 4.730 sec/batch)
2016-04-30 19:13:04.241523: step 5876, loss = 26736814.00 (12.5 examples/sec; 5.111 sec/batch)
2016-04-30 19:13:08.829483: step 5877, loss = 28252530.00 (13.9 examples/sec; 4.588 sec/batch)
2016-04-30 19:13:13.867918: step 5878, loss = 45193588.00 (12.7 examples/sec; 5.038 sec/batch)
2016-04-30 19:13:19.390907: step 5879, loss = 37892720.00 (11.6 examples/sec; 5.523 sec/batch)
2016-04-30 19:13:23.951868: step 5880, loss = 43818592.00 (14.0 examples/sec; 4.561 sec/batch)
2016-04-30 19:13:35.012465: step 5881, loss = 15623134.00 (13.7 examples/sec; 4.672 sec/batch)
2016-04-30 19:13:39.663468: step 5882, loss = 45575216.00 (13.8 examples/sec; 4.651 sec/batch)
2016-04-30 19:13:44.397888: step 5883, loss = 63643748.00 (13.5 examples/sec; 4.734 sec/batch)
2016-04-30 19:13:49.420854: step 5884, loss = 35019088.00 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 19:13:54.330538: step 5885, loss = 23655176.00 (13.0 examples/sec; 4.910 sec/batch)
2016-04-30 19:13:59.203639: step 5886, loss = 41346056.00 (13.1 examples/sec; 4.873 sec/batch)
2016-04-30 19:14:04.215649: step 5887, loss = 39296764.00 (12.8 examples/sec; 5.012 sec/batch)
2016-04-30 19:14:08.879115: step 5888, loss = 36390892.00 (13.7 examples/sec; 4.663 sec/batch)
2016-04-30 19:14:13.850495: step 5889, loss = 45313504.00 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 19:14:18.613008: step 5890, loss = 52749848.00 (13.4 examples/sec; 4.762 sec/batch)
2016-04-30 19:14:30.488629: step 5891, loss = 42235840.00 (14.6 examples/sec; 4.381 sec/batch)
2016-04-30 19:14:35.406058: step 5892, loss = 27878704.00 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 19:14:40.336465: step 5893, loss = 38209044.00 (13.0 examples/sec; 4.930 sec/batch)
2016-04-30 19:14:45.237277: step 5894, loss = 42026408.00 (13.1 examples/sec; 4.901 sec/batch)
2016-04-30 19:14:50.303150: step 5895, loss = 48023224.00 (12.6 examples/sec; 5.066 sec/batch)
2016-04-30 19:14:55.346535: step 5896, loss = 49211692.00 (12.7 examples/sec; 5.043 sec/batch)
2016-04-30 19:15:00.572055: step 5897, loss = 59640904.00 (12.2 examples/sec; 5.225 sec/batch)
2016-04-30 19:15:05.535792: step 5898, loss = 14011885.00 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 19:15:10.826873: step 5899, loss = 34910724.00 (12.1 examples/sec; 5.291 sec/batch)
2016-04-30 19:15:15.782945: step 5900, loss = 43805312.00 (12.9 examples/sec; 4.956 sec/batch)
2016-04-30 19:15:27.007185: step 5901, loss = 69786592.00 (13.8 examples/sec; 4.635 sec/batch)
2016-04-30 19:15:32.464813: step 5902, loss = 51585776.00 (11.7 examples/sec; 5.458 sec/batch)
2016-04-30 19:15:37.561244: step 5903, loss = 42407640.00 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 19:15:42.352471: step 5904, loss = 61050600.00 (13.4 examples/sec; 4.791 sec/batch)
2016-04-30 19:15:47.287438: step 5905, loss = 45045544.00 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 19:15:52.236619: step 5906, loss = 61578056.00 (12.9 examples/sec; 4.949 sec/batch)
2016-04-30 19:15:56.885931: step 5907, loss = 48918476.00 (13.8 examples/sec; 4.649 sec/batch)
2016-04-30 19:16:02.600327: step 5908, loss = 51955660.00 (11.2 examples/sec; 5.714 sec/batch)
2016-04-30 19:16:07.559602: step 5909, loss = 52239096.00 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 19:16:12.331206: step 5910, loss = 41264224.00 (13.4 examples/sec; 4.771 sec/batch)
2016-04-30 19:16:23.691262: step 5911, loss = 65374776.00 (13.9 examples/sec; 4.592 sec/batch)
2016-04-30 19:16:28.566788: step 5912, loss = 38524096.00 (13.1 examples/sec; 4.875 sec/batch)
2016-04-30 19:16:33.212880: step 5913, loss = 113744816.00 (13.8 examples/sec; 4.646 sec/batch)
2016-04-30 19:16:38.606643: step 5914, loss = 16529632.00 (11.9 examples/sec; 5.394 sec/batch)
2016-04-30 19:16:43.613386: step 5915, loss = 51197288.00 (12.8 examples/sec; 5.007 sec/batch)
2016-04-30 19:16:48.535604: step 5916, loss = 51366192.00 (13.0 examples/sec; 4.922 sec/batch)
2016-04-30 19:16:53.640912: step 5917, loss = 77224832.00 (12.5 examples/sec; 5.105 sec/batch)
2016-04-30 19:16:58.765436: step 5918, loss = 61676752.00 (12.5 examples/sec; 5.124 sec/batch)
2016-04-30 19:17:03.822904: step 5919, loss = 67005656.00 (12.7 examples/sec; 5.057 sec/batch)
2016-04-30 19:17:09.076746: step 5920, loss = 68938848.00 (12.2 examples/sec; 5.254 sec/batch)
2016-04-30 19:17:20.391737: step 5921, loss = 61078584.00 (13.6 examples/sec; 4.692 sec/batch)
2016-04-30 19:17:25.643418: step 5922, loss = 73418760.00 (12.2 examples/sec; 5.252 sec/batch)
2016-04-30 19:17:30.402702: step 5923, loss = 73748432.00 (13.4 examples/sec; 4.759 sec/batch)
2016-04-30 19:17:35.341853: step 5924, loss = 46889324.00 (13.0 examples/sec; 4.939 sec/batch)
2016-04-30 19:17:40.825909: step 5925, loss = 59691992.00 (11.7 examples/sec; 5.484 sec/batch)
2016-04-30 19:17:45.785408: step 5926, loss = 44532424.00 (12.9 examples/sec; 4.959 sec/batch)
2016-04-30 19:17:50.590963: step 5927, loss = 55361660.00 (13.3 examples/sec; 4.805 sec/batch)
2016-04-30 19:17:55.681942: step 5928, loss = 75761912.00 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 19:18:00.642912: step 5929, loss = 85650032.00 (12.9 examples/sec; 4.961 sec/batch)
2016-04-30 19:18:05.822673: step 5930, loss = 44316216.00 (12.4 examples/sec; 5.180 sec/batch)
2016-04-30 19:18:18.095057: step 5931, loss = 97088320.00 (12.3 examples/sec; 5.192 sec/batch)
2016-04-30 19:18:22.924413: step 5932, loss = 66884544.00 (13.3 examples/sec; 4.829 sec/batch)
2016-04-30 19:18:27.536659: step 5933, loss = 56826552.00 (13.9 examples/sec; 4.612 sec/batch)
2016-04-30 19:18:32.615843: step 5934, loss = 72827168.00 (12.6 examples/sec; 5.079 sec/batch)
2016-04-30 19:18:37.617556: step 5935, loss = 107388112.00 (12.8 examples/sec; 5.002 sec/batch)
2016-04-30 19:18:42.398696: step 5936, loss = 139763712.00 (13.4 examples/sec; 4.781 sec/batch)
2016-04-30 19:18:48.014295: step 5937, loss = 56448616.00 (11.4 examples/sec; 5.616 sec/batch)
2016-04-30 19:18:52.836135: step 5938, loss = 72908008.00 (13.3 examples/sec; 4.822 sec/batch)
2016-04-30 19:18:57.731913: step 5939, loss = 70074328.00 (13.1 examples/sec; 4.896 sec/batch)
2016-04-30 19:19:02.853219: step 5940, loss = 93320656.00 (12.5 examples/sec; 5.121 sec/batch)
2016-04-30 19:19:14.392016: step 5941, loss = 50063604.00 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 19:19:19.523583: step 5942, loss = 44721536.00 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 19:19:24.414724: step 5943, loss = 107424352.00 (13.1 examples/sec; 4.891 sec/batch)
2016-04-30 19:19:29.365116: step 5944, loss = 65803200.00 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 19:19:34.533144: step 5945, loss = 70739968.00 (12.4 examples/sec; 5.168 sec/batch)
2016-04-30 19:19:39.319347: step 5946, loss = 115632784.00 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 19:19:44.311538: step 5947, loss = 128597896.00 (12.8 examples/sec; 4.992 sec/batch)
2016-04-30 19:19:49.179714: step 5948, loss = 27223848.00 (13.1 examples/sec; 4.868 sec/batch)
2016-04-30 19:19:54.386630: step 5949, loss = 71655472.00 (12.3 examples/sec; 5.207 sec/batch)
2016-04-30 19:19:59.377086: step 5950, loss = 125386144.00 (12.8 examples/sec; 4.990 sec/batch)
2016-04-30 19:20:11.262316: step 5951, loss = 130153280.00 (12.7 examples/sec; 5.037 sec/batch)
2016-04-30 19:20:16.282806: step 5952, loss = 77120136.00 (12.7 examples/sec; 5.020 sec/batch)
2016-04-30 19:20:20.987352: step 5953, loss = 140444208.00 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 19:20:26.528800: step 5954, loss = 119124672.00 (11.5 examples/sec; 5.541 sec/batch)
2016-04-30 19:20:31.702488: step 5955, loss = 33561120.00 (12.4 examples/sec; 5.174 sec/batch)
2016-04-30 19:20:36.447912: step 5956, loss = 82701360.00 (13.5 examples/sec; 4.745 sec/batch)
2016-04-30 19:20:41.525729: step 5957, loss = 88046576.00 (12.6 examples/sec; 5.078 sec/batch)
2016-04-30 19:20:46.534764: step 5958, loss = 140867392.00 (12.8 examples/sec; 5.009 sec/batch)
2016-04-30 19:20:51.391998: step 5959, loss = 70537384.00 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 19:20:56.342039: step 5960, loss = 134819168.00 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 19:21:08.573127: step 5961, loss = 120568480.00 (13.9 examples/sec; 4.609 sec/batch)
2016-04-30 19:21:13.675114: step 5962, loss = 36645496.00 (12.5 examples/sec; 5.102 sec/batch)
2016-04-30 19:21:18.356547: step 5963, loss = 101398240.00 (13.7 examples/sec; 4.681 sec/batch)
2016-04-30 19:21:23.383666: step 5964, loss = 131879864.00 (12.7 examples/sec; 5.027 sec/batch)
2016-04-30 19:21:28.295688: step 5965, loss = 156227360.00 (13.0 examples/sec; 4.912 sec/batch)
2016-04-30 19:21:33.517835: step 5966, loss = 101535608.00 (12.3 examples/sec; 5.222 sec/batch)
2016-04-30 19:21:38.488916: step 5967, loss = 125986816.00 (12.9 examples/sec; 4.971 sec/batch)
2016-04-30 19:21:43.672456: step 5968, loss = 114633776.00 (12.3 examples/sec; 5.183 sec/batch)
2016-04-30 19:21:48.450186: step 5969, loss = 127117160.00 (13.4 examples/sec; 4.778 sec/batch)
2016-04-30 19:21:53.331460: step 5970, loss = 148649920.00 (13.1 examples/sec; 4.881 sec/batch)
2016-04-30 19:22:05.352087: step 5971, loss = 141809648.00 (11.6 examples/sec; 5.536 sec/batch)
2016-04-30 19:22:10.483333: step 5972, loss = 89814816.00 (12.5 examples/sec; 5.131 sec/batch)
2016-04-30 19:22:15.139869: step 5973, loss = 26658152.00 (13.7 examples/sec; 4.656 sec/batch)
2016-04-30 19:22:20.053180: step 5974, loss = 111412848.00 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 19:22:24.695692: step 5975, loss = 192377360.00 (13.8 examples/sec; 4.642 sec/batch)
2016-04-30 19:22:29.520615: step 5976, loss = 141293344.00 (13.3 examples/sec; 4.825 sec/batch)
2016-04-30 19:22:34.273520: step 5977, loss = 109406720.00 (13.5 examples/sec; 4.753 sec/batch)
2016-04-30 19:22:39.434130: step 5978, loss = 155292656.00 (12.4 examples/sec; 5.161 sec/batch)
2016-04-30 19:22:44.467353: step 5979, loss = 183683552.00 (12.7 examples/sec; 5.033 sec/batch)
2016-04-30 19:22:49.540204: step 5980, loss = 81925808.00 (12.6 examples/sec; 5.073 sec/batch)
2016-04-30 19:23:01.333854: step 5981, loss = 161211616.00 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 19:23:06.040497: step 5982, loss = 169735648.00 (13.6 examples/sec; 4.707 sec/batch)
2016-04-30 19:23:11.303792: step 5983, loss = 98541440.00 (12.2 examples/sec; 5.263 sec/batch)
2016-04-30 19:23:16.344253: step 5984, loss = 139424192.00 (12.7 examples/sec; 5.040 sec/batch)
2016-04-30 19:23:21.021927: step 5985, loss = 186502320.00 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 19:23:26.163785: step 5986, loss = 144771584.00 (12.4 examples/sec; 5.142 sec/batch)
2016-04-30 19:23:31.232465: step 5987, loss = 136338208.00 (12.6 examples/sec; 5.069 sec/batch)
2016-04-30 19:23:36.006042: step 5988, loss = 139204912.00 (13.4 examples/sec; 4.774 sec/batch)
2016-04-30 19:23:41.051345: step 5989, loss = 184492688.00 (12.7 examples/sec; 5.045 sec/batch)
2016-04-30 19:23:46.645329: step 5990, loss = 194427552.00 (11.4 examples/sec; 5.594 sec/batch)
2016-04-30 19:23:58.645300: step 5991, loss = 73960416.00 (12.7 examples/sec; 5.023 sec/batch)
2016-04-30 19:24:03.705861: step 5992, loss = 71668256.00 (12.6 examples/sec; 5.060 sec/batch)
2016-04-30 19:24:09.119042: step 5993, loss = 188011040.00 (11.8 examples/sec; 5.413 sec/batch)
2016-04-30 19:24:15.050838: step 5994, loss = 325086464.00 (10.8 examples/sec; 5.932 sec/batch)
2016-04-30 19:24:20.913265: step 5995, loss = 94889424.00 (10.9 examples/sec; 5.862 sec/batch)
2016-04-30 19:24:26.155579: step 5996, loss = 51761928.00 (12.2 examples/sec; 5.242 sec/batch)
2016-04-30 19:24:30.973517: step 5997, loss = 141122752.00 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 19:24:35.878933: step 5998, loss = 243330752.00 (13.0 examples/sec; 4.905 sec/batch)
2016-04-30 19:24:41.200145: step 5999, loss = 277482208.00 (12.0 examples/sec; 5.321 sec/batch)
2016-04-30 19:24:46.057266: step 6000, loss = 101443656.00 (13.2 examples/sec; 4.857 sec/batch)
2016-04-30 19:24:57.566035: step 6001, loss = 222291952.00 (14.5 examples/sec; 4.415 sec/batch)
2016-04-30 19:25:02.698808: step 6002, loss = 176401344.00 (12.5 examples/sec; 5.133 sec/batch)
2016-04-30 19:25:07.736813: step 6003, loss = 228218160.00 (12.7 examples/sec; 5.038 sec/batch)
2016-04-30 19:25:12.528082: step 6004, loss = 191843296.00 (13.4 examples/sec; 4.791 sec/batch)
2016-04-30 19:25:17.612618: step 6005, loss = 196326560.00 (12.6 examples/sec; 5.084 sec/batch)
2016-04-30 19:25:22.637784: step 6006, loss = 240096192.00 (12.7 examples/sec; 5.025 sec/batch)
2016-04-30 19:25:27.569357: step 6007, loss = 225042864.00 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 19:25:32.640873: step 6008, loss = 150231648.00 (12.6 examples/sec; 5.071 sec/batch)
2016-04-30 19:25:37.499062: step 6009, loss = 203829664.00 (13.2 examples/sec; 4.858 sec/batch)
2016-04-30 19:25:42.250878: step 6010, loss = 229128976.00 (13.5 examples/sec; 4.752 sec/batch)
2016-04-30 19:25:53.409855: step 6011, loss = 282071328.00 (13.6 examples/sec; 4.704 sec/batch)
2016-04-30 19:25:58.949928: step 6012, loss = 205806960.00 (11.6 examples/sec; 5.540 sec/batch)
2016-04-30 19:26:03.815500: step 6013, loss = 190996960.00 (13.2 examples/sec; 4.865 sec/batch)
2016-04-30 19:26:08.938484: step 6014, loss = 222342336.00 (12.5 examples/sec; 5.123 sec/batch)
2016-04-30 19:26:13.979269: step 6015, loss = 136268736.00 (12.7 examples/sec; 5.041 sec/batch)
2016-04-30 19:26:18.910714: step 6016, loss = 248982064.00 (13.0 examples/sec; 4.931 sec/batch)
2016-04-30 19:26:23.821357: step 6017, loss = 235585600.00 (13.0 examples/sec; 4.911 sec/batch)
2016-04-30 19:26:29.306605: step 6018, loss = 308991584.00 (11.7 examples/sec; 5.485 sec/batch)
2016-04-30 19:26:33.962811: step 6019, loss = 123581824.00 (13.7 examples/sec; 4.656 sec/batch)
2016-04-30 19:26:38.758207: step 6020, loss = 160858816.00 (13.3 examples/sec; 4.795 sec/batch)
2016-04-30 19:26:50.149003: step 6021, loss = 363564864.00 (12.8 examples/sec; 4.988 sec/batch)
2016-04-30 19:26:54.963144: step 6022, loss = 336222240.00 (13.3 examples/sec; 4.814 sec/batch)
2016-04-30 19:26:59.692119: step 6023, loss = 75113056.00 (13.5 examples/sec; 4.729 sec/batch)
2016-04-30 19:27:05.542655: step 6024, loss = 135559264.00 (10.9 examples/sec; 5.850 sec/batch)
2016-04-30 19:27:10.539295: step 6025, loss = 223944672.00 (12.8 examples/sec; 4.997 sec/batch)
2016-04-30 19:27:15.165739: step 6026, loss = 437971584.00 (13.8 examples/sec; 4.626 sec/batch)
2016-04-30 19:27:20.233743: step 6027, loss = 147731680.00 (12.6 examples/sec; 5.068 sec/batch)
2016-04-30 19:27:25.020165: step 6028, loss = 154216704.00 (13.4 examples/sec; 4.786 sec/batch)
2016-04-30 19:27:29.846179: step 6029, loss = 195442528.00 (13.3 examples/sec; 4.826 sec/batch)
2016-04-30 19:27:35.595620: step 6030, loss = 318256192.00 (11.1 examples/sec; 5.749 sec/batch)
2016-04-30 19:27:46.718357: step 6031, loss = 300468608.00 (13.8 examples/sec; 4.638 sec/batch)
2016-04-30 19:27:51.530650: step 6032, loss = 315913344.00 (13.3 examples/sec; 4.812 sec/batch)
2016-04-30 19:27:56.418348: step 6033, loss = 299499200.00 (13.1 examples/sec; 4.888 sec/batch)
2016-04-30 19:28:01.577217: step 6034, loss = 275889632.00 (12.4 examples/sec; 5.159 sec/batch)
2016-04-30 19:28:06.285242: step 6035, loss = 296311680.00 (13.6 examples/sec; 4.708 sec/batch)
2016-04-30 19:28:12.077400: step 6036, loss = 299620928.00 (11.0 examples/sec; 5.792 sec/batch)
2016-04-30 19:28:17.151879: step 6037, loss = 249154784.00 (12.6 examples/sec; 5.074 sec/batch)
2016-04-30 19:28:21.946353: step 6038, loss = 214536768.00 (13.3 examples/sec; 4.794 sec/batch)
2016-04-30 19:28:27.064471: step 6039, loss = 334567360.00 (12.5 examples/sec; 5.118 sec/batch)
2016-04-30 19:28:32.048298: step 6040, loss = 302180832.00 (12.8 examples/sec; 4.984 sec/batch)
2016-04-30 19:28:43.861245: step 6041, loss = 236168000.00 (12.2 examples/sec; 5.257 sec/batch)
2016-04-30 19:28:48.557645: step 6042, loss = 284720800.00 (13.6 examples/sec; 4.696 sec/batch)
2016-04-30 19:28:53.507046: step 6043, loss = 263395824.00 (12.9 examples/sec; 4.949 sec/batch)
2016-04-30 19:28:58.325405: step 6044, loss = 305401888.00 (13.3 examples/sec; 4.818 sec/batch)
2016-04-30 19:29:03.428437: step 6045, loss = 443430784.00 (12.5 examples/sec; 5.103 sec/batch)
2016-04-30 19:29:08.378130: step 6046, loss = 512164896.00 (12.9 examples/sec; 4.950 sec/batch)
2016-04-30 19:29:13.352493: step 6047, loss = 110497424.00 (12.9 examples/sec; 4.974 sec/batch)
2016-04-30 19:29:18.669655: step 6048, loss = 234440432.00 (12.0 examples/sec; 5.317 sec/batch)
2016-04-30 19:29:23.599047: step 6049, loss = 435824064.00 (13.0 examples/sec; 4.929 sec/batch)
2016-04-30 19:29:28.512173: step 6050, loss = 600703680.00 (13.0 examples/sec; 4.913 sec/batch)
2016-04-30 19:29:39.786915: step 6051, loss = 218321600.00 (14.2 examples/sec; 4.514 sec/batch)
2016-04-30 19:29:44.664415: step 6052, loss = 311763968.00 (13.1 examples/sec; 4.877 sec/batch)
2016-04-30 19:29:50.140537: step 6053, loss = 545009728.00 (11.7 examples/sec; 5.476 sec/batch)
2016-04-30 19:29:54.902824: step 6054, loss = 342708512.00 (13.4 examples/sec; 4.762 sec/batch)
2016-04-30 19:29:59.569053: step 6055, loss = 312783744.00 (13.7 examples/sec; 4.666 sec/batch)
2016-04-30 19:30:04.562705: step 6056, loss = 244454752.00 (12.8 examples/sec; 4.994 sec/batch)
2016-04-30 19:30:09.285013: step 6057, loss = 440539232.00 (13.6 examples/sec; 4.722 sec/batch)
2016-04-30 19:30:14.219781: step 6058, loss = 666957440.00 (13.0 examples/sec; 4.935 sec/batch)
2016-04-30 19:30:19.060078: step 6059, loss = 356511520.00 (13.2 examples/sec; 4.840 sec/batch)
2016-04-30 19:30:24.370252: step 6060, loss = 384468736.00 (12.1 examples/sec; 5.310 sec/batch)
2016-04-30 19:30:35.582515: step 6061, loss = 391190720.00 (13.7 examples/sec; 4.680 sec/batch)
2016-04-30 19:30:40.672441: step 6062, loss = 460573952.00 (12.6 examples/sec; 5.090 sec/batch)
2016-04-30 19:30:45.260203: step 6063, loss = 592917312.00 (14.0 examples/sec; 4.588 sec/batch)
2016-04-30 19:30:50.348170: step 6064, loss = 230923936.00 (12.6 examples/sec; 5.088 sec/batch)
2016-04-30 19:30:56.036062: step 6065, loss = 543314752.00 (11.3 examples/sec; 5.688 sec/batch)
2016-04-30 19:31:00.884975: step 6066, loss = 168440304.00 (13.2 examples/sec; 4.849 sec/batch)
2016-04-30 19:31:06.021835: step 6067, loss = 269668320.00 (12.5 examples/sec; 5.137 sec/batch)
2016-04-30 19:31:11.118135: step 6068, loss = 662289664.00 (12.6 examples/sec; 5.096 sec/batch)
2016-04-30 19:31:16.209438: step 6069, loss = 803003392.00 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 19:31:21.126547: step 6070, loss = 255880800.00 (13.0 examples/sec; 4.917 sec/batch)
2016-04-30 19:31:33.216833: step 6071, loss = 226116048.00 (13.8 examples/sec; 4.628 sec/batch)
2016-04-30 19:31:38.290112: step 6072, loss = 514320384.00 (12.6 examples/sec; 5.073 sec/batch)
2016-04-30 19:31:43.132479: step 6073, loss = 614136064.00 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 19:31:47.974898: step 6074, loss = 660762496.00 (13.2 examples/sec; 4.842 sec/batch)
2016-04-30 19:31:52.923262: step 6075, loss = 440403968.00 (12.9 examples/sec; 4.948 sec/batch)
2016-04-30 19:31:57.765935: step 6076, loss = 722630656.00 (13.2 examples/sec; 4.843 sec/batch)
2016-04-30 19:32:03.461172: step 6077, loss = 360481984.00 (11.2 examples/sec; 5.695 sec/batch)
2016-04-30 19:32:08.425306: step 6078, loss = 653536000.00 (12.9 examples/sec; 4.964 sec/batch)
2016-04-30 19:32:13.317448: step 6079, loss = 581698304.00 (13.1 examples/sec; 4.892 sec/batch)
2016-04-30 19:32:18.349833: step 6080, loss = 605303616.00 (12.7 examples/sec; 5.032 sec/batch)
2016-04-30 19:32:29.684460: step 6081, loss = 328646592.00 (13.6 examples/sec; 4.722 sec/batch)
2016-04-30 19:32:35.060236: step 6082, loss = 385871904.00 (11.9 examples/sec; 5.376 sec/batch)
2016-04-30 19:32:39.738491: step 6083, loss = 706956992.00 (13.7 examples/sec; 4.678 sec/batch)
2016-04-30 19:32:44.829698: step 6084, loss = 794530176.00 (12.6 examples/sec; 5.091 sec/batch)
2016-04-30 19:32:49.816760: step 6085, loss = 481225152.00 (12.8 examples/sec; 4.987 sec/batch)
2016-04-30 19:32:54.624432: step 6086, loss = 281841152.00 (13.3 examples/sec; 4.808 sec/batch)
2016-04-30 19:32:59.477124: step 6087, loss = 637662720.00 (13.2 examples/sec; 4.853 sec/batch)
2016-04-30 19:33:04.385958: step 6088, loss = 540509184.00 (13.0 examples/sec; 4.909 sec/batch)
2016-04-30 19:33:09.672997: step 6089, loss = 703178048.00 (12.1 examples/sec; 5.287 sec/batch)
2016-04-30 19:33:15.391111: step 6090, loss = 364407744.00 (11.2 examples/sec; 5.718 sec/batch)
2016-04-30 19:33:29.453260: step 6091, loss = 898473856.00 (10.2 examples/sec; 6.249 sec/batch)
2016-04-30 19:33:35.898056: step 6092, loss = 707895296.00 (9.9 examples/sec; 6.445 sec/batch)
2016-04-30 19:33:43.058527: step 6093, loss = 621213184.00 (8.9 examples/sec; 7.160 sec/batch)
2016-04-30 19:33:49.682656: step 6094, loss = 467651616.00 (9.7 examples/sec; 6.624 sec/batch)
2016-04-30 19:33:56.101659: step 6095, loss = 45141587670568288649216.00 (10.0 examples/sec; 6.419 sec/batch)
Traceback (most recent call last):
  File "/home/neo/projects/dl/cifar10_train.py", line 142, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/home/neo/projects/dl/cifar10_train.py", line 138, in main
    train()
  File "/home/neo/projects/dl/cifar10_train.py", line 109, in train
    assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
AssertionError: Model diverged with loss = NaN

Process finished with exit code 1
