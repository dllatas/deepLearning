/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-28 22:31:17.920586: step 0, loss = 63.82 (0.3 examples/sec; 243.185 sec/batch)
2016-04-28 22:31:33.767187: step 1, loss = 63.85 (10.2 examples/sec; 6.283 sec/batch)
2016-04-28 22:31:40.036431: step 2, loss = 63.81 (10.2 examples/sec; 6.269 sec/batch)
2016-04-28 22:31:46.456910: step 3, loss = 63.59 (10.0 examples/sec; 6.420 sec/batch)
2016-04-28 22:31:53.193472: step 4, loss = 63.36 (9.5 examples/sec; 6.736 sec/batch)
2016-04-28 22:32:00.054656: step 5, loss = 63.40 (9.3 examples/sec; 6.861 sec/batch)
2016-04-28 22:32:06.275154: step 6, loss = 63.43 (10.3 examples/sec; 6.220 sec/batch)
2016-04-28 22:32:12.579992: step 7, loss = 63.82 (10.2 examples/sec; 6.305 sec/batch)
2016-04-28 22:32:18.840136: step 8, loss = 63.56 (10.2 examples/sec; 6.260 sec/batch)
2016-04-28 22:32:26.481538: step 9, loss = 63.30 (8.4 examples/sec; 7.641 sec/batch)
2016-04-28 22:32:37.090001: step 10, loss = 63.27 (6.0 examples/sec; 10.608 sec/batch)
2016-04-28 22:32:58.650337: step 11, loss = 63.38 (7.9 examples/sec; 8.143 sec/batch)
2016-04-28 22:33:06.280496: step 12, loss = 63.12 (8.4 examples/sec; 7.630 sec/batch)
2016-04-28 22:33:13.290906: step 13, loss = 63.14 (9.1 examples/sec; 7.010 sec/batch)
2016-04-28 22:33:20.307947: step 14, loss = 63.04 (9.1 examples/sec; 7.017 sec/batch)
2016-04-28 22:33:27.122160: step 15, loss = 63.03 (9.4 examples/sec; 6.814 sec/batch)
2016-04-28 22:33:34.227946: step 16, loss = 63.13 (9.0 examples/sec; 7.106 sec/batch)
2016-04-28 22:33:42.285560: step 17, loss = 62.96 (7.9 examples/sec; 8.058 sec/batch)
2016-04-28 22:33:49.547408: step 18, loss = 63.02 (8.8 examples/sec; 7.262 sec/batch)
2016-04-28 22:33:56.428786: step 19, loss = 62.84 (9.3 examples/sec; 6.881 sec/batch)
2016-04-28 22:34:03.822708: step 20, loss = 62.84 (8.7 examples/sec; 7.394 sec/batch)
2016-04-28 22:34:21.322799: step 21, loss = 62.74 (9.0 examples/sec; 7.105 sec/batch)
2016-04-28 22:34:28.824081: step 22, loss = 62.65 (8.5 examples/sec; 7.501 sec/batch)
2016-04-28 22:34:36.462454: step 23, loss = 62.69 (8.4 examples/sec; 7.638 sec/batch)
2016-04-28 22:34:44.341314: step 24, loss = 62.60 (8.1 examples/sec; 7.879 sec/batch)
2016-04-28 22:34:52.172496: step 25, loss = 62.54 (8.2 examples/sec; 7.831 sec/batch)
2016-04-28 22:34:59.827944: step 26, loss = 62.43 (8.4 examples/sec; 7.655 sec/batch)
2016-04-28 22:35:07.355437: step 27, loss = 62.42 (8.5 examples/sec; 7.527 sec/batch)
2016-04-28 22:35:14.951816: step 28, loss = 62.27 (8.4 examples/sec; 7.596 sec/batch)
2016-04-28 22:35:23.431528: step 29, loss = 62.28 (7.5 examples/sec; 8.480 sec/batch)
2016-04-28 22:35:30.645881: step 30, loss = 62.06 (8.9 examples/sec; 7.214 sec/batch)
2016-04-28 22:35:48.398499: step 31, loss = 62.25 (8.3 examples/sec; 7.743 sec/batch)
2016-04-28 22:35:56.550622: step 32, loss = 62.09 (7.9 examples/sec; 8.152 sec/batch)
2016-04-28 22:36:03.944863: step 33, loss = 62.16 (8.7 examples/sec; 7.394 sec/batch)
2016-04-28 22:36:11.659330: step 34, loss = 62.11 (8.3 examples/sec; 7.714 sec/batch)
2016-04-28 22:36:19.146812: step 35, loss = 62.05 (8.5 examples/sec; 7.487 sec/batch)
2016-04-28 22:36:27.251688: step 36, loss = 62.04 (7.9 examples/sec; 8.105 sec/batch)
2016-04-28 22:36:34.632456: step 37, loss = 61.96 (8.7 examples/sec; 7.381 sec/batch)
2016-04-28 22:36:42.355164: step 38, loss = 61.87 (8.3 examples/sec; 7.723 sec/batch)
2016-04-28 22:36:50.010032: step 39, loss = 61.96 (8.4 examples/sec; 7.655 sec/batch)
2016-04-28 22:36:57.522177: step 40, loss = 61.76 (8.5 examples/sec; 7.512 sec/batch)
2016-04-28 22:37:15.427861: step 41, loss = 61.69 (9.1 examples/sec; 7.020 sec/batch)
2016-04-28 22:37:23.180685: step 42, loss = 61.57 (8.3 examples/sec; 7.753 sec/batch)
2016-04-28 22:37:30.414270: step 43, loss = 61.48 (8.8 examples/sec; 7.233 sec/batch)
2016-04-28 22:37:38.723959: step 44, loss = 61.55 (7.7 examples/sec; 8.310 sec/batch)
2016-04-28 22:37:46.067576: step 45, loss = 61.38 (8.7 examples/sec; 7.343 sec/batch)
2016-04-28 22:37:53.872226: step 46, loss = 61.55 (8.2 examples/sec; 7.805 sec/batch)
2016-04-28 22:38:01.514499: step 47, loss = 61.52 (8.4 examples/sec; 7.642 sec/batch)
2016-04-28 22:38:09.484133: step 48, loss = 61.35 (8.0 examples/sec; 7.970 sec/batch)
2016-04-28 22:38:17.038630: step 49, loss = 61.48 (8.5 examples/sec; 7.554 sec/batch)
2016-04-28 22:38:24.248223: step 50, loss = 61.30 (8.9 examples/sec; 7.210 sec/batch)
2016-04-28 22:38:42.469921: step 51, loss = 61.21 (7.8 examples/sec; 8.199 sec/batch)
2016-04-28 22:38:50.083408: step 52, loss = 61.19 (8.4 examples/sec; 7.613 sec/batch)
2016-04-28 22:38:57.280460: step 53, loss = 61.00 (8.9 examples/sec; 7.197 sec/batch)
2016-04-28 22:39:04.948261: step 54, loss = 61.17 (8.3 examples/sec; 7.668 sec/batch)
2016-04-28 22:39:13.140314: step 55, loss = 60.95 (7.8 examples/sec; 8.192 sec/batch)
2016-04-28 22:39:20.713528: step 56, loss = 60.95 (8.5 examples/sec; 7.573 sec/batch)
2016-04-28 22:39:28.173412: step 57, loss = 60.81 (8.6 examples/sec; 7.460 sec/batch)
2016-04-28 22:39:36.021472: step 58, loss = 60.97 (8.2 examples/sec; 7.848 sec/batch)
2016-04-28 22:39:43.418700: step 59, loss = 60.65 (8.7 examples/sec; 7.397 sec/batch)
2016-04-28 22:39:51.713853: step 60, loss = 60.93 (7.7 examples/sec; 8.295 sec/batch)
2016-04-28 22:40:09.187501: step 61, loss = 60.81 (8.7 examples/sec; 7.346 sec/batch)
2016-04-28 22:40:16.940046: step 62, loss = 60.51 (8.3 examples/sec; 7.752 sec/batch)
2016-04-28 22:40:24.835536: step 63, loss = 60.55 (8.1 examples/sec; 7.895 sec/batch)
2016-04-28 22:40:31.784911: step 64, loss = 60.55 (9.2 examples/sec; 6.949 sec/batch)
2016-04-28 22:40:38.573704: step 65, loss = 60.60 (9.4 examples/sec; 6.789 sec/batch)
2016-04-28 22:40:45.400338: step 66, loss = 60.47 (9.4 examples/sec; 6.827 sec/batch)
2016-04-28 22:40:52.338521: step 67, loss = 60.39 (9.2 examples/sec; 6.938 sec/batch)
2016-04-28 22:41:00.146175: step 68, loss = 60.37 (8.2 examples/sec; 7.808 sec/batch)
2016-04-28 22:41:07.375702: step 69, loss = 60.34 (8.9 examples/sec; 7.229 sec/batch)
2016-04-28 22:41:14.762040: step 70, loss = 60.33 (8.7 examples/sec; 7.386 sec/batch)
2016-04-28 22:41:31.687257: step 71, loss = 60.27 (8.6 examples/sec; 7.472 sec/batch)
2016-04-28 22:41:38.821697: step 72, loss = 60.18 (9.0 examples/sec; 7.134 sec/batch)
2016-04-28 22:41:46.095786: step 73, loss = 60.06 (8.8 examples/sec; 7.274 sec/batch)
2016-04-28 22:41:53.528069: step 74, loss = 60.11 (8.6 examples/sec; 7.432 sec/batch)
2016-04-28 22:42:00.467770: step 75, loss = 59.97 (9.2 examples/sec; 6.940 sec/batch)
2016-04-28 22:42:08.503212: step 76, loss = 60.01 (8.0 examples/sec; 8.035 sec/batch)
2016-04-28 22:42:15.477195: step 77, loss = 60.02 (9.2 examples/sec; 6.974 sec/batch)
2016-04-28 22:42:22.698585: step 78, loss = 59.85 (8.9 examples/sec; 7.221 sec/batch)
2016-04-28 22:42:29.977117: step 79, loss = 59.92 (8.8 examples/sec; 7.278 sec/batch)
2016-04-28 22:42:38.034017: step 80, loss = 59.77 (7.9 examples/sec; 8.057 sec/batch)
2016-04-28 22:42:54.691324: step 81, loss = 59.76 (9.4 examples/sec; 6.826 sec/batch)
2016-04-28 22:43:02.163440: step 82, loss = 59.67 (8.6 examples/sec; 7.472 sec/batch)
2016-04-28 22:43:10.369351: step 83, loss = 59.70 (7.8 examples/sec; 8.206 sec/batch)
2016-04-28 22:43:17.823802: step 84, loss = 59.70 (8.6 examples/sec; 7.454 sec/batch)
2016-04-28 22:43:24.892489: step 85, loss = 59.60 (9.1 examples/sec; 7.069 sec/batch)
2016-04-28 22:43:32.082680: step 86, loss = 59.72 (8.9 examples/sec; 7.190 sec/batch)
2016-04-28 22:43:39.504041: step 87, loss = 59.50 (8.6 examples/sec; 7.421 sec/batch)
2016-04-28 22:43:47.571187: step 88, loss = 59.51 (7.9 examples/sec; 8.067 sec/batch)
2016-04-28 22:43:54.874810: step 89, loss = 59.38 (8.8 examples/sec; 7.304 sec/batch)
2016-04-28 22:44:02.634426: step 90, loss = 59.15 (8.2 examples/sec; 7.760 sec/batch)
2016-04-28 22:44:20.609809: step 91, loss = 59.31 (7.9 examples/sec; 8.133 sec/batch)
2016-04-28 22:44:27.966337: step 92, loss = 59.45 (8.7 examples/sec; 7.356 sec/batch)
2016-04-28 22:44:34.995731: step 93, loss = 59.26 (9.1 examples/sec; 7.029 sec/batch)
2016-04-28 22:44:41.979863: step 94, loss = 59.09 (9.2 examples/sec; 6.984 sec/batch)
2016-04-28 22:44:49.788913: step 95, loss = 59.04 (8.2 examples/sec; 7.809 sec/batch)
2016-04-28 22:44:56.946647: step 96, loss = 58.94 (8.9 examples/sec; 7.158 sec/batch)
2016-04-28 22:45:04.134594: step 97, loss = 59.01 (8.9 examples/sec; 7.188 sec/batch)
2016-04-28 22:45:11.488861: step 98, loss = 58.96 (8.7 examples/sec; 7.354 sec/batch)
2016-04-28 22:45:18.433668: step 99, loss = 58.92 (9.2 examples/sec; 6.945 sec/batch)
2016-04-28 22:45:26.536659: step 100, loss = 58.79 (7.9 examples/sec; 8.103 sec/batch)
2016-04-28 22:45:43.000690: step 101, loss = 59.00 (9.2 examples/sec; 6.950 sec/batch)
2016-04-28 22:45:50.274129: step 102, loss = 58.79 (8.8 examples/sec; 7.273 sec/batch)
2016-04-28 22:45:58.018889: step 103, loss = 58.72 (8.3 examples/sec; 7.745 sec/batch)
2016-04-28 22:46:05.312880: step 104, loss = 58.75 (8.8 examples/sec; 7.294 sec/batch)
2016-04-28 22:46:12.526432: step 105, loss = 58.76 (8.9 examples/sec; 7.213 sec/batch)
2016-04-28 22:46:20.207729: step 106, loss = 58.56 (8.3 examples/sec; 7.681 sec/batch)
2016-04-28 22:46:28.810044: step 107, loss = 58.69 (7.4 examples/sec; 8.602 sec/batch)
2016-04-28 22:46:36.099238: step 108, loss = 58.73 (8.8 examples/sec; 7.289 sec/batch)
2016-04-28 22:46:43.355089: step 109, loss = 58.48 (8.8 examples/sec; 7.256 sec/batch)
2016-04-28 22:46:50.629664: step 110, loss = 58.37 (8.8 examples/sec; 7.274 sec/batch)
2016-04-28 22:47:08.211846: step 111, loss = 58.33 (8.9 examples/sec; 7.216 sec/batch)
2016-04-28 22:47:15.237327: step 112, loss = 58.23 (9.1 examples/sec; 7.025 sec/batch)
2016-04-28 22:47:22.422411: step 113, loss = 58.17 (8.9 examples/sec; 7.185 sec/batch)
2016-04-28 22:47:30.024749: step 114, loss = 58.35 (8.4 examples/sec; 7.602 sec/batch)
2016-04-28 22:47:38.270325: step 115, loss = 58.12 (7.8 examples/sec; 8.245 sec/batch)
2016-04-28 22:47:45.414218: step 116, loss = 58.17 (9.0 examples/sec; 7.144 sec/batch)
2016-04-28 22:47:52.494977: step 117, loss = 58.08 (9.0 examples/sec; 7.081 sec/batch)
2016-04-28 22:47:59.905173: step 118, loss = 58.18 (8.6 examples/sec; 7.410 sec/batch)
2016-04-28 22:48:07.628670: step 119, loss = 58.00 (8.3 examples/sec; 7.723 sec/batch)
2016-04-28 22:48:15.017921: step 120, loss = 58.10 (8.7 examples/sec; 7.389 sec/batch)
2016-04-28 22:48:31.899459: step 121, loss = 57.83 (9.2 examples/sec; 6.978 sec/batch)
2016-04-28 22:48:39.628375: step 122, loss = 57.96 (8.3 examples/sec; 7.729 sec/batch)
2016-04-28 22:48:46.994436: step 123, loss = 57.77 (8.7 examples/sec; 7.366 sec/batch)
2016-04-28 22:48:54.096829: step 124, loss = 57.81 (9.0 examples/sec; 7.102 sec/batch)
2016-04-28 22:49:01.446780: step 125, loss = 57.70 (8.7 examples/sec; 7.350 sec/batch)
2016-04-28 22:49:08.790379: step 126, loss = 57.68 (8.7 examples/sec; 7.344 sec/batch)
2016-04-28 22:49:16.499056: step 127, loss = 57.56 (8.3 examples/sec; 7.709 sec/batch)
2016-04-28 22:49:23.742978: step 128, loss = 57.70 (8.8 examples/sec; 7.244 sec/batch)
2016-04-28 22:49:30.681252: step 129, loss = 57.46 (9.2 examples/sec; 6.938 sec/batch)
2016-04-28 22:49:37.655145: step 130, loss = 57.58 (9.2 examples/sec; 6.974 sec/batch)
2016-04-28 22:49:54.800707: step 131, loss = 57.50 (9.4 examples/sec; 6.793 sec/batch)
2016-04-28 22:50:01.983906: step 132, loss = 57.31 (8.9 examples/sec; 7.183 sec/batch)
2016-04-28 22:50:09.059560: step 133, loss = 57.45 (9.0 examples/sec; 7.076 sec/batch)
2016-04-28 22:50:15.823132: step 134, loss = 57.20 (9.5 examples/sec; 6.763 sec/batch)
2016-04-28 22:50:24.152208: step 135, loss = 57.29 (7.7 examples/sec; 8.329 sec/batch)
2016-04-28 22:50:31.236158: step 136, loss = 57.39 (9.0 examples/sec; 7.084 sec/batch)
2016-04-28 22:50:38.469878: step 137, loss = 57.10 (8.8 examples/sec; 7.234 sec/batch)
2016-04-28 22:50:45.497343: step 138, loss = 57.18 (9.1 examples/sec; 7.027 sec/batch)
2016-04-28 22:50:53.360302: step 139, loss = 57.07 (8.1 examples/sec; 7.863 sec/batch)
2016-04-28 22:51:00.758157: step 140, loss = 57.04 (8.7 examples/sec; 7.398 sec/batch)
2016-04-28 22:51:17.692687: step 141, loss = 57.05 (8.8 examples/sec; 7.272 sec/batch)
2016-04-28 22:51:25.835484: step 142, loss = 57.08 (7.9 examples/sec; 8.143 sec/batch)
2016-04-28 22:51:33.150239: step 143, loss = 56.94 (8.7 examples/sec; 7.315 sec/batch)
2016-04-28 22:51:40.166297: step 144, loss = 56.87 (9.1 examples/sec; 7.016 sec/batch)
2016-04-28 22:51:47.370052: step 145, loss = 56.96 (8.9 examples/sec; 7.204 sec/batch)
2016-04-28 22:51:54.626380: step 146, loss = 56.92 (8.8 examples/sec; 7.256 sec/batch)
2016-04-28 22:52:02.669117: step 147, loss = 56.78 (8.0 examples/sec; 8.043 sec/batch)
2016-04-28 22:52:09.926937: step 148, loss = 56.85 (8.8 examples/sec; 7.258 sec/batch)
2016-04-28 22:52:17.065355: step 149, loss = 56.59 (9.0 examples/sec; 7.138 sec/batch)
2016-04-28 22:52:24.100546: step 150, loss = 56.61 (9.1 examples/sec; 7.035 sec/batch)
2016-04-28 22:52:40.775210: step 151, loss = 56.52 (9.8 examples/sec; 6.559 sec/batch)
2016-04-28 22:52:48.195080: step 152, loss = 56.44 (8.6 examples/sec; 7.420 sec/batch)
2016-04-28 22:52:54.981488: step 153, loss = 56.52 (9.4 examples/sec; 6.786 sec/batch)
2016-04-28 22:53:02.155489: step 154, loss = 56.43 (8.9 examples/sec; 7.174 sec/batch)
2016-04-28 22:53:10.043135: step 155, loss = 56.35 (8.1 examples/sec; 7.888 sec/batch)
2016-04-28 22:53:17.352495: step 156, loss = 56.46 (8.8 examples/sec; 7.309 sec/batch)
2016-04-28 22:53:24.450234: step 157, loss = 56.47 (9.0 examples/sec; 7.098 sec/batch)
2016-04-28 22:53:31.727939: step 158, loss = 56.09 (8.8 examples/sec; 7.278 sec/batch)
2016-04-28 22:53:39.587696: step 159, loss = 56.10 (8.1 examples/sec; 7.860 sec/batch)
2016-04-28 22:53:46.610919: step 160, loss = 56.22 (9.1 examples/sec; 7.023 sec/batch)
2016-04-28 22:54:03.566483: step 161, loss = 56.03 (9.0 examples/sec; 7.098 sec/batch)
2016-04-28 22:54:11.504386: step 162, loss = 55.96 (8.1 examples/sec; 7.938 sec/batch)
2016-04-28 22:54:18.487848: step 163, loss = 56.15 (9.2 examples/sec; 6.983 sec/batch)
2016-04-28 22:54:25.853677: step 164, loss = 55.87 (8.7 examples/sec; 7.366 sec/batch)
2016-04-28 22:54:32.918262: step 165, loss = 56.00 (9.1 examples/sec; 7.065 sec/batch)
2016-04-28 22:54:39.911454: step 166, loss = 55.81 (9.2 examples/sec; 6.993 sec/batch)
2016-04-28 22:54:47.767624: step 167, loss = 56.01 (8.1 examples/sec; 7.856 sec/batch)
2016-04-28 22:54:54.710529: step 168, loss = 55.79 (9.2 examples/sec; 6.943 sec/batch)
2016-04-28 22:55:01.931642: step 169, loss = 55.93 (8.9 examples/sec; 7.221 sec/batch)
2016-04-28 22:55:09.072864: step 170, loss = 55.75 (9.0 examples/sec; 7.141 sec/batch)
2016-04-28 22:55:26.185545: step 171, loss = 55.83 (9.5 examples/sec; 6.709 sec/batch)
2016-04-28 22:55:33.404614: step 172, loss = 55.78 (8.9 examples/sec; 7.219 sec/batch)
2016-04-28 22:55:40.307785: step 173, loss = 55.54 (9.3 examples/sec; 6.903 sec/batch)
2016-04-28 22:55:47.630954: step 174, loss = 55.64 (8.7 examples/sec; 7.323 sec/batch)
2016-04-28 22:55:55.255058: step 175, loss = 55.52 (8.4 examples/sec; 7.624 sec/batch)
2016-04-28 22:56:02.982382: step 176, loss = 55.59 (8.3 examples/sec; 7.727 sec/batch)
2016-04-28 22:56:09.924059: step 177, loss = 55.22 (9.2 examples/sec; 6.942 sec/batch)
2016-04-28 22:56:17.033927: step 178, loss = 55.39 (9.0 examples/sec; 7.110 sec/batch)
2016-04-28 22:56:24.895952: step 179, loss = 55.27 (8.1 examples/sec; 7.862 sec/batch)
2016-04-28 22:56:32.219578: step 180, loss = 55.18 (8.7 examples/sec; 7.324 sec/batch)
2016-04-28 22:56:48.419240: step 181, loss = 55.28 (9.4 examples/sec; 6.785 sec/batch)
2016-04-28 22:56:56.620944: step 182, loss = 55.11 (7.8 examples/sec; 8.202 sec/batch)
2016-04-28 22:57:03.725203: step 183, loss = 55.04 (9.0 examples/sec; 7.104 sec/batch)
2016-04-28 22:57:10.839345: step 184, loss = 55.06 (9.0 examples/sec; 7.114 sec/batch)
2016-04-28 22:57:18.505325: step 185, loss = 55.00 (8.3 examples/sec; 7.666 sec/batch)
2016-04-28 22:57:25.415385: step 186, loss = 54.88 (9.3 examples/sec; 6.910 sec/batch)
2016-04-28 22:57:33.555792: step 187, loss = 54.99 (7.9 examples/sec; 8.140 sec/batch)
2016-04-28 22:57:40.427579: step 188, loss = 55.12 (9.3 examples/sec; 6.872 sec/batch)
2016-04-28 22:57:48.128306: step 189, loss = 54.74 (8.3 examples/sec; 7.701 sec/batch)
2016-04-28 22:57:55.169823: step 190, loss = 54.94 (9.1 examples/sec; 7.041 sec/batch)
2016-04-28 22:58:12.658223: step 191, loss = 54.68 (9.4 examples/sec; 6.805 sec/batch)
2016-04-28 22:58:19.959607: step 192, loss = 54.74 (8.8 examples/sec; 7.301 sec/batch)
2016-04-28 22:58:27.211477: step 193, loss = 54.77 (8.8 examples/sec; 7.252 sec/batch)
2016-04-28 22:58:35.299413: step 194, loss = 54.65 (7.9 examples/sec; 8.088 sec/batch)
2016-04-28 22:58:42.309325: step 195, loss = 54.74 (9.1 examples/sec; 7.010 sec/batch)
2016-04-28 22:58:49.419918: step 196, loss = 54.46 (9.0 examples/sec; 7.111 sec/batch)
2016-04-28 22:58:56.818702: step 197, loss = 54.53 (8.7 examples/sec; 7.399 sec/batch)
2016-04-28 22:59:04.024243: step 198, loss = 54.62 (8.9 examples/sec; 7.205 sec/batch)
2016-04-28 22:59:11.663155: step 199, loss = 54.51 (8.4 examples/sec; 7.639 sec/batch)
2016-04-28 22:59:18.476774: step 200, loss = 54.44 (9.4 examples/sec; 6.813 sec/batch)
2016-04-28 22:59:34.759475: step 201, loss = 54.50 (9.6 examples/sec; 6.673 sec/batch)
2016-04-28 22:59:42.853090: step 202, loss = 54.41 (7.9 examples/sec; 8.094 sec/batch)
2016-04-28 22:59:49.922071: step 203, loss = 54.25 (9.1 examples/sec; 7.069 sec/batch)
2016-04-28 22:59:57.099645: step 204, loss = 54.32 (8.9 examples/sec; 7.177 sec/batch)
2016-04-28 23:00:03.875719: step 205, loss = 54.31 (9.4 examples/sec; 6.776 sec/batch)
2016-04-28 23:00:10.586275: step 206, loss = 54.23 (9.5 examples/sec; 6.710 sec/batch)
2016-04-28 23:00:18.218668: step 207, loss = 54.15 (8.4 examples/sec; 7.631 sec/batch)
2016-04-28 23:00:25.298289: step 208, loss = 54.14 (9.0 examples/sec; 7.079 sec/batch)
2016-04-28 23:00:32.261696: step 209, loss = 54.04 (9.2 examples/sec; 6.963 sec/batch)
2016-04-28 23:00:39.550722: step 210, loss = 53.97 (8.8 examples/sec; 7.289 sec/batch)
2016-04-28 23:00:57.130911: step 211, loss = 54.14 (8.9 examples/sec; 7.199 sec/batch)
2016-04-28 23:01:04.300997: step 212, loss = 53.87 (8.9 examples/sec; 7.170 sec/batch)
2016-04-28 23:01:11.075552: step 213, loss = 53.87 (9.4 examples/sec; 6.774 sec/batch)
2016-04-28 23:01:18.906203: step 214, loss = 54.04 (8.2 examples/sec; 7.831 sec/batch)
2016-04-28 23:01:26.274659: step 215, loss = 53.74 (8.7 examples/sec; 7.368 sec/batch)
2016-04-28 23:01:33.485564: step 216, loss = 53.76 (8.9 examples/sec; 7.211 sec/batch)
2016-04-28 23:01:40.478486: step 217, loss = 53.76 (9.2 examples/sec; 6.993 sec/batch)
2016-04-28 23:01:47.807243: step 218, loss = 53.77 (8.7 examples/sec; 7.329 sec/batch)
2016-04-28 23:01:55.689264: step 219, loss = 53.62 (8.1 examples/sec; 7.882 sec/batch)
2016-04-28 23:02:03.379211: step 220, loss = 53.59 (8.3 examples/sec; 7.690 sec/batch)
2016-04-28 23:02:19.670540: step 221, loss = 53.58 (9.6 examples/sec; 6.699 sec/batch)
2016-04-28 23:02:26.991066: step 222, loss = 53.50 (8.7 examples/sec; 7.320 sec/batch)
2016-04-28 23:02:34.399048: step 223, loss = 53.44 (8.6 examples/sec; 7.408 sec/batch)
2016-04-28 23:02:41.330845: step 224, loss = 53.63 (9.2 examples/sec; 6.932 sec/batch)
2016-04-28 23:02:48.554795: step 225, loss = 53.31 (8.9 examples/sec; 7.224 sec/batch)
2016-04-28 23:02:55.348213: step 226, loss = 53.29 (9.4 examples/sec; 6.793 sec/batch)
2016-04-28 23:03:03.460991: step 227, loss = 53.30 (7.9 examples/sec; 8.113 sec/batch)
2016-04-28 23:03:10.474953: step 228, loss = 53.02 (9.1 examples/sec; 7.014 sec/batch)
2016-04-28 23:03:17.737616: step 229, loss = 53.14 (8.8 examples/sec; 7.263 sec/batch)
2016-04-28 23:03:24.953415: step 230, loss = 53.32 (8.9 examples/sec; 7.216 sec/batch)
2016-04-28 23:03:42.454902: step 231, loss = 53.09 (8.9 examples/sec; 7.183 sec/batch)
2016-04-28 23:03:49.469223: step 232, loss = 53.11 (9.1 examples/sec; 7.014 sec/batch)
2016-04-28 23:03:56.733172: step 233, loss = 53.04 (8.8 examples/sec; 7.264 sec/batch)
2016-04-28 23:04:03.910054: step 234, loss = 53.06 (8.9 examples/sec; 7.177 sec/batch)
2016-04-28 23:04:11.598967: step 235, loss = 52.99 (8.3 examples/sec; 7.689 sec/batch)
2016-04-28 23:04:18.657772: step 236, loss = 52.95 (9.1 examples/sec; 7.059 sec/batch)
2016-04-28 23:04:25.708234: step 237, loss = 53.08 (9.1 examples/sec; 7.050 sec/batch)
2016-04-28 23:04:33.139681: step 238, loss = 52.78 (8.6 examples/sec; 7.431 sec/batch)
2016-04-28 23:04:40.973776: step 239, loss = 52.96 (8.2 examples/sec; 7.834 sec/batch)
2016-04-28 23:04:48.513719: step 240, loss = 52.70 (8.5 examples/sec; 7.540 sec/batch)
2016-04-28 23:05:05.422137: step 241, loss = 52.62 (9.1 examples/sec; 7.015 sec/batch)
2016-04-28 23:05:13.076936: step 242, loss = 52.53 (8.4 examples/sec; 7.655 sec/batch)
2016-04-28 23:05:20.521784: step 243, loss = 52.67 (8.6 examples/sec; 7.445 sec/batch)
2016-04-28 23:05:27.890173: step 244, loss = 52.69 (8.7 examples/sec; 7.368 sec/batch)
2016-04-28 23:05:34.863560: step 245, loss = 52.58 (9.2 examples/sec; 6.973 sec/batch)
2016-04-28 23:05:42.346098: step 246, loss = 52.66 (8.6 examples/sec; 7.482 sec/batch)
2016-04-28 23:05:50.090916: step 247, loss = 52.47 (8.3 examples/sec; 7.745 sec/batch)
2016-04-28 23:05:57.136721: step 248, loss = 52.53 (9.1 examples/sec; 7.046 sec/batch)
2016-04-28 23:06:04.360083: step 249, loss = 52.42 (8.9 examples/sec; 7.223 sec/batch)
2016-04-28 23:06:11.369830: step 250, loss = 52.31 (9.1 examples/sec; 7.010 sec/batch)
2016-04-28 23:06:28.609945: step 251, loss = 52.31 (8.9 examples/sec; 7.175 sec/batch)
2016-04-28 23:06:36.057778: step 252, loss = 52.36 (8.6 examples/sec; 7.448 sec/batch)
2016-04-28 23:06:43.137794: step 253, loss = 52.19 (9.0 examples/sec; 7.080 sec/batch)
2016-04-28 23:06:50.070783: step 254, loss = 52.11 (9.2 examples/sec; 6.933 sec/batch)
2016-04-28 23:06:58.150788: step 255, loss = 52.28 (7.9 examples/sec; 8.080 sec/batch)
2016-04-28 23:07:05.224959: step 256, loss = 52.14 (9.0 examples/sec; 7.074 sec/batch)
2016-04-28 23:07:12.589060: step 257, loss = 51.98 (8.7 examples/sec; 7.364 sec/batch)
2016-04-28 23:07:19.729781: step 258, loss = 52.00 (9.0 examples/sec; 7.141 sec/batch)
2016-04-28 23:07:27.851051: step 259, loss = 52.07 (7.9 examples/sec; 8.121 sec/batch)
2016-04-28 23:07:34.936451: step 260, loss = 52.11 (9.0 examples/sec; 7.085 sec/batch)
2016-04-28 23:07:51.542504: step 261, loss = 51.95 (8.7 examples/sec; 7.329 sec/batch)
2016-04-28 23:07:59.234978: step 262, loss = 51.83 (8.3 examples/sec; 7.692 sec/batch)
2016-04-28 23:08:06.501103: step 263, loss = 51.70 (8.8 examples/sec; 7.266 sec/batch)
2016-04-28 23:08:13.964459: step 264, loss = 51.73 (8.6 examples/sec; 7.463 sec/batch)
2016-04-28 23:08:21.377424: step 265, loss = 51.63 (8.6 examples/sec; 7.413 sec/batch)
2016-04-28 23:08:28.632087: step 266, loss = 51.65 (8.8 examples/sec; 7.255 sec/batch)
2016-04-28 23:08:36.735941: step 267, loss = 51.74 (7.9 examples/sec; 8.104 sec/batch)
2016-04-28 23:08:43.702395: step 268, loss = 51.43 (9.2 examples/sec; 6.966 sec/batch)
2016-04-28 23:08:50.875964: step 269, loss = 51.43 (8.9 examples/sec; 7.173 sec/batch)
2016-04-28 23:08:57.942871: step 270, loss = 51.52 (9.1 examples/sec; 7.067 sec/batch)
2016-04-28 23:09:15.678241: step 271, loss = 51.58 (8.8 examples/sec; 7.238 sec/batch)
2016-04-28 23:09:22.644347: step 272, loss = 51.50 (9.2 examples/sec; 6.966 sec/batch)
2016-04-28 23:09:29.966720: step 273, loss = 51.36 (8.7 examples/sec; 7.322 sec/batch)
2016-04-28 23:09:37.832861: step 274, loss = 51.26 (8.1 examples/sec; 7.866 sec/batch)
2016-04-28 23:09:44.941501: step 275, loss = 51.29 (9.0 examples/sec; 7.109 sec/batch)
2016-04-28 23:09:52.223896: step 276, loss = 51.27 (8.8 examples/sec; 7.282 sec/batch)
2016-04-28 23:09:59.761844: step 277, loss = 51.26 (8.5 examples/sec; 7.538 sec/batch)
2016-04-28 23:10:07.011037: step 278, loss = 51.17 (8.8 examples/sec; 7.249 sec/batch)
2016-04-28 23:10:14.866043: step 279, loss = 51.32 (8.1 examples/sec; 7.855 sec/batch)
2016-04-28 23:10:22.040498: step 280, loss = 51.13 (8.9 examples/sec; 7.174 sec/batch)
2016-04-28 23:10:37.593285: step 281, loss = 51.13 (10.8 examples/sec; 5.950 sec/batch)
2016-04-28 23:10:44.137176: step 282, loss = 50.96 (9.8 examples/sec; 6.544 sec/batch)
2016-04-28 23:10:49.849498: step 283, loss = 51.09 (11.2 examples/sec; 5.712 sec/batch)
2016-04-28 23:10:55.463760: step 284, loss = 50.89 (11.4 examples/sec; 5.614 sec/batch)
2016-04-28 23:11:01.370618: step 285, loss = 51.01 (10.8 examples/sec; 5.907 sec/batch)
2016-04-28 23:11:07.035959: step 286, loss = 50.98 (11.3 examples/sec; 5.665 sec/batch)
2016-04-28 23:11:12.715721: step 287, loss = 51.04 (11.3 examples/sec; 5.680 sec/batch)
2016-04-28 23:11:19.134127: step 288, loss = 50.81 (10.0 examples/sec; 6.418 sec/batch)
2016-04-28 23:11:24.763564: step 289, loss = 50.83 (11.4 examples/sec; 5.629 sec/batch)
2016-04-28 23:11:30.521676: step 290, loss = 50.74 (11.1 examples/sec; 5.758 sec/batch)
2016-04-28 23:11:46.988523: step 291, loss = 50.56 (9.3 examples/sec; 6.913 sec/batch)
2016-04-28 23:11:55.026334: step 292, loss = 50.68 (8.0 examples/sec; 8.038 sec/batch)
2016-04-28 23:12:02.074184: step 293, loss = 50.60 (9.1 examples/sec; 7.048 sec/batch)
2016-04-28 23:12:09.355770: step 294, loss = 50.63 (8.8 examples/sec; 7.281 sec/batch)
2016-04-28 23:12:16.429729: step 295, loss = 50.55 (9.0 examples/sec; 7.074 sec/batch)
2016-04-28 23:12:24.324062: step 296, loss = 50.45 (8.1 examples/sec; 7.894 sec/batch)
2016-04-28 23:12:31.139121: step 297, loss = 50.52 (9.4 examples/sec; 6.815 sec/batch)
2016-04-28 23:12:38.122139: step 298, loss = 50.56 (9.2 examples/sec; 6.983 sec/batch)
2016-04-28 23:12:44.986291: step 299, loss = 50.21 (9.3 examples/sec; 6.864 sec/batch)
2016-04-28 23:12:52.101788: step 300, loss = 50.36 (9.0 examples/sec; 7.115 sec/batch)
2016-04-28 23:13:09.602207: step 301, loss = 50.14 (8.9 examples/sec; 7.193 sec/batch)
2016-04-28 23:13:16.798267: step 302, loss = 50.37 (8.9 examples/sec; 7.196 sec/batch)
2016-04-28 23:13:23.821938: step 303, loss = 50.14 (9.1 examples/sec; 7.024 sec/batch)
2016-04-28 23:13:31.772822: step 304, loss = 50.16 (8.0 examples/sec; 7.951 sec/batch)
2016-04-28 23:13:38.557972: step 305, loss = 50.03 (9.4 examples/sec; 6.785 sec/batch)
2016-04-28 23:13:45.904348: step 306, loss = 50.04 (8.7 examples/sec; 7.346 sec/batch)
2016-04-28 23:13:52.965669: step 307, loss = 50.08 (9.1 examples/sec; 7.061 sec/batch)
2016-04-28 23:13:59.765743: step 308, loss = 50.07 (9.4 examples/sec; 6.800 sec/batch)
2016-04-28 23:14:07.515778: step 309, loss = 49.96 (8.3 examples/sec; 7.750 sec/batch)
2016-04-28 23:14:14.688559: step 310, loss = 50.01 (8.9 examples/sec; 7.173 sec/batch)
2016-04-28 23:14:31.675630: step 311, loss = 49.97 (9.3 examples/sec; 6.900 sec/batch)
2016-04-28 23:14:40.017412: step 312, loss = 49.88 (7.7 examples/sec; 8.342 sec/batch)
2016-04-28 23:14:47.174702: step 313, loss = 49.99 (8.9 examples/sec; 7.157 sec/batch)
2016-04-28 23:14:54.211501: step 314, loss = 49.78 (9.1 examples/sec; 7.037 sec/batch)
2016-04-28 23:15:01.423388: step 315, loss = 49.83 (8.9 examples/sec; 7.212 sec/batch)
2016-04-28 23:15:09.524186: step 316, loss = 49.52 (7.9 examples/sec; 8.101 sec/batch)
2016-04-28 23:15:16.593768: step 317, loss = 49.69 (9.1 examples/sec; 7.069 sec/batch)
2016-04-28 23:15:23.494721: step 318, loss = 49.51 (9.3 examples/sec; 6.901 sec/batch)
2016-04-28 23:15:30.702587: step 319, loss = 49.59 (8.9 examples/sec; 7.208 sec/batch)
2016-04-28 23:15:37.657986: step 320, loss = 49.54 (9.2 examples/sec; 6.955 sec/batch)
2016-04-28 23:15:53.765622: step 321, loss = 49.59 (10.3 examples/sec; 6.202 sec/batch)
2016-04-28 23:16:00.297453: step 322, loss = 49.44 (9.8 examples/sec; 6.532 sec/batch)
2016-04-28 23:16:07.274807: step 323, loss = 49.54 (9.2 examples/sec; 6.977 sec/batch)
2016-04-28 23:16:15.089876: step 324, loss = 49.30 (8.2 examples/sec; 7.815 sec/batch)
2016-04-28 23:16:23.204424: step 325, loss = 49.28 (7.9 examples/sec; 8.114 sec/batch)
2016-04-28 23:16:29.542436: step 326, loss = 49.34 (10.1 examples/sec; 6.338 sec/batch)
2016-04-28 23:16:36.158453: step 327, loss = 49.30 (9.7 examples/sec; 6.616 sec/batch)
2016-04-28 23:16:41.772753: step 328, loss = 49.22 (11.4 examples/sec; 5.614 sec/batch)
2016-04-28 23:16:49.291751: step 329, loss = 49.09 (8.5 examples/sec; 7.519 sec/batch)
2016-04-28 23:16:54.589582: step 330, loss = 49.14 (12.1 examples/sec; 5.298 sec/batch)
2016-04-28 23:17:06.670167: step 331, loss = 49.14 (13.1 examples/sec; 4.895 sec/batch)
2016-04-28 23:17:11.633691: step 332, loss = 49.17 (12.9 examples/sec; 4.963 sec/batch)
2016-04-28 23:17:16.828577: step 333, loss = 49.06 (12.3 examples/sec; 5.195 sec/batch)
2016-04-28 23:17:22.206794: step 334, loss = 48.99 (11.9 examples/sec; 5.378 sec/batch)
2016-04-28 23:17:27.317344: step 335, loss = 49.06 (12.5 examples/sec; 5.110 sec/batch)
2016-04-28 23:17:32.179984: step 336, loss = 48.96 (13.2 examples/sec; 4.863 sec/batch)
2016-04-28 23:17:37.402489: step 337, loss = 48.87 (12.3 examples/sec; 5.222 sec/batch)
2016-04-28 23:17:42.571201: step 338, loss = 48.85 (12.4 examples/sec; 5.169 sec/batch)
2016-04-28 23:17:47.495113: step 339, loss = 48.92 (13.0 examples/sec; 4.924 sec/batch)
2016-04-28 23:17:52.815621: step 340, loss = 48.70 (12.0 examples/sec; 5.320 sec/batch)
2016-04-28 23:18:04.799708: step 341, loss = 48.65 (12.2 examples/sec; 5.258 sec/batch)
2016-04-28 23:18:10.199414: step 342, loss = 48.64 (11.9 examples/sec; 5.400 sec/batch)
2016-04-28 23:18:16.264852: step 343, loss = 48.72 (10.6 examples/sec; 6.065 sec/batch)
2016-04-28 23:18:22.704701: step 344, loss = 48.65 (9.9 examples/sec; 6.440 sec/batch)
2016-04-28 23:18:29.400896: step 345, loss = 48.58 (9.6 examples/sec; 6.696 sec/batch)
2016-04-28 23:18:35.556326: step 346, loss = 48.77 (10.4 examples/sec; 6.155 sec/batch)
2016-04-28 23:18:41.898669: step 347, loss = 48.55 (10.1 examples/sec; 6.342 sec/batch)
2016-04-28 23:18:48.292904: step 348, loss = 48.43 (10.0 examples/sec; 6.394 sec/batch)
2016-04-28 23:18:54.839490: step 349, loss = 48.46 (9.8 examples/sec; 6.546 sec/batch)
2016-04-28 23:19:01.737767: step 350, loss = 48.63 (9.3 examples/sec; 6.898 sec/batch)
2016-04-28 23:19:16.598200: step 351, loss = 48.33 (10.2 examples/sec; 6.281 sec/batch)
2016-04-28 23:19:22.818215: step 352, loss = 48.35 (10.3 examples/sec; 6.220 sec/batch)
2016-04-28 23:19:29.302692: step 353, loss = 48.36 (9.9 examples/sec; 6.484 sec/batch)
2016-04-28 23:19:36.799132: step 354, loss = 48.29 (8.5 examples/sec; 7.496 sec/batch)
2016-04-28 23:19:43.182317: step 355, loss = 48.32 (10.0 examples/sec; 6.383 sec/batch)
2016-04-28 23:19:49.618909: step 356, loss = 48.14 (9.9 examples/sec; 6.436 sec/batch)
2016-04-28 23:19:55.893654: step 357, loss = 48.15 (10.2 examples/sec; 6.275 sec/batch)
2016-04-28 23:20:02.184970: step 358, loss = 48.16 (10.2 examples/sec; 6.291 sec/batch)
2016-04-28 23:20:09.167737: step 359, loss = 48.12 (9.2 examples/sec; 6.983 sec/batch)
2016-04-28 23:20:15.516209: step 360, loss = 47.93 (10.1 examples/sec; 6.348 sec/batch)
2016-04-28 23:20:30.177299: step 361, loss = 48.05 (10.0 examples/sec; 6.391 sec/batch)
2016-04-28 23:20:36.701769: step 362, loss = 47.99 (9.8 examples/sec; 6.524 sec/batch)
2016-04-28 23:20:43.859728: step 363, loss = 47.85 (8.9 examples/sec; 7.158 sec/batch)
2016-04-28 23:20:50.280227: step 364, loss = 47.80 (10.0 examples/sec; 6.420 sec/batch)
2016-04-28 23:20:56.632146: step 365, loss = 47.91 (10.1 examples/sec; 6.352 sec/batch)
2016-04-28 23:21:03.440773: step 366, loss = 47.84 (9.4 examples/sec; 6.809 sec/batch)
2016-04-28 23:21:09.793956: step 367, loss = 47.93 (10.1 examples/sec; 6.353 sec/batch)
2016-04-28 23:21:16.915250: step 368, loss = 47.77 (9.0 examples/sec; 7.121 sec/batch)
2016-04-28 23:21:23.137095: step 369, loss = 47.79 (10.3 examples/sec; 6.222 sec/batch)
2016-04-28 23:21:29.348230: step 370, loss = 47.74 (10.3 examples/sec; 6.211 sec/batch)
2016-04-28 23:21:44.808842: step 371, loss = 47.46 (9.3 examples/sec; 6.909 sec/batch)
2016-04-28 23:21:53.673815: step 372, loss = 47.62 (7.2 examples/sec; 8.865 sec/batch)
2016-04-28 23:22:04.243483: step 373, loss = 47.53 (6.1 examples/sec; 10.570 sec/batch)
2016-04-28 23:22:13.898182: step 374, loss = 47.52 (6.6 examples/sec; 9.655 sec/batch)
2016-04-28 23:22:22.443125: step 375, loss = 47.48 (7.5 examples/sec; 8.545 sec/batch)
2016-04-28 23:22:29.526264: step 376, loss = 47.52 (9.0 examples/sec; 7.083 sec/batch)
2016-04-28 23:22:36.599753: step 377, loss = 47.35 (9.0 examples/sec; 7.073 sec/batch)
2016-04-28 23:22:43.959017: step 378, loss = 47.30 (8.7 examples/sec; 7.359 sec/batch)
2016-04-28 23:22:52.051585: step 379, loss = 47.34 (7.9 examples/sec; 8.092 sec/batch)
2016-04-28 23:22:59.081018: step 380, loss = 47.31 (9.1 examples/sec; 7.029 sec/batch)
2016-04-28 23:23:16.194654: step 381, loss = 47.32 (8.8 examples/sec; 7.238 sec/batch)
2016-04-28 23:23:23.820658: step 382, loss = 47.32 (8.4 examples/sec; 7.626 sec/batch)
2016-04-28 23:23:31.484595: step 383, loss = 47.33 (8.4 examples/sec; 7.664 sec/batch)
2016-04-28 23:23:38.408435: step 384, loss = 47.17 (9.2 examples/sec; 6.924 sec/batch)
2016-04-28 23:23:45.491110: step 385, loss = 47.12 (9.0 examples/sec; 7.083 sec/batch)
2016-04-28 23:23:52.929604: step 386, loss = 47.13 (8.6 examples/sec; 7.438 sec/batch)
2016-04-28 23:24:00.869701: step 387, loss = 46.99 (8.1 examples/sec; 7.940 sec/batch)
2016-04-28 23:24:08.177916: step 388, loss = 47.07 (8.8 examples/sec; 7.308 sec/batch)
2016-04-28 23:24:16.697413: step 389, loss = 46.86 (7.5 examples/sec; 8.519 sec/batch)
2016-04-28 23:24:24.176015: step 390, loss = 47.18 (8.6 examples/sec; 7.478 sec/batch)
2016-04-28 23:24:41.627631: step 391, loss = 46.82 (9.4 examples/sec; 6.830 sec/batch)
2016-04-28 23:24:49.050371: step 392, loss = 47.01 (8.6 examples/sec; 7.423 sec/batch)
2016-04-28 23:24:56.194397: step 393, loss = 46.91 (9.0 examples/sec; 7.144 sec/batch)
2016-04-28 23:25:04.415372: step 394, loss = 46.83 (7.8 examples/sec; 8.221 sec/batch)
2016-04-28 23:25:11.502741: step 395, loss = 46.73 (9.0 examples/sec; 7.087 sec/batch)
2016-04-28 23:25:19.061106: step 396, loss = 46.78 (8.5 examples/sec; 7.558 sec/batch)
2016-04-28 23:25:26.289151: step 397, loss = 46.72 (8.9 examples/sec; 7.228 sec/batch)
2016-04-28 23:25:33.494166: step 398, loss = 46.51 (8.9 examples/sec; 7.205 sec/batch)
2016-04-28 23:25:41.266866: step 399, loss = 46.62 (8.2 examples/sec; 7.773 sec/batch)
2016-04-28 23:25:48.142344: step 400, loss = 46.59 (9.3 examples/sec; 6.875 sec/batch)
2016-04-28 23:26:05.144912: step 401, loss = 46.52 (9.1 examples/sec; 7.034 sec/batch)
2016-04-28 23:26:13.204698: step 402, loss = 46.43 (7.9 examples/sec; 8.060 sec/batch)
2016-04-28 23:26:20.119238: step 403, loss = 46.45 (9.3 examples/sec; 6.914 sec/batch)
2016-04-28 23:26:27.329988: step 404, loss = 46.44 (8.9 examples/sec; 7.211 sec/batch)
2016-04-28 23:26:34.727877: step 405, loss = 46.46 (8.7 examples/sec; 7.398 sec/batch)
2016-04-28 23:26:42.610051: step 406, loss = 46.48 (8.1 examples/sec; 7.882 sec/batch)
2016-04-28 23:26:49.738249: step 407, loss = 46.22 (9.0 examples/sec; 7.128 sec/batch)
2016-04-28 23:26:56.844498: step 408, loss = 46.11 (9.0 examples/sec; 7.106 sec/batch)
2016-04-28 23:27:04.466709: step 409, loss = 46.28 (8.4 examples/sec; 7.622 sec/batch)
2016-04-28 23:27:11.585435: step 410, loss = 46.19 (9.0 examples/sec; 7.119 sec/batch)
2016-04-28 23:27:29.085285: step 411, loss = 46.18 (9.4 examples/sec; 6.807 sec/batch)
2016-04-28 23:27:36.182934: step 412, loss = 46.19 (9.0 examples/sec; 7.098 sec/batch)
2016-04-28 23:27:43.391594: step 413, loss = 45.91 (8.9 examples/sec; 7.209 sec/batch)
2016-04-28 23:27:50.899279: step 414, loss = 45.81 (8.5 examples/sec; 7.508 sec/batch)
2016-04-28 23:27:58.143824: step 415, loss = 45.93 (8.8 examples/sec; 7.244 sec/batch)
2016-04-28 23:28:05.114116: step 416, loss = 45.92 (9.2 examples/sec; 6.970 sec/batch)
2016-04-28 23:28:12.219464: step 417, loss = 45.98 (9.0 examples/sec; 7.105 sec/batch)
2016-04-28 23:28:19.659144: step 418, loss = 45.85 (8.6 examples/sec; 7.440 sec/batch)
2016-04-28 23:28:26.811301: step 419, loss = 45.96 (8.9 examples/sec; 7.152 sec/batch)
2016-04-28 23:28:34.078965: step 420, loss = 45.88 (8.8 examples/sec; 7.268 sec/batch)
2016-04-28 23:28:50.585621: step 421, loss = 45.67 (9.2 examples/sec; 6.982 sec/batch)
2016-04-28 23:28:58.462260: step 422, loss = 45.85 (8.1 examples/sec; 7.877 sec/batch)
2016-04-28 23:29:05.720467: step 423, loss = 45.64 (8.8 examples/sec; 7.258 sec/batch)
2016-04-28 23:29:12.943930: step 424, loss = 45.56 (8.9 examples/sec; 7.223 sec/batch)
2016-04-28 23:29:19.863760: step 425, loss = 45.70 (9.2 examples/sec; 6.920 sec/batch)
2016-04-28 23:29:28.027583: step 426, loss = 45.50 (7.8 examples/sec; 8.164 sec/batch)
2016-04-28 23:29:35.099519: step 427, loss = 45.64 (9.0 examples/sec; 7.072 sec/batch)
2016-04-28 23:29:42.311271: step 428, loss = 45.56 (8.9 examples/sec; 7.212 sec/batch)
2016-04-28 23:29:49.454576: step 429, loss = 45.61 (9.0 examples/sec; 7.143 sec/batch)
2016-04-28 23:29:56.478728: step 430, loss = 45.48 (9.1 examples/sec; 7.024 sec/batch)
2016-04-28 23:30:15.657421: step 431, loss = 45.61 (9.1 examples/sec; 6.998 sec/batch)
2016-04-28 23:30:22.853846: step 432, loss = 45.51 (8.9 examples/sec; 7.196 sec/batch)
2016-04-28 23:30:29.936464: step 433, loss = 45.43 (9.0 examples/sec; 7.083 sec/batch)
2016-04-28 23:30:38.094735: step 434, loss = 45.33 (7.8 examples/sec; 8.158 sec/batch)
2016-04-28 23:30:44.975730: step 435, loss = 45.18 (9.3 examples/sec; 6.881 sec/batch)
2016-04-28 23:30:52.450287: step 436, loss = 45.25 (8.6 examples/sec; 7.474 sec/batch)
2016-04-28 23:30:59.518798: step 437, loss = 45.16 (9.1 examples/sec; 7.068 sec/batch)
2016-04-28 23:31:07.671027: step 438, loss = 45.12 (7.9 examples/sec; 8.152 sec/batch)
2016-04-28 23:31:14.668093: step 439, loss = 45.23 (9.1 examples/sec; 6.997 sec/batch)
2016-04-28 23:31:21.917146: step 440, loss = 45.17 (8.8 examples/sec; 7.249 sec/batch)
2016-04-28 23:31:39.824848: step 441, loss = 45.07 (8.1 examples/sec; 7.860 sec/batch)
2016-04-28 23:31:47.116031: step 442, loss = 45.02 (8.8 examples/sec; 7.291 sec/batch)
2016-04-28 23:31:54.353367: step 443, loss = 44.90 (8.8 examples/sec; 7.237 sec/batch)
2016-04-28 23:32:01.474601: step 444, loss = 44.93 (9.0 examples/sec; 7.121 sec/batch)
2016-04-28 23:32:08.411039: step 445, loss = 44.96 (9.2 examples/sec; 6.936 sec/batch)
2016-04-28 23:32:16.560474: step 446, loss = 44.91 (7.9 examples/sec; 8.149 sec/batch)
2016-04-28 23:32:23.479597: step 447, loss = 44.94 (9.2 examples/sec; 6.919 sec/batch)
2016-04-28 23:32:30.448713: step 448, loss = 44.69 (9.2 examples/sec; 6.969 sec/batch)
2016-04-28 23:32:37.645443: step 449, loss = 44.91 (8.9 examples/sec; 7.197 sec/batch)
2016-04-28 23:32:45.366740: step 450, loss = 44.94 (8.3 examples/sec; 7.721 sec/batch)
2016-04-28 23:33:02.131666: step 451, loss = 44.76 (9.0 examples/sec; 7.096 sec/batch)
2016-04-28 23:33:09.044771: step 452, loss = 44.72 (9.3 examples/sec; 6.913 sec/batch)
2016-04-28 23:33:16.439155: step 453, loss = 44.55 (8.7 examples/sec; 7.394 sec/batch)
2016-04-28 23:33:23.941363: step 454, loss = 44.60 (8.5 examples/sec; 7.502 sec/batch)
2016-04-28 23:33:30.753742: step 455, loss = 44.46 (9.4 examples/sec; 6.812 sec/batch)
2016-04-28 23:33:37.598110: step 456, loss = 44.56 (9.4 examples/sec; 6.844 sec/batch)
2016-04-28 23:33:44.653647: step 457, loss = 44.55 (9.1 examples/sec; 7.055 sec/batch)
2016-04-28 23:33:52.639898: step 458, loss = 44.67 (8.0 examples/sec; 7.986 sec/batch)
2016-04-28 23:33:59.674731: step 459, loss = 44.44 (9.1 examples/sec; 7.035 sec/batch)
2016-04-28 23:34:06.691533: step 460, loss = 44.45 (9.1 examples/sec; 7.017 sec/batch)
2016-04-28 23:34:23.981918: step 461, loss = 44.27 (8.6 examples/sec; 7.435 sec/batch)
2016-04-28 23:34:31.315072: step 462, loss = 44.39 (8.7 examples/sec; 7.333 sec/batch)
2016-04-28 23:34:38.452086: step 463, loss = 44.41 (9.0 examples/sec; 7.137 sec/batch)
2016-04-28 23:34:45.265119: step 464, loss = 44.33 (9.4 examples/sec; 6.813 sec/batch)
2016-04-28 23:34:52.535875: step 465, loss = 44.46 (8.8 examples/sec; 7.271 sec/batch)
2016-04-28 23:35:00.154210: step 466, loss = 44.30 (8.4 examples/sec; 7.618 sec/batch)
2016-04-28 23:35:07.596230: step 467, loss = 44.29 (8.6 examples/sec; 7.442 sec/batch)
2016-04-28 23:35:15.041010: step 468, loss = 44.16 (8.6 examples/sec; 7.445 sec/batch)
2016-04-28 23:35:22.570645: step 469, loss = 44.11 (8.5 examples/sec; 7.529 sec/batch)
2016-04-28 23:35:30.356053: step 470, loss = 44.04 (8.2 examples/sec; 7.785 sec/batch)
2016-04-28 23:35:46.903160: step 471, loss = 43.96 (9.3 examples/sec; 6.885 sec/batch)
2016-04-28 23:35:53.791762: step 472, loss = 44.24 (9.3 examples/sec; 6.889 sec/batch)
2016-04-28 23:36:01.394247: step 473, loss = 44.17 (8.4 examples/sec; 7.602 sec/batch)
2016-04-28 23:36:09.131075: step 474, loss = 44.24 (8.3 examples/sec; 7.737 sec/batch)
2016-04-28 23:36:16.355419: step 475, loss = 44.03 (8.9 examples/sec; 7.224 sec/batch)
2016-04-28 23:36:23.527292: step 476, loss = 43.96 (8.9 examples/sec; 7.172 sec/batch)
2016-04-28 23:36:30.378612: step 477, loss = 43.90 (9.3 examples/sec; 6.851 sec/batch)
2016-04-28 23:36:38.554202: step 478, loss = 43.97 (7.8 examples/sec; 8.176 sec/batch)
2016-04-28 23:36:45.685638: step 479, loss = 43.85 (9.0 examples/sec; 7.131 sec/batch)
2016-04-28 23:36:52.805497: step 480, loss = 43.56 (9.0 examples/sec; 7.120 sec/batch)
2016-04-28 23:37:10.196811: step 481, loss = 43.86 (8.2 examples/sec; 7.778 sec/batch)
2016-04-28 23:37:17.507762: step 482, loss = 43.90 (8.8 examples/sec; 7.311 sec/batch)
2016-04-28 23:37:24.566495: step 483, loss = 43.49 (9.1 examples/sec; 7.059 sec/batch)
2016-04-28 23:37:31.955293: step 484, loss = 43.52 (8.7 examples/sec; 7.389 sec/batch)
2016-04-28 23:37:39.115576: step 485, loss = 43.62 (8.9 examples/sec; 7.160 sec/batch)
2016-04-28 23:37:47.215013: step 486, loss = 43.59 (7.9 examples/sec; 8.099 sec/batch)
2016-04-28 23:37:54.424893: step 487, loss = 43.59 (8.9 examples/sec; 7.210 sec/batch)
2016-04-28 23:38:01.977193: step 488, loss = 43.58 (8.5 examples/sec; 7.552 sec/batch)
2016-04-28 23:38:09.098400: step 489, loss = 43.38 (9.0 examples/sec; 7.121 sec/batch)
2016-04-28 23:38:17.163664: step 490, loss = 43.51 (7.9 examples/sec; 8.065 sec/batch)
2016-04-28 23:38:33.919688: step 491, loss = 43.47 (9.3 examples/sec; 6.881 sec/batch)
2016-04-28 23:38:41.272874: step 492, loss = 43.42 (8.7 examples/sec; 7.353 sec/batch)
2016-04-28 23:38:49.178839: step 493, loss = 43.39 (8.1 examples/sec; 7.906 sec/batch)
2016-04-28 23:38:56.204820: step 494, loss = 43.35 (9.1 examples/sec; 7.026 sec/batch)
2016-04-28 23:39:03.185926: step 495, loss = 43.19 (9.2 examples/sec; 6.981 sec/batch)
2016-04-28 23:39:10.269466: step 496, loss = 43.19 (9.0 examples/sec; 7.083 sec/batch)
2016-04-28 23:39:17.465161: step 497, loss = 43.25 (8.9 examples/sec; 7.196 sec/batch)
2016-04-28 23:39:25.406151: step 498, loss = 43.15 (8.1 examples/sec; 7.941 sec/batch)
2016-04-28 23:39:32.658705: step 499, loss = 43.41 (8.8 examples/sec; 7.252 sec/batch)
2016-04-28 23:39:39.561789: step 500, loss = 43.12 (9.3 examples/sec; 6.903 sec/batch)
2016-04-28 23:39:56.835159: step 501, loss = 42.83 (8.6 examples/sec; 7.471 sec/batch)
2016-04-28 23:40:03.674169: step 502, loss = 42.96 (9.4 examples/sec; 6.839 sec/batch)
2016-04-28 23:40:10.810698: step 503, loss = 42.98 (9.0 examples/sec; 7.136 sec/batch)
2016-04-28 23:40:17.745401: step 504, loss = 43.02 (9.2 examples/sec; 6.935 sec/batch)
2016-04-28 23:40:24.868765: step 505, loss = 42.98 (9.0 examples/sec; 7.123 sec/batch)
2016-04-28 23:40:32.830830: step 506, loss = 42.91 (8.0 examples/sec; 7.962 sec/batch)
2016-04-28 23:40:39.779683: step 507, loss = 42.99 (9.2 examples/sec; 6.949 sec/batch)
2016-04-28 23:40:46.924225: step 508, loss = 42.83 (9.0 examples/sec; 7.144 sec/batch)
2016-04-28 23:40:53.888419: step 509, loss = 42.77 (9.2 examples/sec; 6.964 sec/batch)
2016-04-28 23:41:01.970900: step 510, loss = 42.74 (7.9 examples/sec; 8.082 sec/batch)
2016-04-28 23:41:18.795810: step 511, loss = 42.83 (9.2 examples/sec; 6.966 sec/batch)
2016-04-28 23:41:26.052795: step 512, loss = 42.84 (8.8 examples/sec; 7.257 sec/batch)
2016-04-28 23:41:33.968804: step 513, loss = 42.50 (8.1 examples/sec; 7.916 sec/batch)
2016-04-28 23:41:41.735204: step 514, loss = 42.42 (8.2 examples/sec; 7.766 sec/batch)
2016-04-28 23:41:48.616576: step 515, loss = 42.65 (9.3 examples/sec; 6.881 sec/batch)
2016-04-28 23:41:55.998745: step 516, loss = 42.57 (8.7 examples/sec; 7.382 sec/batch)
2016-04-28 23:42:03.269704: step 517, loss = 42.47 (8.8 examples/sec; 7.271 sec/batch)
2016-04-28 23:42:11.339604: step 518, loss = 42.47 (7.9 examples/sec; 8.070 sec/batch)
2016-04-28 23:42:18.246151: step 519, loss = 42.39 (9.3 examples/sec; 6.906 sec/batch)
2016-04-28 23:42:25.123755: step 520, loss = 42.44 (9.3 examples/sec; 6.878 sec/batch)
2016-04-28 23:42:42.531360: step 521, loss = 42.61 (8.4 examples/sec; 7.601 sec/batch)
2016-04-28 23:42:49.307665: step 522, loss = 42.28 (9.4 examples/sec; 6.776 sec/batch)
2016-04-28 23:42:55.969112: step 523, loss = 42.48 (9.6 examples/sec; 6.661 sec/batch)
2016-04-28 23:43:02.775348: step 524, loss = 42.19 (9.4 examples/sec; 6.806 sec/batch)
2016-04-28 23:43:09.515582: step 525, loss = 42.10 (9.5 examples/sec; 6.740 sec/batch)
2016-04-28 23:43:17.296632: step 526, loss = 42.20 (8.2 examples/sec; 7.781 sec/batch)
2016-04-28 23:43:24.482377: step 527, loss = 42.18 (8.9 examples/sec; 7.186 sec/batch)
2016-04-28 23:43:31.953240: step 528, loss = 42.13 (8.6 examples/sec; 7.471 sec/batch)
2016-04-28 23:43:38.981628: step 529, loss = 42.38 (9.1 examples/sec; 7.028 sec/batch)
2016-04-28 23:43:47.286969: step 530, loss = 42.03 (7.7 examples/sec; 8.305 sec/batch)
2016-04-28 23:44:04.565667: step 531, loss = 42.12 (8.6 examples/sec; 7.415 sec/batch)
2016-04-28 23:44:11.703461: step 532, loss = 41.98 (9.0 examples/sec; 7.138 sec/batch)
2016-04-28 23:44:19.619393: step 533, loss = 42.12 (8.1 examples/sec; 7.916 sec/batch)
2016-04-28 23:44:26.721839: step 534, loss = 42.09 (9.0 examples/sec; 7.102 sec/batch)
2016-04-28 23:44:33.686652: step 535, loss = 41.99 (9.2 examples/sec; 6.965 sec/batch)
2016-04-28 23:44:41.039623: step 536, loss = 42.03 (8.7 examples/sec; 7.353 sec/batch)
2016-04-28 23:44:47.845810: step 537, loss = 41.92 (9.4 examples/sec; 6.806 sec/batch)
2016-04-28 23:44:55.449172: step 538, loss = 41.58 (8.4 examples/sec; 7.603 sec/batch)
2016-04-28 23:45:02.759649: step 539, loss = 41.79 (8.8 examples/sec; 7.310 sec/batch)
2016-04-28 23:45:09.701435: step 540, loss = 41.85 (9.2 examples/sec; 6.942 sec/batch)
2016-04-28 23:45:27.122563: step 541, loss = 41.82 (8.2 examples/sec; 7.776 sec/batch)
2016-04-28 23:45:34.274687: step 542, loss = 41.66 (8.9 examples/sec; 7.152 sec/batch)
2016-04-28 23:45:41.343852: step 543, loss = 41.70 (9.1 examples/sec; 7.069 sec/batch)
2016-04-28 23:45:48.410287: step 544, loss = 41.66 (9.1 examples/sec; 7.066 sec/batch)
2016-04-28 23:45:56.009385: step 545, loss = 41.65 (8.4 examples/sec; 7.599 sec/batch)
2016-04-28 23:46:03.608431: step 546, loss = 41.66 (8.4 examples/sec; 7.599 sec/batch)
2016-04-28 23:46:11.004136: step 547, loss = 41.48 (8.7 examples/sec; 7.396 sec/batch)
2016-04-28 23:46:18.003873: step 548, loss = 41.64 (9.1 examples/sec; 7.000 sec/batch)
2016-04-28 23:46:25.278258: step 549, loss = 41.52 (8.8 examples/sec; 7.274 sec/batch)
2016-04-28 23:46:33.034257: step 550, loss = 41.58 (8.3 examples/sec; 7.756 sec/batch)
2016-04-28 23:46:49.910191: step 551, loss = 41.45 (8.8 examples/sec; 7.287 sec/batch)
2016-04-28 23:46:57.079569: step 552, loss = 41.31 (8.9 examples/sec; 7.169 sec/batch)
2016-04-28 23:47:05.054753: step 553, loss = 41.17 (8.0 examples/sec; 7.975 sec/batch)
2016-04-28 23:47:12.057932: step 554, loss = 41.20 (9.1 examples/sec; 7.003 sec/batch)
2016-04-28 23:47:18.972256: step 555, loss = 41.22 (9.3 examples/sec; 6.914 sec/batch)
2016-04-28 23:47:26.382441: step 556, loss = 41.32 (8.6 examples/sec; 7.410 sec/batch)
2016-04-28 23:47:33.570189: step 557, loss = 41.07 (8.9 examples/sec; 7.188 sec/batch)
2016-04-28 23:47:41.724447: step 558, loss = 41.25 (7.8 examples/sec; 8.154 sec/batch)
2016-04-28 23:47:48.959582: step 559, loss = 41.25 (8.8 examples/sec; 7.235 sec/batch)
2016-04-28 23:47:56.356911: step 560, loss = 41.18 (8.7 examples/sec; 7.397 sec/batch)
2016-04-28 23:48:14.168718: step 561, loss = 41.02 (8.2 examples/sec; 7.801 sec/batch)
2016-04-28 23:48:21.485341: step 562, loss = 41.12 (8.7 examples/sec; 7.317 sec/batch)
2016-04-28 23:48:28.527119: step 563, loss = 40.97 (9.1 examples/sec; 7.042 sec/batch)
2016-04-28 23:48:35.615161: step 564, loss = 40.97 (9.0 examples/sec; 7.088 sec/batch)
2016-04-28 23:48:43.614251: step 565, loss = 40.91 (8.0 examples/sec; 7.999 sec/batch)
2016-04-28 23:48:50.990039: step 566, loss = 40.87 (8.7 examples/sec; 7.376 sec/batch)
2016-04-28 23:48:58.013756: step 567, loss = 40.90 (9.1 examples/sec; 7.024 sec/batch)
2016-04-28 23:49:05.517422: step 568, loss = 40.99 (8.5 examples/sec; 7.504 sec/batch)
2016-04-28 23:49:12.338388: step 569, loss = 40.90 (9.4 examples/sec; 6.821 sec/batch)
2016-04-28 23:49:20.080227: step 570, loss = 40.74 (8.3 examples/sec; 7.742 sec/batch)
2016-04-28 23:49:36.424078: step 571, loss = 40.98 (9.3 examples/sec; 6.903 sec/batch)
2016-04-28 23:49:43.927301: step 572, loss = 40.81 (8.5 examples/sec; 7.503 sec/batch)
2016-04-28 23:49:52.768097: step 573, loss = 40.83 (7.2 examples/sec; 8.841 sec/batch)
2016-04-28 23:49:59.925427: step 574, loss = 40.82 (8.9 examples/sec; 7.157 sec/batch)
2016-04-28 23:50:07.081853: step 575, loss = 40.55 (8.9 examples/sec; 7.156 sec/batch)
2016-04-28 23:50:14.182530: step 576, loss = 40.76 (9.0 examples/sec; 7.101 sec/batch)
2016-04-28 23:50:22.181557: step 577, loss = 40.47 (8.0 examples/sec; 7.999 sec/batch)
2016-04-28 23:50:29.385758: step 578, loss = 40.63 (8.9 examples/sec; 7.204 sec/batch)
2016-04-28 23:50:36.502846: step 579, loss = 40.50 (9.0 examples/sec; 7.117 sec/batch)
2016-04-28 23:50:43.329799: step 580, loss = 40.62 (9.4 examples/sec; 6.827 sec/batch)
2016-04-28 23:51:00.390515: step 581, loss = 40.53 (8.6 examples/sec; 7.484 sec/batch)
2016-04-28 23:51:07.475840: step 582, loss = 40.46 (9.0 examples/sec; 7.085 sec/batch)
2016-04-28 23:51:14.636711: step 583, loss = 40.67 (8.9 examples/sec; 7.161 sec/batch)
2016-04-28 23:51:21.823371: step 584, loss = 40.32 (8.9 examples/sec; 7.187 sec/batch)
2016-04-28 23:51:29.891055: step 585, loss = 40.34 (7.9 examples/sec; 8.068 sec/batch)
2016-04-28 23:51:36.930018: step 586, loss = 40.23 (9.1 examples/sec; 7.039 sec/batch)
2016-04-28 23:51:44.303159: step 587, loss = 40.23 (8.7 examples/sec; 7.373 sec/batch)
2016-04-28 23:51:51.579259: step 588, loss = 40.36 (8.8 examples/sec; 7.276 sec/batch)
2016-04-28 23:51:58.990077: step 589, loss = 40.26 (8.6 examples/sec; 7.411 sec/batch)
2016-04-28 23:52:06.581859: step 590, loss = 40.02 (8.4 examples/sec; 7.592 sec/batch)
2016-04-28 23:52:22.895157: step 591, loss = 40.16 (9.2 examples/sec; 6.933 sec/batch)
2016-04-28 23:52:30.046559: step 592, loss = 40.06 (8.9 examples/sec; 7.151 sec/batch)
2016-04-28 23:52:38.123233: step 593, loss = 40.02 (7.9 examples/sec; 8.077 sec/batch)
2016-04-28 23:52:45.259544: step 594, loss = 40.03 (9.0 examples/sec; 7.136 sec/batch)
2016-04-28 23:52:52.494412: step 595, loss = 39.97 (8.8 examples/sec; 7.235 sec/batch)
2016-04-28 23:52:59.718176: step 596, loss = 40.12 (8.9 examples/sec; 7.224 sec/batch)
2016-04-28 23:53:08.064757: step 597, loss = 39.93 (7.7 examples/sec; 8.346 sec/batch)
2016-04-28 23:53:15.241173: step 598, loss = 39.77 (8.9 examples/sec; 7.176 sec/batch)
2016-04-28 23:53:22.654080: step 599, loss = 39.82 (8.6 examples/sec; 7.413 sec/batch)
2016-04-28 23:53:29.881846: step 600, loss = 39.82 (8.9 examples/sec; 7.228 sec/batch)
2016-04-28 23:53:47.650169: step 601, loss = 39.83 (8.5 examples/sec; 7.500 sec/batch)
2016-04-28 23:53:54.746612: step 602, loss = 39.73 (9.0 examples/sec; 7.096 sec/batch)
2016-04-28 23:54:02.228488: step 603, loss = 39.87 (8.6 examples/sec; 7.482 sec/batch)
2016-04-28 23:54:09.233106: step 604, loss = 39.65 (9.1 examples/sec; 7.005 sec/batch)
2016-04-28 23:54:17.401977: step 605, loss = 39.60 (7.8 examples/sec; 8.169 sec/batch)
2016-04-28 23:54:24.647531: step 606, loss = 39.74 (8.8 examples/sec; 7.245 sec/batch)
2016-04-28 23:54:31.967268: step 607, loss = 39.67 (8.7 examples/sec; 7.320 sec/batch)
2016-04-28 23:54:38.927112: step 608, loss = 39.73 (9.2 examples/sec; 6.960 sec/batch)
2016-04-28 23:54:47.068068: step 609, loss = 39.74 (7.9 examples/sec; 8.141 sec/batch)
2016-04-28 23:54:54.078732: step 610, loss = 39.90 (9.1 examples/sec; 7.011 sec/batch)
2016-04-28 23:55:10.530552: step 611, loss = 39.64 (9.2 examples/sec; 6.919 sec/batch)
2016-04-28 23:55:18.398432: step 612, loss = 39.49 (8.1 examples/sec; 7.868 sec/batch)
2016-04-28 23:55:26.340858: step 613, loss = 39.31 (8.1 examples/sec; 7.942 sec/batch)
2016-04-28 23:55:33.637220: step 614, loss = 39.35 (8.8 examples/sec; 7.296 sec/batch)
2016-04-28 23:55:41.164838: step 615, loss = 39.39 (8.5 examples/sec; 7.528 sec/batch)
2016-04-28 23:55:48.281894: step 616, loss = 39.28 (9.0 examples/sec; 7.117 sec/batch)
2016-04-28 23:55:56.703464: step 617, loss = 39.29 (7.6 examples/sec; 8.421 sec/batch)
2016-04-28 23:56:03.798699: step 618, loss = 39.38 (9.0 examples/sec; 7.095 sec/batch)
2016-04-28 23:56:11.141608: step 619, loss = 39.35 (8.7 examples/sec; 7.343 sec/batch)
2016-04-28 23:56:18.240583: step 620, loss = 39.30 (9.0 examples/sec; 7.099 sec/batch)
2016-04-28 23:56:36.082279: step 621, loss = 39.31 (9.1 examples/sec; 7.052 sec/batch)
2016-04-28 23:56:43.216971: step 622, loss = 39.27 (9.0 examples/sec; 7.134 sec/batch)
2016-04-28 23:56:50.484957: step 623, loss = 39.16 (8.8 examples/sec; 7.268 sec/batch)
2016-04-28 23:56:58.332560: step 624, loss = 39.20 (8.2 examples/sec; 7.847 sec/batch)
2016-04-28 23:57:05.940467: step 625, loss = 39.24 (8.4 examples/sec; 7.608 sec/batch)
2016-04-28 23:57:12.974722: step 626, loss = 39.23 (9.1 examples/sec; 7.034 sec/batch)
2016-04-28 23:57:20.412047: step 627, loss = 39.08 (8.6 examples/sec; 7.437 sec/batch)
2016-04-28 23:57:27.711143: step 628, loss = 39.16 (8.8 examples/sec; 7.299 sec/batch)
2016-04-28 23:57:35.657716: step 629, loss = 39.04 (8.1 examples/sec; 7.946 sec/batch)
2016-04-28 23:57:42.616681: step 630, loss = 39.03 (9.2 examples/sec; 6.959 sec/batch)
2016-04-28 23:57:59.791904: step 631, loss = 38.89 (8.6 examples/sec; 7.474 sec/batch)
2016-04-28 23:58:08.117051: step 632, loss = 38.77 (7.7 examples/sec; 8.325 sec/batch)
2016-04-28 23:58:15.456889: step 633, loss = 38.71 (8.7 examples/sec; 7.340 sec/batch)
2016-04-28 23:58:22.775308: step 634, loss = 38.77 (8.7 examples/sec; 7.318 sec/batch)
2016-04-28 23:58:29.937753: step 635, loss = 38.74 (8.9 examples/sec; 7.162 sec/batch)
2016-04-28 23:58:37.806483: step 636, loss = 38.85 (8.1 examples/sec; 7.869 sec/batch)
2016-04-28 23:58:45.052463: step 637, loss = 38.74 (8.8 examples/sec; 7.246 sec/batch)
2016-04-28 23:58:52.298037: step 638, loss = 38.81 (8.8 examples/sec; 7.245 sec/batch)
2016-04-28 23:58:59.642978: step 639, loss = 38.70 (8.7 examples/sec; 7.345 sec/batch)
2016-04-28 23:59:06.832668: step 640, loss = 38.75 (8.9 examples/sec; 7.190 sec/batch)
2016-04-28 23:59:25.000448: step 641, loss = 38.80 (9.0 examples/sec; 7.137 sec/batch)
2016-04-28 23:59:32.586631: step 642, loss = 38.63 (8.4 examples/sec; 7.586 sec/batch)
2016-04-28 23:59:39.927348: step 643, loss = 38.64 (8.7 examples/sec; 7.341 sec/batch)
2016-04-28 23:59:48.264559: step 644, loss = 38.45 (7.7 examples/sec; 8.337 sec/batch)
2016-04-28 23:59:55.763982: step 645, loss = 38.38 (8.5 examples/sec; 7.499 sec/batch)
2016-04-29 00:00:03.413813: step 646, loss = 38.46 (8.4 examples/sec; 7.650 sec/batch)
2016-04-29 00:00:10.742743: step 647, loss = 38.45 (8.7 examples/sec; 7.329 sec/batch)
2016-04-29 00:00:19.256262: step 648, loss = 38.49 (7.5 examples/sec; 8.513 sec/batch)
2016-04-29 00:00:27.079149: step 649, loss = 38.59 (8.2 examples/sec; 7.823 sec/batch)
2016-04-29 00:00:34.504498: step 650, loss = 38.42 (8.6 examples/sec; 7.425 sec/batch)
2016-04-29 00:00:53.435737: step 651, loss = 38.53 (7.7 examples/sec; 8.358 sec/batch)
2016-04-29 00:01:00.880581: step 652, loss = 38.16 (8.6 examples/sec; 7.445 sec/batch)
2016-04-29 00:01:08.556799: step 653, loss = 38.20 (8.3 examples/sec; 7.676 sec/batch)
2016-04-29 00:01:15.767920: step 654, loss = 38.32 (8.9 examples/sec; 7.211 sec/batch)
2016-04-29 00:01:24.030733: step 655, loss = 38.31 (7.7 examples/sec; 8.263 sec/batch)
2016-04-29 00:01:31.703152: step 656, loss = 38.25 (8.3 examples/sec; 7.672 sec/batch)
2016-04-29 00:01:38.984573: step 657, loss = 38.13 (8.8 examples/sec; 7.281 sec/batch)
2016-04-29 00:01:46.256617: step 658, loss = 38.09 (8.8 examples/sec; 7.272 sec/batch)
2016-04-29 00:01:54.039309: step 659, loss = 37.94 (8.2 examples/sec; 7.783 sec/batch)
2016-04-29 00:02:02.527037: step 660, loss = 38.07 (7.5 examples/sec; 8.488 sec/batch)
2016-04-29 00:02:20.358062: step 661, loss = 38.00 (8.2 examples/sec; 7.765 sec/batch)
2016-04-29 00:02:28.230381: step 662, loss = 38.04 (8.1 examples/sec; 7.872 sec/batch)
2016-04-29 00:02:36.661878: step 663, loss = 38.02 (7.6 examples/sec; 8.431 sec/batch)
2016-04-29 00:02:44.131497: step 664, loss = 38.04 (8.6 examples/sec; 7.470 sec/batch)
2016-04-29 00:02:51.995709: step 665, loss = 37.93 (8.1 examples/sec; 7.864 sec/batch)
2016-04-29 00:02:59.691317: step 666, loss = 37.94 (8.3 examples/sec; 7.695 sec/batch)
2016-04-29 00:03:07.979550: step 667, loss = 37.88 (7.7 examples/sec; 8.288 sec/batch)
2016-04-29 00:03:15.781491: step 668, loss = 37.77 (8.2 examples/sec; 7.802 sec/batch)
2016-04-29 00:03:23.564748: step 669, loss = 37.83 (8.2 examples/sec; 7.783 sec/batch)
2016-04-29 00:03:31.154537: step 670, loss = 37.72 (8.4 examples/sec; 7.590 sec/batch)
2016-04-29 00:03:49.817963: step 671, loss = 37.83 (8.7 examples/sec; 7.376 sec/batch)
2016-04-29 00:03:57.513651: step 672, loss = 37.54 (8.3 examples/sec; 7.696 sec/batch)
2016-04-29 00:04:05.367714: step 673, loss = 37.76 (8.1 examples/sec; 7.854 sec/batch)
2016-04-29 00:04:13.764389: step 674, loss = 37.80 (7.6 examples/sec; 8.397 sec/batch)
2016-04-29 00:04:21.547437: step 675, loss = 37.53 (8.2 examples/sec; 7.783 sec/batch)
2016-04-29 00:04:29.174526: step 676, loss = 37.75 (8.4 examples/sec; 7.627 sec/batch)
2016-04-29 00:04:36.055273: step 677, loss = 37.63 (9.3 examples/sec; 6.881 sec/batch)
2016-04-29 00:04:42.845617: step 678, loss = 37.68 (9.4 examples/sec; 6.790 sec/batch)
2016-04-29 00:04:51.352447: step 679, loss = 37.56 (7.5 examples/sec; 8.507 sec/batch)
2016-04-29 00:04:57.684416: step 680, loss = 37.46 (10.1 examples/sec; 6.332 sec/batch)
2016-04-29 00:05:12.780822: step 681, loss = 37.53 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 00:05:19.604943: step 682, loss = 37.55 (9.4 examples/sec; 6.824 sec/batch)
2016-04-29 00:05:25.696789: step 683, loss = 37.32 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 00:05:31.636353: step 684, loss = 37.33 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 00:05:37.696883: step 685, loss = 37.38 (10.6 examples/sec; 6.060 sec/batch)
2016-04-29 00:05:43.899816: step 686, loss = 37.12 (10.3 examples/sec; 6.203 sec/batch)
2016-04-29 00:05:50.302914: step 687, loss = 37.33 (10.0 examples/sec; 6.403 sec/batch)
2016-04-29 00:05:57.162872: step 688, loss = 37.33 (9.3 examples/sec; 6.860 sec/batch)
2016-04-29 00:06:03.371101: step 689, loss = 37.32 (10.3 examples/sec; 6.208 sec/batch)
2016-04-29 00:06:09.509064: step 690, loss = 37.22 (10.4 examples/sec; 6.138 sec/batch)
2016-04-29 00:06:24.821441: step 691, loss = 37.22 (9.5 examples/sec; 6.724 sec/batch)
2016-04-29 00:06:31.165941: step 692, loss = 37.25 (10.1 examples/sec; 6.344 sec/batch)
2016-04-29 00:06:37.422654: step 693, loss = 37.27 (10.2 examples/sec; 6.257 sec/batch)
2016-04-29 00:06:43.526643: step 694, loss = 37.16 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 00:06:49.620408: step 695, loss = 37.02 (10.5 examples/sec; 6.094 sec/batch)
2016-04-29 00:06:55.486621: step 696, loss = 37.08 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 00:07:03.065679: step 697, loss = 37.00 (8.4 examples/sec; 7.579 sec/batch)
2016-04-29 00:07:09.168979: step 698, loss = 36.86 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 00:07:15.240613: step 699, loss = 36.95 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 00:07:21.378487: step 700, loss = 36.98 (10.4 examples/sec; 6.138 sec/batch)
2016-04-29 00:07:36.577877: step 701, loss = 36.91 (10.2 examples/sec; 6.288 sec/batch)
2016-04-29 00:07:42.772052: step 702, loss = 36.79 (10.3 examples/sec; 6.194 sec/batch)
2016-04-29 00:07:49.738465: step 703, loss = 37.12 (9.2 examples/sec; 6.966 sec/batch)
2016-04-29 00:07:57.673573: step 704, loss = 36.99 (8.1 examples/sec; 7.935 sec/batch)
2016-04-29 00:08:05.673101: step 705, loss = 36.75 (8.0 examples/sec; 7.999 sec/batch)
2016-04-29 00:08:11.726332: step 706, loss = 36.83 (10.6 examples/sec; 6.053 sec/batch)
2016-04-29 00:08:18.787978: step 707, loss = 36.70 (9.1 examples/sec; 7.062 sec/batch)
2016-04-29 00:08:25.185268: step 708, loss = 36.71 (10.0 examples/sec; 6.397 sec/batch)
2016-04-29 00:08:30.665750: step 709, loss = 36.51 (11.7 examples/sec; 5.480 sec/batch)
2016-04-29 00:08:36.775391: step 710, loss = 36.66 (10.5 examples/sec; 6.110 sec/batch)
2016-04-29 00:08:49.862095: step 711, loss = 36.50 (12.1 examples/sec; 5.294 sec/batch)
2016-04-29 00:08:55.320179: step 712, loss = 36.67 (11.7 examples/sec; 5.458 sec/batch)
2016-04-29 00:09:00.802986: step 713, loss = 36.57 (11.7 examples/sec; 5.483 sec/batch)
2016-04-29 00:09:06.299472: step 714, loss = 36.56 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 00:09:12.382190: step 715, loss = 36.38 (10.5 examples/sec; 6.083 sec/batch)
2016-04-29 00:09:17.876621: step 716, loss = 36.42 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 00:09:23.355112: step 717, loss = 36.47 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 00:09:28.816448: step 718, loss = 36.16 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 00:09:35.595179: step 719, loss = 36.44 (9.4 examples/sec; 6.777 sec/batch)
2016-04-29 00:09:42.976839: step 720, loss = 36.45 (8.7 examples/sec; 7.382 sec/batch)
2016-04-29 00:09:55.792902: step 721, loss = 36.24 (12.2 examples/sec; 5.245 sec/batch)
2016-04-29 00:10:01.325517: step 722, loss = 36.37 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 00:10:06.899164: step 723, loss = 36.31 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 00:10:12.392486: step 724, loss = 36.19 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 00:10:18.587295: step 725, loss = 35.99 (10.3 examples/sec; 6.195 sec/batch)
2016-04-29 00:10:24.190082: step 726, loss = 35.92 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 00:10:29.742005: step 727, loss = 36.12 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 00:10:34.968246: step 728, loss = 36.11 (12.2 examples/sec; 5.226 sec/batch)
2016-04-29 00:10:40.336068: step 729, loss = 36.06 (11.9 examples/sec; 5.368 sec/batch)
2016-04-29 00:10:45.784473: step 730, loss = 36.17 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 00:10:59.448475: step 731, loss = 36.10 (11.8 examples/sec; 5.415 sec/batch)
2016-04-29 00:11:04.985792: step 732, loss = 36.06 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 00:11:10.637777: step 733, loss = 35.92 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 00:11:16.307551: step 734, loss = 35.99 (11.3 examples/sec; 5.670 sec/batch)
2016-04-29 00:11:22.436820: step 735, loss = 36.02 (10.4 examples/sec; 6.129 sec/batch)
2016-04-29 00:11:28.057208: step 736, loss = 35.99 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 00:11:33.714900: step 737, loss = 35.98 (11.3 examples/sec; 5.658 sec/batch)
2016-04-29 00:11:39.370646: step 738, loss = 35.77 (11.3 examples/sec; 5.656 sec/batch)
2016-04-29 00:11:45.177154: step 739, loss = 35.61 (11.0 examples/sec; 5.806 sec/batch)
2016-04-29 00:11:50.861385: step 740, loss = 35.74 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 00:12:04.643583: step 741, loss = 35.72 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 00:12:10.268319: step 742, loss = 35.69 (11.4 examples/sec; 5.625 sec/batch)
2016-04-29 00:12:15.739303: step 743, loss = 35.60 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 00:12:21.406147: step 744, loss = 35.73 (11.3 examples/sec; 5.667 sec/batch)
2016-04-29 00:12:27.400934: step 745, loss = 35.77 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 00:12:33.092981: step 746, loss = 35.53 (11.2 examples/sec; 5.692 sec/batch)
2016-04-29 00:12:38.466796: step 747, loss = 35.40 (11.9 examples/sec; 5.374 sec/batch)
2016-04-29 00:12:43.805979: step 748, loss = 35.64 (12.0 examples/sec; 5.339 sec/batch)
2016-04-29 00:12:49.342593: step 749, loss = 35.66 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 00:12:55.047976: step 750, loss = 35.40 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 00:13:09.017540: step 751, loss = 35.59 (11.5 examples/sec; 5.576 sec/batch)
2016-04-29 00:13:14.448316: step 752, loss = 35.52 (11.8 examples/sec; 5.430 sec/batch)
2016-04-29 00:13:19.755854: step 753, loss = 35.53 (12.1 examples/sec; 5.307 sec/batch)
2016-04-29 00:13:25.523203: step 754, loss = 35.54 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 00:13:31.195667: step 755, loss = 35.45 (11.3 examples/sec; 5.672 sec/batch)
2016-04-29 00:13:37.667656: step 756, loss = 35.59 (9.9 examples/sec; 6.472 sec/batch)
2016-04-29 00:13:43.231317: step 757, loss = 35.44 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 00:13:48.945921: step 758, loss = 35.34 (11.2 examples/sec; 5.715 sec/batch)
2016-04-29 00:13:54.755996: step 759, loss = 35.13 (11.0 examples/sec; 5.810 sec/batch)
2016-04-29 00:14:00.356733: step 760, loss = 35.11 (11.4 examples/sec; 5.601 sec/batch)
2016-04-29 00:14:14.499728: step 761, loss = 35.23 (11.2 examples/sec; 5.729 sec/batch)
2016-04-29 00:14:20.017698: step 762, loss = 35.28 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 00:14:25.811138: step 763, loss = 35.20 (11.0 examples/sec; 5.793 sec/batch)
2016-04-29 00:14:31.574457: step 764, loss = 35.26 (11.1 examples/sec; 5.763 sec/batch)
2016-04-29 00:14:37.234762: step 765, loss = 35.25 (11.3 examples/sec; 5.660 sec/batch)
2016-04-29 00:14:45.306571: step 766, loss = 35.29 (7.9 examples/sec; 8.072 sec/batch)
2016-04-29 00:14:51.231057: step 767, loss = 34.92 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 00:14:56.781175: step 768, loss = 34.96 (11.5 examples/sec; 5.550 sec/batch)
2016-04-29 00:15:02.009157: step 769, loss = 34.95 (12.2 examples/sec; 5.228 sec/batch)
2016-04-29 00:15:07.595785: step 770, loss = 34.97 (11.5 examples/sec; 5.587 sec/batch)
2016-04-29 00:15:21.611778: step 771, loss = 35.00 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 00:15:27.190674: step 772, loss = 34.88 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 00:15:32.652052: step 773, loss = 34.75 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 00:15:38.088667: step 774, loss = 34.85 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 00:15:43.647784: step 775, loss = 34.86 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 00:15:49.851981: step 776, loss = 34.91 (10.3 examples/sec; 6.204 sec/batch)
2016-04-29 00:15:55.427411: step 777, loss = 34.99 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 00:16:01.183458: step 778, loss = 34.67 (11.1 examples/sec; 5.756 sec/batch)
2016-04-29 00:16:06.892486: step 779, loss = 34.67 (11.2 examples/sec; 5.709 sec/batch)
2016-04-29 00:16:12.571601: step 780, loss = 34.63 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 00:16:26.073417: step 781, loss = 34.65 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 00:16:31.744490: step 782, loss = 34.67 (11.3 examples/sec; 5.671 sec/batch)
2016-04-29 00:16:37.452106: step 783, loss = 34.70 (11.2 examples/sec; 5.708 sec/batch)
2016-04-29 00:16:43.020451: step 784, loss = 34.42 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 00:16:48.526245: step 785, loss = 34.50 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 00:16:54.766468: step 786, loss = 34.53 (10.3 examples/sec; 6.240 sec/batch)
2016-04-29 00:17:00.268003: step 787, loss = 34.36 (11.6 examples/sec; 5.501 sec/batch)
2016-04-29 00:17:05.920055: step 788, loss = 34.63 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 00:17:11.360889: step 789, loss = 34.53 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 00:17:16.771568: step 790, loss = 34.29 (11.8 examples/sec; 5.411 sec/batch)
2016-04-29 00:17:30.581924: step 791, loss = 34.35 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 00:17:36.092212: step 792, loss = 34.40 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 00:17:41.731365: step 793, loss = 34.30 (11.3 examples/sec; 5.639 sec/batch)
2016-04-29 00:17:47.055036: step 794, loss = 34.40 (12.0 examples/sec; 5.324 sec/batch)
2016-04-29 00:17:52.642942: step 795, loss = 34.30 (11.5 examples/sec; 5.588 sec/batch)
2016-04-29 00:17:58.838513: step 796, loss = 34.27 (10.3 examples/sec; 6.195 sec/batch)
2016-04-29 00:18:04.390695: step 797, loss = 34.30 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 00:18:09.970151: step 798, loss = 34.26 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 00:18:15.566501: step 799, loss = 34.24 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 00:18:21.366253: step 800, loss = 33.99 (11.0 examples/sec; 5.800 sec/batch)
2016-04-29 00:18:35.191566: step 801, loss = 34.10 (10.2 examples/sec; 6.244 sec/batch)
2016-04-29 00:18:40.676057: step 802, loss = 34.22 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 00:18:46.420512: step 803, loss = 34.09 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 00:18:52.053802: step 804, loss = 34.26 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 00:18:57.680401: step 805, loss = 33.99 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 00:19:03.971790: step 806, loss = 33.98 (10.2 examples/sec; 6.291 sec/batch)
2016-04-29 00:19:09.782504: step 807, loss = 34.11 (11.0 examples/sec; 5.811 sec/batch)
2016-04-29 00:19:15.417325: step 808, loss = 33.97 (11.4 examples/sec; 5.635 sec/batch)
2016-04-29 00:19:21.014316: step 809, loss = 34.00 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 00:19:26.844526: step 810, loss = 33.81 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 00:19:40.342399: step 811, loss = 33.79 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 00:19:45.934984: step 812, loss = 33.99 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 00:19:54.893275: step 813, loss = 33.89 (7.1 examples/sec; 8.958 sec/batch)
2016-04-29 00:20:01.040667: step 814, loss = 33.86 (10.4 examples/sec; 6.147 sec/batch)
2016-04-29 00:20:07.239526: step 815, loss = 33.74 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 00:20:13.338496: step 816, loss = 33.57 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 00:20:19.022864: step 817, loss = 33.72 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 00:20:24.669380: step 818, loss = 33.74 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 00:20:30.380514: step 819, loss = 33.56 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 00:20:35.898572: step 820, loss = 33.66 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 00:20:49.547387: step 821, loss = 33.79 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 00:20:55.108416: step 822, loss = 33.60 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 00:21:00.781730: step 823, loss = 33.63 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 00:21:06.383456: step 824, loss = 33.60 (11.4 examples/sec; 5.602 sec/batch)
2016-04-29 00:21:12.060515: step 825, loss = 33.42 (11.3 examples/sec; 5.677 sec/batch)
2016-04-29 00:21:18.152299: step 826, loss = 33.48 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 00:21:23.647828: step 827, loss = 33.43 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 00:21:29.206646: step 828, loss = 33.54 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 00:21:34.851349: step 829, loss = 33.48 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 00:21:40.502897: step 830, loss = 33.36 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 00:21:54.463586: step 831, loss = 33.48 (10.7 examples/sec; 5.993 sec/batch)
2016-04-29 00:22:00.173930: step 832, loss = 33.33 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 00:22:05.685580: step 833, loss = 33.27 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 00:22:11.117493: step 834, loss = 33.22 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 00:22:16.827082: step 835, loss = 33.42 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 00:22:23.179120: step 836, loss = 33.29 (10.1 examples/sec; 6.352 sec/batch)
2016-04-29 00:22:28.985822: step 837, loss = 33.28 (11.0 examples/sec; 5.807 sec/batch)
2016-04-29 00:22:34.622413: step 838, loss = 33.25 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 00:22:40.304626: step 839, loss = 33.09 (11.3 examples/sec; 5.682 sec/batch)
2016-04-29 00:22:45.945325: step 840, loss = 33.06 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 00:23:00.112070: step 841, loss = 32.98 (10.0 examples/sec; 6.403 sec/batch)
2016-04-29 00:23:05.858913: step 842, loss = 33.10 (11.1 examples/sec; 5.746 sec/batch)
2016-04-29 00:23:11.695467: step 843, loss = 33.02 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 00:23:17.317629: step 844, loss = 33.18 (11.4 examples/sec; 5.622 sec/batch)
2016-04-29 00:23:22.973523: step 845, loss = 33.06 (11.3 examples/sec; 5.656 sec/batch)
2016-04-29 00:23:29.193545: step 846, loss = 32.88 (10.3 examples/sec; 6.220 sec/batch)
2016-04-29 00:23:34.994626: step 847, loss = 32.95 (11.0 examples/sec; 5.801 sec/batch)
2016-04-29 00:23:40.545164: step 848, loss = 32.84 (11.5 examples/sec; 5.550 sec/batch)
2016-04-29 00:23:46.236397: step 849, loss = 32.92 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 00:23:51.962570: step 850, loss = 32.91 (11.2 examples/sec; 5.726 sec/batch)
2016-04-29 00:24:06.069615: step 851, loss = 32.70 (10.2 examples/sec; 6.274 sec/batch)
2016-04-29 00:24:12.741844: step 852, loss = 32.77 (9.6 examples/sec; 6.670 sec/batch)
2016-04-29 00:24:18.677280: step 853, loss = 32.90 (10.8 examples/sec; 5.935 sec/batch)
2016-04-29 00:24:24.447269: step 854, loss = 32.80 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 00:24:30.169882: step 855, loss = 32.65 (11.2 examples/sec; 5.723 sec/batch)
2016-04-29 00:24:36.279313: step 856, loss = 32.77 (10.5 examples/sec; 6.109 sec/batch)
2016-04-29 00:24:41.967455: step 857, loss = 32.65 (11.3 examples/sec; 5.688 sec/batch)
2016-04-29 00:24:47.358035: step 858, loss = 32.75 (11.9 examples/sec; 5.390 sec/batch)
2016-04-29 00:24:53.058451: step 859, loss = 32.86 (11.2 examples/sec; 5.700 sec/batch)
2016-04-29 00:25:00.837428: step 860, loss = 32.47 (8.2 examples/sec; 7.779 sec/batch)
2016-04-29 00:25:18.382240: step 861, loss = 32.50 (9.1 examples/sec; 7.037 sec/batch)
2016-04-29 00:25:25.256459: step 862, loss = 32.66 (9.3 examples/sec; 6.874 sec/batch)
2016-04-29 00:25:31.158741: step 863, loss = 32.51 (10.8 examples/sec; 5.902 sec/batch)
2016-04-29 00:25:39.522659: step 864, loss = 32.57 (7.7 examples/sec; 8.364 sec/batch)
2016-04-29 00:25:45.917730: step 865, loss = 32.42 (10.0 examples/sec; 6.395 sec/batch)
2016-04-29 00:25:51.491498: step 866, loss = 32.55 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 00:25:57.215111: step 867, loss = 32.71 (11.2 examples/sec; 5.724 sec/batch)
2016-04-29 00:26:02.634488: step 868, loss = 32.49 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 00:26:08.112211: step 869, loss = 32.52 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 00:26:14.449916: step 870, loss = 32.48 (10.1 examples/sec; 6.338 sec/batch)
2016-04-29 00:26:27.992484: step 871, loss = 32.31 (10.9 examples/sec; 5.846 sec/batch)
2016-04-29 00:26:33.841800: step 872, loss = 32.18 (10.9 examples/sec; 5.849 sec/batch)
2016-04-29 00:26:39.446430: step 873, loss = 32.32 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 00:26:44.860878: step 874, loss = 32.39 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 00:26:50.884335: step 875, loss = 32.39 (10.6 examples/sec; 6.023 sec/batch)
2016-04-29 00:26:56.336766: step 876, loss = 32.35 (11.7 examples/sec; 5.452 sec/batch)
2016-04-29 00:27:01.915504: step 877, loss = 32.17 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 00:27:07.504954: step 878, loss = 32.23 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 00:27:13.052418: step 879, loss = 32.33 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 00:27:18.520921: step 880, loss = 32.06 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 00:27:31.635944: step 881, loss = 32.16 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 00:27:37.132454: step 882, loss = 32.17 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 00:27:42.643865: step 883, loss = 32.06 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 00:27:48.091328: step 884, loss = 31.93 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 00:27:54.711744: step 885, loss = 31.88 (9.7 examples/sec; 6.620 sec/batch)
2016-04-29 00:28:00.615112: step 886, loss = 31.91 (10.8 examples/sec; 5.903 sec/batch)
2016-04-29 00:28:06.224028: step 887, loss = 31.79 (11.4 examples/sec; 5.607 sec/batch)
2016-04-29 00:28:11.799315: step 888, loss = 32.06 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 00:28:17.401323: step 889, loss = 31.91 (11.4 examples/sec; 5.602 sec/batch)
2016-04-29 00:28:23.099203: step 890, loss = 32.09 (11.2 examples/sec; 5.698 sec/batch)
2016-04-29 00:28:37.226177: step 891, loss = 31.81 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 00:28:42.819425: step 892, loss = 31.80 (11.4 examples/sec; 5.593 sec/batch)
2016-04-29 00:28:48.290480: step 893, loss = 31.79 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 00:28:53.528410: step 894, loss = 31.76 (12.2 examples/sec; 5.238 sec/batch)
2016-04-29 00:28:59.583262: step 895, loss = 31.80 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 00:29:05.246480: step 896, loss = 31.64 (11.3 examples/sec; 5.663 sec/batch)
2016-04-29 00:29:10.746907: step 897, loss = 31.98 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 00:29:16.546101: step 898, loss = 31.62 (11.0 examples/sec; 5.799 sec/batch)
2016-04-29 00:29:22.117235: step 899, loss = 31.82 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 00:29:27.690328: step 900, loss = 31.65 (11.5 examples/sec; 5.573 sec/batch)
2016-04-29 00:29:41.120469: step 901, loss = 31.55 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 00:29:46.552631: step 902, loss = 31.77 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 00:29:52.440455: step 903, loss = 31.63 (10.9 examples/sec; 5.888 sec/batch)
2016-04-29 00:30:01.902140: step 904, loss = 31.46 (6.8 examples/sec; 9.462 sec/batch)
2016-04-29 00:30:16.481070: step 905, loss = 31.48 (4.4 examples/sec; 14.579 sec/batch)
2016-04-29 00:30:27.308122: step 906, loss = 31.39 (5.9 examples/sec; 10.827 sec/batch)
2016-04-29 00:30:37.171474: step 907, loss = 31.47 (6.5 examples/sec; 9.863 sec/batch)
2016-04-29 00:30:47.449589: step 908, loss = 31.52 (6.2 examples/sec; 10.278 sec/batch)
2016-04-29 00:30:55.027166: step 909, loss = 31.43 (8.4 examples/sec; 7.577 sec/batch)
2016-04-29 00:31:01.536848: step 910, loss = 31.41 (9.8 examples/sec; 6.510 sec/batch)
2016-04-29 00:31:17.367342: step 911, loss = 31.41 (9.2 examples/sec; 6.931 sec/batch)
2016-04-29 00:31:23.871665: step 912, loss = 31.49 (9.8 examples/sec; 6.504 sec/batch)
2016-04-29 00:31:30.806913: step 913, loss = 31.29 (9.2 examples/sec; 6.935 sec/batch)
2016-04-29 00:31:37.632615: step 914, loss = 31.35 (9.4 examples/sec; 6.826 sec/batch)
2016-04-29 00:31:44.360088: step 915, loss = 31.30 (9.5 examples/sec; 6.727 sec/batch)
2016-04-29 00:31:51.796459: step 916, loss = 31.29 (8.6 examples/sec; 7.436 sec/batch)
2016-04-29 00:31:59.130493: step 917, loss = 31.44 (8.7 examples/sec; 7.334 sec/batch)
2016-04-29 00:32:07.275472: step 918, loss = 31.24 (7.9 examples/sec; 8.145 sec/batch)
2016-04-29 00:32:14.485136: step 919, loss = 31.23 (8.9 examples/sec; 7.210 sec/batch)
2016-04-29 00:32:21.102783: step 920, loss = 31.14 (9.7 examples/sec; 6.618 sec/batch)
2016-04-29 00:32:38.103828: step 921, loss = 31.23 (9.0 examples/sec; 7.142 sec/batch)
2016-04-29 00:32:45.817433: step 922, loss = 30.95 (8.3 examples/sec; 7.713 sec/batch)
2016-04-29 00:32:52.700766: step 923, loss = 31.03 (9.3 examples/sec; 6.883 sec/batch)
2016-04-29 00:33:01.155191: step 924, loss = 31.24 (7.6 examples/sec; 8.454 sec/batch)
2016-04-29 00:33:08.407876: step 925, loss = 31.11 (8.8 examples/sec; 7.253 sec/batch)
2016-04-29 00:33:15.073815: step 926, loss = 31.06 (9.6 examples/sec; 6.666 sec/batch)
2016-04-29 00:33:22.146113: step 927, loss = 31.04 (9.0 examples/sec; 7.072 sec/batch)
2016-04-29 00:33:30.019326: step 928, loss = 31.21 (8.1 examples/sec; 7.873 sec/batch)
2016-04-29 00:33:37.644766: step 929, loss = 31.02 (8.4 examples/sec; 7.625 sec/batch)
2016-04-29 00:33:44.240476: step 930, loss = 30.96 (9.7 examples/sec; 6.596 sec/batch)
2016-04-29 00:34:00.564453: step 931, loss = 30.84 (9.3 examples/sec; 6.903 sec/batch)
2016-04-29 00:34:08.061129: step 932, loss = 30.85 (8.5 examples/sec; 7.497 sec/batch)
2016-04-29 00:34:14.568798: step 933, loss = 30.94 (9.8 examples/sec; 6.508 sec/batch)
2016-04-29 00:34:21.831143: step 934, loss = 30.92 (8.8 examples/sec; 7.262 sec/batch)
2016-04-29 00:34:28.918191: step 935, loss = 30.78 (9.0 examples/sec; 7.087 sec/batch)
2016-04-29 00:34:36.520025: step 936, loss = 30.84 (8.4 examples/sec; 7.602 sec/batch)
2016-04-29 00:34:44.368456: step 937, loss = 30.85 (8.2 examples/sec; 7.848 sec/batch)
2016-04-29 00:34:51.804157: step 938, loss = 30.83 (8.6 examples/sec; 7.436 sec/batch)
2016-04-29 00:34:59.068037: step 939, loss = 30.60 (8.8 examples/sec; 7.264 sec/batch)
2016-04-29 00:35:06.441682: step 940, loss = 30.65 (8.7 examples/sec; 7.374 sec/batch)
2016-04-29 00:35:23.458858: step 941, loss = 30.68 (10.1 examples/sec; 6.323 sec/batch)
2016-04-29 00:35:29.539157: step 942, loss = 30.72 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 00:35:35.796957: step 943, loss = 30.55 (10.2 examples/sec; 6.258 sec/batch)
2016-04-29 00:35:41.792434: step 944, loss = 30.61 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 00:35:48.483902: step 945, loss = 30.65 (9.6 examples/sec; 6.691 sec/batch)
2016-04-29 00:35:54.294712: step 946, loss = 30.72 (11.0 examples/sec; 5.811 sec/batch)
2016-04-29 00:35:59.583618: step 947, loss = 30.61 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 00:36:05.264524: step 948, loss = 30.48 (11.3 examples/sec; 5.681 sec/batch)
2016-04-29 00:36:10.773330: step 949, loss = 30.46 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 00:36:16.896342: step 950, loss = 30.36 (10.5 examples/sec; 6.123 sec/batch)
2016-04-29 00:36:29.695652: step 951, loss = 30.42 (12.2 examples/sec; 5.226 sec/batch)
2016-04-29 00:36:35.231006: step 952, loss = 30.33 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 00:36:40.571617: step 953, loss = 30.33 (12.0 examples/sec; 5.341 sec/batch)
2016-04-29 00:36:45.917299: step 954, loss = 30.60 (12.0 examples/sec; 5.346 sec/batch)
2016-04-29 00:36:51.993912: step 955, loss = 30.19 (10.5 examples/sec; 6.077 sec/batch)
2016-04-29 00:36:57.105192: step 956, loss = 30.42 (12.5 examples/sec; 5.111 sec/batch)
2016-04-29 00:37:02.636632: step 957, loss = 30.16 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 00:37:07.980796: step 958, loss = 30.35 (12.0 examples/sec; 5.344 sec/batch)
2016-04-29 00:37:13.473916: step 959, loss = 30.35 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 00:37:18.839911: step 960, loss = 30.19 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 00:37:31.926141: step 961, loss = 30.23 (12.0 examples/sec; 5.342 sec/batch)
2016-04-29 00:37:37.556954: step 962, loss = 30.37 (11.4 examples/sec; 5.631 sec/batch)
2016-04-29 00:37:43.209752: step 963, loss = 30.26 (11.3 examples/sec; 5.653 sec/batch)
2016-04-29 00:37:48.643671: step 964, loss = 30.15 (11.8 examples/sec; 5.434 sec/batch)
2016-04-29 00:37:53.889914: step 965, loss = 30.13 (12.2 examples/sec; 5.246 sec/batch)
2016-04-29 00:37:59.604223: step 966, loss = 30.15 (11.2 examples/sec; 5.714 sec/batch)
2016-04-29 00:38:05.570195: step 967, loss = 30.01 (10.7 examples/sec; 5.966 sec/batch)
2016-04-29 00:38:10.959200: step 968, loss = 30.10 (11.9 examples/sec; 5.389 sec/batch)
2016-04-29 00:38:16.537911: step 969, loss = 30.20 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 00:38:22.189691: step 970, loss = 30.18 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 00:38:35.845034: step 971, loss = 29.92 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 00:38:41.238610: step 972, loss = 30.02 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 00:38:46.683947: step 973, loss = 29.75 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 00:38:52.231931: step 974, loss = 29.99 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 00:38:57.530805: step 975, loss = 29.83 (12.1 examples/sec; 5.299 sec/batch)
2016-04-29 00:39:03.683311: step 976, loss = 29.92 (10.4 examples/sec; 6.152 sec/batch)
2016-04-29 00:39:08.947004: step 977, loss = 29.97 (12.2 examples/sec; 5.264 sec/batch)
2016-04-29 00:39:14.396920: step 978, loss = 29.60 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 00:39:19.928254: step 979, loss = 29.78 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 00:39:25.451434: step 980, loss = 29.74 (11.6 examples/sec; 5.523 sec/batch)
2016-04-29 00:39:38.685120: step 981, loss = 29.85 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 00:39:44.106299: step 982, loss = 29.68 (11.8 examples/sec; 5.421 sec/batch)
2016-04-29 00:39:49.675743: step 983, loss = 29.57 (11.5 examples/sec; 5.569 sec/batch)
2016-04-29 00:39:55.096456: step 984, loss = 29.71 (11.8 examples/sec; 5.421 sec/batch)
2016-04-29 00:40:00.489575: step 985, loss = 29.38 (11.9 examples/sec; 5.393 sec/batch)
2016-04-29 00:40:06.097317: step 986, loss = 29.52 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 00:40:12.187070: step 987, loss = 29.61 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 00:40:17.577353: step 988, loss = 29.60 (11.9 examples/sec; 5.390 sec/batch)
2016-04-29 00:40:22.961965: step 989, loss = 29.65 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 00:40:29.004727: step 990, loss = 29.50 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 00:40:42.106445: step 991, loss = 29.56 (11.1 examples/sec; 5.781 sec/batch)
2016-04-29 00:40:47.663703: step 992, loss = 29.65 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 00:40:53.084003: step 993, loss = 29.60 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 00:40:58.559806: step 994, loss = 29.56 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 00:41:04.179503: step 995, loss = 29.43 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 00:41:09.707126: step 996, loss = 29.35 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 00:41:15.907321: step 997, loss = 29.34 (10.3 examples/sec; 6.200 sec/batch)
2016-04-29 00:41:21.148459: step 998, loss = 29.32 (12.2 examples/sec; 5.241 sec/batch)
2016-04-29 00:41:26.882458: step 999, loss = 29.30 (11.2 examples/sec; 5.734 sec/batch)
2016-04-29 00:41:32.508829: step 1000, loss = 29.44 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 00:41:45.347562: step 1001, loss = 29.26 (11.8 examples/sec; 5.408 sec/batch)
2016-04-29 00:41:51.215443: step 1002, loss = 29.33 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 00:41:56.704984: step 1003, loss = 29.14 (11.7 examples/sec; 5.489 sec/batch)
2016-04-29 00:42:02.230341: step 1004, loss = 29.25 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 00:42:07.930064: step 1005, loss = 29.16 (11.2 examples/sec; 5.700 sec/batch)
2016-04-29 00:42:13.609394: step 1006, loss = 29.08 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 00:42:19.042496: step 1007, loss = 28.96 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 00:42:24.937796: step 1008, loss = 29.02 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 00:42:30.223775: step 1009, loss = 29.20 (12.1 examples/sec; 5.286 sec/batch)
2016-04-29 00:42:35.874768: step 1010, loss = 28.97 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 00:42:49.741213: step 1011, loss = 29.25 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 00:42:56.033247: step 1012, loss = 29.19 (10.2 examples/sec; 6.292 sec/batch)
2016-04-29 00:43:01.648205: step 1013, loss = 29.08 (11.4 examples/sec; 5.615 sec/batch)
2016-04-29 00:43:07.168482: step 1014, loss = 29.06 (11.6 examples/sec; 5.520 sec/batch)
2016-04-29 00:43:12.317406: step 1015, loss = 28.85 (12.4 examples/sec; 5.149 sec/batch)
2016-04-29 00:43:17.773261: step 1016, loss = 28.80 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 00:43:23.289586: step 1017, loss = 28.81 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 00:43:29.359276: step 1018, loss = 28.84 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 00:43:35.243130: step 1019, loss = 29.07 (10.9 examples/sec; 5.884 sec/batch)
2016-04-29 00:43:40.624043: step 1020, loss = 29.02 (11.9 examples/sec; 5.381 sec/batch)
2016-04-29 00:43:53.651124: step 1021, loss = 28.84 (11.1 examples/sec; 5.773 sec/batch)
2016-04-29 00:43:59.805387: step 1022, loss = 28.81 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 00:44:05.441437: step 1023, loss = 28.79 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 00:44:10.721781: step 1024, loss = 28.69 (12.1 examples/sec; 5.280 sec/batch)
2016-04-29 00:44:16.193320: step 1025, loss = 28.82 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 00:44:21.857418: step 1026, loss = 28.73 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 00:44:27.099760: step 1027, loss = 28.77 (12.2 examples/sec; 5.242 sec/batch)
2016-04-29 00:44:33.135500: step 1028, loss = 28.50 (10.6 examples/sec; 6.036 sec/batch)
2016-04-29 00:44:38.891360: step 1029, loss = 28.51 (11.1 examples/sec; 5.756 sec/batch)
2016-04-29 00:44:44.045693: step 1030, loss = 28.74 (12.4 examples/sec; 5.154 sec/batch)
2016-04-29 00:44:56.651687: step 1031, loss = 28.51 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 00:45:02.303068: step 1032, loss = 28.63 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 00:45:08.323608: step 1033, loss = 28.57 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 00:45:13.784131: step 1034, loss = 28.55 (11.7 examples/sec; 5.460 sec/batch)
2016-04-29 00:45:19.110870: step 1035, loss = 28.70 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 00:45:24.550635: step 1036, loss = 28.70 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 00:45:30.035936: step 1037, loss = 28.48 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 00:45:35.785735: step 1038, loss = 28.58 (11.1 examples/sec; 5.750 sec/batch)
2016-04-29 00:45:41.738449: step 1039, loss = 28.38 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 00:45:47.172758: step 1040, loss = 28.49 (11.8 examples/sec; 5.434 sec/batch)
2016-04-29 00:45:59.662915: step 1041, loss = 28.40 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 00:46:05.581317: step 1042, loss = 28.17 (10.8 examples/sec; 5.918 sec/batch)
2016-04-29 00:46:12.032761: step 1043, loss = 28.45 (9.9 examples/sec; 6.451 sec/batch)
2016-04-29 00:46:17.674582: step 1044, loss = 28.35 (11.3 examples/sec; 5.642 sec/batch)
2016-04-29 00:46:23.076041: step 1045, loss = 28.41 (11.8 examples/sec; 5.401 sec/batch)
2016-04-29 00:46:28.711864: step 1046, loss = 28.21 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 00:46:34.200493: step 1047, loss = 28.12 (11.7 examples/sec; 5.489 sec/batch)
2016-04-29 00:46:39.322789: step 1048, loss = 28.38 (12.5 examples/sec; 5.122 sec/batch)
2016-04-29 00:46:45.370550: step 1049, loss = 28.36 (10.6 examples/sec; 6.048 sec/batch)
2016-04-29 00:46:50.779474: step 1050, loss = 28.40 (11.8 examples/sec; 5.409 sec/batch)
2016-04-29 00:47:03.760799: step 1051, loss = 28.17 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 00:47:08.972027: step 1052, loss = 28.03 (12.3 examples/sec; 5.211 sec/batch)
2016-04-29 00:47:14.253286: step 1053, loss = 28.08 (12.1 examples/sec; 5.281 sec/batch)
2016-04-29 00:47:20.460242: step 1054, loss = 28.01 (10.3 examples/sec; 6.207 sec/batch)
2016-04-29 00:47:25.957935: step 1055, loss = 27.95 (11.6 examples/sec; 5.498 sec/batch)
2016-04-29 00:47:31.343493: step 1056, loss = 28.04 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 00:47:36.754278: step 1057, loss = 28.05 (11.8 examples/sec; 5.411 sec/batch)
2016-04-29 00:47:42.058590: step 1058, loss = 28.05 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 00:47:47.553870: step 1059, loss = 28.00 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 00:47:53.707666: step 1060, loss = 27.92 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 00:48:06.943251: step 1061, loss = 27.88 (11.8 examples/sec; 5.405 sec/batch)
2016-04-29 00:48:12.276366: step 1062, loss = 27.96 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 00:48:17.987926: step 1063, loss = 27.97 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 00:48:23.987823: step 1064, loss = 28.09 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 00:48:29.442374: step 1065, loss = 27.91 (11.7 examples/sec; 5.454 sec/batch)
2016-04-29 00:48:35.039927: step 1066, loss = 27.83 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 00:48:40.368144: step 1067, loss = 27.84 (12.0 examples/sec; 5.328 sec/batch)
2016-04-29 00:48:45.955353: step 1068, loss = 27.79 (11.5 examples/sec; 5.587 sec/batch)
2016-04-29 00:48:51.378288: step 1069, loss = 27.85 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 00:48:57.464078: step 1070, loss = 27.77 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 00:49:10.653271: step 1071, loss = 27.82 (11.7 examples/sec; 5.479 sec/batch)
2016-04-29 00:49:15.980491: step 1072, loss = 27.78 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 00:49:21.263735: step 1073, loss = 27.80 (12.1 examples/sec; 5.283 sec/batch)
2016-04-29 00:49:27.096480: step 1074, loss = 27.65 (11.0 examples/sec; 5.833 sec/batch)
2016-04-29 00:49:33.110116: step 1075, loss = 27.71 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 00:49:38.489172: step 1076, loss = 27.63 (11.9 examples/sec; 5.379 sec/batch)
2016-04-29 00:49:43.947465: step 1077, loss = 27.63 (11.7 examples/sec; 5.458 sec/batch)
2016-04-29 00:49:49.707571: step 1078, loss = 27.69 (11.1 examples/sec; 5.760 sec/batch)
2016-04-29 00:49:55.277535: step 1079, loss = 27.46 (11.5 examples/sec; 5.570 sec/batch)
2016-04-29 00:50:00.825863: step 1080, loss = 27.46 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 00:50:14.183831: step 1081, loss = 27.65 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 00:50:20.068426: step 1082, loss = 27.47 (10.9 examples/sec; 5.885 sec/batch)
2016-04-29 00:50:25.380359: step 1083, loss = 27.55 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 00:50:30.951821: step 1084, loss = 27.57 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 00:50:37.154067: step 1085, loss = 27.53 (10.3 examples/sec; 6.202 sec/batch)
2016-04-29 00:50:42.280505: step 1086, loss = 27.48 (12.5 examples/sec; 5.126 sec/batch)
2016-04-29 00:50:47.870157: step 1087, loss = 27.61 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 00:50:53.672543: step 1088, loss = 27.45 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 00:50:59.144855: step 1089, loss = 27.54 (11.7 examples/sec; 5.472 sec/batch)
2016-04-29 00:51:04.613700: step 1090, loss = 27.36 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 00:51:17.668326: step 1091, loss = 27.26 (12.2 examples/sec; 5.247 sec/batch)
2016-04-29 00:51:23.100452: step 1092, loss = 27.42 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 00:51:28.627894: step 1093, loss = 27.36 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 00:51:34.071220: step 1094, loss = 27.27 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 00:51:39.542703: step 1095, loss = 27.25 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 00:51:45.486874: step 1096, loss = 27.25 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 00:51:51.000465: step 1097, loss = 27.16 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 00:51:56.490549: step 1098, loss = 27.21 (11.7 examples/sec; 5.490 sec/batch)
2016-04-29 00:52:02.268822: step 1099, loss = 27.29 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 00:52:07.788343: step 1100, loss = 27.13 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 00:52:21.521335: step 1101, loss = 27.17 (11.5 examples/sec; 5.558 sec/batch)
2016-04-29 00:52:26.886172: step 1102, loss = 27.13 (11.9 examples/sec; 5.363 sec/batch)
2016-04-29 00:52:32.387763: step 1103, loss = 27.00 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 00:52:38.028869: step 1104, loss = 26.99 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 00:52:43.675712: step 1105, loss = 27.11 (11.3 examples/sec; 5.647 sec/batch)
2016-04-29 00:52:49.773224: step 1106, loss = 27.09 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 00:52:55.432073: step 1107, loss = 27.10 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 00:53:00.939274: step 1108, loss = 27.07 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 00:53:06.196355: step 1109, loss = 27.18 (12.2 examples/sec; 5.257 sec/batch)
2016-04-29 00:53:11.537130: step 1110, loss = 27.06 (12.0 examples/sec; 5.341 sec/batch)
2016-04-29 00:53:24.471801: step 1111, loss = 26.89 (11.0 examples/sec; 5.829 sec/batch)
2016-04-29 00:53:29.955607: step 1112, loss = 27.15 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 00:53:35.581527: step 1113, loss = 27.02 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 00:53:41.352525: step 1114, loss = 26.81 (11.1 examples/sec; 5.771 sec/batch)
2016-04-29 00:53:46.754235: step 1115, loss = 26.80 (11.8 examples/sec; 5.402 sec/batch)
2016-04-29 00:53:52.006853: step 1116, loss = 26.78 (12.2 examples/sec; 5.253 sec/batch)
2016-04-29 00:53:58.060129: step 1117, loss = 26.56 (10.6 examples/sec; 6.053 sec/batch)
2016-04-29 00:54:03.425384: step 1118, loss = 26.86 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 00:54:08.868669: step 1119, loss = 26.65 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 00:54:14.288738: step 1120, loss = 26.76 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 00:54:27.242104: step 1121, loss = 26.68 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 00:54:32.906827: step 1122, loss = 26.75 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 00:54:38.254428: step 1123, loss = 26.56 (12.0 examples/sec; 5.347 sec/batch)
2016-04-29 00:54:43.739488: step 1124, loss = 26.69 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 00:54:48.932710: step 1125, loss = 26.72 (12.3 examples/sec; 5.193 sec/batch)
2016-04-29 00:54:54.219629: step 1126, loss = 26.58 (12.1 examples/sec; 5.287 sec/batch)
2016-04-29 00:54:59.963993: step 1127, loss = 26.57 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 00:55:05.468074: step 1128, loss = 26.52 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 00:55:10.900396: step 1129, loss = 26.62 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 00:55:16.313316: step 1130, loss = 26.64 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 00:55:29.061481: step 1131, loss = 26.39 (11.8 examples/sec; 5.405 sec/batch)
2016-04-29 00:55:34.862228: step 1132, loss = 26.45 (11.0 examples/sec; 5.800 sec/batch)
2016-04-29 00:55:40.394625: step 1133, loss = 26.61 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 00:55:46.030713: step 1134, loss = 26.58 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 00:55:51.275746: step 1135, loss = 26.35 (12.2 examples/sec; 5.245 sec/batch)
2016-04-29 00:55:56.868753: step 1136, loss = 26.45 (11.4 examples/sec; 5.593 sec/batch)
2016-04-29 00:56:02.654091: step 1137, loss = 26.39 (11.1 examples/sec; 5.785 sec/batch)
2016-04-29 00:56:08.662857: step 1138, loss = 26.23 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 00:56:13.889417: step 1139, loss = 26.61 (12.2 examples/sec; 5.226 sec/batch)
2016-04-29 00:56:19.060116: step 1140, loss = 26.11 (12.4 examples/sec; 5.171 sec/batch)
2016-04-29 00:56:32.280779: step 1141, loss = 26.37 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 00:56:38.235253: step 1142, loss = 26.50 (10.7 examples/sec; 5.954 sec/batch)
2016-04-29 00:56:43.803732: step 1143, loss = 26.25 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 00:56:49.192160: step 1144, loss = 26.29 (11.9 examples/sec; 5.388 sec/batch)
2016-04-29 00:56:54.724826: step 1145, loss = 26.20 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 00:57:00.192654: step 1146, loss = 26.24 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 00:57:05.639776: step 1147, loss = 26.14 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 00:57:11.405078: step 1148, loss = 26.08 (11.1 examples/sec; 5.765 sec/batch)
2016-04-29 00:57:16.937115: step 1149, loss = 26.25 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 00:57:22.211187: step 1150, loss = 26.10 (12.1 examples/sec; 5.274 sec/batch)
2016-04-29 00:57:34.968070: step 1151, loss = 26.01 (11.5 examples/sec; 5.570 sec/batch)
2016-04-29 00:57:40.428060: step 1152, loss = 26.08 (11.7 examples/sec; 5.460 sec/batch)
2016-04-29 00:57:46.497928: step 1153, loss = 25.91 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 00:57:51.883993: step 1154, loss = 26.19 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 00:57:57.200506: step 1155, loss = 26.16 (12.0 examples/sec; 5.316 sec/batch)
2016-04-29 00:58:02.958953: step 1156, loss = 26.17 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 00:58:08.268136: step 1157, loss = 26.06 (12.1 examples/sec; 5.309 sec/batch)
2016-04-29 00:58:13.677277: step 1158, loss = 25.77 (11.8 examples/sec; 5.409 sec/batch)
2016-04-29 00:58:19.728429: step 1159, loss = 26.04 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 00:58:24.984130: step 1160, loss = 25.88 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 00:58:37.717333: step 1161, loss = 26.05 (12.1 examples/sec; 5.271 sec/batch)
2016-04-29 00:58:43.314801: step 1162, loss = 26.04 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 00:58:48.648445: step 1163, loss = 25.79 (12.0 examples/sec; 5.334 sec/batch)
2016-04-29 00:58:54.808081: step 1164, loss = 25.99 (10.4 examples/sec; 6.159 sec/batch)
2016-04-29 00:59:00.420303: step 1165, loss = 26.02 (11.4 examples/sec; 5.612 sec/batch)
2016-04-29 00:59:06.044507: step 1166, loss = 25.84 (11.4 examples/sec; 5.624 sec/batch)
2016-04-29 00:59:11.391732: step 1167, loss = 25.78 (12.0 examples/sec; 5.347 sec/batch)
2016-04-29 00:59:16.879957: step 1168, loss = 25.74 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 00:59:22.082237: step 1169, loss = 25.65 (12.3 examples/sec; 5.202 sec/batch)
2016-04-29 00:59:27.900623: step 1170, loss = 25.85 (11.0 examples/sec; 5.818 sec/batch)
2016-04-29 00:59:40.708778: step 1171, loss = 25.65 (12.1 examples/sec; 5.284 sec/batch)
2016-04-29 00:59:46.012199: step 1172, loss = 25.66 (12.1 examples/sec; 5.303 sec/batch)
2016-04-29 00:59:51.870914: step 1173, loss = 25.81 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 00:59:57.843196: step 1174, loss = 25.48 (10.7 examples/sec; 5.972 sec/batch)
2016-04-29 01:00:03.439389: step 1175, loss = 25.64 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 01:00:08.750440: step 1176, loss = 25.64 (12.1 examples/sec; 5.311 sec/batch)
2016-04-29 01:00:14.237334: step 1177, loss = 25.70 (11.7 examples/sec; 5.487 sec/batch)
2016-04-29 01:00:19.557551: step 1178, loss = 25.63 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 01:00:24.615226: step 1179, loss = 25.41 (12.7 examples/sec; 5.058 sec/batch)
2016-04-29 01:00:30.447028: step 1180, loss = 25.40 (11.0 examples/sec; 5.832 sec/batch)
2016-04-29 01:00:43.033681: step 1181, loss = 25.51 (12.6 examples/sec; 5.076 sec/batch)
2016-04-29 01:00:48.397412: step 1182, loss = 25.73 (11.9 examples/sec; 5.364 sec/batch)
2016-04-29 01:00:53.759121: step 1183, loss = 25.33 (11.9 examples/sec; 5.362 sec/batch)
2016-04-29 01:00:59.109920: step 1184, loss = 25.49 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 01:01:04.984346: step 1185, loss = 25.45 (10.9 examples/sec; 5.874 sec/batch)
2016-04-29 01:01:10.181937: step 1186, loss = 25.55 (12.3 examples/sec; 5.197 sec/batch)
2016-04-29 01:01:15.512472: step 1187, loss = 25.36 (12.0 examples/sec; 5.330 sec/batch)
2016-04-29 01:01:21.150150: step 1188, loss = 25.34 (11.4 examples/sec; 5.638 sec/batch)
2016-04-29 01:01:26.642630: step 1189, loss = 25.28 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 01:01:32.088841: step 1190, loss = 25.41 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 01:01:44.938387: step 1191, loss = 25.43 (11.9 examples/sec; 5.360 sec/batch)
2016-04-29 01:01:50.227519: step 1192, loss = 25.31 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 01:01:55.409572: step 1193, loss = 25.25 (12.4 examples/sec; 5.182 sec/batch)
2016-04-29 01:02:00.711807: step 1194, loss = 25.37 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 01:02:05.980921: step 1195, loss = 25.20 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 01:02:11.704886: step 1196, loss = 25.24 (11.2 examples/sec; 5.724 sec/batch)
2016-04-29 01:02:17.140198: step 1197, loss = 25.33 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 01:02:22.097149: step 1198, loss = 25.09 (12.9 examples/sec; 4.957 sec/batch)
2016-04-29 01:02:27.168942: step 1199, loss = 25.24 (12.6 examples/sec; 5.072 sec/batch)
2016-04-29 01:02:32.659104: step 1200, loss = 25.12 (11.7 examples/sec; 5.490 sec/batch)
2016-04-29 01:02:45.514540: step 1201, loss = 25.16 (11.2 examples/sec; 5.737 sec/batch)
2016-04-29 01:02:50.966010: step 1202, loss = 25.21 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 01:02:56.434797: step 1203, loss = 25.13 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 01:03:02.040692: step 1204, loss = 25.06 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 01:03:07.105305: step 1205, loss = 25.13 (12.6 examples/sec; 5.065 sec/batch)
2016-04-29 01:03:12.581181: step 1206, loss = 25.07 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 01:03:18.503767: step 1207, loss = 25.00 (10.8 examples/sec; 5.922 sec/batch)
2016-04-29 01:03:23.906165: step 1208, loss = 25.15 (11.8 examples/sec; 5.402 sec/batch)
2016-04-29 01:03:29.140840: step 1209, loss = 24.93 (12.2 examples/sec; 5.235 sec/batch)
2016-04-29 01:03:34.134672: step 1210, loss = 24.92 (12.8 examples/sec; 4.994 sec/batch)
2016-04-29 01:03:46.662575: step 1211, loss = 24.96 (12.4 examples/sec; 5.171 sec/batch)
2016-04-29 01:03:52.476529: step 1212, loss = 25.02 (11.0 examples/sec; 5.814 sec/batch)
2016-04-29 01:03:58.017839: step 1213, loss = 24.93 (11.5 examples/sec; 5.541 sec/batch)
2016-04-29 01:04:03.643233: step 1214, loss = 24.93 (11.4 examples/sec; 5.625 sec/batch)
2016-04-29 01:04:08.923761: step 1215, loss = 24.87 (12.1 examples/sec; 5.280 sec/batch)
2016-04-29 01:04:14.295284: step 1216, loss = 24.76 (11.9 examples/sec; 5.371 sec/batch)
2016-04-29 01:04:19.628583: step 1217, loss = 24.90 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 01:04:25.429418: step 1218, loss = 24.84 (11.0 examples/sec; 5.801 sec/batch)
2016-04-29 01:04:30.750073: step 1219, loss = 25.01 (12.0 examples/sec; 5.321 sec/batch)
2016-04-29 01:04:36.214241: step 1220, loss = 24.91 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 01:04:48.492472: step 1221, loss = 24.72 (12.4 examples/sec; 5.181 sec/batch)
2016-04-29 01:04:54.460014: step 1222, loss = 24.62 (10.7 examples/sec; 5.967 sec/batch)
2016-04-29 01:05:00.068926: step 1223, loss = 24.66 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 01:05:05.550648: step 1224, loss = 24.70 (11.7 examples/sec; 5.482 sec/batch)
2016-04-29 01:05:11.024697: step 1225, loss = 24.73 (11.7 examples/sec; 5.474 sec/batch)
2016-04-29 01:05:16.190988: step 1226, loss = 24.67 (12.4 examples/sec; 5.166 sec/batch)
2016-04-29 01:05:21.430775: step 1227, loss = 24.65 (12.2 examples/sec; 5.240 sec/batch)
2016-04-29 01:05:27.465279: step 1228, loss = 24.63 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 01:05:32.866746: step 1229, loss = 24.66 (11.8 examples/sec; 5.401 sec/batch)
2016-04-29 01:05:38.447552: step 1230, loss = 24.72 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 01:05:50.961392: step 1231, loss = 24.40 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 01:05:56.524199: step 1232, loss = 24.54 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 01:06:02.568650: step 1233, loss = 24.69 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 01:06:07.820639: step 1234, loss = 24.53 (12.2 examples/sec; 5.252 sec/batch)
2016-04-29 01:06:12.999276: step 1235, loss = 24.63 (12.4 examples/sec; 5.179 sec/batch)
2016-04-29 01:06:18.454095: step 1236, loss = 24.58 (11.7 examples/sec; 5.455 sec/batch)
2016-04-29 01:06:24.042756: step 1237, loss = 24.36 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 01:06:29.401461: step 1238, loss = 24.39 (11.9 examples/sec; 5.359 sec/batch)
2016-04-29 01:06:35.296672: step 1239, loss = 24.37 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 01:06:40.576199: step 1240, loss = 24.51 (12.1 examples/sec; 5.279 sec/batch)
2016-04-29 01:06:53.064362: step 1241, loss = 24.42 (12.2 examples/sec; 5.237 sec/batch)
2016-04-29 01:06:58.370676: step 1242, loss = 24.41 (12.1 examples/sec; 5.306 sec/batch)
2016-04-29 01:07:04.154787: step 1243, loss = 24.41 (11.1 examples/sec; 5.784 sec/batch)
2016-04-29 01:07:09.830145: step 1244, loss = 24.56 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 01:07:15.393314: step 1245, loss = 24.22 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 01:07:20.881173: step 1246, loss = 24.45 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 01:07:26.207072: step 1247, loss = 24.43 (12.0 examples/sec; 5.326 sec/batch)
2016-04-29 01:07:31.393294: step 1248, loss = 24.33 (12.3 examples/sec; 5.186 sec/batch)
2016-04-29 01:07:36.905478: step 1249, loss = 24.41 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 01:07:42.874111: step 1250, loss = 24.24 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 01:07:55.621657: step 1251, loss = 24.08 (12.1 examples/sec; 5.284 sec/batch)
2016-04-29 01:08:01.160448: step 1252, loss = 24.16 (11.6 examples/sec; 5.539 sec/batch)
2016-04-29 01:08:06.779975: step 1253, loss = 24.02 (11.4 examples/sec; 5.619 sec/batch)
2016-04-29 01:08:12.750773: step 1254, loss = 24.12 (10.7 examples/sec; 5.971 sec/batch)
2016-04-29 01:08:18.399519: step 1255, loss = 24.03 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 01:08:23.733857: step 1256, loss = 23.99 (12.0 examples/sec; 5.334 sec/batch)
2016-04-29 01:08:29.073473: step 1257, loss = 24.24 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 01:08:34.422548: step 1258, loss = 24.18 (12.0 examples/sec; 5.349 sec/batch)
2016-04-29 01:08:39.723744: step 1259, loss = 24.10 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 01:08:45.871548: step 1260, loss = 24.19 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 01:08:58.534725: step 1261, loss = 23.86 (12.3 examples/sec; 5.199 sec/batch)
2016-04-29 01:09:03.981885: step 1262, loss = 23.98 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 01:09:09.634088: step 1263, loss = 23.99 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 01:09:15.279535: step 1264, loss = 24.10 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 01:09:21.202675: step 1265, loss = 24.03 (10.8 examples/sec; 5.923 sec/batch)
2016-04-29 01:09:26.617094: step 1266, loss = 23.93 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 01:09:32.252626: step 1267, loss = 24.11 (11.4 examples/sec; 5.635 sec/batch)
2016-04-29 01:09:37.352169: step 1268, loss = 23.89 (12.6 examples/sec; 5.099 sec/batch)
2016-04-29 01:09:42.815958: step 1269, loss = 23.85 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 01:09:48.187469: step 1270, loss = 23.80 (11.9 examples/sec; 5.371 sec/batch)
2016-04-29 01:10:01.378756: step 1271, loss = 23.75 (12.1 examples/sec; 5.303 sec/batch)
2016-04-29 01:10:06.798824: step 1272, loss = 23.85 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 01:10:12.476932: step 1273, loss = 23.84 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 01:10:17.802223: step 1274, loss = 23.88 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 01:10:23.496221: step 1275, loss = 23.66 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 01:10:29.140530: step 1276, loss = 23.97 (11.3 examples/sec; 5.644 sec/batch)
2016-04-29 01:10:34.266122: step 1277, loss = 23.67 (12.5 examples/sec; 5.126 sec/batch)
2016-04-29 01:10:39.514542: step 1278, loss = 23.77 (12.2 examples/sec; 5.248 sec/batch)
2016-04-29 01:10:44.865302: step 1279, loss = 23.76 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 01:10:50.333729: step 1280, loss = 23.74 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 01:11:03.593876: step 1281, loss = 23.64 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 01:11:09.151800: step 1282, loss = 23.66 (11.5 examples/sec; 5.558 sec/batch)
2016-04-29 01:11:14.665279: step 1283, loss = 23.66 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 01:11:20.246646: step 1284, loss = 23.54 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 01:11:25.633276: step 1285, loss = 23.58 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 01:11:31.503111: step 1286, loss = 23.57 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 01:11:36.984433: step 1287, loss = 23.56 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 01:11:42.494511: step 1288, loss = 23.52 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 01:11:48.318449: step 1289, loss = 23.58 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 01:11:53.750060: step 1290, loss = 23.61 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 01:12:07.040003: step 1291, loss = 23.55 (11.0 examples/sec; 5.800 sec/batch)
2016-04-29 01:12:12.729196: step 1292, loss = 23.49 (11.2 examples/sec; 5.689 sec/batch)
2016-04-29 01:12:18.194305: step 1293, loss = 23.55 (11.7 examples/sec; 5.465 sec/batch)
2016-04-29 01:12:23.728958: step 1294, loss = 23.56 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 01:12:29.223340: step 1295, loss = 23.38 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 01:12:34.383317: step 1296, loss = 23.56 (12.4 examples/sec; 5.160 sec/batch)
2016-04-29 01:12:40.418909: step 1297, loss = 23.49 (10.6 examples/sec; 6.036 sec/batch)
2016-04-29 01:12:45.812923: step 1298, loss = 23.52 (11.9 examples/sec; 5.394 sec/batch)
2016-04-29 01:12:51.118866: step 1299, loss = 23.29 (12.1 examples/sec; 5.306 sec/batch)
2016-04-29 01:12:56.629924: step 1300, loss = 23.37 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 01:13:09.757156: step 1301, loss = 23.37 (11.0 examples/sec; 5.794 sec/batch)
2016-04-29 01:13:15.450591: step 1302, loss = 23.27 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 01:13:20.957504: step 1303, loss = 23.36 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 01:13:26.531240: step 1304, loss = 23.22 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 01:13:32.321335: step 1305, loss = 23.17 (11.1 examples/sec; 5.790 sec/batch)
2016-04-29 01:13:37.477757: step 1306, loss = 23.14 (12.4 examples/sec; 5.156 sec/batch)
2016-04-29 01:13:43.155134: step 1307, loss = 23.17 (11.3 examples/sec; 5.677 sec/batch)
2016-04-29 01:13:48.537757: step 1308, loss = 23.48 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 01:13:53.962548: step 1309, loss = 23.11 (11.8 examples/sec; 5.425 sec/batch)
2016-04-29 01:13:59.347945: step 1310, loss = 23.21 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 01:14:11.827043: step 1311, loss = 23.10 (12.3 examples/sec; 5.211 sec/batch)
2016-04-29 01:14:17.903837: step 1312, loss = 23.16 (10.5 examples/sec; 6.077 sec/batch)
2016-04-29 01:14:23.417339: step 1313, loss = 23.18 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 01:14:28.788319: step 1314, loss = 23.15 (11.9 examples/sec; 5.371 sec/batch)
2016-04-29 01:14:34.114300: step 1315, loss = 22.94 (12.0 examples/sec; 5.326 sec/batch)
2016-04-29 01:14:39.399554: step 1316, loss = 23.08 (12.1 examples/sec; 5.285 sec/batch)
2016-04-29 01:14:44.800419: step 1317, loss = 22.94 (11.9 examples/sec; 5.401 sec/batch)
2016-04-29 01:14:50.782309: step 1318, loss = 23.08 (10.7 examples/sec; 5.982 sec/batch)
2016-04-29 01:14:56.115579: step 1319, loss = 22.88 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 01:15:01.555164: step 1320, loss = 23.16 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 01:15:14.508495: step 1321, loss = 23.07 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 01:15:19.677942: step 1322, loss = 22.95 (12.4 examples/sec; 5.169 sec/batch)
2016-04-29 01:15:25.669001: step 1323, loss = 22.86 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 01:15:31.015311: step 1324, loss = 23.00 (12.0 examples/sec; 5.346 sec/batch)
2016-04-29 01:15:36.521644: step 1325, loss = 22.92 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 01:15:42.441535: step 1326, loss = 23.00 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 01:15:48.134853: step 1327, loss = 22.80 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 01:15:54.154072: step 1328, loss = 22.96 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 01:15:59.516570: step 1329, loss = 22.76 (11.9 examples/sec; 5.362 sec/batch)
2016-04-29 01:16:04.661666: step 1330, loss = 22.67 (12.4 examples/sec; 5.145 sec/batch)
2016-04-29 01:16:17.424642: step 1331, loss = 22.72 (11.9 examples/sec; 5.384 sec/batch)
2016-04-29 01:16:22.557463: step 1332, loss = 22.94 (12.5 examples/sec; 5.133 sec/batch)
2016-04-29 01:16:28.508403: step 1333, loss = 22.81 (10.8 examples/sec; 5.951 sec/batch)
2016-04-29 01:16:34.015570: step 1334, loss = 22.88 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 01:16:39.173011: step 1335, loss = 22.90 (12.4 examples/sec; 5.157 sec/batch)
2016-04-29 01:16:44.649792: step 1336, loss = 22.75 (11.7 examples/sec; 5.477 sec/batch)
2016-04-29 01:16:50.211522: step 1337, loss = 22.76 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 01:16:55.274698: step 1338, loss = 22.81 (12.6 examples/sec; 5.063 sec/batch)
2016-04-29 01:17:01.579837: step 1339, loss = 22.67 (10.2 examples/sec; 6.305 sec/batch)
2016-04-29 01:17:06.845171: step 1340, loss = 22.66 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 01:17:19.449653: step 1341, loss = 22.59 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 01:17:25.123712: step 1342, loss = 22.64 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 01:17:30.626882: step 1343, loss = 22.71 (11.6 examples/sec; 5.503 sec/batch)
2016-04-29 01:17:36.624857: step 1344, loss = 22.43 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 01:17:42.034885: step 1345, loss = 22.56 (11.8 examples/sec; 5.410 sec/batch)
2016-04-29 01:17:47.480336: step 1346, loss = 22.41 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 01:17:52.768975: step 1347, loss = 22.46 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 01:17:58.135698: step 1348, loss = 22.45 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 01:18:03.824521: step 1349, loss = 22.49 (11.3 examples/sec; 5.689 sec/batch)
2016-04-29 01:18:09.457808: step 1350, loss = 22.56 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 01:18:21.974421: step 1351, loss = 22.52 (12.4 examples/sec; 5.174 sec/batch)
2016-04-29 01:18:27.539346: step 1352, loss = 22.50 (11.5 examples/sec; 5.565 sec/batch)
2016-04-29 01:18:33.124340: step 1353, loss = 22.38 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 01:18:38.892676: step 1354, loss = 22.22 (11.1 examples/sec; 5.768 sec/batch)
2016-04-29 01:18:44.340512: step 1355, loss = 22.41 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 01:18:49.580871: step 1356, loss = 22.25 (12.2 examples/sec; 5.240 sec/batch)
2016-04-29 01:18:54.964412: step 1357, loss = 22.45 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 01:19:00.802910: step 1358, loss = 22.37 (11.0 examples/sec; 5.838 sec/batch)
2016-04-29 01:19:06.211316: step 1359, loss = 22.44 (11.8 examples/sec; 5.408 sec/batch)
2016-04-29 01:19:12.445925: step 1360, loss = 22.33 (10.3 examples/sec; 6.235 sec/batch)
2016-04-29 01:19:24.928734: step 1361, loss = 22.16 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 01:19:30.508929: step 1362, loss = 22.25 (11.5 examples/sec; 5.580 sec/batch)
2016-04-29 01:19:36.016291: step 1363, loss = 22.34 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 01:19:41.535102: step 1364, loss = 22.19 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 01:19:47.387242: step 1365, loss = 22.28 (10.9 examples/sec; 5.852 sec/batch)
2016-04-29 01:19:52.736162: step 1366, loss = 22.37 (12.0 examples/sec; 5.349 sec/batch)
2016-04-29 01:19:58.115097: step 1367, loss = 22.20 (11.9 examples/sec; 5.379 sec/batch)
2016-04-29 01:20:03.501222: step 1368, loss = 22.18 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 01:20:08.871669: step 1369, loss = 22.23 (11.9 examples/sec; 5.370 sec/batch)
2016-04-29 01:20:14.254537: step 1370, loss = 22.00 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 01:20:27.276619: step 1371, loss = 22.14 (11.9 examples/sec; 5.374 sec/batch)
2016-04-29 01:20:32.735435: step 1372, loss = 21.98 (11.7 examples/sec; 5.459 sec/batch)
2016-04-29 01:20:37.928928: step 1373, loss = 22.18 (12.3 examples/sec; 5.193 sec/batch)
2016-04-29 01:20:43.296432: step 1374, loss = 21.98 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 01:20:48.780861: step 1375, loss = 22.12 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 01:20:54.508437: step 1376, loss = 21.91 (11.2 examples/sec; 5.727 sec/batch)
2016-04-29 01:20:59.816202: step 1377, loss = 22.06 (12.1 examples/sec; 5.308 sec/batch)
2016-04-29 01:21:05.456515: step 1378, loss = 21.79 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 01:21:11.019085: step 1379, loss = 22.03 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 01:21:16.298087: step 1380, loss = 21.91 (12.1 examples/sec; 5.279 sec/batch)
2016-04-29 01:21:29.469442: step 1381, loss = 22.00 (11.8 examples/sec; 5.434 sec/batch)
2016-04-29 01:21:34.588567: step 1382, loss = 21.96 (12.5 examples/sec; 5.119 sec/batch)
2016-04-29 01:21:40.094806: step 1383, loss = 21.88 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 01:21:45.395504: step 1384, loss = 21.91 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 01:21:50.746006: step 1385, loss = 21.95 (12.0 examples/sec; 5.350 sec/batch)
2016-04-29 01:21:56.047871: step 1386, loss = 21.86 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 01:22:02.004851: step 1387, loss = 21.84 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 01:22:07.274661: step 1388, loss = 21.96 (12.1 examples/sec; 5.270 sec/batch)
2016-04-29 01:22:12.993134: step 1389, loss = 21.87 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 01:22:18.291348: step 1390, loss = 21.64 (12.1 examples/sec; 5.298 sec/batch)
2016-04-29 01:22:31.424518: step 1391, loss = 21.85 (10.8 examples/sec; 5.903 sec/batch)
2016-04-29 01:22:36.950879: step 1392, loss = 22.00 (11.6 examples/sec; 5.526 sec/batch)
2016-04-29 01:22:42.479695: step 1393, loss = 21.74 (11.6 examples/sec; 5.529 sec/batch)
2016-04-29 01:22:47.923689: step 1394, loss = 21.88 (11.8 examples/sec; 5.444 sec/batch)
2016-04-29 01:22:53.149331: step 1395, loss = 21.75 (12.2 examples/sec; 5.226 sec/batch)
2016-04-29 01:22:58.488625: step 1396, loss = 21.79 (12.0 examples/sec; 5.339 sec/batch)
2016-04-29 01:23:04.566534: step 1397, loss = 21.64 (10.5 examples/sec; 6.078 sec/batch)
2016-04-29 01:23:09.768572: step 1398, loss = 21.51 (12.3 examples/sec; 5.202 sec/batch)
2016-04-29 01:23:15.218904: step 1399, loss = 21.76 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 01:23:20.385212: step 1400, loss = 21.66 (12.4 examples/sec; 5.166 sec/batch)
2016-04-29 01:23:33.261879: step 1401, loss = 21.54 (11.7 examples/sec; 5.477 sec/batch)
2016-04-29 01:23:39.052901: step 1402, loss = 21.71 (11.1 examples/sec; 5.791 sec/batch)
2016-04-29 01:23:44.317561: step 1403, loss = 21.66 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 01:23:49.898752: step 1404, loss = 21.61 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 01:23:55.913119: step 1405, loss = 21.44 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 01:24:01.602859: step 1406, loss = 21.58 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 01:24:07.286654: step 1407, loss = 21.37 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 01:24:15.094777: step 1408, loss = 21.35 (8.2 examples/sec; 7.808 sec/batch)
2016-04-29 01:24:21.455761: step 1409, loss = 21.46 (10.1 examples/sec; 6.361 sec/batch)
2016-04-29 01:24:27.318242: step 1410, loss = 21.53 (10.9 examples/sec; 5.862 sec/batch)
2016-04-29 01:24:40.063422: step 1411, loss = 21.31 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 01:24:45.991609: step 1412, loss = 21.51 (10.8 examples/sec; 5.928 sec/batch)
2016-04-29 01:24:51.290146: step 1413, loss = 21.37 (12.1 examples/sec; 5.298 sec/batch)
2016-04-29 01:24:56.886054: step 1414, loss = 21.51 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 01:25:02.318345: step 1415, loss = 21.47 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 01:25:07.679637: step 1416, loss = 21.37 (11.9 examples/sec; 5.361 sec/batch)
2016-04-29 01:25:13.173296: step 1417, loss = 21.44 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 01:25:19.029616: step 1418, loss = 21.30 (10.9 examples/sec; 5.856 sec/batch)
2016-04-29 01:25:24.534850: step 1419, loss = 21.18 (11.6 examples/sec; 5.505 sec/batch)
2016-04-29 01:25:29.937465: step 1420, loss = 21.40 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 01:25:42.657850: step 1421, loss = 21.37 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 01:25:48.465184: step 1422, loss = 21.34 (11.0 examples/sec; 5.807 sec/batch)
2016-04-29 01:25:54.064987: step 1423, loss = 21.18 (11.4 examples/sec; 5.600 sec/batch)
2016-04-29 01:25:59.056904: step 1424, loss = 21.19 (12.8 examples/sec; 4.992 sec/batch)
2016-04-29 01:26:04.631305: step 1425, loss = 21.30 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 01:26:10.390835: step 1426, loss = 21.19 (11.1 examples/sec; 5.759 sec/batch)
2016-04-29 01:26:15.883167: step 1427, loss = 21.22 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 01:26:21.812299: step 1428, loss = 21.07 (10.8 examples/sec; 5.929 sec/batch)
2016-04-29 01:26:27.347540: step 1429, loss = 21.29 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 01:26:33.039190: step 1430, loss = 21.09 (11.2 examples/sec; 5.692 sec/batch)
2016-04-29 01:26:45.718335: step 1431, loss = 21.21 (12.1 examples/sec; 5.309 sec/batch)
2016-04-29 01:26:51.421998: step 1432, loss = 21.05 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 01:26:57.573811: step 1433, loss = 21.13 (10.4 examples/sec; 6.152 sec/batch)
2016-04-29 01:27:03.273398: step 1434, loss = 21.03 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 01:27:08.727592: step 1435, loss = 21.14 (11.7 examples/sec; 5.454 sec/batch)
2016-04-29 01:27:13.980550: step 1436, loss = 21.09 (12.2 examples/sec; 5.253 sec/batch)
2016-04-29 01:27:19.294374: step 1437, loss = 20.97 (12.0 examples/sec; 5.314 sec/batch)
2016-04-29 01:27:24.718407: step 1438, loss = 20.99 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 01:27:30.728128: step 1439, loss = 21.20 (10.6 examples/sec; 6.010 sec/batch)
2016-04-29 01:27:36.194676: step 1440, loss = 20.90 (11.7 examples/sec; 5.466 sec/batch)
2016-04-29 01:27:49.234274: step 1441, loss = 20.98 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 01:27:54.907915: step 1442, loss = 21.02 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 01:28:01.768242: step 1443, loss = 20.96 (9.3 examples/sec; 6.859 sec/batch)
2016-04-29 01:28:07.355188: step 1444, loss = 20.90 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 01:28:13.182194: step 1445, loss = 20.93 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 01:28:18.705332: step 1446, loss = 21.03 (11.6 examples/sec; 5.523 sec/batch)
2016-04-29 01:28:24.079470: step 1447, loss = 20.82 (11.9 examples/sec; 5.374 sec/batch)
2016-04-29 01:28:29.227616: step 1448, loss = 20.86 (12.4 examples/sec; 5.148 sec/batch)
2016-04-29 01:28:35.178326: step 1449, loss = 20.74 (10.8 examples/sec; 5.951 sec/batch)
2016-04-29 01:28:40.491978: step 1450, loss = 20.70 (12.0 examples/sec; 5.314 sec/batch)
2016-04-29 01:28:52.849296: step 1451, loss = 20.99 (12.7 examples/sec; 5.036 sec/batch)
2016-04-29 01:28:58.680148: step 1452, loss = 20.85 (11.0 examples/sec; 5.831 sec/batch)
2016-04-29 01:29:04.345502: step 1453, loss = 20.70 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 01:29:10.310180: step 1454, loss = 20.67 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 01:29:16.015366: step 1455, loss = 20.90 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 01:29:21.682179: step 1456, loss = 20.92 (11.3 examples/sec; 5.667 sec/batch)
2016-04-29 01:29:27.010534: step 1457, loss = 20.78 (12.0 examples/sec; 5.328 sec/batch)
2016-04-29 01:29:32.013990: step 1458, loss = 20.65 (12.8 examples/sec; 5.003 sec/batch)
2016-04-29 01:29:37.483302: step 1459, loss = 20.46 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 01:29:43.301205: step 1460, loss = 20.64 (11.0 examples/sec; 5.818 sec/batch)
2016-04-29 01:29:55.673810: step 1461, loss = 20.68 (12.8 examples/sec; 5.010 sec/batch)
2016-04-29 01:30:01.140935: step 1462, loss = 20.68 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 01:30:06.715881: step 1463, loss = 20.56 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 01:30:12.241832: step 1464, loss = 20.62 (11.6 examples/sec; 5.526 sec/batch)
2016-04-29 01:30:17.904434: step 1465, loss = 20.59 (11.3 examples/sec; 5.662 sec/batch)
2016-04-29 01:30:23.027083: step 1466, loss = 20.61 (12.5 examples/sec; 5.123 sec/batch)
2016-04-29 01:30:28.356205: step 1467, loss = 20.63 (12.0 examples/sec; 5.329 sec/batch)
2016-04-29 01:30:34.008108: step 1468, loss = 20.60 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 01:30:39.432441: step 1469, loss = 20.60 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 01:30:44.841304: step 1470, loss = 20.57 (11.8 examples/sec; 5.409 sec/batch)
2016-04-29 01:30:57.957162: step 1471, loss = 20.38 (11.5 examples/sec; 5.565 sec/batch)
2016-04-29 01:31:03.508620: step 1472, loss = 20.49 (11.5 examples/sec; 5.551 sec/batch)
2016-04-29 01:31:08.984112: step 1473, loss = 20.60 (11.7 examples/sec; 5.475 sec/batch)
2016-04-29 01:31:14.091119: step 1474, loss = 20.53 (12.5 examples/sec; 5.107 sec/batch)
2016-04-29 01:31:20.040426: step 1475, loss = 20.27 (10.8 examples/sec; 5.949 sec/batch)
2016-04-29 01:31:25.536456: step 1476, loss = 20.40 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 01:31:31.065969: step 1477, loss = 20.53 (11.6 examples/sec; 5.529 sec/batch)
2016-04-29 01:31:36.636821: step 1478, loss = 20.37 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 01:31:42.117665: step 1479, loss = 20.37 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 01:31:47.370643: step 1480, loss = 20.48 (12.2 examples/sec; 5.253 sec/batch)
2016-04-29 01:32:00.720107: step 1481, loss = 20.48 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 01:32:06.033731: step 1482, loss = 20.38 (12.0 examples/sec; 5.314 sec/batch)
2016-04-29 01:32:11.465024: step 1483, loss = 20.51 (11.8 examples/sec; 5.431 sec/batch)
2016-04-29 01:32:16.629386: step 1484, loss = 20.44 (12.4 examples/sec; 5.164 sec/batch)
2016-04-29 01:32:22.186391: step 1485, loss = 20.35 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 01:32:27.979187: step 1486, loss = 20.26 (11.0 examples/sec; 5.793 sec/batch)
2016-04-29 01:32:33.628451: step 1487, loss = 20.35 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 01:32:38.755192: step 1488, loss = 20.29 (12.5 examples/sec; 5.127 sec/batch)
2016-04-29 01:32:43.936363: step 1489, loss = 20.25 (12.4 examples/sec; 5.181 sec/batch)
2016-04-29 01:32:49.340326: step 1490, loss = 20.27 (11.8 examples/sec; 5.404 sec/batch)
2016-04-29 01:33:02.528262: step 1491, loss = 20.25 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 01:33:07.785978: step 1492, loss = 20.29 (12.2 examples/sec; 5.258 sec/batch)
2016-04-29 01:33:13.248198: step 1493, loss = 20.05 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 01:33:18.802925: step 1494, loss = 19.93 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 01:33:24.109971: step 1495, loss = 20.24 (12.1 examples/sec; 5.307 sec/batch)
2016-04-29 01:33:29.365488: step 1496, loss = 20.15 (12.2 examples/sec; 5.255 sec/batch)
2016-04-29 01:33:35.400110: step 1497, loss = 20.17 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 01:33:40.846138: step 1498, loss = 20.03 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 01:33:46.321885: step 1499, loss = 20.02 (11.7 examples/sec; 5.476 sec/batch)

Process finished with exit code 0
