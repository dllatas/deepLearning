/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-28 18:10:57.910663: step 0, loss = 125.79 (0.3 examples/sec; 208.515 sec/batch)
2016-04-28 18:11:44.161759: step 1, loss = 125.85 (3.2 examples/sec; 20.052 sec/batch)
2016-04-28 18:12:03.890136: step 2, loss = 125.53 (3.2 examples/sec; 19.728 sec/batch)
2016-04-28 18:12:23.223507: step 3, loss = 125.52 (3.3 examples/sec; 19.333 sec/batch)
2016-04-28 18:12:51.639081: step 4, loss = 125.22 (2.3 examples/sec; 28.415 sec/batch)
2016-04-28 18:13:14.275036: step 5, loss = 125.22 (2.8 examples/sec; 22.636 sec/batch)
2016-04-28 18:13:35.541940: step 6, loss = 125.61 (3.0 examples/sec; 21.267 sec/batch)
2016-04-28 18:13:58.209473: step 7, loss = 125.06 (2.8 examples/sec; 22.667 sec/batch)
2016-04-28 18:14:19.684763: step 8, loss = 124.99 (3.0 examples/sec; 21.475 sec/batch)
2016-04-28 18:14:40.094236: step 9, loss = 124.88 (3.1 examples/sec; 20.409 sec/batch)
2016-04-28 18:15:02.624941: step 10, loss = 124.81 (2.8 examples/sec; 22.530 sec/batch)
2016-04-28 18:15:50.910422: step 11, loss = 124.72 (3.1 examples/sec; 20.684 sec/batch)
2016-04-28 18:16:13.136199: step 12, loss = 124.49 (2.9 examples/sec; 22.226 sec/batch)
2016-04-28 18:16:34.148705: step 13, loss = 124.49 (3.0 examples/sec; 21.012 sec/batch)
2016-04-28 18:16:55.551117: step 14, loss = 124.41 (3.0 examples/sec; 21.402 sec/batch)
2016-04-28 18:17:16.695950: step 15, loss = 124.50 (3.0 examples/sec; 21.145 sec/batch)
2016-04-28 18:17:38.215116: step 16, loss = 124.20 (3.0 examples/sec; 21.519 sec/batch)
2016-04-28 18:17:58.777337: step 17, loss = 124.11 (3.1 examples/sec; 20.562 sec/batch)
2016-04-28 18:18:20.189065: step 18, loss = 123.92 (3.0 examples/sec; 21.412 sec/batch)
2016-04-28 18:18:41.218695: step 19, loss = 123.94 (3.0 examples/sec; 21.030 sec/batch)
2016-04-28 18:19:02.090295: step 20, loss = 123.72 (3.1 examples/sec; 20.872 sec/batch)
2016-04-28 18:19:50.770031: step 21, loss = 123.52 (3.0 examples/sec; 21.156 sec/batch)
2016-04-28 18:20:11.498341: step 22, loss = 123.85 (3.1 examples/sec; 20.728 sec/batch)
2016-04-28 18:20:32.632288: step 23, loss = 123.44 (3.0 examples/sec; 21.134 sec/batch)
2016-04-28 18:20:55.132490: step 24, loss = 123.49 (2.8 examples/sec; 22.500 sec/batch)
2016-04-28 18:21:15.924977: step 25, loss = 123.24 (3.1 examples/sec; 20.792 sec/batch)
2016-04-28 18:21:37.124862: step 26, loss = 123.25 (3.0 examples/sec; 21.200 sec/batch)
2016-04-28 18:21:58.466184: step 27, loss = 123.20 (3.0 examples/sec; 21.341 sec/batch)
2016-04-28 18:22:19.334131: step 28, loss = 122.98 (3.1 examples/sec; 20.868 sec/batch)
2016-04-28 18:22:40.395538: step 29, loss = 122.71 (3.0 examples/sec; 21.061 sec/batch)
2016-04-28 18:23:01.584201: step 30, loss = 122.81 (3.0 examples/sec; 21.189 sec/batch)
2016-04-28 18:23:48.279042: step 31, loss = 122.60 (3.1 examples/sec; 20.910 sec/batch)
2016-04-28 18:24:09.779080: step 32, loss = 122.68 (3.0 examples/sec; 21.500 sec/batch)
2016-04-28 18:24:30.159038: step 33, loss = 122.43 (3.1 examples/sec; 20.380 sec/batch)
2016-04-28 18:24:51.284489: step 34, loss = 122.38 (3.0 examples/sec; 21.125 sec/batch)
2016-04-28 18:25:12.011035: step 35, loss = 122.23 (3.1 examples/sec; 20.726 sec/batch)
2016-04-28 18:25:33.863237: step 36, loss = 122.21 (2.9 examples/sec; 21.852 sec/batch)
2016-04-28 18:25:55.160608: step 37, loss = 122.13 (3.0 examples/sec; 21.297 sec/batch)
2016-04-28 18:26:15.692895: step 38, loss = 121.98 (3.1 examples/sec; 20.532 sec/batch)
2016-04-28 18:26:37.684486: step 39, loss = 121.82 (2.9 examples/sec; 21.991 sec/batch)
2016-04-28 18:26:58.911533: step 40, loss = 121.81 (3.0 examples/sec; 21.227 sec/batch)
2016-04-28 18:27:45.143195: step 41, loss = 121.53 (3.1 examples/sec; 20.688 sec/batch)
2016-04-28 18:28:07.345318: step 42, loss = 121.59 (2.9 examples/sec; 22.202 sec/batch)
2016-04-28 18:28:28.560527: step 43, loss = 121.46 (3.0 examples/sec; 21.215 sec/batch)
2016-04-28 18:28:49.581866: step 44, loss = 121.34 (3.0 examples/sec; 21.021 sec/batch)
2016-04-28 18:29:10.765584: step 45, loss = 121.31 (3.0 examples/sec; 21.184 sec/batch)
2016-04-28 18:29:31.400581: step 46, loss = 121.28 (3.1 examples/sec; 20.635 sec/batch)
2016-04-28 18:29:52.398776: step 47, loss = 120.95 (3.0 examples/sec; 20.998 sec/batch)
2016-04-28 18:30:13.444957: step 48, loss = 121.01 (3.0 examples/sec; 21.046 sec/batch)
2016-04-28 18:30:33.628681: step 49, loss = 120.91 (3.2 examples/sec; 20.184 sec/batch)
2016-04-28 18:30:54.668450: step 50, loss = 120.63 (3.0 examples/sec; 21.040 sec/batch)
2016-04-28 18:31:42.314876: step 51, loss = 120.93 (3.1 examples/sec; 20.967 sec/batch)
2016-04-28 18:32:03.666849: step 52, loss = 120.56 (3.0 examples/sec; 21.352 sec/batch)
2016-04-28 18:32:25.244082: step 53, loss = 120.49 (3.0 examples/sec; 21.577 sec/batch)
2016-04-28 18:32:45.531877: step 54, loss = 120.44 (3.2 examples/sec; 20.288 sec/batch)
2016-04-28 18:33:07.254562: step 55, loss = 120.33 (2.9 examples/sec; 21.723 sec/batch)
2016-04-28 18:33:28.347597: step 56, loss = 120.30 (3.0 examples/sec; 21.093 sec/batch)
2016-04-28 18:33:48.775171: step 57, loss = 120.11 (3.1 examples/sec; 20.427 sec/batch)
2016-04-28 18:34:09.711847: step 58, loss = 119.82 (3.1 examples/sec; 20.937 sec/batch)
2016-04-28 18:34:30.273493: step 59, loss = 119.90 (3.1 examples/sec; 20.562 sec/batch)
2016-04-28 18:34:51.283026: step 60, loss = 119.71 (3.0 examples/sec; 21.009 sec/batch)
2016-04-28 18:35:44.001632: step 61, loss = 119.63 (2.5 examples/sec; 25.856 sec/batch)
2016-04-28 18:36:06.419430: step 62, loss = 119.60 (2.9 examples/sec; 22.418 sec/batch)
2016-04-28 18:36:27.650335: step 63, loss = 119.60 (3.0 examples/sec; 21.231 sec/batch)
2016-04-28 18:36:52.867020: step 64, loss = 119.64 (2.5 examples/sec; 25.217 sec/batch)
2016-04-28 18:37:12.988768: step 65, loss = 119.30 (3.2 examples/sec; 20.122 sec/batch)
2016-04-28 18:37:32.520985: step 66, loss = 119.21 (3.3 examples/sec; 19.532 sec/batch)
2016-04-28 18:37:54.252179: step 67, loss = 119.27 (2.9 examples/sec; 21.731 sec/batch)
2016-04-28 18:38:15.545535: step 68, loss = 119.09 (3.0 examples/sec; 21.293 sec/batch)
2016-04-28 18:38:36.358212: step 69, loss = 118.93 (3.1 examples/sec; 20.813 sec/batch)
2016-04-28 18:38:59.226748: step 70, loss = 119.02 (2.8 examples/sec; 22.868 sec/batch)
2016-04-28 18:39:45.273731: step 71, loss = 118.77 (3.3 examples/sec; 19.520 sec/batch)
2016-04-28 18:40:07.131677: step 72, loss = 118.81 (2.9 examples/sec; 21.855 sec/batch)
2016-04-28 18:40:28.069149: step 73, loss = 118.79 (3.1 examples/sec; 20.934 sec/batch)
2016-04-28 18:40:47.769323: step 74, loss = 118.61 (3.2 examples/sec; 19.696 sec/batch)
2016-04-28 18:41:07.380033: step 75, loss = 118.48 (3.3 examples/sec; 19.600 sec/batch)
2016-04-28 18:41:29.146596: step 76, loss = 118.27 (2.9 examples/sec; 21.751 sec/batch)
2016-04-28 18:41:58.100678: step 77, loss = 118.22 (2.2 examples/sec; 28.943 sec/batch)
2016-04-28 18:42:21.828496: step 78, loss = 118.11 (2.7 examples/sec; 23.717 sec/batch)
2016-04-28 18:42:50.944047: step 79, loss = 118.09 (2.2 examples/sec; 29.100 sec/batch)
2016-04-28 18:43:25.645951: step 80, loss = 117.85 (1.8 examples/sec; 34.689 sec/batch)
2016-04-28 18:44:33.999335: step 81, loss = 117.95 (2.3 examples/sec; 27.937 sec/batch)
2016-04-28 18:45:14.889402: step 82, loss = 117.86 (1.6 examples/sec; 40.879 sec/batch)
2016-04-28 18:45:37.937854: step 83, loss = 117.85 (2.8 examples/sec; 23.046 sec/batch)
2016-04-28 18:46:00.611379: step 84, loss = 117.58 (2.8 examples/sec; 22.673 sec/batch)
2016-04-28 18:46:25.222906: step 85, loss = 117.49 (2.6 examples/sec; 24.611 sec/batch)
2016-04-28 18:46:46.214438: step 86, loss = 117.42 (3.0 examples/sec; 20.991 sec/batch)
2016-04-28 18:47:10.988421: step 87, loss = 117.32 (2.6 examples/sec; 24.774 sec/batch)
2016-04-28 18:47:40.451563: step 88, loss = 117.26 (2.2 examples/sec; 29.463 sec/batch)
2016-04-28 18:48:07.216938: step 89, loss = 117.17 (2.4 examples/sec; 26.765 sec/batch)
2016-04-28 18:48:24.339479: step 90, loss = 117.00 (3.7 examples/sec; 17.122 sec/batch)
2016-04-28 18:49:03.532546: step 91, loss = 117.15 (3.7 examples/sec; 17.309 sec/batch)
2016-04-28 18:49:20.889822: step 92, loss = 116.81 (3.7 examples/sec; 17.357 sec/batch)
2016-04-28 18:49:38.370590: step 93, loss = 116.85 (3.7 examples/sec; 17.481 sec/batch)
2016-04-28 18:49:56.694317: step 94, loss = 116.79 (3.5 examples/sec; 18.324 sec/batch)
2016-04-28 18:50:14.682808: step 95, loss = 116.78 (3.6 examples/sec; 17.988 sec/batch)
2016-04-28 18:50:31.847481: step 96, loss = 116.47 (3.7 examples/sec; 17.165 sec/batch)
2016-04-28 18:50:49.452700: step 97, loss = 116.60 (3.6 examples/sec; 17.605 sec/batch)
2016-04-28 18:51:06.584693: step 98, loss = 116.20 (3.7 examples/sec; 17.132 sec/batch)
2016-04-28 18:51:24.145802: step 99, loss = 116.32 (3.6 examples/sec; 17.561 sec/batch)
2016-04-28 18:51:41.382254: step 100, loss = 116.09 (3.7 examples/sec; 17.236 sec/batch)
2016-04-28 18:52:20.294694: step 101, loss = 116.07 (3.7 examples/sec; 17.363 sec/batch)
2016-04-28 18:52:38.171049: step 102, loss = 115.88 (3.6 examples/sec; 17.876 sec/batch)
2016-04-28 18:52:55.595576: step 103, loss = 115.88 (3.7 examples/sec; 17.424 sec/batch)
2016-04-28 18:53:17.951122: step 104, loss = 115.59 (2.9 examples/sec; 22.355 sec/batch)
2016-04-28 18:53:40.228862: step 105, loss = 115.69 (2.9 examples/sec; 22.277 sec/batch)
2016-04-28 18:54:00.234538: step 106, loss = 115.64 (3.2 examples/sec; 20.005 sec/batch)
2016-04-28 18:54:21.531755: step 107, loss = 115.56 (3.0 examples/sec; 21.297 sec/batch)
2016-04-28 18:54:42.364391: step 108, loss = 115.38 (3.1 examples/sec; 20.833 sec/batch)
2016-04-28 18:55:02.330222: step 109, loss = 115.24 (3.2 examples/sec; 19.966 sec/batch)
2016-04-28 18:55:22.840660: step 110, loss = 115.15 (3.1 examples/sec; 20.510 sec/batch)
2016-04-28 18:56:08.684038: step 111, loss = 115.15 (3.2 examples/sec; 19.990 sec/batch)
2016-04-28 18:56:29.192935: step 112, loss = 115.13 (3.1 examples/sec; 20.508 sec/batch)
2016-04-28 18:56:49.763620: step 113, loss = 114.81 (3.1 examples/sec; 20.571 sec/batch)
2016-04-28 18:57:09.649421: step 114, loss = 114.92 (3.2 examples/sec; 19.886 sec/batch)
2016-04-28 18:57:30.020817: step 115, loss = 114.84 (3.1 examples/sec; 20.371 sec/batch)
2016-04-28 18:57:51.287067: step 116, loss = 114.48 (3.0 examples/sec; 21.266 sec/batch)
2016-04-28 18:58:15.934534: step 117, loss = 114.60 (2.6 examples/sec; 24.637 sec/batch)
2016-04-28 18:58:36.067814: step 118, loss = 114.45 (3.2 examples/sec; 20.124 sec/batch)
2016-04-28 18:58:55.119930: step 119, loss = 114.58 (3.4 examples/sec; 19.052 sec/batch)
2016-04-28 18:59:14.978391: step 120, loss = 114.33 (3.2 examples/sec; 19.858 sec/batch)
2016-04-28 18:59:59.346953: step 121, loss = 114.05 (3.3 examples/sec; 19.147 sec/batch)
2016-04-28 19:00:18.195300: step 122, loss = 114.17 (3.4 examples/sec; 18.848 sec/batch)
2016-04-28 19:00:38.972983: step 123, loss = 114.06 (3.1 examples/sec; 20.778 sec/batch)
2016-04-28 19:00:58.766729: step 124, loss = 113.91 (3.2 examples/sec; 19.794 sec/batch)
2016-04-28 19:01:19.112841: step 125, loss = 113.86 (3.1 examples/sec; 20.346 sec/batch)
2016-04-28 19:01:38.560719: step 126, loss = 113.83 (3.3 examples/sec; 19.448 sec/batch)
2016-04-28 19:01:59.274413: step 127, loss = 113.76 (3.1 examples/sec; 20.714 sec/batch)
2016-04-28 19:02:19.493553: step 128, loss = 113.64 (3.2 examples/sec; 20.219 sec/batch)
2016-04-28 19:02:38.623997: step 129, loss = 113.46 (3.3 examples/sec; 19.130 sec/batch)
2016-04-28 19:02:57.939667: step 130, loss = 113.44 (3.3 examples/sec; 19.316 sec/batch)
2016-04-28 19:03:49.574347: step 131, loss = 113.29 (3.2 examples/sec; 19.963 sec/batch)
2016-04-28 19:04:10.522319: step 132, loss = 113.14 (3.1 examples/sec; 20.948 sec/batch)
2016-04-28 19:04:31.321465: step 133, loss = 113.16 (3.1 examples/sec; 20.799 sec/batch)
2016-04-28 19:04:52.043296: step 134, loss = 112.96 (3.1 examples/sec; 20.722 sec/batch)
2016-04-28 19:05:13.937183: step 135, loss = 112.87 (2.9 examples/sec; 21.894 sec/batch)
2016-04-28 19:05:35.107676: step 136, loss = 112.75 (3.0 examples/sec; 21.170 sec/batch)
2016-04-28 19:05:55.751603: step 137, loss = 112.70 (3.1 examples/sec; 20.644 sec/batch)
2016-04-28 19:06:17.288352: step 138, loss = 112.62 (3.0 examples/sec; 21.537 sec/batch)
2016-04-28 19:06:38.118269: step 139, loss = 112.56 (3.1 examples/sec; 20.830 sec/batch)
2016-04-28 19:06:59.539997: step 140, loss = 112.53 (3.0 examples/sec; 21.422 sec/batch)
2016-04-28 19:07:47.241766: step 141, loss = 112.40 (3.0 examples/sec; 21.062 sec/batch)
2016-04-28 19:08:08.229784: step 142, loss = 112.23 (3.0 examples/sec; 20.988 sec/batch)
2016-04-28 19:08:29.498099: step 143, loss = 112.25 (3.0 examples/sec; 21.268 sec/batch)
2016-04-28 19:08:50.120058: step 144, loss = 112.15 (3.1 examples/sec; 20.622 sec/batch)
2016-04-28 19:09:11.732243: step 145, loss = 112.06 (3.0 examples/sec; 21.612 sec/batch)
2016-04-28 19:09:33.030703: step 146, loss = 112.07 (3.0 examples/sec; 21.298 sec/batch)
2016-04-28 19:09:53.860421: step 147, loss = 111.80 (3.1 examples/sec; 20.830 sec/batch)
2016-04-28 19:10:15.651961: step 148, loss = 111.85 (2.9 examples/sec; 21.791 sec/batch)
2016-04-28 19:10:38.086355: step 149, loss = 111.49 (2.9 examples/sec; 22.434 sec/batch)
2016-04-28 19:10:59.131207: step 150, loss = 111.59 (3.0 examples/sec; 21.045 sec/batch)
2016-04-28 19:11:48.790577: step 151, loss = 111.44 (3.0 examples/sec; 21.692 sec/batch)
2016-04-28 19:12:10.171892: step 152, loss = 111.45 (3.0 examples/sec; 21.381 sec/batch)
2016-04-28 19:12:31.818376: step 153, loss = 111.34 (3.0 examples/sec; 21.646 sec/batch)
2016-04-28 19:12:53.448140: step 154, loss = 111.23 (3.0 examples/sec; 21.630 sec/batch)
2016-04-28 19:13:13.901455: step 155, loss = 111.13 (3.1 examples/sec; 20.453 sec/batch)
2016-04-28 19:13:34.558740: step 156, loss = 111.19 (3.1 examples/sec; 20.657 sec/batch)
2016-04-28 19:13:54.978320: step 157, loss = 111.05 (3.1 examples/sec; 20.419 sec/batch)
2016-04-28 19:14:15.640334: step 158, loss = 110.79 (3.1 examples/sec; 20.662 sec/batch)
2016-04-28 19:14:36.876715: step 159, loss = 110.99 (3.0 examples/sec; 21.236 sec/batch)
2016-04-28 19:14:58.512685: step 160, loss = 110.66 (3.0 examples/sec; 21.636 sec/batch)
2016-04-28 19:15:47.543664: step 161, loss = 110.75 (2.9 examples/sec; 22.256 sec/batch)
2016-04-28 19:16:10.844078: step 162, loss = 110.71 (2.7 examples/sec; 23.300 sec/batch)
2016-04-28 19:16:32.496735: step 163, loss = 110.55 (3.0 examples/sec; 21.653 sec/batch)
2016-04-28 19:16:54.556296: step 164, loss = 110.33 (2.9 examples/sec; 22.059 sec/batch)
2016-04-28 19:17:16.715978: step 165, loss = 110.32 (2.9 examples/sec; 22.160 sec/batch)
2016-04-28 19:17:38.657699: step 166, loss = 110.15 (2.9 examples/sec; 21.942 sec/batch)
2016-04-28 19:18:01.865655: step 167, loss = 109.99 (2.8 examples/sec; 23.208 sec/batch)
2016-04-28 19:18:23.626562: step 168, loss = 110.17 (2.9 examples/sec; 21.761 sec/batch)
2016-04-28 19:18:43.926114: step 169, loss = 110.01 (3.2 examples/sec; 20.299 sec/batch)
2016-04-28 19:19:04.226056: step 170, loss = 109.78 (3.2 examples/sec; 20.300 sec/batch)
2016-04-28 19:19:54.783372: step 171, loss = 109.72 (3.1 examples/sec; 20.767 sec/batch)
2016-04-28 19:20:14.732593: step 172, loss = 109.61 (3.2 examples/sec; 19.949 sec/batch)
2016-04-28 19:20:34.549528: step 173, loss = 109.52 (3.2 examples/sec; 19.817 sec/batch)
2016-04-28 19:20:53.548222: step 174, loss = 109.41 (3.4 examples/sec; 18.999 sec/batch)
2016-04-28 19:21:12.681818: step 175, loss = 109.35 (3.3 examples/sec; 19.134 sec/batch)
2016-04-28 19:21:31.184456: step 176, loss = 109.31 (3.5 examples/sec; 18.503 sec/batch)
2016-04-28 19:21:50.180071: step 177, loss = 109.40 (3.4 examples/sec; 18.996 sec/batch)
2016-04-28 19:22:08.887264: step 178, loss = 109.03 (3.4 examples/sec; 18.707 sec/batch)
2016-04-28 19:22:27.412464: step 179, loss = 109.15 (3.5 examples/sec; 18.525 sec/batch)
2016-04-28 19:22:46.297839: step 180, loss = 109.00 (3.4 examples/sec; 18.885 sec/batch)
2016-04-28 19:23:28.313477: step 181, loss = 108.90 (3.4 examples/sec; 19.008 sec/batch)
2016-04-28 19:23:47.545379: step 182, loss = 108.72 (3.3 examples/sec; 19.232 sec/batch)
2016-04-28 19:24:06.054775: step 183, loss = 108.80 (3.5 examples/sec; 18.509 sec/batch)
2016-04-28 19:24:25.488772: step 184, loss = 108.73 (3.3 examples/sec; 19.434 sec/batch)
2016-04-28 19:24:45.869043: step 185, loss = 108.57 (3.1 examples/sec; 20.380 sec/batch)
2016-04-28 19:25:14.391159: step 186, loss = 108.33 (2.2 examples/sec; 28.522 sec/batch)
2016-04-28 19:25:40.583966: step 187, loss = 108.42 (2.4 examples/sec; 26.192 sec/batch)
2016-04-28 19:26:05.856766: step 188, loss = 108.23 (2.5 examples/sec; 25.273 sec/batch)
2016-04-28 19:26:28.904174: step 189, loss = 108.38 (2.8 examples/sec; 23.047 sec/batch)
2016-04-28 19:26:52.203992: step 190, loss = 108.21 (2.7 examples/sec; 23.300 sec/batch)
2016-04-28 19:27:48.513776: step 191, loss = 108.06 (2.4 examples/sec; 26.569 sec/batch)
2016-04-28 19:28:15.263717: step 192, loss = 107.87 (2.4 examples/sec; 26.750 sec/batch)
2016-04-28 19:28:40.794915: step 193, loss = 108.07 (2.5 examples/sec; 25.531 sec/batch)
2016-04-28 19:29:07.557079: step 194, loss = 107.77 (2.4 examples/sec; 26.762 sec/batch)
2016-04-28 19:29:35.644892: step 195, loss = 107.73 (2.3 examples/sec; 28.088 sec/batch)
2016-04-28 19:30:03.940023: step 196, loss = 107.64 (2.3 examples/sec; 28.295 sec/batch)
2016-04-28 19:30:30.434780: step 197, loss = 107.51 (2.4 examples/sec; 26.495 sec/batch)
2016-04-28 19:30:55.459030: step 198, loss = 107.42 (2.6 examples/sec; 25.024 sec/batch)
2016-04-28 19:31:22.094191: step 199, loss = 107.44 (2.4 examples/sec; 26.635 sec/batch)
2016-04-28 19:31:48.723111: step 200, loss = 107.24 (2.4 examples/sec; 26.629 sec/batch)
2016-04-28 19:32:43.522416: step 201, loss = 107.15 (2.7 examples/sec; 23.895 sec/batch)
2016-04-28 19:33:06.639805: step 202, loss = 107.13 (2.8 examples/sec; 23.117 sec/batch)
2016-04-28 19:33:30.894591: step 203, loss = 107.00 (2.6 examples/sec; 24.255 sec/batch)
2016-04-28 19:33:55.089682: step 204, loss = 106.95 (2.6 examples/sec; 24.195 sec/batch)
2016-04-28 19:34:18.968047: step 205, loss = 106.99 (2.7 examples/sec; 23.878 sec/batch)
2016-04-28 19:34:42.429988: step 206, loss = 106.84 (2.7 examples/sec; 23.462 sec/batch)
2016-04-28 19:35:06.286716: step 207, loss = 106.74 (2.7 examples/sec; 23.857 sec/batch)
2016-04-28 19:35:30.024733: step 208, loss = 106.70 (2.7 examples/sec; 23.738 sec/batch)
2016-04-28 19:35:53.325523: step 209, loss = 106.52 (2.7 examples/sec; 23.301 sec/batch)
2016-04-28 19:36:17.843975: step 210, loss = 106.48 (2.6 examples/sec; 24.518 sec/batch)
2016-04-28 19:37:16.299787: step 211, loss = 106.43 (2.4 examples/sec; 26.359 sec/batch)
2016-04-28 19:37:41.785511: step 212, loss = 106.30 (2.5 examples/sec; 25.486 sec/batch)
2016-04-28 19:38:07.884229: step 213, loss = 106.19 (2.5 examples/sec; 26.099 sec/batch)
2016-04-28 19:38:33.431090: step 214, loss = 106.09 (2.5 examples/sec; 25.547 sec/batch)
2016-04-28 19:38:58.520982: step 215, loss = 106.13 (2.6 examples/sec; 25.090 sec/batch)
2016-04-28 19:39:24.973255: step 216, loss = 106.00 (2.4 examples/sec; 26.452 sec/batch)
2016-04-28 19:39:51.536336: step 217, loss = 105.93 (2.4 examples/sec; 26.563 sec/batch)
2016-04-28 19:40:16.923493: step 218, loss = 105.67 (2.5 examples/sec; 25.387 sec/batch)
2016-04-28 19:40:42.279654: step 219, loss = 105.77 (2.5 examples/sec; 25.356 sec/batch)
2016-04-28 19:41:08.012821: step 220, loss = 105.38 (2.5 examples/sec; 25.733 sec/batch)
2016-04-28 19:42:05.873063: step 221, loss = 105.55 (2.4 examples/sec; 26.495 sec/batch)
2016-04-28 19:42:31.222969: step 222, loss = 105.46 (2.5 examples/sec; 25.350 sec/batch)
2016-04-28 19:42:56.021327: step 223, loss = 105.53 (2.6 examples/sec; 24.798 sec/batch)
2016-04-28 19:43:21.831069: step 224, loss = 105.34 (2.5 examples/sec; 25.810 sec/batch)
2016-04-28 19:43:46.498458: step 225, loss = 105.14 (2.6 examples/sec; 24.667 sec/batch)
2016-04-28 19:44:06.857224: step 226, loss = 105.15 (3.1 examples/sec; 20.359 sec/batch)
2016-04-28 19:44:25.898614: step 227, loss = 104.97 (3.4 examples/sec; 19.041 sec/batch)
2016-04-28 19:44:44.130047: step 228, loss = 105.02 (3.5 examples/sec; 18.231 sec/batch)
2016-04-28 19:45:03.453753: step 229, loss = 104.88 (3.3 examples/sec; 19.324 sec/batch)
2016-04-28 19:45:31.112374: step 230, loss = 104.75 (2.3 examples/sec; 27.659 sec/batch)
2016-04-28 19:46:15.421528: step 231, loss = 104.53 (3.3 examples/sec; 19.186 sec/batch)
2016-04-28 19:46:34.786094: step 232, loss = 104.60 (3.3 examples/sec; 19.364 sec/batch)
2016-04-28 19:46:55.629408: step 233, loss = 104.48 (3.1 examples/sec; 20.843 sec/batch)
2016-04-28 19:47:16.925251: step 234, loss = 104.43 (3.0 examples/sec; 21.296 sec/batch)
2016-04-28 19:47:36.757151: step 235, loss = 104.54 (3.2 examples/sec; 19.832 sec/batch)
2016-04-28 19:47:55.389797: step 236, loss = 104.27 (3.4 examples/sec; 18.633 sec/batch)
2016-04-28 19:48:15.183187: step 237, loss = 104.42 (3.2 examples/sec; 19.793 sec/batch)
2016-04-28 19:48:34.343630: step 238, loss = 104.08 (3.3 examples/sec; 19.160 sec/batch)
2016-04-28 19:48:52.873757: step 239, loss = 104.02 (3.5 examples/sec; 18.530 sec/batch)
2016-04-28 19:49:12.120480: step 240, loss = 103.90 (3.3 examples/sec; 19.247 sec/batch)
2016-04-28 19:49:56.467540: step 241, loss = 104.00 (3.6 examples/sec; 17.928 sec/batch)
2016-04-28 19:50:13.709651: step 242, loss = 103.71 (3.7 examples/sec; 17.242 sec/batch)
2016-04-28 19:50:30.516430: step 243, loss = 103.81 (3.8 examples/sec; 16.807 sec/batch)
2016-04-28 19:50:47.852243: step 244, loss = 103.76 (3.7 examples/sec; 17.336 sec/batch)
2016-04-28 19:51:05.351550: step 245, loss = 103.60 (3.7 examples/sec; 17.499 sec/batch)
2016-04-28 19:51:22.419788: step 246, loss = 103.43 (3.7 examples/sec; 17.068 sec/batch)
2016-04-28 19:51:39.253656: step 247, loss = 103.47 (3.8 examples/sec; 16.834 sec/batch)
2016-04-28 19:51:56.271788: step 248, loss = 103.17 (3.8 examples/sec; 17.018 sec/batch)
2016-04-28 19:52:13.147879: step 249, loss = 103.20 (3.8 examples/sec; 16.876 sec/batch)
2016-04-28 19:52:30.313532: step 250, loss = 103.03 (3.7 examples/sec; 17.166 sec/batch)
2016-04-28 19:53:08.298685: step 251, loss = 103.17 (3.7 examples/sec; 17.153 sec/batch)
2016-04-28 19:53:25.546611: step 252, loss = 103.11 (3.7 examples/sec; 17.248 sec/batch)
2016-04-28 19:53:43.085075: step 253, loss = 103.05 (3.6 examples/sec; 17.538 sec/batch)
2016-04-28 19:53:59.632581: step 254, loss = 102.97 (3.9 examples/sec; 16.547 sec/batch)
2016-04-28 19:54:17.280666: step 255, loss = 102.59 (3.6 examples/sec; 17.648 sec/batch)
2016-04-28 19:54:42.892268: step 256, loss = 102.68 (2.5 examples/sec; 25.612 sec/batch)
2016-04-28 19:55:02.526711: step 257, loss = 102.77 (3.3 examples/sec; 19.634 sec/batch)
2016-04-28 19:55:23.073647: step 258, loss = 102.50 (3.1 examples/sec; 20.547 sec/batch)
2016-04-28 19:55:42.519428: step 259, loss = 102.39 (3.3 examples/sec; 19.446 sec/batch)
2016-04-28 19:56:01.992554: step 260, loss = 102.35 (3.3 examples/sec; 19.473 sec/batch)
2016-04-28 19:56:46.323637: step 261, loss = 102.18 (3.3 examples/sec; 19.335 sec/batch)
2016-04-28 19:57:06.293251: step 262, loss = 102.08 (3.2 examples/sec; 19.970 sec/batch)
2016-04-28 19:57:25.912597: step 263, loss = 101.97 (3.3 examples/sec; 19.619 sec/batch)
2016-04-28 19:57:45.391427: step 264, loss = 102.03 (3.3 examples/sec; 19.479 sec/batch)
2016-04-28 19:58:04.822571: step 265, loss = 102.05 (3.3 examples/sec; 19.431 sec/batch)
2016-04-28 19:58:23.928406: step 266, loss = 101.88 (3.3 examples/sec; 19.106 sec/batch)
2016-04-28 19:58:43.466296: step 267, loss = 101.77 (3.3 examples/sec; 19.538 sec/batch)
2016-04-28 19:59:03.280441: step 268, loss = 101.66 (3.2 examples/sec; 19.814 sec/batch)
2016-04-28 19:59:23.091957: step 269, loss = 101.51 (3.2 examples/sec; 19.811 sec/batch)
2016-04-28 19:59:42.776565: step 270, loss = 101.44 (3.3 examples/sec; 19.685 sec/batch)
2016-04-28 20:00:26.269289: step 271, loss = 101.58 (3.3 examples/sec; 19.318 sec/batch)
2016-04-28 20:00:45.799996: step 272, loss = 101.37 (3.3 examples/sec; 19.531 sec/batch)
2016-04-28 20:01:05.208582: step 273, loss = 101.28 (3.3 examples/sec; 19.409 sec/batch)
2016-04-28 20:01:25.024620: step 274, loss = 101.13 (3.2 examples/sec; 19.816 sec/batch)
2016-04-28 20:01:44.234122: step 275, loss = 101.09 (3.3 examples/sec; 19.209 sec/batch)
2016-04-28 20:02:04.058975: step 276, loss = 101.03 (3.2 examples/sec; 19.825 sec/batch)
2016-04-28 20:02:24.242571: step 277, loss = 100.86 (3.2 examples/sec; 20.183 sec/batch)
2016-04-28 20:02:48.400881: step 278, loss = 100.89 (2.6 examples/sec; 24.158 sec/batch)
2016-04-28 20:03:12.946381: step 279, loss = 100.84 (2.6 examples/sec; 24.545 sec/batch)
2016-04-28 20:03:33.054132: step 280, loss = 100.76 (3.2 examples/sec; 20.108 sec/batch)
2016-04-28 20:04:15.038403: step 281, loss = 100.60 (3.4 examples/sec; 18.979 sec/batch)
2016-04-28 20:04:35.434173: step 282, loss = 100.61 (3.1 examples/sec; 20.396 sec/batch)
2016-04-28 20:04:55.113811: step 283, loss = 100.50 (3.3 examples/sec; 19.680 sec/batch)
2016-04-28 20:05:16.455743: step 284, loss = 100.34 (3.0 examples/sec; 21.342 sec/batch)
2016-04-28 20:05:37.907454: step 285, loss = 100.50 (3.0 examples/sec; 21.452 sec/batch)
2016-04-28 20:05:58.180042: step 286, loss = 100.17 (3.2 examples/sec; 20.273 sec/batch)
2016-04-28 20:06:18.934167: step 287, loss = 100.17 (3.1 examples/sec; 20.754 sec/batch)
2016-04-28 20:06:38.559561: step 288, loss = 100.07 (3.3 examples/sec; 19.625 sec/batch)
2016-04-28 20:06:59.181577: step 289, loss = 100.14 (3.1 examples/sec; 20.622 sec/batch)
2016-04-28 20:07:17.750446: step 290, loss = 100.11 (3.4 examples/sec; 18.569 sec/batch)
2016-04-28 20:08:03.791173: step 291, loss = 99.81 (2.7 examples/sec; 23.423 sec/batch)
2016-04-28 20:08:27.756801: step 292, loss = 99.82 (2.7 examples/sec; 23.960 sec/batch)
2016-04-28 20:08:47.756588: step 293, loss = 99.62 (3.2 examples/sec; 19.998 sec/batch)
2016-04-28 20:09:08.761953: step 294, loss = 99.57 (3.0 examples/sec; 21.005 sec/batch)
2016-04-28 20:09:28.533396: step 295, loss = 99.74 (3.2 examples/sec; 19.771 sec/batch)
2016-04-28 20:09:48.254260: step 296, loss = 99.38 (3.2 examples/sec; 19.721 sec/batch)
2016-04-28 20:10:08.509292: step 297, loss = 99.62 (3.2 examples/sec; 20.255 sec/batch)
2016-04-28 20:10:28.051063: step 298, loss = 99.38 (3.3 examples/sec; 19.542 sec/batch)
2016-04-28 20:10:47.910413: step 299, loss = 99.11 (3.2 examples/sec; 19.859 sec/batch)
2016-04-28 20:11:08.113865: step 300, loss = 99.22 (3.2 examples/sec; 20.203 sec/batch)
2016-04-28 20:11:52.400285: step 301, loss = 99.00 (3.2 examples/sec; 19.825 sec/batch)
2016-04-28 20:12:11.934649: step 302, loss = 98.96 (3.3 examples/sec; 19.534 sec/batch)
2016-04-28 20:12:31.421523: step 303, loss = 99.02 (3.3 examples/sec; 19.487 sec/batch)
2016-04-28 20:12:51.372545: step 304, loss = 98.85 (3.2 examples/sec; 19.951 sec/batch)
2016-04-28 20:13:11.040557: step 305, loss = 98.67 (3.3 examples/sec; 19.668 sec/batch)
2016-04-28 20:13:30.970365: step 306, loss = 98.77 (3.2 examples/sec; 19.930 sec/batch)
2016-04-28 20:13:50.412570: step 307, loss = 98.72 (3.3 examples/sec; 19.442 sec/batch)
2016-04-28 20:14:10.453886: step 308, loss = 98.46 (3.2 examples/sec; 20.041 sec/batch)
2016-04-28 20:14:30.516413: step 309, loss = 98.25 (3.2 examples/sec; 20.062 sec/batch)
2016-04-28 20:14:50.148978: step 310, loss = 98.44 (3.3 examples/sec; 19.632 sec/batch)
2016-04-28 20:15:36.821170: step 311, loss = 98.33 (3.2 examples/sec; 19.818 sec/batch)
2016-04-28 20:15:56.421713: step 312, loss = 98.41 (3.3 examples/sec; 19.600 sec/batch)
2016-04-28 20:16:16.498507: step 313, loss = 98.12 (3.2 examples/sec; 20.077 sec/batch)
2016-04-28 20:16:36.077449: step 314, loss = 98.03 (3.3 examples/sec; 19.579 sec/batch)
2016-04-28 20:16:56.214603: step 315, loss = 98.08 (3.2 examples/sec; 20.137 sec/batch)
2016-04-28 20:17:16.499867: step 316, loss = 97.91 (3.2 examples/sec; 20.285 sec/batch)
2016-04-28 20:17:35.750351: step 317, loss = 97.73 (3.3 examples/sec; 19.250 sec/batch)
2016-04-28 20:17:56.100077: step 318, loss = 97.56 (3.1 examples/sec; 20.350 sec/batch)
2016-04-28 20:18:15.723827: step 319, loss = 97.88 (3.3 examples/sec; 19.624 sec/batch)
2016-04-28 20:18:35.760168: step 320, loss = 97.50 (3.2 examples/sec; 20.036 sec/batch)
2016-04-28 20:19:20.505828: step 321, loss = 97.62 (3.3 examples/sec; 19.212 sec/batch)
2016-04-28 20:19:40.422041: step 322, loss = 97.69 (3.2 examples/sec; 19.916 sec/batch)
2016-04-28 20:19:59.958192: step 323, loss = 97.30 (3.3 examples/sec; 19.536 sec/batch)
2016-04-28 20:20:19.645448: step 324, loss = 97.37 (3.3 examples/sec; 19.687 sec/batch)
2016-04-28 20:20:39.260439: step 325, loss = 97.30 (3.3 examples/sec; 19.615 sec/batch)
2016-04-28 20:20:58.500030: step 326, loss = 97.13 (3.3 examples/sec; 19.240 sec/batch)
2016-04-28 20:21:19.477030: step 327, loss = 97.18 (3.1 examples/sec; 20.977 sec/batch)
2016-04-28 20:21:39.453700: step 328, loss = 96.93 (3.2 examples/sec; 19.977 sec/batch)
2016-04-28 20:21:59.463505: step 329, loss = 97.05 (3.2 examples/sec; 20.010 sec/batch)
2016-04-28 20:22:19.351175: step 330, loss = 96.73 (3.2 examples/sec; 19.888 sec/batch)
2016-04-28 20:23:03.154969: step 331, loss = 96.71 (3.4 examples/sec; 18.990 sec/batch)
2016-04-28 20:23:23.309169: step 332, loss = 96.56 (3.2 examples/sec; 20.154 sec/batch)
2016-04-28 20:23:43.297018: step 333, loss = 96.43 (3.2 examples/sec; 19.988 sec/batch)
2016-04-28 20:24:03.509130: step 334, loss = 96.51 (3.2 examples/sec; 20.212 sec/batch)
2016-04-28 20:24:23.469063: step 335, loss = 96.46 (3.2 examples/sec; 19.960 sec/batch)
2016-04-28 20:24:42.754216: step 336, loss = 96.40 (3.3 examples/sec; 19.285 sec/batch)
2016-04-28 20:25:03.002926: step 337, loss = 96.33 (3.2 examples/sec; 20.249 sec/batch)
2016-04-28 20:25:22.448332: step 338, loss = 96.22 (3.3 examples/sec; 19.445 sec/batch)
2016-04-28 20:25:42.764264: step 339, loss = 96.15 (3.2 examples/sec; 20.316 sec/batch)
2016-04-28 20:26:02.746776: step 340, loss = 96.18 (3.2 examples/sec; 19.982 sec/batch)
2016-04-28 20:26:47.206922: step 341, loss = 96.04 (3.2 examples/sec; 20.042 sec/batch)
2016-04-28 20:27:07.565621: step 342, loss = 95.80 (3.1 examples/sec; 20.359 sec/batch)
2016-04-28 20:27:27.159770: step 343, loss = 95.95 (3.3 examples/sec; 19.594 sec/batch)
2016-04-28 20:27:47.447093: step 344, loss = 95.78 (3.2 examples/sec; 20.287 sec/batch)
2016-04-28 20:28:06.990963: step 345, loss = 95.74 (3.3 examples/sec; 19.544 sec/batch)
2016-04-28 20:28:27.267835: step 346, loss = 95.58 (3.2 examples/sec; 20.277 sec/batch)
2016-04-28 20:28:47.245338: step 347, loss = 95.59 (3.2 examples/sec; 19.977 sec/batch)
2016-04-28 20:29:06.697355: step 348, loss = 95.35 (3.3 examples/sec; 19.452 sec/batch)
2016-04-28 20:29:27.042727: step 349, loss = 95.31 (3.1 examples/sec; 20.345 sec/batch)
2016-04-28 20:29:46.606083: step 350, loss = 95.28 (3.3 examples/sec; 19.563 sec/batch)
2016-04-28 20:30:31.010697: step 351, loss = 95.18 (3.3 examples/sec; 19.563 sec/batch)
2016-04-28 20:30:50.513349: step 352, loss = 95.14 (3.3 examples/sec; 19.503 sec/batch)
2016-04-28 20:31:11.174676: step 353, loss = 95.09 (3.1 examples/sec; 20.661 sec/batch)
2016-04-28 20:31:31.005578: step 354, loss = 95.20 (3.2 examples/sec; 19.831 sec/batch)
2016-04-28 20:31:50.681729: step 355, loss = 95.05 (3.3 examples/sec; 19.676 sec/batch)
2016-04-28 20:32:11.118620: step 356, loss = 94.81 (3.1 examples/sec; 20.437 sec/batch)
2016-04-28 20:32:30.805813: step 357, loss = 94.71 (3.3 examples/sec; 19.687 sec/batch)
2016-04-28 20:32:50.936958: step 358, loss = 94.74 (3.2 examples/sec; 20.131 sec/batch)
2016-04-28 20:33:11.266734: step 359, loss = 94.58 (3.1 examples/sec; 20.330 sec/batch)
2016-04-28 20:33:30.729869: step 360, loss = 94.61 (3.3 examples/sec; 19.463 sec/batch)
2016-04-28 20:34:15.929166: step 361, loss = 94.41 (3.2 examples/sec; 20.134 sec/batch)
2016-04-28 20:34:35.434525: step 362, loss = 94.51 (3.3 examples/sec; 19.505 sec/batch)
2016-04-28 20:34:55.765709: step 363, loss = 94.32 (3.1 examples/sec; 20.331 sec/batch)
2016-04-28 20:35:15.218981: step 364, loss = 94.37 (3.3 examples/sec; 19.453 sec/batch)
2016-04-28 20:35:35.287849: step 365, loss = 94.00 (3.2 examples/sec; 20.069 sec/batch)
2016-04-28 20:35:54.929171: step 366, loss = 94.17 (3.3 examples/sec; 19.641 sec/batch)
2016-04-28 20:36:14.534247: step 367, loss = 94.04 (3.3 examples/sec; 19.605 sec/batch)
2016-04-28 20:36:34.611302: step 368, loss = 94.02 (3.2 examples/sec; 20.077 sec/batch)
2016-04-28 20:36:54.317227: step 369, loss = 93.96 (3.2 examples/sec; 19.706 sec/batch)
2016-04-28 20:37:14.621623: step 370, loss = 93.58 (3.2 examples/sec; 20.304 sec/batch)
2016-04-28 20:37:59.435038: step 371, loss = 93.80 (3.2 examples/sec; 19.767 sec/batch)
2016-04-28 20:38:20.035325: step 372, loss = 93.60 (3.1 examples/sec; 20.600 sec/batch)
2016-04-28 20:38:39.795298: step 373, loss = 93.73 (3.2 examples/sec; 19.760 sec/batch)
2016-04-28 20:38:59.569469: step 374, loss = 93.40 (3.2 examples/sec; 19.774 sec/batch)
2016-04-28 20:39:19.796368: step 375, loss = 93.39 (3.2 examples/sec; 20.227 sec/batch)
2016-04-28 20:39:39.793217: step 376, loss = 93.36 (3.2 examples/sec; 19.997 sec/batch)
2016-04-28 20:39:59.932746: step 377, loss = 93.26 (3.2 examples/sec; 20.139 sec/batch)
2016-04-28 20:40:20.332338: step 378, loss = 93.07 (3.1 examples/sec; 20.400 sec/batch)
2016-04-28 20:40:39.772398: step 379, loss = 93.14 (3.3 examples/sec; 19.440 sec/batch)
2016-04-28 20:40:59.637693: step 380, loss = 93.09 (3.2 examples/sec; 19.865 sec/batch)
2016-04-28 20:41:44.080081: step 381, loss = 93.11 (3.2 examples/sec; 19.949 sec/batch)
2016-04-28 20:42:04.110042: step 382, loss = 92.94 (3.2 examples/sec; 20.030 sec/batch)
2016-04-28 20:42:23.609188: step 383, loss = 92.94 (3.3 examples/sec; 19.499 sec/batch)
2016-04-28 20:42:43.424163: step 384, loss = 92.70 (3.2 examples/sec; 19.815 sec/batch)
2016-04-28 20:43:03.036790: step 385, loss = 92.79 (3.3 examples/sec; 19.613 sec/batch)
2016-04-28 20:43:22.587894: step 386, loss = 92.54 (3.3 examples/sec; 19.551 sec/batch)
2016-04-28 20:43:42.916156: step 387, loss = 92.47 (3.1 examples/sec; 20.328 sec/batch)
2016-04-28 20:44:02.418040: step 388, loss = 92.51 (3.3 examples/sec; 19.502 sec/batch)
2016-04-28 20:44:22.650724: step 389, loss = 92.41 (3.2 examples/sec; 20.233 sec/batch)
2016-04-28 20:44:42.079013: step 390, loss = 92.44 (3.3 examples/sec; 19.428 sec/batch)
2016-04-28 20:45:25.767492: step 391, loss = 92.23 (3.3 examples/sec; 19.631 sec/batch)
2016-04-28 20:45:45.192220: step 392, loss = 92.47 (3.3 examples/sec; 19.425 sec/batch)
2016-04-28 20:46:05.431029: step 393, loss = 92.22 (3.2 examples/sec; 20.239 sec/batch)
2016-04-28 20:46:25.544114: step 394, loss = 92.25 (3.2 examples/sec; 20.113 sec/batch)
2016-04-28 20:46:44.852458: step 395, loss = 92.01 (3.3 examples/sec; 19.308 sec/batch)
2016-04-28 20:47:04.970170: step 396, loss = 92.03 (3.2 examples/sec; 20.118 sec/batch)
2016-04-28 20:47:24.652006: step 397, loss = 91.87 (3.3 examples/sec; 19.682 sec/batch)
2016-04-28 20:47:44.460220: step 398, loss = 91.73 (3.2 examples/sec; 19.808 sec/batch)
2016-04-28 20:48:04.319559: step 399, loss = 91.80 (3.2 examples/sec; 19.859 sec/batch)
2016-04-28 20:48:24.037132: step 400, loss = 91.55 (3.2 examples/sec; 19.717 sec/batch)
2016-04-28 20:49:09.173386: step 401, loss = 91.56 (3.1 examples/sec; 20.387 sec/batch)
2016-04-28 20:49:28.731227: step 402, loss = 91.56 (3.3 examples/sec; 19.558 sec/batch)
2016-04-28 20:49:48.792766: step 403, loss = 91.42 (3.2 examples/sec; 20.061 sec/batch)
2016-04-28 20:50:08.531448: step 404, loss = 91.38 (3.2 examples/sec; 19.739 sec/batch)
2016-04-28 20:50:28.562331: step 405, loss = 91.22 (3.2 examples/sec; 20.031 sec/batch)
2016-04-28 20:50:48.481532: step 406, loss = 91.22 (3.2 examples/sec; 19.919 sec/batch)
2016-04-28 20:51:07.987886: step 407, loss = 91.33 (3.3 examples/sec; 19.506 sec/batch)
2016-04-28 20:51:28.182512: step 408, loss = 90.93 (3.2 examples/sec; 20.195 sec/batch)
2016-04-28 20:51:48.838723: step 409, loss = 90.98 (3.1 examples/sec; 20.656 sec/batch)
2016-04-28 20:52:08.948369: step 410, loss = 91.04 (3.2 examples/sec; 20.109 sec/batch)
2016-04-28 20:52:52.507042: step 411, loss = 90.78 (3.3 examples/sec; 19.175 sec/batch)
2016-04-28 20:53:13.903753: step 412, loss = 90.74 (3.0 examples/sec; 21.397 sec/batch)
2016-04-28 20:53:36.580003: step 413, loss = 90.68 (2.8 examples/sec; 22.676 sec/batch)
2016-04-28 20:54:00.600721: step 414, loss = 90.66 (2.7 examples/sec; 24.021 sec/batch)
2016-04-28 20:54:19.663378: step 415, loss = 90.46 (3.4 examples/sec; 19.063 sec/batch)
2016-04-28 20:54:38.811798: step 416, loss = 90.48 (3.3 examples/sec; 19.148 sec/batch)
2016-04-28 20:55:00.675253: step 417, loss = 90.25 (2.9 examples/sec; 21.863 sec/batch)
2016-04-28 20:55:25.425788: step 418, loss = 90.28 (2.6 examples/sec; 24.750 sec/batch)
2016-04-28 20:55:46.970908: step 419, loss = 90.24 (3.0 examples/sec; 21.545 sec/batch)
2016-04-28 20:56:07.964641: step 420, loss = 90.14 (3.0 examples/sec; 20.994 sec/batch)
2016-04-28 20:56:56.382844: step 421, loss = 90.28 (3.0 examples/sec; 21.678 sec/batch)
2016-04-28 20:57:18.463843: step 422, loss = 90.17 (2.9 examples/sec; 22.081 sec/batch)
2016-04-28 20:57:39.392616: step 423, loss = 90.03 (3.1 examples/sec; 20.929 sec/batch)
2016-04-28 20:58:00.642500: step 424, loss = 89.87 (3.0 examples/sec; 21.250 sec/batch)
2016-04-28 20:58:21.545849: step 425, loss = 89.88 (3.1 examples/sec; 20.903 sec/batch)
2016-04-28 20:58:42.900593: step 426, loss = 89.78 (3.0 examples/sec; 21.355 sec/batch)
2016-04-28 20:59:04.412684: step 427, loss = 89.54 (3.0 examples/sec; 21.512 sec/batch)
2016-04-28 20:59:24.987020: step 428, loss = 89.75 (3.1 examples/sec; 20.574 sec/batch)
2016-04-28 20:59:46.354000: step 429, loss = 89.59 (3.0 examples/sec; 21.367 sec/batch)
2016-04-28 21:00:07.935772: step 430, loss = 89.47 (3.0 examples/sec; 21.582 sec/batch)
2016-04-28 21:01:08.379722: step 431, loss = 89.26 (2.8 examples/sec; 22.781 sec/batch)
2016-04-28 21:01:31.177032: step 432, loss = 89.24 (2.8 examples/sec; 22.797 sec/batch)
2016-04-28 21:01:53.915724: step 433, loss = 89.03 (2.8 examples/sec; 22.739 sec/batch)
2016-04-28 21:02:19.591052: step 434, loss = 89.10 (2.5 examples/sec; 25.675 sec/batch)
2016-04-28 21:02:43.106489: step 435, loss = 89.14 (2.7 examples/sec; 23.515 sec/batch)
2016-04-28 21:03:04.689404: step 436, loss = 89.10 (3.0 examples/sec; 21.583 sec/batch)
2016-04-28 21:03:25.718430: step 437, loss = 88.85 (3.0 examples/sec; 21.029 sec/batch)
2016-04-28 21:03:46.642082: step 438, loss = 88.92 (3.1 examples/sec; 20.924 sec/batch)
2016-04-28 21:04:07.503336: step 439, loss = 88.89 (3.1 examples/sec; 20.861 sec/batch)
2016-04-28 21:04:28.053094: step 440, loss = 88.82 (3.1 examples/sec; 20.550 sec/batch)
2016-04-28 21:05:14.984534: step 441, loss = 88.54 (3.1 examples/sec; 20.853 sec/batch)
2016-04-28 21:05:35.071035: step 442, loss = 88.64 (3.2 examples/sec; 20.086 sec/batch)
2016-04-28 21:05:56.105526: step 443, loss = 88.64 (3.0 examples/sec; 21.034 sec/batch)
2016-04-28 21:06:16.849293: step 444, loss = 88.52 (3.1 examples/sec; 20.744 sec/batch)
2016-04-28 21:06:37.674474: step 445, loss = 88.48 (3.1 examples/sec; 20.825 sec/batch)
2016-04-28 21:06:58.440643: step 446, loss = 88.44 (3.1 examples/sec; 20.766 sec/batch)
2016-04-28 21:07:22.723025: step 447, loss = 88.37 (2.6 examples/sec; 24.282 sec/batch)
2016-04-28 21:07:53.814524: step 448, loss = 88.31 (2.1 examples/sec; 31.091 sec/batch)
2016-04-28 21:08:21.705318: step 449, loss = 88.17 (2.3 examples/sec; 27.891 sec/batch)
2016-04-28 21:08:49.060620: step 450, loss = 88.08 (2.3 examples/sec; 27.355 sec/batch)
2016-04-28 21:09:49.232832: step 451, loss = 88.08 (2.3 examples/sec; 27.669 sec/batch)
2016-04-28 21:10:11.079029: step 452, loss = 88.14 (2.9 examples/sec; 21.846 sec/batch)
2016-04-28 21:10:30.506474: step 453, loss = 87.80 (3.3 examples/sec; 19.427 sec/batch)
2016-04-28 21:10:50.206560: step 454, loss = 87.88 (3.2 examples/sec; 19.700 sec/batch)
2016-04-28 21:11:11.109341: step 455, loss = 87.90 (3.1 examples/sec; 20.903 sec/batch)
2016-04-28 21:11:30.960565: step 456, loss = 87.69 (3.2 examples/sec; 19.851 sec/batch)
2016-04-28 21:11:50.651218: step 457, loss = 87.55 (3.3 examples/sec; 19.691 sec/batch)
2016-04-28 21:12:10.219584: step 458, loss = 87.31 (3.3 examples/sec; 19.568 sec/batch)
2016-04-28 21:12:30.522463: step 459, loss = 87.57 (3.2 examples/sec; 20.303 sec/batch)
2016-04-28 21:12:50.130669: step 460, loss = 87.27 (3.3 examples/sec; 19.608 sec/batch)
2016-04-28 21:13:38.243256: step 461, loss = 87.40 (3.0 examples/sec; 21.587 sec/batch)
2016-04-28 21:13:59.461943: step 462, loss = 87.25 (3.0 examples/sec; 21.219 sec/batch)
2016-04-28 21:14:20.756024: step 463, loss = 87.13 (3.0 examples/sec; 21.294 sec/batch)
2016-04-28 21:14:42.786224: step 464, loss = 87.00 (2.9 examples/sec; 22.030 sec/batch)
2016-04-28 21:15:14.311902: step 465, loss = 87.09 (2.0 examples/sec; 31.524 sec/batch)
2016-04-28 21:15:36.689677: step 466, loss = 86.84 (2.9 examples/sec; 22.377 sec/batch)
2016-04-28 21:15:59.037354: step 467, loss = 86.88 (2.9 examples/sec; 22.348 sec/batch)
2016-04-28 21:16:21.900738: step 468, loss = 86.89 (2.8 examples/sec; 22.863 sec/batch)
2016-04-28 21:16:43.994553: step 469, loss = 86.76 (2.9 examples/sec; 22.094 sec/batch)
2016-04-28 21:17:06.284692: step 470, loss = 86.78 (2.9 examples/sec; 22.290 sec/batch)
2016-04-28 21:17:56.219098: step 471, loss = 86.60 (2.9 examples/sec; 22.109 sec/batch)
2016-04-28 21:18:18.233226: step 472, loss = 86.42 (2.9 examples/sec; 22.014 sec/batch)
2016-04-28 21:18:43.412795: step 473, loss = 86.35 (2.5 examples/sec; 25.180 sec/batch)
2016-04-28 21:19:07.437046: step 474, loss = 86.46 (2.7 examples/sec; 24.024 sec/batch)
2016-04-28 21:19:30.738049: step 475, loss = 86.37 (2.7 examples/sec; 23.301 sec/batch)
2016-04-28 21:19:53.906557: step 476, loss = 86.36 (2.8 examples/sec; 23.168 sec/batch)
2016-04-28 21:20:16.853451: step 477, loss = 86.19 (2.8 examples/sec; 22.947 sec/batch)
2016-04-28 21:20:39.694992: step 478, loss = 86.24 (2.8 examples/sec; 22.841 sec/batch)
2016-04-28 21:21:02.490595: step 479, loss = 86.12 (2.8 examples/sec; 22.796 sec/batch)
2016-04-28 21:21:25.527282: step 480, loss = 85.95 (2.8 examples/sec; 23.037 sec/batch)
2016-04-28 21:22:19.038452: step 481, loss = 86.06 (2.5 examples/sec; 25.931 sec/batch)
2016-04-28 21:22:46.571856: step 482, loss = 85.89 (2.3 examples/sec; 27.533 sec/batch)
2016-04-28 21:23:10.120873: step 483, loss = 85.87 (2.7 examples/sec; 23.549 sec/batch)
2016-04-28 21:23:31.625816: step 484, loss = 85.84 (3.0 examples/sec; 21.505 sec/batch)
2016-04-28 21:23:53.755011: step 485, loss = 85.44 (2.9 examples/sec; 22.129 sec/batch)
2016-04-28 21:24:18.371981: step 486, loss = 85.50 (2.6 examples/sec; 24.617 sec/batch)
2016-04-28 21:24:42.249724: step 487, loss = 85.62 (2.7 examples/sec; 23.878 sec/batch)
2016-04-28 21:25:09.873125: step 488, loss = 85.50 (2.3 examples/sec; 27.623 sec/batch)
2016-04-28 21:25:30.224015: step 489, loss = 85.19 (3.1 examples/sec; 20.351 sec/batch)
2016-04-28 21:25:51.618978: step 490, loss = 85.40 (3.0 examples/sec; 21.395 sec/batch)
2016-04-28 21:26:38.487139: step 491, loss = 85.26 (3.2 examples/sec; 20.270 sec/batch)
2016-04-28 21:27:02.346560: step 492, loss = 85.24 (2.7 examples/sec; 23.859 sec/batch)
2016-04-28 21:27:31.477970: step 493, loss = 85.11 (2.2 examples/sec; 29.131 sec/batch)
2016-04-28 21:27:56.506465: step 494, loss = 85.02 (2.6 examples/sec; 25.028 sec/batch)
2016-04-28 21:28:20.550502: step 495, loss = 85.08 (2.7 examples/sec; 24.044 sec/batch)
2016-04-28 21:28:44.559864: step 496, loss = 84.98 (2.7 examples/sec; 24.009 sec/batch)
2016-04-28 21:29:15.279355: step 497, loss = 84.90 (2.1 examples/sec; 30.719 sec/batch)
2016-04-28 21:30:03.854421: step 498, loss = 84.76 (1.3 examples/sec; 48.574 sec/batch)
2016-04-28 21:30:52.559079: step 499, loss = 84.82 (1.3 examples/sec; 48.704 sec/batch)

Process finished with exit code 0
