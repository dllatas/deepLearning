/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
W tensorflow/core/common_runtime/executor.cc:1102] 0x7fb2b0078760 Compute status: Cancelled: Enqueue operation was cancelled
	 [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device="/job:localhost/replica:0/task:0/cpu:0"](input_producer, input_producer/RandomShuffle)]]
I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-21 12:53:58.208113: step 0, loss = 37.11 (0.7 examples/sec; 97.865 sec/batch)
2016-04-21 13:02:52.787666: step 50, loss = 33.81 (5.9 examples/sec; 10.933 sec/batch)
2016-04-21 13:12:36.371057: step 100, loss = 32.48 (4.3 examples/sec; 14.932 sec/batch)
2016-04-21 13:22:52.234520: step 150, loss = 31.21 (5.3 examples/sec; 12.041 sec/batch)
2016-04-21 13:33:16.621168: step 200, loss = 29.98 (4.6 examples/sec; 14.062 sec/batch)
2016-04-21 13:43:54.406871: step 250, loss = 28.81 (6.1 examples/sec; 10.419 sec/batch)
2016-04-21 13:53:45.645389: step 300, loss = 27.68 (5.6 examples/sec; 11.452 sec/batch)
2016-04-21 14:03:34.635607: step 350, loss = 26.59 (5.9 examples/sec; 10.769 sec/batch)
2016-04-21 14:13:12.160955: step 400, loss = 25.55 (5.4 examples/sec; 11.927 sec/batch)
2016-04-21 14:22:47.412383: step 450, loss = 24.55 (6.4 examples/sec; 10.028 sec/batch)
2016-04-21 14:31:11.315881: step 500, loss = 23.58 (6.7 examples/sec; 9.513 sec/batch)
2016-04-21 14:38:53.829730: step 550, loss = 22.66 (6.9 examples/sec; 9.264 sec/batch)
2016-04-21 14:46:28.798233: step 600, loss = 21.77 (7.2 examples/sec; 8.892 sec/batch)
2016-04-21 14:54:00.352058: step 650, loss = 20.92 (7.2 examples/sec; 8.883 sec/batch)
2016-04-21 15:01:30.174211: step 700, loss = 20.10 (7.2 examples/sec; 8.927 sec/batch)
2016-04-21 15:09:01.386561: step 750, loss = 19.31 (5.8 examples/sec; 11.104 sec/batch)
2016-04-21 15:16:37.065922: step 800, loss = 18.55 (6.4 examples/sec; 10.066 sec/batch)
2016-04-21 15:25:26.585299: step 850, loss = 17.82 (7.2 examples/sec; 8.938 sec/batch)
2016-04-21 15:34:53.791727: step 900, loss = 17.13 (5.5 examples/sec; 11.554 sec/batch)
2016-04-21 15:44:17.527072: step 950, loss = 16.46 (6.7 examples/sec; 9.502 sec/batch)
2016-04-21 15:51:46.153368: step 1000, loss = 15.81 (7.4 examples/sec; 8.613 sec/batch)
2016-04-21 15:59:25.660395: step 1050, loss = 15.19 (6.8 examples/sec; 9.448 sec/batch)
2016-04-21 16:07:13.315176: step 1100, loss = 14.59 (7.0 examples/sec; 9.196 sec/batch)
2016-04-21 16:15:02.961995: step 1150, loss = 14.02 (6.6 examples/sec; 9.695 sec/batch)
2016-04-21 16:22:46.691964: step 1200, loss = 13.47 (7.1 examples/sec; 9.006 sec/batch)
2016-04-21 16:30:31.300429: step 1250, loss = 12.94 (6.9 examples/sec; 9.311 sec/batch)
2016-04-21 16:38:17.237810: step 1300, loss = 12.43 (7.1 examples/sec; 9.029 sec/batch)
2016-04-21 16:46:03.684065: step 1350, loss = 11.95 (6.7 examples/sec; 9.493 sec/batch)
2016-04-21 16:53:39.463384: step 1400, loss = 11.48 (7.1 examples/sec; 9.021 sec/batch)
2016-04-21 17:01:08.021790: step 1450, loss = 11.03 (7.0 examples/sec; 9.130 sec/batch)
2016-04-21 17:08:41.103878: step 1500, loss = 10.60 (7.1 examples/sec; 9.004 sec/batch)
2016-04-21 17:16:31.533588: step 1550, loss = 10.18 (7.3 examples/sec; 8.777 sec/batch)
2016-04-21 17:24:02.803333: step 1600, loss = 9.78 (6.9 examples/sec; 9.280 sec/batch)
2016-04-21 17:31:36.615340: step 1650, loss = 9.40 (7.1 examples/sec; 9.077 sec/batch)
2016-04-21 17:39:06.264679: step 1700, loss = 9.03 (6.9 examples/sec; 9.268 sec/batch)
2016-04-21 17:46:36.744840: step 1750, loss = 8.68 (7.3 examples/sec; 8.818 sec/batch)
2016-04-21 17:54:10.180767: step 1800, loss = 8.34 (6.8 examples/sec; 9.389 sec/batch)
2016-04-21 18:01:44.580089: step 1850, loss = 8.01 (7.1 examples/sec; 9.022 sec/batch)
2016-04-21 18:09:19.296509: step 1900, loss = 7.69 (7.1 examples/sec; 9.051 sec/batch)
2016-04-21 18:16:53.229284: step 1950, loss = 7.39 (7.1 examples/sec; 9.036 sec/batch)
2016-04-21 18:24:32.366844: step 2000, loss = 7.10 (7.2 examples/sec; 8.843 sec/batch)
2016-04-21 18:32:19.424083: step 2050, loss = 6.82 (7.0 examples/sec; 9.192 sec/batch)
2016-04-21 18:39:52.169186: step 2100, loss = 6.56 (7.2 examples/sec; 8.873 sec/batch)
2016-04-21 18:47:30.476993: step 2150, loss = 6.30 (7.0 examples/sec; 9.116 sec/batch)
2016-04-21 18:55:06.579066: step 2200, loss = 6.05 (6.9 examples/sec; 9.225 sec/batch)
2016-04-21 19:02:45.369627: step 2250, loss = 5.81 (6.9 examples/sec; 9.220 sec/batch)
2016-04-21 19:10:21.659845: step 2300, loss = 5.59 (7.1 examples/sec; 9.069 sec/batch)
2016-04-21 19:17:58.954955: step 2350, loss = 5.37 (7.1 examples/sec; 9.058 sec/batch)
2016-04-21 19:25:34.276519: step 2400, loss = 5.16 (7.0 examples/sec; 9.124 sec/batch)
2016-04-21 19:33:10.475154: step 2450, loss = 4.95 (7.0 examples/sec; 9.135 sec/batch)
2016-04-21 19:40:44.480378: step 2500, loss = 4.76 (7.0 examples/sec; 9.094 sec/batch)
2016-04-21 19:48:32.012230: step 2550, loss = 4.57 (7.0 examples/sec; 9.168 sec/batch)
2016-04-21 19:56:10.065196: step 2600, loss = 4.40 (6.8 examples/sec; 9.400 sec/batch)
2016-04-21 20:03:50.781949: step 2650, loss = 4.22 (6.9 examples/sec; 9.213 sec/batch)
2016-04-21 20:11:29.456726: step 2700, loss = 4.06 (7.2 examples/sec; 8.919 sec/batch)
2016-04-21 20:19:09.292223: step 2750, loss = 3.90 (6.9 examples/sec; 9.253 sec/batch)
2016-04-21 20:26:47.671503: step 2800, loss = 3.75 (7.0 examples/sec; 9.108 sec/batch)
2016-04-21 20:34:26.519630: step 2850, loss = 3.60 (7.0 examples/sec; 9.119 sec/batch)
2016-04-21 20:42:05.638136: step 2900, loss = 3.46 (6.7 examples/sec; 9.528 sec/batch)
2016-04-21 20:49:45.489963: step 2950, loss = 3.32 (6.9 examples/sec; 9.285 sec/batch)
2016-04-21 20:57:29.391966: step 3000, loss = 3.19 (6.8 examples/sec; 9.398 sec/batch)
2016-04-21 21:05:24.188555: step 3050, loss = 3.07 (6.9 examples/sec; 9.271 sec/batch)
2016-04-21 21:13:03.563392: step 3100, loss = 2.95 (6.9 examples/sec; 9.246 sec/batch)
2016-04-21 21:20:43.602301: step 3150, loss = 2.83 (7.0 examples/sec; 9.105 sec/batch)
2016-04-21 21:28:23.706685: step 3200, loss = 2.72 (7.1 examples/sec; 8.952 sec/batch)
2016-04-21 21:36:05.262424: step 3250, loss = 2.61 (6.8 examples/sec; 9.428 sec/batch)
2016-04-21 21:43:49.182163: step 3300, loss = 2.51 (6.9 examples/sec; 9.312 sec/batch)
2016-04-21 21:51:34.918879: step 3350, loss = 2.41 (7.0 examples/sec; 9.098 sec/batch)
2016-04-21 21:59:18.339611: step 3400, loss = 2.32 (6.9 examples/sec; 9.228 sec/batch)
2016-04-21 22:07:01.105585: step 3450, loss = 2.23 (7.0 examples/sec; 9.192 sec/batch)
2016-04-21 22:14:43.825995: step 3500, loss = 2.14 (7.0 examples/sec; 9.129 sec/batch)
2016-04-21 22:22:37.724171: step 3550, loss = 2.06 (7.0 examples/sec; 9.184 sec/batch)
2016-04-21 22:30:19.725624: step 3600, loss = 1.98 (6.9 examples/sec; 9.211 sec/batch)
2016-04-21 22:38:02.661854: step 3650, loss = 1.90 (6.8 examples/sec; 9.481 sec/batch)
2016-04-21 22:45:46.620788: step 3700, loss = 1.82 (7.2 examples/sec; 8.924 sec/batch)
2016-04-21 22:53:30.770397: step 3750, loss = 1.75 (7.0 examples/sec; 9.119 sec/batch)
2016-04-21 23:01:16.213542: step 3800, loss = 1.68 (6.8 examples/sec; 9.343 sec/batch)
2016-04-21 23:09:00.644869: step 3850, loss = 1.62 (6.8 examples/sec; 9.363 sec/batch)
2016-04-21 23:16:47.421327: step 3900, loss = 1.55 (7.0 examples/sec; 9.197 sec/batch)
2016-04-21 23:24:31.676950: step 3950, loss = 1.49 (6.9 examples/sec; 9.311 sec/batch)
2016-04-21 23:32:14.676687: step 4000, loss = 1.43 (6.9 examples/sec; 9.210 sec/batch)
2016-04-21 23:40:10.237705: step 4050, loss = 1.38 (6.8 examples/sec; 9.447 sec/batch)
2016-04-21 23:47:55.791085: step 4100, loss = 1.32 (6.8 examples/sec; 9.460 sec/batch)
2016-04-21 23:55:38.099390: step 4150, loss = 1.27 (6.9 examples/sec; 9.287 sec/batch)
2016-04-22 00:03:24.723904: step 4200, loss = 1.22 (7.1 examples/sec; 9.066 sec/batch)
2016-04-22 00:11:11.307168: step 4250, loss = 1.17 (6.7 examples/sec; 9.565 sec/batch)
2016-04-22 00:18:57.215872: step 4300, loss = 1.13 (7.0 examples/sec; 9.186 sec/batch)
2016-04-22 00:26:41.336125: step 4350, loss = 1.08 (6.9 examples/sec; 9.270 sec/batch)
2016-04-22 00:34:26.220131: step 4400, loss = 1.04 (6.9 examples/sec; 9.239 sec/batch)
2016-04-22 00:42:13.451398: step 4450, loss = 1.00 (6.4 examples/sec; 9.985 sec/batch)
2016-04-22 00:49:58.094748: step 4500, loss = 0.96 (6.9 examples/sec; 9.279 sec/batch)
2016-04-22 00:57:51.716471: step 4550, loss = 0.92 (7.0 examples/sec; 9.114 sec/batch)
2016-04-22 01:05:39.975964: step 4600, loss = 0.89 (7.0 examples/sec; 9.205 sec/batch)
2016-04-22 01:16:01.130118: step 4650, loss = 0.85 (3.7 examples/sec; 17.140 sec/batch)
2016-04-22 01:28:14.408297: step 4700, loss = 0.82 (5.4 examples/sec; 11.945 sec/batch)
2016-04-22 01:40:33.507784: step 4750, loss = 0.79 (4.4 examples/sec; 14.431 sec/batch)
2016-04-22 01:53:19.828013: step 4800, loss = 0.76 (4.2 examples/sec; 15.104 sec/batch)
2016-04-22 02:06:06.757507: step 4850, loss = 0.73 (4.1 examples/sec; 15.784 sec/batch)
2016-04-22 02:18:33.263924: step 4900, loss = 0.70 (4.3 examples/sec; 14.882 sec/batch)
2016-04-22 02:30:38.317982: step 4950, loss = 0.67 (4.3 examples/sec; 14.918 sec/batch)
