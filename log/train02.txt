/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-29 12:43:35.426256: step 0, loss = 63.83 (0.3 examples/sec; 210.748 sec/batch)
2016-04-29 12:43:47.503075: step 1, loss = 63.48 (12.5 examples/sec; 5.138 sec/batch)
2016-04-29 12:43:52.776029: step 2, loss = 63.60 (12.1 examples/sec; 5.273 sec/batch)
2016-04-29 12:43:57.976292: step 3, loss = 63.71 (12.3 examples/sec; 5.200 sec/batch)
2016-04-29 12:44:02.971762: step 4, loss = 63.43 (12.8 examples/sec; 4.995 sec/batch)
2016-04-29 12:44:08.749961: step 5, loss = 63.39 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 12:44:14.099845: step 6, loss = 63.53 (12.0 examples/sec; 5.350 sec/batch)
2016-04-29 12:44:19.627342: step 7, loss = 63.44 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 12:44:25.065089: step 8, loss = 63.68 (11.8 examples/sec; 5.438 sec/batch)
2016-04-29 12:44:30.376786: step 9, loss = 63.39 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 12:44:35.436671: step 10, loss = 63.22 (12.6 examples/sec; 5.060 sec/batch)
2016-04-29 12:44:48.106901: step 11, loss = 63.12 (12.9 examples/sec; 4.961 sec/batch)
2016-04-29 12:44:53.380907: step 12, loss = 64.01 (12.1 examples/sec; 5.274 sec/batch)
2016-04-29 12:44:58.677105: step 13, loss = 63.07 (12.1 examples/sec; 5.296 sec/batch)
2016-04-29 12:45:03.942242: step 14, loss = 63.23 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 12:45:09.032337: step 15, loss = 62.89 (12.6 examples/sec; 5.090 sec/batch)
2016-04-29 12:45:14.736063: step 16, loss = 62.91 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 12:45:19.955320: step 17, loss = 63.08 (12.3 examples/sec; 5.219 sec/batch)
2016-04-29 12:45:28.102755: step 18, loss = 62.79 (7.9 examples/sec; 8.147 sec/batch)
2016-04-29 12:45:36.837277: step 19, loss = 62.80 (7.3 examples/sec; 8.734 sec/batch)
2016-04-29 12:45:43.449745: step 20, loss = 62.73 (9.7 examples/sec; 6.612 sec/batch)
2016-04-29 12:45:56.906860: step 21, loss = 62.72 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 12:46:04.113491: step 22, loss = 62.69 (8.9 examples/sec; 7.207 sec/batch)
2016-04-29 12:46:10.216936: step 23, loss = 64.78 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 12:46:16.223994: step 24, loss = 62.65 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 12:46:22.544494: step 25, loss = 62.66 (10.1 examples/sec; 6.320 sec/batch)
2016-04-29 12:46:28.190992: step 26, loss = 62.63 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 12:46:34.031703: step 27, loss = 62.49 (11.0 examples/sec; 5.841 sec/batch)
2016-04-29 12:46:39.917820: step 28, loss = 62.41 (10.9 examples/sec; 5.886 sec/batch)
2016-04-29 12:46:45.020433: step 29, loss = 62.43 (12.5 examples/sec; 5.103 sec/batch)
2016-04-29 12:46:51.224890: step 30, loss = 62.43 (10.3 examples/sec; 6.204 sec/batch)
2016-04-29 12:47:04.174695: step 31, loss = 62.28 (12.1 examples/sec; 5.307 sec/batch)
2016-04-29 12:47:09.537911: step 32, loss = 62.39 (11.9 examples/sec; 5.363 sec/batch)
2016-04-29 12:47:14.791521: step 33, loss = 61.89 (12.2 examples/sec; 5.254 sec/batch)
2016-04-29 12:47:20.279468: step 34, loss = 62.07 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 12:47:27.225569: step 35, loss = 61.97 (9.2 examples/sec; 6.946 sec/batch)
2016-04-29 12:47:32.875705: step 36, loss = 61.91 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 12:47:39.244451: step 37, loss = 61.93 (10.0 examples/sec; 6.369 sec/batch)
2016-04-29 12:47:45.992027: step 38, loss = 61.86 (9.5 examples/sec; 6.747 sec/batch)
2016-04-29 12:47:50.978657: step 39, loss = 61.86 (12.8 examples/sec; 4.986 sec/batch)
2016-04-29 12:47:56.380103: step 40, loss = 61.87 (11.8 examples/sec; 5.401 sec/batch)
2016-04-29 12:48:08.032334: step 41, loss = 61.69 (13.1 examples/sec; 4.875 sec/batch)
2016-04-29 12:48:12.786924: step 42, loss = 61.72 (13.5 examples/sec; 4.755 sec/batch)
2016-04-29 12:48:17.813989: step 43, loss = 61.61 (12.7 examples/sec; 5.027 sec/batch)
2016-04-29 12:48:22.887472: step 44, loss = 61.69 (12.6 examples/sec; 5.073 sec/batch)
2016-04-29 12:48:27.847993: step 45, loss = 61.48 (12.9 examples/sec; 4.960 sec/batch)
2016-04-29 12:48:33.047148: step 46, loss = 61.51 (12.3 examples/sec; 5.199 sec/batch)
2016-04-29 12:48:37.912008: step 47, loss = 61.50 (13.2 examples/sec; 4.865 sec/batch)
2016-04-29 12:48:42.797921: step 48, loss = 61.39 (13.1 examples/sec; 4.886 sec/batch)
2016-04-29 12:48:47.645605: step 49, loss = 61.30 (13.2 examples/sec; 4.848 sec/batch)
2016-04-29 12:48:52.828478: step 50, loss = 61.48 (12.3 examples/sec; 5.183 sec/batch)
2016-04-29 12:49:05.034685: step 51, loss = 61.04 (11.4 examples/sec; 5.634 sec/batch)
2016-04-29 12:49:09.942004: step 52, loss = 61.19 (13.0 examples/sec; 4.907 sec/batch)
2016-04-29 12:49:14.659038: step 53, loss = 61.21 (13.6 examples/sec; 4.717 sec/batch)
2016-04-29 12:49:19.674998: step 54, loss = 61.12 (12.8 examples/sec; 5.016 sec/batch)
2016-04-29 12:49:24.551378: step 55, loss = 61.05 (13.1 examples/sec; 4.876 sec/batch)
2016-04-29 12:49:29.337514: step 56, loss = 61.10 (13.4 examples/sec; 4.786 sec/batch)
2016-04-29 12:49:34.480093: step 57, loss = 60.82 (12.4 examples/sec; 5.142 sec/batch)
2016-04-29 12:49:39.407402: step 58, loss = 60.90 (13.0 examples/sec; 4.927 sec/batch)
2016-04-29 12:49:44.256456: step 59, loss = 60.77 (13.2 examples/sec; 4.849 sec/batch)
2016-04-29 12:49:49.152840: step 60, loss = 60.70 (13.1 examples/sec; 4.896 sec/batch)
2016-04-29 12:50:00.279873: step 61, loss = 60.80 (13.8 examples/sec; 4.646 sec/batch)
2016-04-29 12:50:05.076334: step 62, loss = 60.56 (13.3 examples/sec; 4.796 sec/batch)
2016-04-29 12:50:10.502469: step 63, loss = 60.72 (11.8 examples/sec; 5.426 sec/batch)
2016-04-29 12:50:15.134408: step 64, loss = 60.74 (13.8 examples/sec; 4.632 sec/batch)
2016-04-29 12:50:20.059039: step 65, loss = 60.69 (13.0 examples/sec; 4.925 sec/batch)
2016-04-29 12:50:24.957411: step 66, loss = 60.49 (13.1 examples/sec; 4.898 sec/batch)
2016-04-29 12:50:29.606444: step 67, loss = 60.14 (13.8 examples/sec; 4.649 sec/batch)
2016-04-29 12:50:34.583496: step 68, loss = 60.57 (12.9 examples/sec; 4.977 sec/batch)
2016-04-29 12:50:39.219633: step 69, loss = 60.46 (13.8 examples/sec; 4.636 sec/batch)
2016-04-29 12:50:44.586490: step 70, loss = 60.15 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 12:50:55.890937: step 71, loss = 60.34 (13.3 examples/sec; 4.797 sec/batch)
2016-04-29 12:51:00.904698: step 72, loss = 60.21 (12.8 examples/sec; 5.014 sec/batch)
2016-04-29 12:51:05.656436: step 73, loss = 60.05 (13.5 examples/sec; 4.752 sec/batch)
2016-04-29 12:51:10.643447: step 74, loss = 60.20 (12.8 examples/sec; 4.987 sec/batch)
2016-04-29 12:51:16.082565: step 75, loss = 60.21 (11.8 examples/sec; 5.439 sec/batch)
2016-04-29 12:51:20.842939: step 76, loss = 60.18 (13.4 examples/sec; 4.760 sec/batch)
2016-04-29 12:51:25.776407: step 77, loss = 60.00 (13.0 examples/sec; 4.933 sec/batch)
2016-04-29 12:51:30.520982: step 78, loss = 59.89 (13.5 examples/sec; 4.744 sec/batch)
2016-04-29 12:51:35.308485: step 79, loss = 59.86 (13.4 examples/sec; 4.787 sec/batch)
2016-04-29 12:51:40.224308: step 80, loss = 59.90 (13.0 examples/sec; 4.916 sec/batch)
2016-04-29 12:51:52.171060: step 81, loss = 59.76 (13.3 examples/sec; 4.816 sec/batch)
2016-04-29 12:51:56.808944: step 82, loss = 59.64 (13.8 examples/sec; 4.638 sec/batch)
2016-04-29 12:52:01.705759: step 83, loss = 59.82 (13.1 examples/sec; 4.897 sec/batch)
2016-04-29 12:52:06.398999: step 84, loss = 59.61 (13.6 examples/sec; 4.693 sec/batch)
2016-04-29 12:52:11.311691: step 85, loss = 59.60 (13.0 examples/sec; 4.913 sec/batch)
2016-04-29 12:52:16.286780: step 86, loss = 59.56 (12.9 examples/sec; 4.975 sec/batch)
2016-04-29 12:52:21.686381: step 87, loss = 59.26 (11.9 examples/sec; 5.400 sec/batch)
2016-04-29 12:52:26.412868: step 88, loss = 59.50 (13.5 examples/sec; 4.726 sec/batch)
2016-04-29 12:52:31.440978: step 89, loss = 59.47 (12.7 examples/sec; 5.028 sec/batch)
2016-04-29 12:52:36.328166: step 90, loss = 59.43 (13.1 examples/sec; 4.887 sec/batch)
2016-04-29 12:52:51.234549: step 91, loss = 59.33 (8.1 examples/sec; 7.883 sec/batch)
2016-04-29 12:52:59.915236: step 92, loss = 59.31 (7.4 examples/sec; 8.681 sec/batch)
2016-04-29 12:53:05.633452: step 93, loss = 59.27 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 12:53:10.916452: step 94, loss = 59.21 (12.1 examples/sec; 5.283 sec/batch)
2016-04-29 12:53:16.499385: step 95, loss = 59.09 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 12:53:21.721805: step 96, loss = 59.13 (12.3 examples/sec; 5.222 sec/batch)
2016-04-29 12:53:28.092301: step 97, loss = 59.16 (10.0 examples/sec; 6.370 sec/batch)
2016-04-29 12:53:33.267495: step 98, loss = 58.88 (12.4 examples/sec; 5.175 sec/batch)
2016-04-29 12:53:38.642462: step 99, loss = 58.91 (11.9 examples/sec; 5.375 sec/batch)
2016-04-29 12:53:44.103308: step 100, loss = 58.82 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 12:53:56.389717: step 101, loss = 58.82 (12.6 examples/sec; 5.087 sec/batch)
2016-04-29 12:54:02.440934: step 102, loss = 58.86 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 12:54:07.951236: step 103, loss = 58.84 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 12:54:13.366504: step 104, loss = 58.69 (11.8 examples/sec; 5.415 sec/batch)
2016-04-29 12:54:18.484747: step 105, loss = 58.51 (12.5 examples/sec; 5.118 sec/batch)
2016-04-29 12:54:24.518419: step 106, loss = 58.72 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 12:54:30.670882: step 107, loss = 58.60 (10.4 examples/sec; 6.152 sec/batch)
2016-04-29 12:54:39.776168: step 108, loss = 58.46 (7.0 examples/sec; 9.105 sec/batch)
2016-04-29 12:54:48.538740: step 109, loss = 58.44 (7.3 examples/sec; 8.762 sec/batch)
2016-04-29 12:54:55.248466: step 110, loss = 58.51 (9.5 examples/sec; 6.710 sec/batch)
2016-04-29 12:55:10.195490: step 111, loss = 58.31 (9.2 examples/sec; 6.962 sec/batch)
2016-04-29 12:55:16.053593: step 112, loss = 58.54 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 12:55:22.186041: step 113, loss = 58.23 (10.4 examples/sec; 6.132 sec/batch)
2016-04-29 12:55:29.162166: step 114, loss = 58.27 (9.2 examples/sec; 6.976 sec/batch)
2016-04-29 12:55:35.461618: step 115, loss = 58.24 (10.2 examples/sec; 6.299 sec/batch)
2016-04-29 12:55:42.531159: step 116, loss = 58.16 (9.1 examples/sec; 7.069 sec/batch)
2016-04-29 12:55:49.360632: step 117, loss = 58.22 (9.4 examples/sec; 6.829 sec/batch)
2016-04-29 12:55:55.731639: step 118, loss = 58.05 (10.0 examples/sec; 6.371 sec/batch)
2016-04-29 12:56:02.497341: step 119, loss = 57.95 (9.5 examples/sec; 6.766 sec/batch)
2016-04-29 12:56:09.300602: step 120, loss = 58.08 (9.4 examples/sec; 6.803 sec/batch)
2016-04-29 12:56:27.304327: step 121, loss = 57.82 (10.2 examples/sec; 6.281 sec/batch)
2016-04-29 12:56:34.762657: step 122, loss = 58.02 (8.6 examples/sec; 7.458 sec/batch)
2016-04-29 12:56:41.236193: step 123, loss = 57.98 (9.9 examples/sec; 6.473 sec/batch)
2016-04-29 12:56:47.513709: step 124, loss = 57.85 (10.2 examples/sec; 6.277 sec/batch)
2016-04-29 12:56:52.992478: step 125, loss = 57.72 (11.7 examples/sec; 5.479 sec/batch)
2016-04-29 12:56:58.372515: step 126, loss = 57.70 (11.9 examples/sec; 5.380 sec/batch)
2016-04-29 12:57:03.944008: step 127, loss = 57.79 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 12:57:09.104384: step 128, loss = 57.69 (12.4 examples/sec; 5.160 sec/batch)
2016-04-29 12:57:14.416646: step 129, loss = 57.57 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 12:57:20.661949: step 130, loss = 57.65 (10.2 examples/sec; 6.245 sec/batch)
2016-04-29 12:57:33.451451: step 131, loss = 57.39 (12.5 examples/sec; 5.139 sec/batch)
2016-04-29 12:57:38.945276: step 132, loss = 57.47 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 12:57:44.345862: step 133, loss = 57.34 (11.9 examples/sec; 5.400 sec/batch)
2016-04-29 12:57:50.045508: step 134, loss = 57.26 (11.2 examples/sec; 5.700 sec/batch)
2016-04-29 12:57:56.225970: step 135, loss = 57.32 (10.4 examples/sec; 6.180 sec/batch)
2016-04-29 12:58:02.082640: step 136, loss = 57.35 (10.9 examples/sec; 5.857 sec/batch)
2016-04-29 12:58:07.707232: step 137, loss = 57.22 (11.4 examples/sec; 5.625 sec/batch)
2016-04-29 12:58:13.347578: step 138, loss = 57.02 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 12:58:18.642091: step 139, loss = 57.19 (12.1 examples/sec; 5.294 sec/batch)
2016-04-29 12:58:24.133822: step 140, loss = 56.91 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 12:58:39.833799: step 141, loss = 56.90 (12.0 examples/sec; 5.317 sec/batch)
2016-04-29 12:58:45.143441: step 142, loss = 57.14 (12.1 examples/sec; 5.310 sec/batch)
2016-04-29 12:58:50.783834: step 143, loss = 56.94 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 12:58:56.560952: step 144, loss = 57.07 (11.1 examples/sec; 5.777 sec/batch)
2016-04-29 12:59:04.214927: step 145, loss = 56.88 (8.4 examples/sec; 7.654 sec/batch)
2016-04-29 12:59:09.471384: step 146, loss = 56.71 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 12:59:15.132211: step 147, loss = 56.63 (11.3 examples/sec; 5.661 sec/batch)
2016-04-29 12:59:20.709244: step 148, loss = 56.65 (11.5 examples/sec; 5.577 sec/batch)
2016-04-29 12:59:26.608801: step 149, loss = 56.62 (10.8 examples/sec; 5.899 sec/batch)
2016-04-29 12:59:32.976173: step 150, loss = 56.64 (10.1 examples/sec; 6.367 sec/batch)
2016-04-29 12:59:46.260821: step 151, loss = 56.44 (11.9 examples/sec; 5.399 sec/batch)
2016-04-29 12:59:51.980486: step 152, loss = 56.37 (11.2 examples/sec; 5.720 sec/batch)
2016-04-29 12:59:57.785862: step 153, loss = 56.27 (11.0 examples/sec; 5.805 sec/batch)
2016-04-29 13:00:03.616100: step 154, loss = 56.32 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 13:00:09.519945: step 155, loss = 56.26 (10.8 examples/sec; 5.904 sec/batch)
2016-04-29 13:00:18.403604: step 156, loss = 56.41 (7.2 examples/sec; 8.884 sec/batch)
2016-04-29 13:00:25.063680: step 157, loss = 56.23 (9.6 examples/sec; 6.660 sec/batch)
2016-04-29 13:00:30.469329: step 158, loss = 56.23 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 13:00:35.930905: step 159, loss = 56.02 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 13:00:42.013488: step 160, loss = 56.38 (10.5 examples/sec; 6.082 sec/batch)
2016-04-29 13:01:00.346239: step 161, loss = 56.19 (6.5 examples/sec; 9.896 sec/batch)
2016-04-29 13:01:06.757385: step 162, loss = 55.99 (10.0 examples/sec; 6.411 sec/batch)
2016-04-29 13:01:14.484401: step 163, loss = 56.01 (8.3 examples/sec; 7.727 sec/batch)
2016-04-29 13:01:20.548106: step 164, loss = 55.90 (10.6 examples/sec; 6.064 sec/batch)
2016-04-29 13:01:26.186723: step 165, loss = 55.94 (11.4 examples/sec; 5.639 sec/batch)
2016-04-29 13:01:31.897573: step 166, loss = 55.89 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 13:01:37.496458: step 167, loss = 55.66 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 13:01:43.249137: step 168, loss = 55.96 (11.1 examples/sec; 5.753 sec/batch)
2016-04-29 13:01:49.405243: step 169, loss = 55.79 (10.4 examples/sec; 6.156 sec/batch)
2016-04-29 13:01:54.803022: step 170, loss = 55.76 (11.9 examples/sec; 5.398 sec/batch)
2016-04-29 13:02:07.990757: step 171, loss = 55.81 (11.3 examples/sec; 5.639 sec/batch)
2016-04-29 13:02:14.181088: step 172, loss = 55.71 (10.3 examples/sec; 6.190 sec/batch)
2016-04-29 13:02:20.524834: step 173, loss = 55.65 (10.1 examples/sec; 6.344 sec/batch)
2016-04-29 13:02:26.285911: step 174, loss = 55.63 (11.1 examples/sec; 5.761 sec/batch)
2016-04-29 13:02:32.011404: step 175, loss = 55.42 (11.2 examples/sec; 5.725 sec/batch)
2016-04-29 13:02:37.946666: step 176, loss = 55.48 (10.8 examples/sec; 5.935 sec/batch)
2016-04-29 13:02:43.397082: step 177, loss = 55.60 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 13:02:49.090362: step 178, loss = 55.41 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 13:02:55.354853: step 179, loss = 55.43 (10.2 examples/sec; 6.264 sec/batch)
2016-04-29 13:03:01.036202: step 180, loss = 55.26 (11.3 examples/sec; 5.681 sec/batch)
2016-04-29 13:03:14.481802: step 181, loss = 55.30 (10.9 examples/sec; 5.893 sec/batch)
2016-04-29 13:03:20.051046: step 182, loss = 55.10 (11.5 examples/sec; 5.569 sec/batch)
2016-04-29 13:03:26.704450: step 183, loss = 55.29 (9.6 examples/sec; 6.653 sec/batch)
2016-04-29 13:03:32.369675: step 184, loss = 55.12 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 13:03:38.025267: step 185, loss = 55.16 (11.3 examples/sec; 5.655 sec/batch)
2016-04-29 13:03:43.847567: step 186, loss = 55.04 (11.0 examples/sec; 5.822 sec/batch)
2016-04-29 13:03:49.260354: step 187, loss = 54.92 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 13:03:55.065669: step 188, loss = 54.99 (11.0 examples/sec; 5.805 sec/batch)
2016-04-29 13:04:02.071538: step 189, loss = 54.96 (9.1 examples/sec; 7.006 sec/batch)
2016-04-29 13:04:07.886946: step 190, loss = 54.69 (11.0 examples/sec; 5.815 sec/batch)
2016-04-29 13:04:21.378822: step 191, loss = 54.78 (11.1 examples/sec; 5.756 sec/batch)
2016-04-29 13:04:27.062046: step 192, loss = 54.69 (11.3 examples/sec; 5.683 sec/batch)
2016-04-29 13:04:33.288527: step 193, loss = 54.79 (10.3 examples/sec; 6.226 sec/batch)
2016-04-29 13:04:39.124651: step 194, loss = 54.52 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 13:04:44.968769: step 195, loss = 54.70 (11.0 examples/sec; 5.844 sec/batch)
2016-04-29 13:04:50.874482: step 196, loss = 54.56 (10.8 examples/sec; 5.906 sec/batch)
2016-04-29 13:04:56.500133: step 197, loss = 54.66 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 13:05:02.999102: step 198, loss = 54.58 (9.8 examples/sec; 6.499 sec/batch)
2016-04-29 13:05:08.494765: step 199, loss = 54.51 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 13:05:14.318751: step 200, loss = 54.39 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 13:05:28.427353: step 201, loss = 54.52 (10.9 examples/sec; 5.863 sec/batch)
2016-04-29 13:05:36.040343: step 202, loss = 54.41 (8.4 examples/sec; 7.613 sec/batch)
2016-04-29 13:05:41.927494: step 203, loss = 54.40 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 13:05:47.991616: step 204, loss = 54.30 (10.6 examples/sec; 6.064 sec/batch)
2016-04-29 13:05:53.510203: step 205, loss = 54.22 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 13:05:59.042667: step 206, loss = 54.16 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 13:06:04.968573: step 207, loss = 54.19 (10.8 examples/sec; 5.926 sec/batch)
2016-04-29 13:06:11.587973: step 208, loss = 54.17 (9.7 examples/sec; 6.619 sec/batch)
2016-04-29 13:06:17.548436: step 209, loss = 54.00 (10.7 examples/sec; 5.960 sec/batch)
2016-04-29 13:06:23.296584: step 210, loss = 53.97 (11.1 examples/sec; 5.748 sec/batch)
2016-04-29 13:06:37.573474: step 211, loss = 53.94 (11.0 examples/sec; 5.809 sec/batch)
2016-04-29 13:06:43.613413: step 212, loss = 53.84 (10.6 examples/sec; 6.040 sec/batch)
2016-04-29 13:06:48.895902: step 213, loss = 53.84 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 13:06:54.453967: step 214, loss = 53.69 (11.5 examples/sec; 5.558 sec/batch)
2016-04-29 13:07:00.086368: step 215, loss = 53.84 (11.4 examples/sec; 5.632 sec/batch)
2016-04-29 13:07:05.697193: step 216, loss = 53.86 (11.4 examples/sec; 5.611 sec/batch)
2016-04-29 13:07:11.303329: step 217, loss = 53.78 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 13:07:17.257214: step 218, loss = 53.58 (10.7 examples/sec; 5.954 sec/batch)
2016-04-29 13:07:22.853191: step 219, loss = 53.51 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 13:07:28.275514: step 220, loss = 53.69 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 13:07:40.933176: step 221, loss = 53.38 (12.2 examples/sec; 5.261 sec/batch)
2016-04-29 13:07:46.326518: step 222, loss = 53.48 (11.9 examples/sec; 5.393 sec/batch)
2016-04-29 13:07:52.291411: step 223, loss = 53.63 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 13:07:57.513895: step 224, loss = 53.59 (12.3 examples/sec; 5.222 sec/batch)
2016-04-29 13:08:03.147277: step 225, loss = 53.37 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 13:08:08.784561: step 226, loss = 53.37 (11.4 examples/sec; 5.637 sec/batch)
2016-04-29 13:08:14.438975: step 227, loss = 53.37 (11.3 examples/sec; 5.654 sec/batch)
2016-04-29 13:08:20.577503: step 228, loss = 53.32 (10.4 examples/sec; 6.138 sec/batch)
2016-04-29 13:08:26.123043: step 229, loss = 53.32 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 13:08:31.589225: step 230, loss = 53.10 (11.7 examples/sec; 5.466 sec/batch)
2016-04-29 13:08:44.054162: step 231, loss = 53.06 (12.1 examples/sec; 5.281 sec/batch)
2016-04-29 13:08:49.499575: step 232, loss = 53.04 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 13:08:55.674817: step 233, loss = 53.12 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 13:09:01.380886: step 234, loss = 53.00 (11.2 examples/sec; 5.706 sec/batch)
2016-04-29 13:09:06.740546: step 235, loss = 52.93 (11.9 examples/sec; 5.360 sec/batch)
2016-04-29 13:09:12.218342: step 236, loss = 53.03 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 13:09:17.604457: step 237, loss = 52.86 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 13:09:23.113563: step 238, loss = 52.90 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 13:09:29.111907: step 239, loss = 52.76 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 13:09:34.489708: step 240, loss = 52.91 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 13:09:47.045671: step 241, loss = 52.66 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 13:09:52.507872: step 242, loss = 52.63 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 13:09:57.707613: step 243, loss = 52.61 (12.3 examples/sec; 5.200 sec/batch)
2016-04-29 13:10:04.486523: step 244, loss = 52.56 (9.4 examples/sec; 6.779 sec/batch)
2016-04-29 13:10:10.474845: step 245, loss = 52.56 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 13:10:16.317089: step 246, loss = 52.65 (11.0 examples/sec; 5.842 sec/batch)
2016-04-29 13:10:22.063801: step 247, loss = 52.63 (11.1 examples/sec; 5.747 sec/batch)
2016-04-29 13:10:27.668422: step 248, loss = 52.52 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 13:10:34.179510: step 249, loss = 52.39 (9.8 examples/sec; 6.511 sec/batch)
2016-04-29 13:10:39.641289: step 250, loss = 52.35 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 13:10:53.155367: step 251, loss = 52.08 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 13:10:58.858729: step 252, loss = 52.36 (11.2 examples/sec; 5.703 sec/batch)
2016-04-29 13:11:05.700938: step 253, loss = 52.16 (9.4 examples/sec; 6.842 sec/batch)
2016-04-29 13:11:11.773237: step 254, loss = 52.07 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 13:11:17.943725: step 255, loss = 52.11 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 13:11:24.404026: step 256, loss = 52.03 (9.9 examples/sec; 6.460 sec/batch)
2016-04-29 13:11:29.819723: step 257, loss = 52.12 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 13:11:35.276106: step 258, loss = 52.04 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 13:11:41.667420: step 259, loss = 52.04 (10.0 examples/sec; 6.391 sec/batch)
2016-04-29 13:11:47.020962: step 260, loss = 51.85 (12.0 examples/sec; 5.353 sec/batch)
2016-04-29 13:12:01.691915: step 261, loss = 51.79 (9.6 examples/sec; 6.665 sec/batch)
2016-04-29 13:12:07.395970: step 262, loss = 51.92 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 13:12:14.244038: step 263, loss = 51.96 (9.3 examples/sec; 6.848 sec/batch)
2016-04-29 13:12:21.060301: step 264, loss = 51.79 (9.4 examples/sec; 6.816 sec/batch)
2016-04-29 13:12:27.293586: step 265, loss = 51.90 (10.3 examples/sec; 6.233 sec/batch)
2016-04-29 13:12:33.058034: step 266, loss = 51.66 (11.1 examples/sec; 5.764 sec/batch)
2016-04-29 13:12:38.702764: step 267, loss = 51.63 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 13:12:45.244017: step 268, loss = 51.55 (9.8 examples/sec; 6.541 sec/batch)
2016-04-29 13:12:51.404085: step 269, loss = 51.55 (10.4 examples/sec; 6.160 sec/batch)
2016-04-29 13:12:57.300445: step 270, loss = 51.61 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 13:13:11.286604: step 271, loss = 51.56 (10.4 examples/sec; 6.144 sec/batch)
2016-04-29 13:13:17.617050: step 272, loss = 51.52 (10.1 examples/sec; 6.330 sec/batch)
2016-04-29 13:13:23.902319: step 273, loss = 51.36 (10.2 examples/sec; 6.285 sec/batch)
2016-04-29 13:13:29.473433: step 274, loss = 51.29 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 13:13:34.967624: step 275, loss = 51.33 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 13:13:40.671546: step 276, loss = 51.36 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 13:13:46.018735: step 277, loss = 51.29 (12.0 examples/sec; 5.347 sec/batch)
2016-04-29 13:13:52.498806: step 278, loss = 51.36 (9.9 examples/sec; 6.480 sec/batch)
2016-04-29 13:13:58.834763: step 279, loss = 51.28 (10.1 examples/sec; 6.336 sec/batch)
2016-04-29 13:14:05.699169: step 280, loss = 51.03 (9.3 examples/sec; 6.864 sec/batch)
2016-04-29 13:14:23.786019: step 281, loss = 51.09 (9.1 examples/sec; 7.052 sec/batch)
2016-04-29 13:14:31.866692: step 282, loss = 51.18 (7.9 examples/sec; 8.081 sec/batch)
2016-04-29 13:14:38.144661: step 283, loss = 50.93 (10.2 examples/sec; 6.278 sec/batch)
2016-04-29 13:14:44.079236: step 284, loss = 51.06 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 13:14:50.199154: step 285, loss = 51.15 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 13:14:56.931352: step 286, loss = 50.99 (9.5 examples/sec; 6.732 sec/batch)
2016-04-29 13:15:03.508459: step 287, loss = 50.91 (9.7 examples/sec; 6.577 sec/batch)
2016-04-29 13:15:09.594625: step 288, loss = 50.78 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 13:15:15.952799: step 289, loss = 50.83 (10.1 examples/sec; 6.358 sec/batch)
2016-04-29 13:15:22.638193: step 290, loss = 50.92 (9.6 examples/sec; 6.685 sec/batch)
2016-04-29 13:15:37.737149: step 291, loss = 50.55 (10.6 examples/sec; 6.054 sec/batch)
2016-04-29 13:15:43.846821: step 292, loss = 50.78 (10.5 examples/sec; 6.110 sec/batch)
2016-04-29 13:15:49.933511: step 293, loss = 50.57 (10.5 examples/sec; 6.087 sec/batch)
2016-04-29 13:15:56.121541: step 294, loss = 50.63 (10.3 examples/sec; 6.188 sec/batch)
2016-04-29 13:16:02.821801: step 295, loss = 50.44 (9.6 examples/sec; 6.700 sec/batch)
2016-04-29 13:16:09.222952: step 296, loss = 50.62 (10.0 examples/sec; 6.401 sec/batch)
2016-04-29 13:16:14.933706: step 297, loss = 50.42 (11.2 examples/sec; 5.709 sec/batch)
2016-04-29 13:16:20.770434: step 298, loss = 50.49 (11.0 examples/sec; 5.837 sec/batch)
2016-04-29 13:16:26.596835: step 299, loss = 50.22 (11.0 examples/sec; 5.826 sec/batch)
2016-04-29 13:16:33.183990: step 300, loss = 50.36 (9.7 examples/sec; 6.587 sec/batch)
2016-04-29 13:16:47.801874: step 301, loss = 50.58 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 13:16:53.723723: step 302, loss = 50.42 (10.8 examples/sec; 5.922 sec/batch)
2016-04-29 13:16:59.912995: step 303, loss = 50.18 (10.3 examples/sec; 6.189 sec/batch)
2016-04-29 13:17:06.320903: step 304, loss = 50.19 (10.0 examples/sec; 6.408 sec/batch)
2016-04-29 13:17:13.314848: step 305, loss = 50.15 (9.2 examples/sec; 6.994 sec/batch)
2016-04-29 13:17:19.415219: step 306, loss = 50.07 (10.5 examples/sec; 6.100 sec/batch)
2016-04-29 13:17:25.727118: step 307, loss = 49.89 (10.1 examples/sec; 6.312 sec/batch)
2016-04-29 13:17:31.913442: step 308, loss = 49.98 (10.3 examples/sec; 6.186 sec/batch)
2016-04-29 13:17:38.300473: step 309, loss = 50.11 (10.0 examples/sec; 6.387 sec/batch)
2016-04-29 13:17:45.538521: step 310, loss = 50.06 (8.8 examples/sec; 7.238 sec/batch)
2016-04-29 13:17:59.788950: step 311, loss = 49.78 (10.9 examples/sec; 5.848 sec/batch)
2016-04-29 13:18:06.482915: step 312, loss = 49.97 (9.6 examples/sec; 6.694 sec/batch)
2016-04-29 13:18:13.248021: step 313, loss = 49.82 (9.5 examples/sec; 6.765 sec/batch)
2016-04-29 13:18:20.395781: step 314, loss = 49.86 (9.0 examples/sec; 7.148 sec/batch)
2016-04-29 13:18:26.628223: step 315, loss = 49.89 (10.3 examples/sec; 6.232 sec/batch)
2016-04-29 13:18:32.916090: step 316, loss = 49.94 (10.2 examples/sec; 6.288 sec/batch)
2016-04-29 13:18:39.643118: step 317, loss = 49.56 (9.5 examples/sec; 6.727 sec/batch)
2016-04-29 13:18:46.090421: step 318, loss = 49.74 (9.9 examples/sec; 6.447 sec/batch)
2016-04-29 13:18:52.811951: step 319, loss = 49.70 (9.5 examples/sec; 6.721 sec/batch)
2016-04-29 13:18:59.036828: step 320, loss = 49.55 (10.3 examples/sec; 6.225 sec/batch)
2016-04-29 13:19:13.260484: step 321, loss = 49.62 (10.7 examples/sec; 5.982 sec/batch)
2016-04-29 13:19:20.238191: step 322, loss = 49.51 (9.2 examples/sec; 6.978 sec/batch)
2016-04-29 13:19:27.044248: step 323, loss = 49.42 (9.4 examples/sec; 6.806 sec/batch)
2016-04-29 13:19:33.413996: step 324, loss = 49.32 (10.0 examples/sec; 6.370 sec/batch)
2016-04-29 13:19:39.376224: step 325, loss = 49.45 (10.7 examples/sec; 5.962 sec/batch)
2016-04-29 13:19:45.590807: step 326, loss = 49.12 (10.3 examples/sec; 6.214 sec/batch)
2016-04-29 13:19:51.617444: step 327, loss = 49.28 (10.6 examples/sec; 6.027 sec/batch)
2016-04-29 13:19:58.272213: step 328, loss = 49.31 (9.6 examples/sec; 6.655 sec/batch)
2016-04-29 13:20:04.366716: step 329, loss = 49.34 (10.5 examples/sec; 6.094 sec/batch)
2016-04-29 13:20:10.760347: step 330, loss = 49.07 (10.0 examples/sec; 6.394 sec/batch)
2016-04-29 13:20:25.967349: step 331, loss = 49.23 (10.3 examples/sec; 6.191 sec/batch)
2016-04-29 13:20:32.684481: step 332, loss = 49.02 (9.5 examples/sec; 6.717 sec/batch)
2016-04-29 13:20:38.590831: step 333, loss = 49.10 (10.8 examples/sec; 5.906 sec/batch)
2016-04-29 13:20:44.235647: step 334, loss = 49.18 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 13:20:50.183068: step 335, loss = 49.04 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 13:20:56.491825: step 336, loss = 48.97 (10.1 examples/sec; 6.309 sec/batch)
2016-04-29 13:21:02.935025: step 337, loss = 48.96 (9.9 examples/sec; 6.443 sec/batch)
2016-04-29 13:21:08.577817: step 338, loss = 48.87 (11.3 examples/sec; 5.643 sec/batch)
2016-04-29 13:21:14.452160: step 339, loss = 48.96 (10.9 examples/sec; 5.874 sec/batch)
2016-04-29 13:21:20.034012: step 340, loss = 48.88 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 13:21:33.664903: step 341, loss = 48.66 (11.1 examples/sec; 5.789 sec/batch)
2016-04-29 13:21:40.436203: step 342, loss = 48.75 (9.5 examples/sec; 6.771 sec/batch)
2016-04-29 13:21:46.341334: step 343, loss = 48.73 (10.8 examples/sec; 5.905 sec/batch)
2016-04-29 13:21:53.060589: step 344, loss = 48.76 (9.5 examples/sec; 6.719 sec/batch)
2016-04-29 13:21:59.229379: step 345, loss = 48.53 (10.4 examples/sec; 6.169 sec/batch)
2016-04-29 13:22:05.299366: step 346, loss = 48.63 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 13:22:12.171499: step 347, loss = 48.47 (9.3 examples/sec; 6.872 sec/batch)
2016-04-29 13:22:18.865046: step 348, loss = 48.56 (9.6 examples/sec; 6.693 sec/batch)
2016-04-29 13:22:24.968759: step 349, loss = 48.42 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 13:22:31.135589: step 350, loss = 48.52 (10.4 examples/sec; 6.167 sec/batch)
2016-04-29 13:22:46.845963: step 351, loss = 48.50 (8.8 examples/sec; 7.239 sec/batch)
2016-04-29 13:22:53.139112: step 352, loss = 48.43 (10.2 examples/sec; 6.293 sec/batch)
2016-04-29 13:22:59.528881: step 353, loss = 48.33 (10.0 examples/sec; 6.390 sec/batch)
2016-04-29 13:23:06.045330: step 354, loss = 48.12 (9.8 examples/sec; 6.516 sec/batch)
2016-04-29 13:23:12.192301: step 355, loss = 48.22 (10.4 examples/sec; 6.147 sec/batch)
2016-04-29 13:23:19.180341: step 356, loss = 48.08 (9.2 examples/sec; 6.988 sec/batch)
2016-04-29 13:23:25.283979: step 357, loss = 48.15 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 13:23:31.644977: step 358, loss = 48.22 (10.1 examples/sec; 6.361 sec/batch)
2016-04-29 13:23:37.875932: step 359, loss = 48.07 (10.3 examples/sec; 6.231 sec/batch)
2016-04-29 13:23:44.208085: step 360, loss = 48.10 (10.1 examples/sec; 6.332 sec/batch)
2016-04-29 13:24:00.109188: step 361, loss = 48.01 (10.3 examples/sec; 6.184 sec/batch)
2016-04-29 13:24:06.703504: step 362, loss = 48.01 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 13:24:14.264451: step 363, loss = 48.03 (8.5 examples/sec; 7.561 sec/batch)
2016-04-29 13:24:21.309913: step 364, loss = 47.95 (9.1 examples/sec; 7.045 sec/batch)
2016-04-29 13:24:27.415401: step 365, loss = 47.85 (10.5 examples/sec; 6.105 sec/batch)
2016-04-29 13:24:33.511596: step 366, loss = 47.72 (10.5 examples/sec; 6.096 sec/batch)
2016-04-29 13:24:39.530569: step 367, loss = 47.72 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 13:24:45.881552: step 368, loss = 47.81 (10.1 examples/sec; 6.351 sec/batch)
2016-04-29 13:24:52.562267: step 369, loss = 47.74 (9.6 examples/sec; 6.681 sec/batch)
2016-04-29 13:24:59.337417: step 370, loss = 47.71 (9.4 examples/sec; 6.775 sec/batch)
2016-04-29 13:25:13.365956: step 371, loss = 47.65 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 13:25:19.282073: step 372, loss = 47.72 (10.8 examples/sec; 5.916 sec/batch)
2016-04-29 13:25:26.195288: step 373, loss = 47.49 (9.3 examples/sec; 6.913 sec/batch)
2016-04-29 13:25:33.443125: step 374, loss = 47.40 (8.8 examples/sec; 7.248 sec/batch)
2016-04-29 13:25:39.731561: step 375, loss = 47.59 (10.2 examples/sec; 6.288 sec/batch)
2016-04-29 13:25:45.955422: step 376, loss = 47.51 (10.3 examples/sec; 6.224 sec/batch)
2016-04-29 13:25:52.000599: step 377, loss = 47.55 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 13:25:58.354887: step 378, loss = 47.34 (10.1 examples/sec; 6.354 sec/batch)
2016-04-29 13:26:05.634606: step 379, loss = 47.31 (8.8 examples/sec; 7.280 sec/batch)
2016-04-29 13:26:11.928360: step 380, loss = 47.35 (10.2 examples/sec; 6.294 sec/batch)
2016-04-29 13:26:26.102691: step 381, loss = 47.14 (10.6 examples/sec; 6.027 sec/batch)
2016-04-29 13:26:32.753121: step 382, loss = 47.29 (9.6 examples/sec; 6.650 sec/batch)
2016-04-29 13:26:38.823273: step 383, loss = 47.26 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 13:26:45.022466: step 384, loss = 47.19 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 13:26:51.191586: step 385, loss = 47.19 (10.4 examples/sec; 6.169 sec/batch)
2016-04-29 13:26:57.365900: step 386, loss = 47.15 (10.4 examples/sec; 6.174 sec/batch)
2016-04-29 13:27:03.367791: step 387, loss = 46.85 (10.7 examples/sec; 6.002 sec/batch)
2016-04-29 13:27:10.078083: step 388, loss = 46.95 (9.5 examples/sec; 6.710 sec/batch)
2016-04-29 13:27:15.899171: step 389, loss = 47.09 (11.0 examples/sec; 5.821 sec/batch)
2016-04-29 13:27:21.767544: step 390, loss = 46.96 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 13:27:35.967165: step 391, loss = 46.99 (11.0 examples/sec; 5.843 sec/batch)
2016-04-29 13:27:42.583193: step 392, loss = 47.02 (9.7 examples/sec; 6.616 sec/batch)
2016-04-29 13:27:48.602491: step 393, loss = 46.62 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 13:27:54.760341: step 394, loss = 46.71 (10.4 examples/sec; 6.158 sec/batch)
2016-04-29 13:28:01.012387: step 395, loss = 46.63 (10.2 examples/sec; 6.252 sec/batch)
2016-04-29 13:28:06.971571: step 396, loss = 46.83 (10.7 examples/sec; 5.959 sec/batch)
2016-04-29 13:28:13.980875: step 397, loss = 46.67 (9.1 examples/sec; 7.009 sec/batch)
2016-04-29 13:28:19.858322: step 398, loss = 46.72 (10.9 examples/sec; 5.877 sec/batch)
2016-04-29 13:28:26.182099: step 399, loss = 46.58 (10.1 examples/sec; 6.324 sec/batch)
2016-04-29 13:28:32.079971: step 400, loss = 46.50 (10.9 examples/sec; 5.898 sec/batch)
2016-04-29 13:28:46.528535: step 401, loss = 46.64 (9.6 examples/sec; 6.636 sec/batch)
2016-04-29 13:28:52.368821: step 402, loss = 46.49 (11.0 examples/sec; 5.840 sec/batch)
2016-04-29 13:28:58.273680: step 403, loss = 46.43 (10.8 examples/sec; 5.905 sec/batch)
2016-04-29 13:29:04.311687: step 404, loss = 46.51 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 13:29:10.398031: step 405, loss = 46.39 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 13:29:16.644333: step 406, loss = 46.38 (10.2 examples/sec; 6.246 sec/batch)
2016-04-29 13:29:23.379208: step 407, loss = 46.19 (9.5 examples/sec; 6.735 sec/batch)
2016-04-29 13:29:29.571153: step 408, loss = 46.40 (10.3 examples/sec; 6.192 sec/batch)
2016-04-29 13:29:35.452175: step 409, loss = 46.19 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 13:29:41.427033: step 410, loss = 46.18 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 13:29:56.099253: step 411, loss = 46.20 (9.5 examples/sec; 6.708 sec/batch)
2016-04-29 13:30:02.066839: step 412, loss = 46.08 (10.7 examples/sec; 5.968 sec/batch)
2016-04-29 13:30:08.164063: step 413, loss = 46.27 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 13:30:14.048302: step 414, loss = 46.12 (10.9 examples/sec; 5.884 sec/batch)
2016-04-29 13:30:20.175527: step 415, loss = 46.08 (10.4 examples/sec; 6.127 sec/batch)
2016-04-29 13:30:26.768749: step 416, loss = 45.90 (9.7 examples/sec; 6.593 sec/batch)
2016-04-29 13:30:32.685892: step 417, loss = 45.87 (10.8 examples/sec; 5.917 sec/batch)
2016-04-29 13:30:38.790630: step 418, loss = 46.00 (10.5 examples/sec; 6.105 sec/batch)
2016-04-29 13:30:44.811133: step 419, loss = 46.05 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 13:30:50.742489: step 420, loss = 45.73 (10.8 examples/sec; 5.931 sec/batch)
2016-04-29 13:31:05.745543: step 421, loss = 45.60 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 13:31:11.772067: step 422, loss = 45.69 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 13:31:17.606670: step 423, loss = 45.77 (11.0 examples/sec; 5.835 sec/batch)
2016-04-29 13:31:23.796881: step 424, loss = 45.82 (10.3 examples/sec; 6.190 sec/batch)
2016-04-29 13:31:30.676838: step 425, loss = 45.54 (9.3 examples/sec; 6.880 sec/batch)
2016-04-29 13:31:36.783660: step 426, loss = 45.58 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 13:31:42.891453: step 427, loss = 45.43 (10.5 examples/sec; 6.108 sec/batch)
2016-04-29 13:31:48.793565: step 428, loss = 45.65 (10.8 examples/sec; 5.902 sec/batch)
2016-04-29 13:31:54.920844: step 429, loss = 45.49 (10.4 examples/sec; 6.127 sec/batch)
2016-04-29 13:32:00.939306: step 430, loss = 45.55 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 13:32:16.440431: step 431, loss = 45.49 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 13:32:22.305976: step 432, loss = 45.43 (10.9 examples/sec; 5.865 sec/batch)
2016-04-29 13:32:28.761521: step 433, loss = 45.45 (9.9 examples/sec; 6.455 sec/batch)
2016-04-29 13:32:34.850293: step 434, loss = 45.21 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 13:32:41.412876: step 435, loss = 45.33 (9.8 examples/sec; 6.562 sec/batch)
2016-04-29 13:32:47.481009: step 436, loss = 45.19 (10.5 examples/sec; 6.068 sec/batch)
2016-04-29 13:32:53.663102: step 437, loss = 45.21 (10.4 examples/sec; 6.182 sec/batch)
2016-04-29 13:32:59.149216: step 438, loss = 45.24 (11.7 examples/sec; 5.486 sec/batch)
2016-04-29 13:33:04.563631: step 439, loss = 45.21 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 13:33:10.230649: step 440, loss = 45.22 (11.3 examples/sec; 5.667 sec/batch)
2016-04-29 13:33:22.987909: step 441, loss = 45.12 (11.9 examples/sec; 5.368 sec/batch)
2016-04-29 13:33:28.513163: step 442, loss = 45.00 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 13:33:33.769003: step 443, loss = 44.93 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 13:33:38.947613: step 444, loss = 45.04 (12.4 examples/sec; 5.178 sec/batch)
2016-04-29 13:33:44.926201: step 445, loss = 44.96 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 13:33:50.629001: step 446, loss = 44.79 (11.2 examples/sec; 5.703 sec/batch)
2016-04-29 13:33:56.709194: step 447, loss = 44.81 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 13:34:03.419958: step 448, loss = 45.02 (9.5 examples/sec; 6.706 sec/batch)
2016-04-29 13:34:09.513143: step 449, loss = 44.81 (10.5 examples/sec; 6.093 sec/batch)
2016-04-29 13:34:16.909486: step 450, loss = 44.73 (8.7 examples/sec; 7.396 sec/batch)
2016-04-29 13:34:30.677576: step 451, loss = 44.67 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 13:34:36.610195: step 452, loss = 44.78 (10.8 examples/sec; 5.932 sec/batch)
2016-04-29 13:34:42.544483: step 453, loss = 44.73 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 13:34:48.867014: step 454, loss = 44.69 (10.1 examples/sec; 6.322 sec/batch)
2016-04-29 13:34:54.712458: step 455, loss = 44.56 (10.9 examples/sec; 5.845 sec/batch)
2016-04-29 13:35:00.925717: step 456, loss = 44.50 (10.3 examples/sec; 6.213 sec/batch)
2016-04-29 13:35:06.728447: step 457, loss = 44.61 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 13:35:12.639756: step 458, loss = 44.55 (10.8 examples/sec; 5.911 sec/batch)
2016-04-29 13:35:18.318723: step 459, loss = 44.53 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 13:35:25.025615: step 460, loss = 44.60 (9.5 examples/sec; 6.707 sec/batch)
2016-04-29 13:35:38.789232: step 461, loss = 44.45 (11.3 examples/sec; 5.671 sec/batch)
2016-04-29 13:35:44.689597: step 462, loss = 44.49 (10.8 examples/sec; 5.900 sec/batch)
2016-04-29 13:35:50.662328: step 463, loss = 44.32 (10.7 examples/sec; 5.973 sec/batch)
2016-04-29 13:35:57.318256: step 464, loss = 44.40 (9.6 examples/sec; 6.656 sec/batch)
2016-04-29 13:36:03.343480: step 465, loss = 44.15 (10.6 examples/sec; 6.025 sec/batch)
2016-04-29 13:36:09.387111: step 466, loss = 44.17 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 13:36:15.353406: step 467, loss = 44.30 (10.7 examples/sec; 5.966 sec/batch)
2016-04-29 13:36:21.447920: step 468, loss = 44.21 (10.5 examples/sec; 6.094 sec/batch)
2016-04-29 13:36:28.160900: step 469, loss = 44.28 (9.5 examples/sec; 6.713 sec/batch)
2016-04-29 13:36:34.025477: step 470, loss = 44.18 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 13:36:47.742561: step 471, loss = 43.86 (11.2 examples/sec; 5.721 sec/batch)
2016-04-29 13:36:53.760118: step 472, loss = 43.98 (10.6 examples/sec; 6.017 sec/batch)
2016-04-29 13:37:00.309430: step 473, loss = 43.97 (9.8 examples/sec; 6.549 sec/batch)
2016-04-29 13:37:06.257798: step 474, loss = 43.89 (10.8 examples/sec; 5.948 sec/batch)
2016-04-29 13:37:12.355499: step 475, loss = 43.89 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 13:37:18.390317: step 476, loss = 43.71 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 13:37:24.238820: step 477, loss = 43.77 (10.9 examples/sec; 5.848 sec/batch)
2016-04-29 13:37:30.113436: step 478, loss = 43.82 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 13:37:36.804965: step 479, loss = 43.97 (9.6 examples/sec; 6.691 sec/batch)
2016-04-29 13:37:42.878851: step 480, loss = 43.79 (10.5 examples/sec; 6.074 sec/batch)
2016-04-29 13:37:56.818075: step 481, loss = 43.63 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 13:38:02.616574: step 482, loss = 43.73 (11.0 examples/sec; 5.798 sec/batch)
2016-04-29 13:38:09.709645: step 483, loss = 43.65 (9.0 examples/sec; 7.093 sec/batch)
2016-04-29 13:38:15.983682: step 484, loss = 43.76 (10.2 examples/sec; 6.274 sec/batch)
2016-04-29 13:38:23.215823: step 485, loss = 43.54 (8.8 examples/sec; 7.232 sec/batch)
2016-04-29 13:38:29.853269: step 486, loss = 43.62 (9.6 examples/sec; 6.637 sec/batch)
2016-04-29 13:38:35.438168: step 487, loss = 43.53 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 13:38:41.790439: step 488, loss = 43.59 (10.1 examples/sec; 6.352 sec/batch)
2016-04-29 13:38:47.784547: step 489, loss = 43.55 (10.7 examples/sec; 5.994 sec/batch)
2016-04-29 13:38:54.019057: step 490, loss = 43.40 (10.3 examples/sec; 6.234 sec/batch)
2016-04-29 13:39:07.908637: step 491, loss = 43.48 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 13:39:14.080233: step 492, loss = 43.44 (10.4 examples/sec; 6.171 sec/batch)
2016-04-29 13:39:19.625772: step 493, loss = 43.41 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 13:39:24.868208: step 494, loss = 43.30 (12.2 examples/sec; 5.242 sec/batch)
2016-04-29 13:39:30.578641: step 495, loss = 43.14 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 13:39:35.966409: step 496, loss = 43.22 (11.9 examples/sec; 5.388 sec/batch)
2016-04-29 13:39:41.680093: step 497, loss = 43.26 (11.2 examples/sec; 5.714 sec/batch)
2016-04-29 13:39:47.798316: step 498, loss = 43.11 (10.5 examples/sec; 6.118 sec/batch)
2016-04-29 13:39:53.360534: step 499, loss = 43.10 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 13:39:59.097986: step 500, loss = 43.26 (11.2 examples/sec; 5.737 sec/batch)
2016-04-29 13:40:12.126421: step 501, loss = 43.01 (11.8 examples/sec; 5.412 sec/batch)
2016-04-29 13:40:18.349292: step 502, loss = 43.04 (10.3 examples/sec; 6.223 sec/batch)
2016-04-29 13:40:24.119366: step 503, loss = 43.07 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 13:40:29.654633: step 504, loss = 42.87 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 13:40:35.139503: step 505, loss = 42.78 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 13:40:40.730559: step 506, loss = 42.76 (11.4 examples/sec; 5.591 sec/batch)
2016-04-29 13:40:46.186188: step 507, loss = 42.63 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 13:40:52.358875: step 508, loss = 42.62 (10.4 examples/sec; 6.173 sec/batch)
2016-04-29 13:40:58.006835: step 509, loss = 42.80 (11.3 examples/sec; 5.648 sec/batch)
2016-04-29 13:41:03.611942: step 510, loss = 42.74 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 13:41:16.620106: step 511, loss = 42.80 (12.0 examples/sec; 5.328 sec/batch)
2016-04-29 13:41:22.043112: step 512, loss = 42.79 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 13:41:28.257290: step 513, loss = 42.68 (10.3 examples/sec; 6.214 sec/batch)
2016-04-29 13:41:33.856619: step 514, loss = 42.50 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 13:41:39.350607: step 515, loss = 42.62 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 13:41:45.098488: step 516, loss = 42.36 (11.1 examples/sec; 5.748 sec/batch)
2016-04-29 13:41:50.812434: step 517, loss = 42.59 (11.2 examples/sec; 5.714 sec/batch)
2016-04-29 13:41:56.919573: step 518, loss = 42.61 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 13:42:02.995963: step 519, loss = 42.40 (10.5 examples/sec; 6.076 sec/batch)
2016-04-29 13:42:08.863895: step 520, loss = 42.52 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 13:42:21.819684: step 521, loss = 42.46 (12.2 examples/sec; 5.233 sec/batch)
2016-04-29 13:42:27.332468: step 522, loss = 42.37 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 13:42:33.457424: step 523, loss = 42.29 (10.4 examples/sec; 6.125 sec/batch)
2016-04-29 13:42:39.003401: step 524, loss = 42.21 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 13:42:44.649914: step 525, loss = 42.43 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 13:42:50.352297: step 526, loss = 42.26 (11.2 examples/sec; 5.702 sec/batch)
2016-04-29 13:42:56.031293: step 527, loss = 42.34 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 13:43:01.474196: step 528, loss = 42.22 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 13:43:07.562957: step 529, loss = 42.11 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 13:43:13.203303: step 530, loss = 41.91 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 13:43:26.210536: step 531, loss = 42.08 (12.0 examples/sec; 5.313 sec/batch)
2016-04-29 13:43:31.730331: step 532, loss = 42.00 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 13:43:37.971710: step 533, loss = 42.19 (10.3 examples/sec; 6.241 sec/batch)
2016-04-29 13:43:43.332519: step 534, loss = 42.01 (11.9 examples/sec; 5.361 sec/batch)
2016-04-29 13:43:48.683040: step 535, loss = 41.89 (12.0 examples/sec; 5.350 sec/batch)
2016-04-29 13:43:54.331110: step 536, loss = 41.81 (11.3 examples/sec; 5.648 sec/batch)
2016-04-29 13:43:59.815484: step 537, loss = 41.83 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 13:44:05.833887: step 538, loss = 41.76 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 13:44:12.293892: step 539, loss = 41.79 (9.9 examples/sec; 6.460 sec/batch)
2016-04-29 13:44:18.061666: step 540, loss = 41.69 (11.1 examples/sec; 5.768 sec/batch)
2016-04-29 13:44:31.212440: step 541, loss = 41.91 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 13:44:36.843825: step 542, loss = 41.85 (11.4 examples/sec; 5.631 sec/batch)
2016-04-29 13:44:43.118038: step 543, loss = 41.68 (10.2 examples/sec; 6.274 sec/batch)
2016-04-29 13:44:48.628467: step 544, loss = 41.64 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 13:44:54.207619: step 545, loss = 41.66 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 13:44:59.761665: step 546, loss = 41.45 (11.5 examples/sec; 5.554 sec/batch)
2016-04-29 13:45:05.435335: step 547, loss = 41.48 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 13:45:11.181632: step 548, loss = 41.59 (11.1 examples/sec; 5.746 sec/batch)
2016-04-29 13:45:17.547262: step 549, loss = 41.60 (10.1 examples/sec; 6.366 sec/batch)
2016-04-29 13:45:22.997998: step 550, loss = 41.57 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 13:45:35.878959: step 551, loss = 41.33 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 13:45:41.469064: step 552, loss = 41.34 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 13:45:47.298609: step 553, loss = 41.16 (11.0 examples/sec; 5.829 sec/batch)
2016-04-29 13:45:53.178952: step 554, loss = 41.28 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 13:45:58.747009: step 555, loss = 41.29 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 13:46:04.628153: step 556, loss = 41.24 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 13:46:10.220612: step 557, loss = 41.20 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 13:46:15.866283: step 558, loss = 41.20 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 13:46:22.057197: step 559, loss = 41.15 (10.3 examples/sec; 6.191 sec/batch)
2016-04-29 13:46:27.631204: step 560, loss = 41.10 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 13:46:40.254134: step 561, loss = 41.10 (12.8 examples/sec; 5.005 sec/batch)
2016-04-29 13:46:45.880449: step 562, loss = 41.06 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 13:46:51.456782: step 563, loss = 41.07 (11.5 examples/sec; 5.576 sec/batch)
2016-04-29 13:46:57.706171: step 564, loss = 41.12 (10.2 examples/sec; 6.249 sec/batch)
2016-04-29 13:47:03.151556: step 565, loss = 41.04 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 13:47:08.697092: step 566, loss = 40.93 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 13:47:14.257639: step 567, loss = 41.00 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 13:47:19.428442: step 568, loss = 40.85 (12.4 examples/sec; 5.171 sec/batch)
2016-04-29 13:47:24.766173: step 569, loss = 40.69 (12.0 examples/sec; 5.338 sec/batch)
2016-04-29 13:47:30.886062: step 570, loss = 40.76 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 13:47:43.687450: step 571, loss = 40.83 (12.2 examples/sec; 5.248 sec/batch)
2016-04-29 13:47:49.032453: step 572, loss = 40.85 (12.0 examples/sec; 5.345 sec/batch)
2016-04-29 13:47:54.469951: step 573, loss = 40.67 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 13:48:00.839980: step 574, loss = 40.57 (10.0 examples/sec; 6.370 sec/batch)
2016-04-29 13:48:06.357884: step 575, loss = 40.80 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 13:48:11.989921: step 576, loss = 40.69 (11.4 examples/sec; 5.632 sec/batch)
2016-04-29 13:48:17.597982: step 577, loss = 40.68 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 13:48:23.243508: step 578, loss = 40.74 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 13:48:28.629203: step 579, loss = 40.55 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 13:48:34.728771: step 580, loss = 40.41 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 13:48:47.495540: step 581, loss = 40.26 (12.0 examples/sec; 5.324 sec/batch)
2016-04-29 13:48:52.995953: step 582, loss = 40.53 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 13:48:58.433210: step 583, loss = 40.42 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 13:49:03.910283: step 584, loss = 40.45 (11.7 examples/sec; 5.477 sec/batch)
2016-04-29 13:49:10.018323: step 585, loss = 40.33 (10.5 examples/sec; 6.108 sec/batch)
2016-04-29 13:49:15.794481: step 586, loss = 40.25 (11.1 examples/sec; 5.776 sec/batch)
2016-04-29 13:49:21.303416: step 587, loss = 40.41 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 13:49:26.892043: step 588, loss = 40.17 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 13:49:32.393554: step 589, loss = 40.08 (11.6 examples/sec; 5.501 sec/batch)
2016-04-29 13:49:37.783643: step 590, loss = 40.31 (11.9 examples/sec; 5.390 sec/batch)
2016-04-29 13:49:51.418088: step 591, loss = 40.16 (11.9 examples/sec; 5.389 sec/batch)
2016-04-29 13:49:56.986002: step 592, loss = 40.15 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 13:50:02.694701: step 593, loss = 40.00 (11.2 examples/sec; 5.709 sec/batch)
2016-04-29 13:50:08.227851: step 594, loss = 40.10 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 13:50:14.378166: step 595, loss = 40.02 (10.4 examples/sec; 6.150 sec/batch)
2016-04-29 13:50:20.019287: step 596, loss = 40.08 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 13:50:25.311808: step 597, loss = 40.04 (12.1 examples/sec; 5.292 sec/batch)
2016-04-29 13:50:30.897584: step 598, loss = 39.97 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 13:50:36.463326: step 599, loss = 39.83 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 13:50:42.024465: step 600, loss = 39.80 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 13:50:55.664666: step 601, loss = 39.76 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 13:51:01.208992: step 602, loss = 39.80 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 13:51:06.737147: step 603, loss = 39.75 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 13:51:12.217844: step 604, loss = 39.69 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 13:51:18.504036: step 605, loss = 39.81 (10.2 examples/sec; 6.286 sec/batch)
2016-04-29 13:51:24.010825: step 606, loss = 39.59 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 13:51:29.636886: step 607, loss = 39.75 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 13:51:35.289585: step 608, loss = 39.51 (11.3 examples/sec; 5.653 sec/batch)
2016-04-29 13:51:40.472248: step 609, loss = 39.74 (12.3 examples/sec; 5.183 sec/batch)
2016-04-29 13:51:45.986454: step 610, loss = 39.53 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 13:51:59.533637: step 611, loss = 39.54 (11.9 examples/sec; 5.361 sec/batch)
2016-04-29 13:52:05.212416: step 612, loss = 39.46 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 13:52:10.820319: step 613, loss = 39.58 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 13:52:16.214051: step 614, loss = 39.35 (11.9 examples/sec; 5.394 sec/batch)
2016-04-29 13:52:21.865259: step 615, loss = 39.58 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 13:52:27.850705: step 616, loss = 39.40 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 13:52:33.593495: step 617, loss = 39.49 (11.1 examples/sec; 5.743 sec/batch)
2016-04-29 13:52:39.199003: step 618, loss = 39.31 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 13:52:44.717997: step 619, loss = 39.49 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 13:52:50.237311: step 620, loss = 39.21 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 13:53:03.707583: step 621, loss = 39.32 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 13:53:09.345561: step 622, loss = 39.39 (11.4 examples/sec; 5.638 sec/batch)
2016-04-29 13:53:14.807652: step 623, loss = 39.22 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 13:53:20.073460: step 624, loss = 39.18 (12.2 examples/sec; 5.266 sec/batch)
2016-04-29 13:53:25.359002: step 625, loss = 39.10 (12.1 examples/sec; 5.285 sec/batch)
2016-04-29 13:53:31.222629: step 626, loss = 38.89 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 13:53:37.025507: step 627, loss = 39.11 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 13:53:42.544034: step 628, loss = 39.06 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 13:53:48.060448: step 629, loss = 38.98 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 13:53:53.588437: step 630, loss = 38.95 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 13:54:07.121394: step 631, loss = 39.05 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 13:54:12.593950: step 632, loss = 38.92 (11.7 examples/sec; 5.472 sec/batch)
2016-04-29 13:54:18.132426: step 633, loss = 38.72 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 13:54:23.670406: step 634, loss = 38.70 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 13:54:29.126852: step 635, loss = 38.93 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 13:54:34.380743: step 636, loss = 38.77 (12.2 examples/sec; 5.254 sec/batch)
2016-04-29 13:54:40.511449: step 637, loss = 38.73 (10.4 examples/sec; 6.131 sec/batch)
2016-04-29 13:54:46.056122: step 638, loss = 38.75 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 13:54:51.503574: step 639, loss = 38.72 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 13:54:57.097025: step 640, loss = 38.59 (11.4 examples/sec; 5.593 sec/batch)
2016-04-29 13:55:10.540196: step 641, loss = 38.75 (11.0 examples/sec; 5.825 sec/batch)
2016-04-29 13:55:16.168606: step 642, loss = 38.62 (11.4 examples/sec; 5.628 sec/batch)
2016-04-29 13:55:21.714880: step 643, loss = 38.45 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 13:55:27.234662: step 644, loss = 38.45 (11.6 examples/sec; 5.520 sec/batch)
2016-04-29 13:55:32.855079: step 645, loss = 38.44 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 13:55:38.478583: step 646, loss = 38.40 (11.4 examples/sec; 5.623 sec/batch)
2016-04-29 13:55:44.513463: step 647, loss = 38.58 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 13:55:49.981698: step 648, loss = 38.44 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 13:55:55.460084: step 649, loss = 38.23 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 13:56:00.986014: step 650, loss = 38.41 (11.6 examples/sec; 5.526 sec/batch)
2016-04-29 13:56:14.042819: step 651, loss = 38.38 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 13:56:20.155725: step 652, loss = 38.33 (10.5 examples/sec; 6.113 sec/batch)
2016-04-29 13:56:25.441217: step 653, loss = 38.32 (12.1 examples/sec; 5.285 sec/batch)
2016-04-29 13:56:31.114490: step 654, loss = 38.36 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 13:56:36.686288: step 655, loss = 38.37 (11.5 examples/sec; 5.572 sec/batch)
2016-04-29 13:56:42.156198: step 656, loss = 38.15 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 13:56:48.298549: step 657, loss = 38.23 (10.4 examples/sec; 6.142 sec/batch)
2016-04-29 13:56:53.874328: step 658, loss = 38.13 (11.5 examples/sec; 5.576 sec/batch)
2016-04-29 13:56:59.351170: step 659, loss = 38.25 (11.7 examples/sec; 5.477 sec/batch)
2016-04-29 13:57:04.872079: step 660, loss = 37.94 (11.6 examples/sec; 5.521 sec/batch)
2016-04-29 13:57:17.882424: step 661, loss = 38.04 (11.9 examples/sec; 5.395 sec/batch)
2016-04-29 13:57:24.012802: step 662, loss = 37.99 (10.4 examples/sec; 6.130 sec/batch)
2016-04-29 13:57:29.449254: step 663, loss = 37.89 (11.8 examples/sec; 5.436 sec/batch)
2016-04-29 13:57:34.877608: step 664, loss = 37.99 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 13:57:40.294231: step 665, loss = 37.95 (11.8 examples/sec; 5.417 sec/batch)
2016-04-29 13:57:45.746858: step 666, loss = 37.94 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 13:57:51.490016: step 667, loss = 38.02 (11.1 examples/sec; 5.743 sec/batch)
2016-04-29 13:57:57.688862: step 668, loss = 37.79 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 13:58:03.242805: step 669, loss = 37.55 (11.5 examples/sec; 5.554 sec/batch)
2016-04-29 13:58:09.100038: step 670, loss = 37.58 (10.9 examples/sec; 5.857 sec/batch)
2016-04-29 13:58:21.867872: step 671, loss = 37.68 (12.0 examples/sec; 5.345 sec/batch)
2016-04-29 13:58:28.008172: step 672, loss = 37.65 (10.4 examples/sec; 6.140 sec/batch)
2016-04-29 13:58:33.488195: step 673, loss = 37.73 (11.7 examples/sec; 5.480 sec/batch)
2016-04-29 13:58:39.062716: step 674, loss = 37.57 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 13:58:44.470773: step 675, loss = 37.63 (11.8 examples/sec; 5.408 sec/batch)
2016-04-29 13:58:49.744782: step 676, loss = 37.67 (12.1 examples/sec; 5.274 sec/batch)
2016-04-29 13:58:54.973955: step 677, loss = 37.75 (12.2 examples/sec; 5.229 sec/batch)
2016-04-29 13:59:01.293216: step 678, loss = 37.67 (10.1 examples/sec; 6.319 sec/batch)
2016-04-29 13:59:06.782251: step 679, loss = 37.38 (11.7 examples/sec; 5.489 sec/batch)
2016-04-29 13:59:12.224045: step 680, loss = 37.52 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 13:59:24.882459: step 681, loss = 37.54 (12.0 examples/sec; 5.342 sec/batch)
2016-04-29 13:59:30.573775: step 682, loss = 37.35 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 13:59:36.828537: step 683, loss = 37.31 (10.2 examples/sec; 6.255 sec/batch)
2016-04-29 13:59:42.340323: step 684, loss = 37.45 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 13:59:47.713253: step 685, loss = 37.49 (11.9 examples/sec; 5.373 sec/batch)
2016-04-29 13:59:53.200155: step 686, loss = 37.14 (11.7 examples/sec; 5.487 sec/batch)
2016-04-29 13:59:58.560950: step 687, loss = 37.43 (11.9 examples/sec; 5.361 sec/batch)
2016-04-29 14:00:03.984990: step 688, loss = 37.31 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 14:00:10.238602: step 689, loss = 37.21 (10.2 examples/sec; 6.254 sec/batch)
2016-04-29 14:00:16.005534: step 690, loss = 37.13 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 14:00:28.852181: step 691, loss = 37.32 (12.5 examples/sec; 5.108 sec/batch)
2016-04-29 14:00:34.390446: step 692, loss = 37.18 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 14:00:40.286077: step 693, loss = 37.12 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 14:00:45.542249: step 694, loss = 37.02 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 14:00:51.070850: step 695, loss = 37.07 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 14:00:56.603653: step 696, loss = 37.00 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 14:01:02.160431: step 697, loss = 36.97 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 14:01:07.364076: step 698, loss = 37.03 (12.3 examples/sec; 5.204 sec/batch)
2016-04-29 14:01:13.541436: step 699, loss = 36.92 (10.4 examples/sec; 6.177 sec/batch)
2016-04-29 14:01:19.011296: step 700, loss = 36.98 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 14:01:31.827628: step 701, loss = 36.93 (12.4 examples/sec; 5.176 sec/batch)
2016-04-29 14:01:37.215634: step 702, loss = 36.65 (11.9 examples/sec; 5.388 sec/batch)
2016-04-29 14:01:42.722504: step 703, loss = 36.90 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 14:01:48.937402: step 704, loss = 36.76 (10.3 examples/sec; 6.215 sec/batch)
2016-04-29 14:01:54.467186: step 705, loss = 36.83 (11.6 examples/sec; 5.530 sec/batch)
2016-04-29 14:01:59.838811: step 706, loss = 36.79 (11.9 examples/sec; 5.372 sec/batch)
2016-04-29 14:02:05.352544: step 707, loss = 36.49 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 14:02:10.940196: step 708, loss = 36.86 (11.5 examples/sec; 5.588 sec/batch)
2016-04-29 14:02:16.536086: step 709, loss = 36.64 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 14:02:22.554202: step 710, loss = 36.65 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 14:02:35.784858: step 711, loss = 36.63 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 14:02:41.226177: step 712, loss = 36.60 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 14:02:46.452471: step 713, loss = 36.45 (12.2 examples/sec; 5.226 sec/batch)
2016-04-29 14:02:52.098017: step 714, loss = 36.49 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 14:02:58.177986: step 715, loss = 36.46 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 14:03:03.597096: step 716, loss = 36.49 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 14:03:09.161224: step 717, loss = 36.58 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 14:03:14.580352: step 718, loss = 36.33 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 14:03:19.934740: step 719, loss = 36.40 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:03:25.469929: step 720, loss = 36.67 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 14:03:39.023745: step 721, loss = 36.43 (11.7 examples/sec; 5.465 sec/batch)
2016-04-29 14:03:44.606310: step 722, loss = 36.37 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 14:03:49.945228: step 723, loss = 36.40 (12.0 examples/sec; 5.339 sec/batch)
2016-04-29 14:03:55.247494: step 724, loss = 36.39 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 14:04:01.473975: step 725, loss = 36.16 (10.3 examples/sec; 6.226 sec/batch)
2016-04-29 14:04:06.954547: step 726, loss = 36.16 (11.7 examples/sec; 5.480 sec/batch)
2016-04-29 14:04:12.576189: step 727, loss = 36.26 (11.4 examples/sec; 5.622 sec/batch)
2016-04-29 14:04:18.151342: step 728, loss = 36.15 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 14:04:23.869077: step 729, loss = 36.07 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 14:04:29.222473: step 730, loss = 36.00 (12.0 examples/sec; 5.353 sec/batch)
2016-04-29 14:04:42.472496: step 731, loss = 36.00 (12.0 examples/sec; 5.335 sec/batch)
2016-04-29 14:04:47.947190: step 732, loss = 36.00 (11.7 examples/sec; 5.475 sec/batch)
2016-04-29 14:04:53.485546: step 733, loss = 36.02 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 14:04:58.795614: step 734, loss = 36.18 (12.1 examples/sec; 5.310 sec/batch)
2016-04-29 14:05:04.488171: step 735, loss = 36.24 (11.2 examples/sec; 5.692 sec/batch)
2016-04-29 14:05:10.422522: step 736, loss = 35.70 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 14:05:16.014150: step 737, loss = 35.95 (11.4 examples/sec; 5.591 sec/batch)
2016-04-29 14:05:21.593970: step 738, loss = 35.97 (11.5 examples/sec; 5.580 sec/batch)
2016-04-29 14:05:27.033701: step 739, loss = 35.65 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 14:05:32.614920: step 740, loss = 35.66 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 14:05:45.790007: step 741, loss = 35.62 (11.9 examples/sec; 5.364 sec/batch)
2016-04-29 14:05:51.407203: step 742, loss = 35.77 (11.4 examples/sec; 5.617 sec/batch)
2016-04-29 14:05:56.851309: step 743, loss = 35.60 (11.8 examples/sec; 5.444 sec/batch)
2016-04-29 14:06:02.519047: step 744, loss = 35.64 (11.3 examples/sec; 5.668 sec/batch)
2016-04-29 14:06:07.823158: step 745, loss = 35.47 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 14:06:13.993836: step 746, loss = 35.70 (10.4 examples/sec; 6.171 sec/batch)
2016-04-29 14:06:19.406023: step 747, loss = 35.56 (11.8 examples/sec; 5.412 sec/batch)
2016-04-29 14:06:24.941214: step 748, loss = 35.54 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 14:06:30.364869: step 749, loss = 35.41 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 14:06:35.959234: step 750, loss = 35.54 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 14:06:49.444476: step 751, loss = 35.59 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 14:06:54.991390: step 752, loss = 35.53 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 14:07:00.799482: step 753, loss = 35.36 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 14:07:06.287802: step 754, loss = 35.58 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 14:07:11.885273: step 755, loss = 35.37 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 14:07:17.475043: step 756, loss = 35.24 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 14:07:23.565264: step 757, loss = 35.31 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 14:07:28.977489: step 758, loss = 35.45 (11.8 examples/sec; 5.412 sec/batch)
2016-04-29 14:07:34.317470: step 759, loss = 35.23 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 14:07:39.759390: step 760, loss = 35.39 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 14:07:52.930017: step 761, loss = 35.17 (11.2 examples/sec; 5.716 sec/batch)
2016-04-29 14:07:58.284611: step 762, loss = 35.09 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:08:03.555386: step 763, loss = 35.11 (12.1 examples/sec; 5.271 sec/batch)
2016-04-29 14:08:09.141424: step 764, loss = 35.16 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 14:08:14.689007: step 765, loss = 35.28 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 14:08:20.221518: step 766, loss = 35.16 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 14:08:26.283140: step 767, loss = 35.00 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 14:08:31.557522: step 768, loss = 34.99 (12.1 examples/sec; 5.274 sec/batch)
2016-04-29 14:08:37.105697: step 769, loss = 35.15 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 14:08:42.711457: step 770, loss = 34.84 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 14:08:55.357603: step 771, loss = 35.03 (12.4 examples/sec; 5.152 sec/batch)
2016-04-29 14:09:01.675517: step 772, loss = 34.81 (10.1 examples/sec; 6.318 sec/batch)
2016-04-29 14:09:07.095954: step 773, loss = 34.96 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 14:09:12.660194: step 774, loss = 34.78 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 14:09:18.350065: step 775, loss = 34.88 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 14:09:23.693155: step 776, loss = 34.70 (12.0 examples/sec; 5.343 sec/batch)
2016-04-29 14:09:29.033077: step 777, loss = 34.72 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 14:09:35.600233: step 778, loss = 34.77 (9.7 examples/sec; 6.567 sec/batch)
2016-04-29 14:09:41.023189: step 779, loss = 34.85 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 14:09:46.495913: step 780, loss = 34.48 (11.7 examples/sec; 5.473 sec/batch)
2016-04-29 14:09:59.274824: step 781, loss = 34.76 (12.0 examples/sec; 5.330 sec/batch)
2016-04-29 14:10:05.415532: step 782, loss = 34.80 (10.4 examples/sec; 6.141 sec/batch)
2016-04-29 14:10:11.004045: step 783, loss = 34.80 (11.5 examples/sec; 5.588 sec/batch)
2016-04-29 14:10:16.540636: step 784, loss = 34.56 (11.6 examples/sec; 5.536 sec/batch)
2016-04-29 14:10:22.043806: step 785, loss = 34.59 (11.6 examples/sec; 5.503 sec/batch)
2016-04-29 14:10:27.490798: step 786, loss = 34.42 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 14:10:32.942732: step 787, loss = 34.55 (11.7 examples/sec; 5.452 sec/batch)
2016-04-29 14:10:39.112586: step 788, loss = 34.47 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 14:10:44.698094: step 789, loss = 34.48 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 14:10:50.035794: step 790, loss = 34.28 (12.0 examples/sec; 5.338 sec/batch)
2016-04-29 14:11:02.880767: step 791, loss = 34.19 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 14:11:08.532261: step 792, loss = 34.32 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 14:11:14.566433: step 793, loss = 34.34 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 14:11:19.958348: step 794, loss = 34.34 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 14:11:25.278521: step 795, loss = 34.19 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 14:11:30.906451: step 796, loss = 34.36 (11.4 examples/sec; 5.628 sec/batch)
2016-04-29 14:11:36.524921: step 797, loss = 34.26 (11.4 examples/sec; 5.618 sec/batch)
2016-04-29 14:11:42.513912: step 798, loss = 34.20 (10.7 examples/sec; 5.989 sec/batch)
2016-04-29 14:11:48.427938: step 799, loss = 34.22 (10.8 examples/sec; 5.914 sec/batch)
2016-04-29 14:11:53.994844: step 800, loss = 34.12 (11.5 examples/sec; 5.567 sec/batch)
2016-04-29 14:12:06.750370: step 801, loss = 34.10 (12.1 examples/sec; 5.297 sec/batch)
2016-04-29 14:12:12.282531: step 802, loss = 34.14 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 14:12:18.372235: step 803, loss = 33.97 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 14:12:23.878117: step 804, loss = 33.88 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 14:12:29.155837: step 805, loss = 34.18 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 14:12:34.537717: step 806, loss = 33.99 (11.9 examples/sec; 5.382 sec/batch)
2016-04-29 14:12:39.984103: step 807, loss = 33.90 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 14:12:45.451659: step 808, loss = 33.81 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 14:12:51.486260: step 809, loss = 33.80 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 14:12:56.945063: step 810, loss = 33.95 (11.7 examples/sec; 5.459 sec/batch)
2016-04-29 14:13:09.588468: step 811, loss = 33.73 (11.9 examples/sec; 5.375 sec/batch)
2016-04-29 14:13:15.094658: step 812, loss = 33.98 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 14:13:20.642789: step 813, loss = 33.83 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 14:13:26.790935: step 814, loss = 33.89 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 14:13:32.350173: step 815, loss = 33.81 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 14:13:37.655348: step 816, loss = 33.80 (12.1 examples/sec; 5.305 sec/batch)
2016-04-29 14:13:43.111670: step 817, loss = 33.78 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 14:13:48.552677: step 818, loss = 33.65 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 14:13:54.559659: step 819, loss = 33.70 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 14:13:59.994944: step 820, loss = 33.68 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 14:14:12.924451: step 821, loss = 33.68 (11.9 examples/sec; 5.380 sec/batch)
2016-04-29 14:14:18.356666: step 822, loss = 33.63 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 14:14:23.970890: step 823, loss = 33.58 (11.4 examples/sec; 5.614 sec/batch)
2016-04-29 14:14:30.016190: step 824, loss = 33.51 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 14:14:35.543894: step 825, loss = 33.53 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 14:14:40.742378: step 826, loss = 33.67 (12.3 examples/sec; 5.198 sec/batch)
2016-04-29 14:14:46.275723: step 827, loss = 33.35 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 14:14:51.841808: step 828, loss = 33.44 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 14:14:57.380388: step 829, loss = 33.52 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 14:15:03.567175: step 830, loss = 33.40 (10.3 examples/sec; 6.187 sec/batch)
2016-04-29 14:15:16.441284: step 831, loss = 33.31 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 14:15:22.036062: step 832, loss = 33.57 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 14:15:27.672127: step 833, loss = 33.33 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 14:15:33.742590: step 834, loss = 33.44 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 14:15:39.262007: step 835, loss = 33.20 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 14:15:44.571240: step 836, loss = 33.37 (12.1 examples/sec; 5.309 sec/batch)
2016-04-29 14:15:49.757008: step 837, loss = 33.12 (12.3 examples/sec; 5.186 sec/batch)
2016-04-29 14:15:55.017940: step 838, loss = 33.01 (12.2 examples/sec; 5.261 sec/batch)
2016-04-29 14:16:00.867652: step 839, loss = 33.17 (10.9 examples/sec; 5.850 sec/batch)
2016-04-29 14:16:06.975301: step 840, loss = 33.04 (10.5 examples/sec; 6.108 sec/batch)
2016-04-29 14:16:20.003014: step 841, loss = 33.07 (12.3 examples/sec; 5.209 sec/batch)
2016-04-29 14:16:25.471068: step 842, loss = 33.05 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 14:16:30.917016: step 843, loss = 33.04 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 14:16:36.425642: step 844, loss = 33.25 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 14:16:42.423674: step 845, loss = 32.83 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 14:16:47.906947: step 846, loss = 32.93 (11.7 examples/sec; 5.483 sec/batch)
2016-04-29 14:16:53.356182: step 847, loss = 32.98 (11.7 examples/sec; 5.449 sec/batch)
2016-04-29 14:16:58.707362: step 848, loss = 32.96 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 14:17:04.430349: step 849, loss = 33.08 (11.2 examples/sec; 5.723 sec/batch)
2016-04-29 14:17:09.875000: step 850, loss = 32.89 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 14:17:23.198607: step 851, loss = 32.96 (12.5 examples/sec; 5.108 sec/batch)
2016-04-29 14:17:28.759078: step 852, loss = 32.79 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 14:17:34.625111: step 853, loss = 32.73 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 14:17:40.045343: step 854, loss = 32.74 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 14:17:46.105333: step 855, loss = 32.76 (10.6 examples/sec; 6.060 sec/batch)
2016-04-29 14:17:51.641152: step 856, loss = 32.86 (11.6 examples/sec; 5.536 sec/batch)
2016-04-29 14:17:57.132215: step 857, loss = 32.63 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:18:02.739393: step 858, loss = 32.90 (11.4 examples/sec; 5.607 sec/batch)
2016-04-29 14:18:08.047645: step 859, loss = 32.63 (12.1 examples/sec; 5.308 sec/batch)
2016-04-29 14:18:13.472495: step 860, loss = 32.64 (11.8 examples/sec; 5.425 sec/batch)
2016-04-29 14:18:27.215409: step 861, loss = 32.63 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 14:18:32.593074: step 862, loss = 32.61 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 14:18:37.926685: step 863, loss = 32.68 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 14:18:43.452123: step 864, loss = 32.55 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 14:18:48.792291: step 865, loss = 32.65 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 14:18:55.002337: step 866, loss = 32.42 (10.3 examples/sec; 6.210 sec/batch)
2016-04-29 14:19:00.548102: step 867, loss = 32.46 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 14:19:06.154144: step 868, loss = 32.56 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 14:19:11.600128: step 869, loss = 32.40 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 14:19:16.840746: step 870, loss = 32.43 (12.2 examples/sec; 5.241 sec/batch)
2016-04-29 14:19:30.410338: step 871, loss = 32.32 (12.3 examples/sec; 5.221 sec/batch)
2016-04-29 14:19:36.136106: step 872, loss = 32.37 (11.2 examples/sec; 5.726 sec/batch)
2016-04-29 14:19:41.546236: step 873, loss = 32.23 (11.8 examples/sec; 5.410 sec/batch)
2016-04-29 14:19:46.900067: step 874, loss = 32.33 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:19:52.487199: step 875, loss = 32.20 (11.5 examples/sec; 5.587 sec/batch)
2016-04-29 14:19:58.653971: step 876, loss = 32.29 (10.4 examples/sec; 6.167 sec/batch)
2016-04-29 14:20:04.231957: step 877, loss = 32.12 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 14:20:09.748507: step 878, loss = 32.23 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 14:20:15.243276: step 879, loss = 32.03 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 14:20:20.698129: step 880, loss = 32.16 (11.7 examples/sec; 5.455 sec/batch)
2016-04-29 14:20:33.770585: step 881, loss = 32.17 (10.9 examples/sec; 5.889 sec/batch)
2016-04-29 14:20:39.773355: step 882, loss = 31.92 (10.7 examples/sec; 6.003 sec/batch)
2016-04-29 14:20:45.284258: step 883, loss = 32.10 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 14:20:50.879079: step 884, loss = 32.01 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 14:20:56.365174: step 885, loss = 32.06 (11.7 examples/sec; 5.486 sec/batch)
2016-04-29 14:21:01.835445: step 886, loss = 31.99 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 14:21:07.934725: step 887, loss = 31.85 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 14:21:13.613283: step 888, loss = 32.03 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 14:21:19.437246: step 889, loss = 31.90 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 14:21:25.015523: step 890, loss = 32.02 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 14:21:38.514917: step 891, loss = 31.99 (11.2 examples/sec; 5.729 sec/batch)
2016-04-29 14:21:43.941502: step 892, loss = 31.68 (11.8 examples/sec; 5.426 sec/batch)
2016-04-29 14:21:49.645895: step 893, loss = 31.73 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 14:21:55.288658: step 894, loss = 31.70 (11.3 examples/sec; 5.643 sec/batch)
2016-04-29 14:22:00.968989: step 895, loss = 31.74 (11.3 examples/sec; 5.680 sec/batch)
2016-04-29 14:22:06.509419: step 896, loss = 31.80 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 14:22:12.706135: step 897, loss = 31.78 (10.3 examples/sec; 6.197 sec/batch)
2016-04-29 14:22:18.521465: step 898, loss = 31.72 (11.0 examples/sec; 5.815 sec/batch)
2016-04-29 14:22:23.996452: step 899, loss = 31.64 (11.7 examples/sec; 5.475 sec/batch)
2016-04-29 14:22:29.400224: step 900, loss = 31.63 (11.8 examples/sec; 5.404 sec/batch)
2016-04-29 14:22:42.492372: step 901, loss = 31.51 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 14:22:48.386163: step 902, loss = 31.62 (10.9 examples/sec; 5.894 sec/batch)
2016-04-29 14:22:53.967470: step 903, loss = 31.62 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 14:22:59.436446: step 904, loss = 31.68 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 14:23:04.738864: step 905, loss = 31.38 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 14:23:10.127148: step 906, loss = 31.56 (11.9 examples/sec; 5.388 sec/batch)
2016-04-29 14:23:16.260203: step 907, loss = 31.52 (10.4 examples/sec; 6.133 sec/batch)
2016-04-29 14:23:21.994354: step 908, loss = 31.53 (11.2 examples/sec; 5.734 sec/batch)
2016-04-29 14:23:27.509255: step 909, loss = 31.47 (11.6 examples/sec; 5.515 sec/batch)
2016-04-29 14:23:33.000230: step 910, loss = 31.31 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:23:45.505374: step 911, loss = 31.24 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 14:23:51.725973: step 912, loss = 31.33 (10.3 examples/sec; 6.220 sec/batch)
2016-04-29 14:23:57.308148: step 913, loss = 31.43 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 14:24:03.111117: step 914, loss = 31.22 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 14:24:08.838877: step 915, loss = 31.27 (11.2 examples/sec; 5.728 sec/batch)
2016-04-29 14:24:15.583097: step 916, loss = 31.43 (9.5 examples/sec; 6.744 sec/batch)
2016-04-29 14:24:21.397278: step 917, loss = 31.32 (11.0 examples/sec; 5.814 sec/batch)
2016-04-29 14:24:27.199848: step 918, loss = 31.18 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 14:24:32.527140: step 919, loss = 31.11 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 14:24:37.858523: step 920, loss = 31.12 (12.0 examples/sec; 5.331 sec/batch)
2016-04-29 14:24:50.781161: step 921, loss = 31.10 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 14:24:56.918321: step 922, loss = 31.19 (10.4 examples/sec; 6.137 sec/batch)
2016-04-29 14:25:02.330966: step 923, loss = 31.12 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 14:25:07.823410: step 924, loss = 31.25 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 14:25:13.233528: step 925, loss = 31.09 (11.8 examples/sec; 5.410 sec/batch)
2016-04-29 14:25:18.896016: step 926, loss = 31.07 (11.3 examples/sec; 5.662 sec/batch)
2016-04-29 14:25:24.417749: step 927, loss = 30.85 (11.6 examples/sec; 5.522 sec/batch)
2016-04-29 14:25:30.626736: step 928, loss = 31.04 (10.3 examples/sec; 6.209 sec/batch)
2016-04-29 14:25:36.260927: step 929, loss = 30.93 (11.4 examples/sec; 5.634 sec/batch)
2016-04-29 14:25:41.580907: step 930, loss = 31.03 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 14:25:54.415723: step 931, loss = 31.08 (11.8 examples/sec; 5.415 sec/batch)
2016-04-29 14:26:00.164696: step 932, loss = 30.95 (11.1 examples/sec; 5.749 sec/batch)
2016-04-29 14:26:05.974290: step 933, loss = 30.87 (11.0 examples/sec; 5.810 sec/batch)
2016-04-29 14:26:11.298980: step 934, loss = 30.82 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 14:26:16.768718: step 935, loss = 30.81 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 14:26:22.393230: step 936, loss = 30.87 (11.4 examples/sec; 5.624 sec/batch)
2016-04-29 14:26:27.971761: step 937, loss = 30.68 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 14:26:34.251736: step 938, loss = 30.77 (10.2 examples/sec; 6.280 sec/batch)
2016-04-29 14:26:40.007371: step 939, loss = 30.84 (11.1 examples/sec; 5.756 sec/batch)
2016-04-29 14:26:45.518031: step 940, loss = 30.54 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 14:26:58.348136: step 941, loss = 30.75 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 14:27:03.987035: step 942, loss = 30.60 (11.3 examples/sec; 5.639 sec/batch)
2016-04-29 14:27:10.072156: step 943, loss = 30.65 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 14:27:15.751096: step 944, loss = 30.63 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 14:27:21.236055: step 945, loss = 30.60 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 14:27:26.917308: step 946, loss = 30.64 (11.3 examples/sec; 5.681 sec/batch)
2016-04-29 14:27:32.199047: step 947, loss = 30.59 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 14:27:37.699310: step 948, loss = 30.38 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 14:27:43.791324: step 949, loss = 30.58 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 14:27:49.533591: step 950, loss = 30.34 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 14:28:02.615047: step 951, loss = 30.37 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 14:28:07.789343: step 952, loss = 30.44 (12.4 examples/sec; 5.174 sec/batch)
2016-04-29 14:28:13.904452: step 953, loss = 30.39 (10.5 examples/sec; 6.115 sec/batch)
2016-04-29 14:28:19.406763: step 954, loss = 30.33 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 14:28:24.966008: step 955, loss = 30.36 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 14:28:30.499802: step 956, loss = 30.13 (11.6 examples/sec; 5.534 sec/batch)
2016-04-29 14:28:36.024934: step 957, loss = 30.17 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 14:28:41.449905: step 958, loss = 30.34 (11.8 examples/sec; 5.425 sec/batch)
2016-04-29 14:28:47.492927: step 959, loss = 30.16 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 14:28:52.762045: step 960, loss = 30.33 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 14:29:05.857833: step 961, loss = 30.20 (12.2 examples/sec; 5.264 sec/batch)
2016-04-29 14:29:11.323061: step 962, loss = 30.11 (11.7 examples/sec; 5.465 sec/batch)
2016-04-29 14:29:16.689408: step 963, loss = 30.11 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 14:29:22.644332: step 964, loss = 30.08 (10.7 examples/sec; 5.955 sec/batch)
2016-04-29 14:29:28.035673: step 965, loss = 30.04 (11.9 examples/sec; 5.391 sec/batch)
2016-04-29 14:29:33.628016: step 966, loss = 30.05 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 14:29:39.147429: step 967, loss = 30.14 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 14:29:44.353462: step 968, loss = 30.14 (12.3 examples/sec; 5.206 sec/batch)
2016-04-29 14:29:49.753437: step 969, loss = 30.19 (11.9 examples/sec; 5.400 sec/batch)
2016-04-29 14:29:55.667619: step 970, loss = 30.15 (10.8 examples/sec; 5.914 sec/batch)
2016-04-29 14:30:08.765393: step 971, loss = 30.07 (11.9 examples/sec; 5.358 sec/batch)
2016-04-29 14:30:14.094582: step 972, loss = 29.70 (12.0 examples/sec; 5.329 sec/batch)
2016-04-29 14:30:19.677363: step 973, loss = 29.96 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 14:30:25.843438: step 974, loss = 29.94 (10.4 examples/sec; 6.166 sec/batch)
2016-04-29 14:30:31.215387: step 975, loss = 29.89 (11.9 examples/sec; 5.372 sec/batch)
2016-04-29 14:30:36.835934: step 976, loss = 29.75 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 14:30:42.277995: step 977, loss = 29.88 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 14:30:47.842999: step 978, loss = 29.91 (11.5 examples/sec; 5.565 sec/batch)
2016-04-29 14:30:53.235624: step 979, loss = 29.78 (11.9 examples/sec; 5.393 sec/batch)
2016-04-29 14:30:59.082300: step 980, loss = 29.72 (10.9 examples/sec; 5.847 sec/batch)
2016-04-29 14:31:12.211888: step 981, loss = 29.78 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 14:31:17.701809: step 982, loss = 29.61 (11.7 examples/sec; 5.490 sec/batch)
2016-04-29 14:31:23.003606: step 983, loss = 29.68 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 14:31:28.654062: step 984, loss = 29.79 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 14:31:34.955518: step 985, loss = 29.66 (10.2 examples/sec; 6.301 sec/batch)
2016-04-29 14:31:40.452520: step 986, loss = 29.68 (11.6 examples/sec; 5.497 sec/batch)
2016-04-29 14:31:46.217290: step 987, loss = 29.65 (11.1 examples/sec; 5.765 sec/batch)
2016-04-29 14:31:51.914074: step 988, loss = 29.83 (11.2 examples/sec; 5.697 sec/batch)
2016-04-29 14:31:57.541354: step 989, loss = 29.67 (11.4 examples/sec; 5.627 sec/batch)
2016-04-29 14:32:03.888635: step 990, loss = 29.55 (10.1 examples/sec; 6.347 sec/batch)
2016-04-29 14:32:16.947655: step 991, loss = 29.57 (12.1 examples/sec; 5.276 sec/batch)
2016-04-29 14:32:22.343764: step 992, loss = 29.49 (11.9 examples/sec; 5.396 sec/batch)
2016-04-29 14:32:27.960420: step 993, loss = 29.49 (11.4 examples/sec; 5.617 sec/batch)
2016-04-29 14:32:33.530126: step 994, loss = 29.19 (11.5 examples/sec; 5.570 sec/batch)
2016-04-29 14:32:39.573697: step 995, loss = 29.45 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 14:32:44.994218: step 996, loss = 29.26 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 14:32:50.456519: step 997, loss = 29.36 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 14:32:55.829340: step 998, loss = 29.45 (11.9 examples/sec; 5.373 sec/batch)
2016-04-29 14:33:01.674527: step 999, loss = 29.32 (10.9 examples/sec; 5.845 sec/batch)
2016-04-29 14:33:07.118084: step 1000, loss = 29.36 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 14:33:20.545482: step 1001, loss = 29.27 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 14:33:26.223754: step 1002, loss = 29.33 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 14:33:31.735606: step 1003, loss = 29.27 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 14:33:37.207364: step 1004, loss = 29.20 (11.7 examples/sec; 5.472 sec/batch)
2016-04-29 14:33:43.423905: step 1005, loss = 29.27 (10.3 examples/sec; 6.216 sec/batch)
2016-04-29 14:33:48.982235: step 1006, loss = 29.19 (11.5 examples/sec; 5.558 sec/batch)
2016-04-29 14:33:54.711005: step 1007, loss = 29.02 (11.2 examples/sec; 5.729 sec/batch)
2016-04-29 14:34:00.049696: step 1008, loss = 29.17 (12.0 examples/sec; 5.339 sec/batch)
2016-04-29 14:34:05.466418: step 1009, loss = 29.30 (11.8 examples/sec; 5.417 sec/batch)
2016-04-29 14:34:10.957874: step 1010, loss = 29.15 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:34:24.528375: step 1011, loss = 28.96 (11.9 examples/sec; 5.359 sec/batch)
2016-04-29 14:34:30.137023: step 1012, loss = 29.11 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 14:34:35.469580: step 1013, loss = 28.99 (12.0 examples/sec; 5.332 sec/batch)
2016-04-29 14:34:40.803365: step 1014, loss = 29.03 (12.0 examples/sec; 5.334 sec/batch)
2016-04-29 14:34:46.399162: step 1015, loss = 28.87 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 14:34:52.488351: step 1016, loss = 28.93 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 14:34:58.473405: step 1017, loss = 28.97 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 14:35:04.758697: step 1018, loss = 28.91 (10.2 examples/sec; 6.285 sec/batch)
2016-04-29 14:35:10.197253: step 1019, loss = 28.76 (11.8 examples/sec; 5.438 sec/batch)
2016-04-29 14:35:16.005370: step 1020, loss = 29.06 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 14:35:29.407146: step 1021, loss = 28.69 (12.4 examples/sec; 5.161 sec/batch)
2016-04-29 14:35:34.774170: step 1022, loss = 28.84 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 14:35:40.358113: step 1023, loss = 28.98 (11.5 examples/sec; 5.584 sec/batch)
2016-04-29 14:35:46.029062: step 1024, loss = 28.70 (11.3 examples/sec; 5.671 sec/batch)
2016-04-29 14:35:51.940481: step 1025, loss = 28.72 (10.8 examples/sec; 5.911 sec/batch)
2016-04-29 14:35:58.046961: step 1026, loss = 28.61 (10.5 examples/sec; 6.106 sec/batch)
2016-04-29 14:36:03.764717: step 1027, loss = 28.81 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 14:36:09.311457: step 1028, loss = 28.77 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 14:36:14.785947: step 1029, loss = 28.63 (11.7 examples/sec; 5.474 sec/batch)
2016-04-29 14:36:20.156241: step 1030, loss = 28.60 (11.9 examples/sec; 5.370 sec/batch)
2016-04-29 14:36:33.766392: step 1031, loss = 28.62 (11.9 examples/sec; 5.397 sec/batch)
2016-04-29 14:36:39.434716: step 1032, loss = 28.42 (11.3 examples/sec; 5.668 sec/batch)
2016-04-29 14:36:44.847637: step 1033, loss = 28.56 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 14:36:50.188774: step 1034, loss = 28.63 (12.0 examples/sec; 5.341 sec/batch)
2016-04-29 14:36:55.834149: step 1035, loss = 28.44 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 14:37:02.083159: step 1036, loss = 28.60 (10.2 examples/sec; 6.249 sec/batch)
2016-04-29 14:37:07.469427: step 1037, loss = 28.55 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 14:37:12.935305: step 1038, loss = 28.48 (11.7 examples/sec; 5.466 sec/batch)
2016-04-29 14:37:18.476048: step 1039, loss = 28.44 (11.6 examples/sec; 5.541 sec/batch)
2016-04-29 14:37:24.127076: step 1040, loss = 28.44 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 14:37:37.432546: step 1041, loss = 28.37 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 14:37:42.943083: step 1042, loss = 28.48 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 14:37:48.548928: step 1043, loss = 28.33 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 14:37:54.067513: step 1044, loss = 28.44 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 14:37:59.246007: step 1045, loss = 28.37 (12.4 examples/sec; 5.178 sec/batch)
2016-04-29 14:38:05.029441: step 1046, loss = 28.37 (11.1 examples/sec; 5.783 sec/batch)
2016-04-29 14:38:11.184489: step 1047, loss = 28.42 (10.4 examples/sec; 6.155 sec/batch)
2016-04-29 14:38:16.839729: step 1048, loss = 28.12 (11.3 examples/sec; 5.655 sec/batch)
2016-04-29 14:38:22.511418: step 1049, loss = 28.44 (11.3 examples/sec; 5.672 sec/batch)
2016-04-29 14:38:28.027669: step 1050, loss = 28.00 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 14:38:41.498935: step 1051, loss = 28.20 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 14:38:47.187854: step 1052, loss = 28.13 (11.3 examples/sec; 5.689 sec/batch)
2016-04-29 14:38:52.769655: step 1053, loss = 27.87 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 14:38:58.427648: step 1054, loss = 28.17 (11.3 examples/sec; 5.658 sec/batch)
2016-04-29 14:39:04.186503: step 1055, loss = 28.03 (11.1 examples/sec; 5.759 sec/batch)
2016-04-29 14:39:09.667359: step 1056, loss = 27.97 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 14:39:15.724330: step 1057, loss = 28.13 (10.6 examples/sec; 6.057 sec/batch)
2016-04-29 14:39:21.450705: step 1058, loss = 27.95 (11.2 examples/sec; 5.726 sec/batch)
2016-04-29 14:39:26.900424: step 1059, loss = 27.98 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 14:39:32.234956: step 1060, loss = 28.09 (12.0 examples/sec; 5.334 sec/batch)
2016-04-29 14:39:45.153320: step 1061, loss = 28.05 (12.0 examples/sec; 5.348 sec/batch)
2016-04-29 14:39:51.417297: step 1062, loss = 28.03 (10.2 examples/sec; 6.264 sec/batch)
2016-04-29 14:39:56.922129: step 1063, loss = 27.90 (11.6 examples/sec; 5.505 sec/batch)
2016-04-29 14:40:02.594908: step 1064, loss = 27.97 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 14:40:07.902572: step 1065, loss = 27.90 (12.1 examples/sec; 5.308 sec/batch)
2016-04-29 14:40:13.510510: step 1066, loss = 27.78 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 14:40:19.654544: step 1067, loss = 27.74 (10.4 examples/sec; 6.144 sec/batch)
2016-04-29 14:40:25.116047: step 1068, loss = 27.77 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 14:40:30.585169: step 1069, loss = 27.53 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 14:40:36.019001: step 1070, loss = 27.80 (11.8 examples/sec; 5.434 sec/batch)
2016-04-29 14:40:48.783651: step 1071, loss = 27.85 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 14:40:54.868626: step 1072, loss = 27.58 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 14:41:00.255414: step 1073, loss = 27.73 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 14:41:06.025360: step 1074, loss = 27.90 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 14:41:11.151700: step 1075, loss = 27.74 (12.5 examples/sec; 5.126 sec/batch)
2016-04-29 14:41:16.851337: step 1076, loss = 27.64 (11.2 examples/sec; 5.700 sec/batch)
2016-04-29 14:41:22.294956: step 1077, loss = 27.59 (11.8 examples/sec; 5.444 sec/batch)
2016-04-29 14:41:28.519870: step 1078, loss = 27.76 (10.3 examples/sec; 6.225 sec/batch)
2016-04-29 14:41:33.936419: step 1079, loss = 27.55 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 14:41:39.469296: step 1080, loss = 27.63 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 14:41:52.117656: step 1081, loss = 27.71 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 14:41:58.144995: step 1082, loss = 27.59 (10.6 examples/sec; 6.027 sec/batch)
2016-04-29 14:42:03.727583: step 1083, loss = 27.58 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 14:42:09.323802: step 1084, loss = 27.50 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 14:42:14.932074: step 1085, loss = 27.68 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 14:42:20.188964: step 1086, loss = 27.51 (12.2 examples/sec; 5.257 sec/batch)
2016-04-29 14:42:25.633105: step 1087, loss = 27.56 (11.8 examples/sec; 5.444 sec/batch)
2016-04-29 14:42:31.896393: step 1088, loss = 27.34 (10.2 examples/sec; 6.263 sec/batch)
2016-04-29 14:42:37.425174: step 1089, loss = 27.39 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 14:42:43.090623: step 1090, loss = 27.41 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 14:42:55.920554: step 1091, loss = 27.39 (12.0 examples/sec; 5.317 sec/batch)
2016-04-29 14:43:01.571011: step 1092, loss = 27.38 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 14:43:07.714646: step 1093, loss = 27.35 (10.4 examples/sec; 6.144 sec/batch)
2016-04-29 14:43:13.109400: step 1094, loss = 27.32 (11.9 examples/sec; 5.395 sec/batch)
2016-04-29 14:43:18.754404: step 1095, loss = 27.16 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 14:43:24.272146: step 1096, loss = 27.13 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 14:43:29.534555: step 1097, loss = 27.30 (12.2 examples/sec; 5.262 sec/batch)
2016-04-29 14:43:34.778563: step 1098, loss = 27.09 (12.2 examples/sec; 5.244 sec/batch)
2016-04-29 14:43:41.011201: step 1099, loss = 27.23 (10.3 examples/sec; 6.233 sec/batch)
2016-04-29 14:43:46.681516: step 1100, loss = 27.23 (11.3 examples/sec; 5.670 sec/batch)
2016-04-29 14:43:59.614162: step 1101, loss = 27.28 (12.2 examples/sec; 5.229 sec/batch)
2016-04-29 14:44:04.966228: step 1102, loss = 28.68 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 14:44:11.273840: step 1103, loss = 28.13 (10.1 examples/sec; 6.308 sec/batch)
2016-04-29 14:44:16.740233: step 1104, loss = 28.06 (11.7 examples/sec; 5.466 sec/batch)
2016-04-29 14:44:22.062336: step 1105, loss = 27.14 (12.0 examples/sec; 5.322 sec/batch)
2016-04-29 14:44:27.794526: step 1106, loss = 27.25 (11.2 examples/sec; 5.732 sec/batch)
2016-04-29 14:44:33.341023: step 1107, loss = 27.04 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 14:44:38.685475: step 1108, loss = 27.15 (12.0 examples/sec; 5.344 sec/batch)
2016-04-29 14:44:44.819034: step 1109, loss = 27.16 (10.4 examples/sec; 6.133 sec/batch)
2016-04-29 14:44:50.205952: step 1110, loss = 27.08 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 14:45:03.126327: step 1111, loss = 26.97 (12.0 examples/sec; 5.314 sec/batch)
2016-04-29 14:45:08.328521: step 1112, loss = 26.99 (12.3 examples/sec; 5.202 sec/batch)
2016-04-29 14:45:13.808783: step 1113, loss = 27.12 (11.7 examples/sec; 5.480 sec/batch)
2016-04-29 14:45:19.801120: step 1114, loss = 28.79 (10.7 examples/sec; 5.992 sec/batch)
2016-04-29 14:45:25.223214: step 1115, loss = 30.15 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 14:45:30.832531: step 1116, loss = 26.97 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 14:45:36.255954: step 1117, loss = 26.83 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 14:45:41.803910: step 1118, loss = 26.89 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 14:45:47.094500: step 1119, loss = 26.93 (12.1 examples/sec; 5.291 sec/batch)
2016-04-29 14:45:53.448478: step 1120, loss = 26.83 (10.1 examples/sec; 6.354 sec/batch)
2016-04-29 14:46:06.476493: step 1121, loss = 26.97 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 14:46:12.087949: step 1122, loss = 26.79 (11.4 examples/sec; 5.611 sec/batch)
2016-04-29 14:46:17.552741: step 1123, loss = 26.95 (11.7 examples/sec; 5.465 sec/batch)
2016-04-29 14:46:23.910749: step 1124, loss = 26.78 (10.1 examples/sec; 6.358 sec/batch)
2016-04-29 14:46:29.469332: step 1125, loss = 26.98 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 14:46:34.887156: step 1126, loss = 26.74 (11.8 examples/sec; 5.418 sec/batch)
2016-04-29 14:46:40.470146: step 1127, loss = 26.82 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 14:46:46.200502: step 1128, loss = 26.73 (11.2 examples/sec; 5.730 sec/batch)
2016-04-29 14:46:51.923410: step 1129, loss = 26.60 (11.2 examples/sec; 5.723 sec/batch)
2016-04-29 14:46:58.053059: step 1130, loss = 26.47 (10.4 examples/sec; 6.130 sec/batch)
2016-04-29 14:47:10.890334: step 1131, loss = 26.82 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 14:47:16.888470: step 1132, loss = 26.84 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 14:47:22.543022: step 1133, loss = 26.55 (11.3 examples/sec; 5.654 sec/batch)
2016-04-29 14:47:28.814926: step 1134, loss = 26.72 (10.2 examples/sec; 6.272 sec/batch)
2016-04-29 14:47:34.305592: step 1135, loss = 26.51 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:47:39.904410: step 1136, loss = 26.84 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 14:47:45.467188: step 1137, loss = 26.72 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 14:47:51.242556: step 1138, loss = 26.94 (11.1 examples/sec; 5.775 sec/batch)
2016-04-29 14:47:56.701974: step 1139, loss = 29.33 (11.7 examples/sec; 5.459 sec/batch)
2016-04-29 14:48:03.116919: step 1140, loss = 28.01 (10.0 examples/sec; 6.415 sec/batch)
2016-04-29 14:48:16.472459: step 1141, loss = 28.83 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 14:48:22.207291: step 1142, loss = 28.40 (11.2 examples/sec; 5.735 sec/batch)
2016-04-29 14:48:28.023952: step 1143, loss = 28.63 (11.0 examples/sec; 5.817 sec/batch)
2016-04-29 14:48:34.204311: step 1144, loss = 27.92 (10.4 examples/sec; 6.180 sec/batch)
2016-04-29 14:48:39.759444: step 1145, loss = 28.26 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 14:48:45.237233: step 1146, loss = 27.21 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 14:48:50.727893: step 1147, loss = 35.62 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:48:56.275590: step 1148, loss = 39.39 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 14:49:01.964290: step 1149, loss = 26.24 (11.3 examples/sec; 5.689 sec/batch)
2016-04-29 14:49:07.929381: step 1150, loss = 26.49 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 14:49:21.038290: step 1151, loss = 26.21 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 14:49:26.756722: step 1152, loss = 26.56 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 14:49:32.110647: step 1153, loss = 26.11 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:49:37.878598: step 1154, loss = 26.01 (11.1 examples/sec; 5.768 sec/batch)
2016-04-29 14:49:43.868965: step 1155, loss = 26.15 (10.7 examples/sec; 5.990 sec/batch)
2016-04-29 14:49:49.330288: step 1156, loss = 26.16 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 14:49:54.851633: step 1157, loss = 26.07 (11.6 examples/sec; 5.521 sec/batch)
2016-04-29 14:50:00.401994: step 1158, loss = 26.13 (11.5 examples/sec; 5.550 sec/batch)
2016-04-29 14:50:06.066584: step 1159, loss = 25.96 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 14:50:11.420948: step 1160, loss = 25.76 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:50:25.719761: step 1161, loss = 26.03 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 14:50:31.261396: step 1162, loss = 26.01 (11.5 examples/sec; 5.542 sec/batch)
2016-04-29 14:50:36.834258: step 1163, loss = 25.71 (11.5 examples/sec; 5.573 sec/batch)
2016-04-29 14:50:42.440204: step 1164, loss = 26.13 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 14:50:48.612027: step 1165, loss = 26.89 (10.4 examples/sec; 6.172 sec/batch)
2016-04-29 14:50:53.995702: step 1166, loss = 27.16 (11.9 examples/sec; 5.384 sec/batch)
2016-04-29 14:50:59.349383: step 1167, loss = 27.30 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 14:51:04.876084: step 1168, loss = 28.75 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 14:51:10.438794: step 1169, loss = 26.24 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 14:51:16.025415: step 1170, loss = 26.03 (11.5 examples/sec; 5.587 sec/batch)
2016-04-29 14:51:29.682489: step 1171, loss = 26.02 (12.1 examples/sec; 5.291 sec/batch)
2016-04-29 14:51:35.258385: step 1172, loss = 27.20 (11.5 examples/sec; 5.576 sec/batch)
2016-04-29 14:51:40.907069: step 1173, loss = 27.36 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 14:51:46.511296: step 1174, loss = 27.34 (11.4 examples/sec; 5.604 sec/batch)
2016-04-29 14:51:52.781700: step 1175, loss = 28.24 (10.2 examples/sec; 6.270 sec/batch)
2016-04-29 14:51:58.440941: step 1176, loss = 28.32 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 14:52:04.172132: step 1177, loss = 28.15 (11.2 examples/sec; 5.731 sec/batch)
2016-04-29 14:52:09.691644: step 1178, loss = 25.91 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 14:52:15.256074: step 1179, loss = 26.01 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 14:52:20.565984: step 1180, loss = 25.72 (12.1 examples/sec; 5.310 sec/batch)
2016-04-29 14:52:34.322125: step 1181, loss = 25.48 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 14:52:39.929781: step 1182, loss = 25.64 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 14:52:45.621753: step 1183, loss = 27.59 (11.2 examples/sec; 5.692 sec/batch)
2016-04-29 14:52:51.280659: step 1184, loss = 35.13 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 14:52:56.991130: step 1185, loss = 73.73 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 14:53:03.297219: step 1186, loss = 25.39 (10.1 examples/sec; 6.306 sec/batch)
2016-04-29 14:53:08.595026: step 1187, loss = 25.45 (12.1 examples/sec; 5.298 sec/batch)
2016-04-29 14:53:14.190320: step 1188, loss = 25.42 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 14:53:19.908198: step 1189, loss = 25.55 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 14:53:25.443037: step 1190, loss = 25.51 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 14:53:39.455803: step 1191, loss = 25.38 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 14:53:44.898550: step 1192, loss = 25.44 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 14:53:50.283275: step 1193, loss = 25.43 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 14:53:55.677744: step 1194, loss = 25.40 (11.9 examples/sec; 5.394 sec/batch)
2016-04-29 14:54:01.382960: step 1195, loss = 25.36 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 14:54:07.389807: step 1196, loss = 25.23 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 14:54:12.988187: step 1197, loss = 25.06 (11.4 examples/sec; 5.598 sec/batch)
2016-04-29 14:54:18.457302: step 1198, loss = 25.25 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 14:54:23.863989: step 1199, loss = 25.17 (11.8 examples/sec; 5.407 sec/batch)
2016-04-29 14:54:29.267546: step 1200, loss = 25.41 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 14:54:42.780095: step 1201, loss = 25.29 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 14:54:48.324286: step 1202, loss = 25.13 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 14:54:53.668470: step 1203, loss = 25.17 (12.0 examples/sec; 5.344 sec/batch)
2016-04-29 14:54:59.172438: step 1204, loss = 25.01 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 14:55:04.613397: step 1205, loss = 25.00 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 14:55:10.810355: step 1206, loss = 24.94 (10.3 examples/sec; 6.197 sec/batch)
2016-04-29 14:55:16.342395: step 1207, loss = 24.98 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 14:55:21.850039: step 1208, loss = 25.10 (11.6 examples/sec; 5.508 sec/batch)
2016-04-29 14:55:27.474774: step 1209, loss = 24.94 (11.4 examples/sec; 5.625 sec/batch)
2016-04-29 14:55:32.728477: step 1210, loss = 24.97 (12.2 examples/sec; 5.254 sec/batch)
2016-04-29 14:55:46.162608: step 1211, loss = 25.25 (10.7 examples/sec; 6.003 sec/batch)
2016-04-29 14:55:51.647399: step 1212, loss = 24.90 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 14:55:57.068707: step 1213, loss = 24.82 (11.8 examples/sec; 5.421 sec/batch)
2016-04-29 14:56:03.280334: step 1214, loss = 24.97 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 14:56:08.811915: step 1215, loss = 24.96 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 14:56:14.230069: step 1216, loss = 24.81 (11.8 examples/sec; 5.418 sec/batch)
2016-04-29 14:56:20.266139: step 1217, loss = 24.95 (10.6 examples/sec; 6.036 sec/batch)
2016-04-29 14:56:26.046343: step 1218, loss = 24.81 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 14:56:31.474049: step 1219, loss = 24.72 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 14:56:37.074132: step 1220, loss = 24.91 (11.4 examples/sec; 5.600 sec/batch)
2016-04-29 14:56:50.313276: step 1221, loss = 24.68 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 14:56:56.172038: step 1222, loss = 24.79 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 14:57:01.618747: step 1223, loss = 24.87 (11.8 examples/sec; 5.447 sec/batch)
2016-04-29 14:57:07.056428: step 1224, loss = 24.70 (11.8 examples/sec; 5.438 sec/batch)
2016-04-29 14:57:12.581468: step 1225, loss = 24.67 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 14:57:17.931388: step 1226, loss = 24.80 (12.0 examples/sec; 5.350 sec/batch)
2016-04-29 14:57:24.110177: step 1227, loss = 24.62 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 14:57:29.282920: step 1228, loss = 24.61 (12.4 examples/sec; 5.173 sec/batch)
2016-04-29 14:57:34.718384: step 1229, loss = 24.68 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 14:57:40.182148: step 1230, loss = 24.63 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 14:57:52.773975: step 1231, loss = 24.68 (12.0 examples/sec; 5.341 sec/batch)
2016-04-29 14:57:58.892330: step 1232, loss = 24.62 (10.5 examples/sec; 6.118 sec/batch)
2016-04-29 14:58:04.376014: step 1233, loss = 24.43 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 14:58:09.922690: step 1234, loss = 24.52 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 14:58:15.428445: step 1235, loss = 24.68 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 14:58:20.609735: step 1236, loss = 24.51 (12.4 examples/sec; 5.181 sec/batch)
2016-04-29 14:58:26.249043: step 1237, loss = 24.58 (11.3 examples/sec; 5.639 sec/batch)
2016-04-29 14:58:32.179308: step 1238, loss = 24.65 (10.8 examples/sec; 5.930 sec/batch)
2016-04-29 14:58:37.683828: step 1239, loss = 24.52 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 14:58:43.191511: step 1240, loss = 24.55 (11.6 examples/sec; 5.508 sec/batch)
2016-04-29 14:58:56.160313: step 1241, loss = 24.35 (11.8 examples/sec; 5.417 sec/batch)
2016-04-29 14:59:02.323974: step 1242, loss = 24.40 (10.4 examples/sec; 6.164 sec/batch)
2016-04-29 14:59:07.639365: step 1243, loss = 24.50 (12.0 examples/sec; 5.315 sec/batch)
2016-04-29 14:59:13.147634: step 1244, loss = 24.41 (11.6 examples/sec; 5.508 sec/batch)
2016-04-29 14:59:18.859230: step 1245, loss = 24.46 (11.2 examples/sec; 5.712 sec/batch)
2016-04-29 14:59:24.521962: step 1246, loss = 24.39 (11.3 examples/sec; 5.663 sec/batch)
2016-04-29 14:59:29.848325: step 1247, loss = 24.25 (12.0 examples/sec; 5.326 sec/batch)
2016-04-29 14:59:35.738793: step 1248, loss = 24.43 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 14:59:41.105628: step 1249, loss = 24.39 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 14:59:46.642648: step 1250, loss = 24.46 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 14:59:59.473430: step 1251, loss = 24.30 (12.7 examples/sec; 5.023 sec/batch)
2016-04-29 15:00:04.968556: step 1252, loss = 24.31 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 15:00:11.104424: step 1253, loss = 24.99 (10.4 examples/sec; 6.136 sec/batch)
2016-04-29 15:00:16.716443: step 1254, loss = 24.91 (11.4 examples/sec; 5.612 sec/batch)
2016-04-29 15:00:22.331763: step 1255, loss = 25.52 (11.4 examples/sec; 5.615 sec/batch)
2016-04-29 15:00:28.011524: step 1256, loss = 24.41 (11.3 examples/sec; 5.680 sec/batch)
2016-04-29 15:00:33.453756: step 1257, loss = 24.60 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 15:00:38.562627: step 1258, loss = 24.42 (12.5 examples/sec; 5.109 sec/batch)
2016-04-29 15:00:44.741511: step 1259, loss = 25.22 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 15:00:50.215121: step 1260, loss = 24.99 (11.7 examples/sec; 5.474 sec/batch)
2016-04-29 15:01:02.940781: step 1261, loss = 24.35 (12.3 examples/sec; 5.207 sec/batch)
2016-04-29 15:01:08.300415: step 1262, loss = 26.00 (11.9 examples/sec; 5.360 sec/batch)
2016-04-29 15:01:14.237763: step 1263, loss = 28.52 (10.8 examples/sec; 5.937 sec/batch)
2016-04-29 15:01:19.785452: step 1264, loss = 25.96 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 15:01:25.245202: step 1265, loss = 24.41 (11.7 examples/sec; 5.460 sec/batch)
2016-04-29 15:01:30.573737: step 1266, loss = 24.06 (12.0 examples/sec; 5.328 sec/batch)
2016-04-29 15:01:36.080449: step 1267, loss = 24.00 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 15:01:41.261528: step 1268, loss = 23.90 (12.4 examples/sec; 5.181 sec/batch)
2016-04-29 15:01:47.320690: step 1269, loss = 24.16 (10.6 examples/sec; 6.059 sec/batch)
2016-04-29 15:01:52.759972: step 1270, loss = 26.98 (11.8 examples/sec; 5.439 sec/batch)
2016-04-29 15:02:05.789967: step 1271, loss = 29.89 (11.8 examples/sec; 5.432 sec/batch)
2016-04-29 15:02:11.209761: step 1272, loss = 30.35 (11.8 examples/sec; 5.420 sec/batch)
2016-04-29 15:02:16.931667: step 1273, loss = 24.83 (11.2 examples/sec; 5.722 sec/batch)
2016-04-29 15:02:23.164515: step 1274, loss = 24.25 (10.3 examples/sec; 6.233 sec/batch)
2016-04-29 15:02:28.585957: step 1275, loss = 24.33 (11.8 examples/sec; 5.421 sec/batch)
2016-04-29 15:02:34.113395: step 1276, loss = 24.04 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 15:02:39.770684: step 1277, loss = 23.92 (11.3 examples/sec; 5.657 sec/batch)
2016-04-29 15:02:44.919279: step 1278, loss = 23.93 (12.4 examples/sec; 5.148 sec/batch)
2016-04-29 15:02:50.272036: step 1279, loss = 24.04 (12.0 examples/sec; 5.353 sec/batch)
2016-04-29 15:02:56.348524: step 1280, loss = 23.83 (10.5 examples/sec; 6.076 sec/batch)
2016-04-29 15:03:09.474528: step 1281, loss = 24.16 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 15:03:14.820324: step 1282, loss = 29.33 (12.0 examples/sec; 5.346 sec/batch)
2016-04-29 15:03:20.087336: step 1283, loss = 30.25 (12.2 examples/sec; 5.267 sec/batch)
2016-04-29 15:03:26.410579: step 1284, loss = 30.53 (10.1 examples/sec; 6.323 sec/batch)
2016-04-29 15:03:32.008478: step 1285, loss = 28.46 (11.4 examples/sec; 5.598 sec/batch)
2016-04-29 15:03:37.539816: step 1286, loss = 24.68 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 15:03:43.257621: step 1287, loss = 24.74 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 15:03:48.757467: step 1288, loss = 24.27 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 15:03:54.334637: step 1289, loss = 23.72 (11.5 examples/sec; 5.577 sec/batch)
2016-04-29 15:04:00.582984: step 1290, loss = 23.81 (10.2 examples/sec; 6.248 sec/batch)
2016-04-29 15:04:13.443173: step 1291, loss = 24.16 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 15:04:19.091600: step 1292, loss = 26.46 (11.3 examples/sec; 5.648 sec/batch)
2016-04-29 15:04:24.738670: step 1293, loss = 33.32 (11.3 examples/sec; 5.647 sec/batch)
2016-04-29 15:04:30.244664: step 1294, loss = 30.49 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 15:04:36.423412: step 1295, loss = 26.66 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 15:04:41.909771: step 1296, loss = 25.84 (11.7 examples/sec; 5.486 sec/batch)
2016-04-29 15:04:47.262412: step 1297, loss = 25.85 (12.0 examples/sec; 5.353 sec/batch)
2016-04-29 15:04:53.017917: step 1298, loss = 23.95 (11.1 examples/sec; 5.755 sec/batch)
2016-04-29 15:04:58.683885: step 1299, loss = 25.41 (11.3 examples/sec; 5.666 sec/batch)
2016-04-29 15:05:04.816814: step 1300, loss = 36.09 (10.4 examples/sec; 6.133 sec/batch)
2016-04-29 15:05:17.573870: step 1301, loss = 60.05 (12.3 examples/sec; 5.193 sec/batch)
2016-04-29 15:05:23.092466: step 1302, loss = 24.63 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 15:05:28.577608: step 1303, loss = 23.29 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 15:05:33.910247: step 1304, loss = 23.39 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 15:05:40.093939: step 1305, loss = 23.28 (10.3 examples/sec; 6.184 sec/batch)
2016-04-29 15:05:45.581439: step 1306, loss = 23.14 (11.7 examples/sec; 5.487 sec/batch)
2016-04-29 15:05:51.035323: step 1307, loss = 23.17 (11.7 examples/sec; 5.454 sec/batch)
2016-04-29 15:05:56.488838: step 1308, loss = 23.23 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 15:06:02.352623: step 1309, loss = 23.04 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 15:06:07.956450: step 1310, loss = 23.19 (11.4 examples/sec; 5.604 sec/batch)
2016-04-29 15:06:21.834422: step 1311, loss = 23.17 (12.0 examples/sec; 5.315 sec/batch)
2016-04-29 15:06:27.383602: step 1312, loss = 23.19 (11.5 examples/sec; 5.549 sec/batch)
2016-04-29 15:06:32.738309: step 1313, loss = 23.19 (12.0 examples/sec; 5.355 sec/batch)
2016-04-29 15:06:38.067310: step 1314, loss = 23.22 (12.0 examples/sec; 5.329 sec/batch)
2016-04-29 15:06:44.043797: step 1315, loss = 23.03 (10.7 examples/sec; 5.976 sec/batch)
2016-04-29 15:06:49.818737: step 1316, loss = 23.06 (11.1 examples/sec; 5.775 sec/batch)
2016-04-29 15:06:55.313935: step 1317, loss = 23.06 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 15:07:01.005335: step 1318, loss = 23.13 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 15:07:06.505615: step 1319, loss = 22.93 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 15:07:11.967996: step 1320, loss = 23.02 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 15:07:25.190017: step 1321, loss = 23.02 (12.0 examples/sec; 5.336 sec/batch)
2016-04-29 15:07:30.864897: step 1322, loss = 22.87 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 15:07:36.540052: step 1323, loss = 22.81 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 15:07:42.101723: step 1324, loss = 22.91 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 15:07:47.543698: step 1325, loss = 22.89 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 15:07:53.575227: step 1326, loss = 22.81 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 15:07:59.128061: step 1327, loss = 22.98 (11.5 examples/sec; 5.553 sec/batch)
2016-04-29 15:08:04.686411: step 1328, loss = 22.86 (11.5 examples/sec; 5.558 sec/batch)
2016-04-29 15:08:10.362888: step 1329, loss = 22.79 (11.3 examples/sec; 5.676 sec/batch)
2016-04-29 15:08:15.742147: step 1330, loss = 22.77 (11.9 examples/sec; 5.379 sec/batch)
2016-04-29 15:08:29.151381: step 1331, loss = 22.88 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 15:08:34.712795: step 1332, loss = 22.83 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 15:08:40.428958: step 1333, loss = 22.82 (11.2 examples/sec; 5.716 sec/batch)
2016-04-29 15:08:46.045989: step 1334, loss = 22.72 (11.4 examples/sec; 5.617 sec/batch)
2016-04-29 15:08:51.494391: step 1335, loss = 22.67 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 15:08:57.477908: step 1336, loss = 22.82 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 15:09:02.583432: step 1337, loss = 22.67 (12.5 examples/sec; 5.105 sec/batch)
2016-04-29 15:09:08.237767: step 1338, loss = 22.72 (11.3 examples/sec; 5.654 sec/batch)
2016-04-29 15:09:13.850308: step 1339, loss = 22.67 (11.4 examples/sec; 5.612 sec/batch)
2016-04-29 15:09:19.257586: step 1340, loss = 22.77 (11.8 examples/sec; 5.407 sec/batch)
2016-04-29 15:09:32.373803: step 1341, loss = 22.49 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 15:09:37.850036: step 1342, loss = 22.60 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 15:09:43.415724: step 1343, loss = 22.67 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 15:09:48.839099: step 1344, loss = 22.58 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 15:09:54.200343: step 1345, loss = 22.68 (11.9 examples/sec; 5.361 sec/batch)
2016-04-29 15:09:59.435146: step 1346, loss = 22.48 (12.2 examples/sec; 5.235 sec/batch)
2016-04-29 15:10:05.445775: step 1347, loss = 22.68 (10.6 examples/sec; 6.011 sec/batch)
2016-04-29 15:10:11.042349: step 1348, loss = 22.61 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 15:10:16.635687: step 1349, loss = 22.39 (11.4 examples/sec; 5.593 sec/batch)
2016-04-29 15:10:22.168460: step 1350, loss = 22.46 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 15:10:35.107767: step 1351, loss = 22.49 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 15:10:40.620346: step 1352, loss = 22.43 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 15:10:46.217663: step 1353, loss = 22.58 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 15:10:51.709516: step 1354, loss = 22.33 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 15:10:56.828588: step 1355, loss = 22.53 (12.5 examples/sec; 5.119 sec/batch)
2016-04-29 15:11:02.239675: step 1356, loss = 22.43 (11.8 examples/sec; 5.411 sec/batch)
2016-04-29 15:11:08.572228: step 1357, loss = 22.50 (10.1 examples/sec; 6.332 sec/batch)
2016-04-29 15:11:14.103868: step 1358, loss = 22.41 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 15:11:19.596886: step 1359, loss = 22.43 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 15:11:24.869969: step 1360, loss = 22.36 (12.1 examples/sec; 5.273 sec/batch)
2016-04-29 15:11:37.371930: step 1361, loss = 22.38 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 15:11:43.365202: step 1362, loss = 22.29 (10.7 examples/sec; 5.993 sec/batch)
2016-04-29 15:11:48.903930: step 1363, loss = 22.19 (11.6 examples/sec; 5.539 sec/batch)
2016-04-29 15:11:54.274033: step 1364, loss = 22.18 (11.9 examples/sec; 5.370 sec/batch)
2016-04-29 15:11:59.578048: step 1365, loss = 22.26 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 15:12:05.046396: step 1366, loss = 22.11 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 15:12:10.469345: step 1367, loss = 22.26 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 15:12:16.725753: step 1368, loss = 21.99 (10.2 examples/sec; 6.256 sec/batch)
2016-04-29 15:12:22.367951: step 1369, loss = 22.24 (11.3 examples/sec; 5.642 sec/batch)
2016-04-29 15:12:27.990028: step 1370, loss = 22.29 (11.4 examples/sec; 5.622 sec/batch)
2016-04-29 15:12:40.560681: step 1371, loss = 22.15 (11.9 examples/sec; 5.391 sec/batch)
2016-04-29 15:12:46.665078: step 1372, loss = 22.04 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 15:12:52.366507: step 1373, loss = 22.03 (11.2 examples/sec; 5.701 sec/batch)
2016-04-29 15:12:57.927140: step 1374, loss = 21.95 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 15:13:03.467843: step 1375, loss = 21.90 (11.6 examples/sec; 5.541 sec/batch)
2016-04-29 15:13:08.617846: step 1376, loss = 22.06 (12.4 examples/sec; 5.150 sec/batch)
2016-04-29 15:13:14.194261: step 1377, loss = 22.02 (11.5 examples/sec; 5.576 sec/batch)
2016-04-29 15:13:20.192557: step 1378, loss = 22.09 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 15:13:25.554955: step 1379, loss = 21.87 (11.9 examples/sec; 5.362 sec/batch)
2016-04-29 15:13:31.093427: step 1380, loss = 21.95 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 15:13:43.647912: step 1381, loss = 22.15 (12.0 examples/sec; 5.330 sec/batch)
2016-04-29 15:13:49.274656: step 1382, loss = 22.09 (11.4 examples/sec; 5.627 sec/batch)
2016-04-29 15:13:55.446354: step 1383, loss = 22.04 (10.4 examples/sec; 6.172 sec/batch)
2016-04-29 15:14:01.247258: step 1384, loss = 21.82 (11.0 examples/sec; 5.801 sec/batch)
2016-04-29 15:14:06.850437: step 1385, loss = 21.92 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 15:14:12.138498: step 1386, loss = 22.04 (12.1 examples/sec; 5.288 sec/batch)
2016-04-29 15:14:17.544157: step 1387, loss = 22.03 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 15:14:23.068250: step 1388, loss = 21.88 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 15:14:29.225378: step 1389, loss = 21.70 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 15:14:34.650596: step 1390, loss = 21.81 (11.8 examples/sec; 5.425 sec/batch)
2016-04-29 15:14:47.148836: step 1391, loss = 21.71 (12.2 examples/sec; 5.257 sec/batch)
2016-04-29 15:14:52.642032: step 1392, loss = 21.73 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 15:14:58.783952: step 1393, loss = 22.06 (10.4 examples/sec; 6.142 sec/batch)
2016-04-29 15:15:04.324432: step 1394, loss = 22.32 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 15:15:10.063403: step 1395, loss = 24.61 (11.2 examples/sec; 5.739 sec/batch)
2016-04-29 15:15:15.629662: step 1396, loss = 23.01 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 15:15:21.218776: step 1397, loss = 21.85 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 15:15:26.551628: step 1398, loss = 22.06 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 15:15:32.630823: step 1399, loss = 22.27 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 15:15:38.164230: step 1400, loss = 23.40 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 15:15:50.679635: step 1401, loss = 23.54 (12.7 examples/sec; 5.044 sec/batch)
2016-04-29 15:15:56.374533: step 1402, loss = 22.88 (11.2 examples/sec; 5.695 sec/batch)
2016-04-29 15:16:01.879183: step 1403, loss = 25.05 (11.6 examples/sec; 5.505 sec/batch)
2016-04-29 15:16:07.883289: step 1404, loss = 27.20 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 15:16:13.532292: step 1405, loss = 28.12 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 15:16:19.049297: step 1406, loss = 24.81 (11.6 examples/sec; 5.517 sec/batch)
2016-04-29 15:16:24.693957: step 1407, loss = 23.48 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 15:16:30.050278: step 1408, loss = 22.23 (11.9 examples/sec; 5.356 sec/batch)
2016-04-29 15:16:35.347317: step 1409, loss = 22.06 (12.1 examples/sec; 5.297 sec/batch)
2016-04-29 15:16:41.339435: step 1410, loss = 21.54 (10.7 examples/sec; 5.992 sec/batch)
2016-04-29 15:16:53.905595: step 1411, loss = 22.14 (12.8 examples/sec; 5.009 sec/batch)
2016-04-29 15:16:59.316681: step 1412, loss = 21.61 (11.8 examples/sec; 5.411 sec/batch)
2016-04-29 15:17:04.931763: step 1413, loss = 22.47 (11.4 examples/sec; 5.615 sec/batch)
2016-04-29 15:17:11.271367: step 1414, loss = 22.64 (10.1 examples/sec; 6.340 sec/batch)
2016-04-29 15:17:16.677657: step 1415, loss = 22.80 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 15:17:22.259253: step 1416, loss = 22.85 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 15:17:27.945438: step 1417, loss = 22.90 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 15:17:33.230634: step 1418, loss = 22.22 (12.1 examples/sec; 5.285 sec/batch)
2016-04-29 15:17:38.647122: step 1419, loss = 31.15 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 15:17:44.952759: step 1420, loss = 34.75 (10.1 examples/sec; 6.306 sec/batch)
2016-04-29 15:17:57.733835: step 1421, loss = 25.79 (12.0 examples/sec; 5.331 sec/batch)
2016-04-29 15:18:03.386188: step 1422, loss = 24.75 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 15:18:08.688376: step 1423, loss = 23.11 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 15:18:14.235107: step 1424, loss = 21.91 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 15:18:20.441546: step 1425, loss = 21.82 (10.3 examples/sec; 6.206 sec/batch)
2016-04-29 15:18:25.992980: step 1426, loss = 21.68 (11.5 examples/sec; 5.551 sec/batch)
2016-04-29 15:18:31.469238: step 1427, loss = 21.88 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 15:18:37.054514: step 1428, loss = 31.66 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 15:18:42.480334: step 1429, loss = 62.84 (11.8 examples/sec; 5.426 sec/batch)
2016-04-29 15:18:47.799879: step 1430, loss = 28.81 (12.0 examples/sec; 5.319 sec/batch)
2016-04-29 15:19:01.649696: step 1431, loss = 32.66 (11.7 examples/sec; 5.466 sec/batch)
2016-04-29 15:19:07.036229: step 1432, loss = 21.46 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 15:19:12.420102: step 1433, loss = 22.11 (11.9 examples/sec; 5.384 sec/batch)
2016-04-29 15:19:17.803579: step 1434, loss = 23.08 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 15:19:23.846996: step 1435, loss = 21.87 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 15:19:29.399479: step 1436, loss = 21.68 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 15:19:34.822003: step 1437, loss = 21.64 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 15:19:40.271220: step 1438, loss = 21.46 (11.7 examples/sec; 5.449 sec/batch)
2016-04-29 15:19:45.674265: step 1439, loss = 22.08 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 15:19:51.101934: step 1440, loss = 21.76 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 15:20:04.819122: step 1441, loss = 21.80 (11.6 examples/sec; 5.499 sec/batch)
2016-04-29 15:20:10.248941: step 1442, loss = 21.99 (11.8 examples/sec; 5.430 sec/batch)
2016-04-29 15:20:15.658059: step 1443, loss = 22.57 (11.8 examples/sec; 5.409 sec/batch)
2016-04-29 15:20:20.946909: step 1444, loss = 23.13 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 15:20:26.524358: step 1445, loss = 21.59 (11.5 examples/sec; 5.577 sec/batch)
2016-04-29 15:20:32.953074: step 1446, loss = 22.68 (10.0 examples/sec; 6.429 sec/batch)
2016-04-29 15:20:38.288772: step 1447, loss = 32.52 (12.0 examples/sec; 5.336 sec/batch)
2016-04-29 15:20:43.699509: step 1448, loss = 34.14 (11.8 examples/sec; 5.411 sec/batch)
2016-04-29 15:20:49.268466: step 1449, loss = 37.42 (11.5 examples/sec; 5.569 sec/batch)
2016-04-29 15:20:54.868187: step 1450, loss = 45.23 (11.4 examples/sec; 5.600 sec/batch)
2016-04-29 15:21:08.091691: step 1451, loss = 43.89 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 15:21:13.600877: step 1452, loss = 22.17 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 15:21:19.204064: step 1453, loss = 21.52 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 15:21:24.869604: step 1454, loss = 25.37 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 15:21:29.987515: step 1455, loss = 21.13 (12.5 examples/sec; 5.118 sec/batch)
2016-04-29 15:21:35.945546: step 1456, loss = 23.03 (10.7 examples/sec; 5.958 sec/batch)
2016-04-29 15:21:41.459255: step 1457, loss = 24.98 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 15:21:47.011624: step 1458, loss = 30.95 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 15:21:52.603684: step 1459, loss = 34.31 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 15:21:58.147253: step 1460, loss = 29.69 (11.5 examples/sec; 5.543 sec/batch)
2016-04-29 15:22:11.659578: step 1461, loss = 30.49 (10.5 examples/sec; 6.068 sec/batch)
2016-04-29 15:22:17.435746: step 1462, loss = 30.99 (11.1 examples/sec; 5.776 sec/batch)
2016-04-29 15:22:22.967739: step 1463, loss = 26.55 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 15:22:28.553992: step 1464, loss = 39.00 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 15:22:34.002893: step 1465, loss = 54.76 (11.7 examples/sec; 5.449 sec/batch)
2016-04-29 15:22:39.295747: step 1466, loss = 47.52 (12.1 examples/sec; 5.293 sec/batch)
2016-04-29 15:22:45.299874: step 1467, loss = 45.88 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 15:22:50.826532: step 1468, loss = 23.22 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 15:22:56.337339: step 1469, loss = 21.76 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 15:23:01.994364: step 1470, loss = 21.53 (11.3 examples/sec; 5.657 sec/batch)
2016-04-29 15:23:15.246515: step 1471, loss = 22.84 (11.2 examples/sec; 5.726 sec/batch)
2016-04-29 15:23:20.562633: step 1472, loss = 24.84 (12.0 examples/sec; 5.316 sec/batch)
2016-04-29 15:23:26.175540: step 1473, loss = 23.33 (11.4 examples/sec; 5.613 sec/batch)
2016-04-29 15:23:31.774138: step 1474, loss = 25.08 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 15:23:37.318392: step 1475, loss = 30.76 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 15:23:42.914783: step 1476, loss = 59.38 (11.4 examples/sec; 5.596 sec/batch)
2016-04-29 15:23:49.383731: step 1477, loss = 74.87 (9.9 examples/sec; 6.469 sec/batch)
2016-04-29 15:23:54.939266: step 1478, loss = 61.85 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 15:24:00.322158: step 1479, loss = 24.10 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 15:24:05.720083: step 1480, loss = 22.16 (11.9 examples/sec; 5.398 sec/batch)
2016-04-29 15:24:20.660853: step 1481, loss = 23.15 (10.3 examples/sec; 6.193 sec/batch)
2016-04-29 15:24:26.113117: step 1482, loss = 24.84 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 15:24:31.622356: step 1483, loss = 26.17 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 15:24:37.289127: step 1484, loss = 35.36 (11.3 examples/sec; 5.667 sec/batch)
2016-04-29 15:24:42.544427: step 1485, loss = 34.86 (12.2 examples/sec; 5.255 sec/batch)
2016-04-29 15:24:48.030251: step 1486, loss = 42.95 (11.7 examples/sec; 5.486 sec/batch)
2016-04-29 15:24:54.338686: step 1487, loss = 48.64 (10.1 examples/sec; 6.308 sec/batch)
2016-04-29 15:24:59.819435: step 1488, loss = 28.86 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 15:25:05.356427: step 1489, loss = 25.22 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 15:25:10.758208: step 1490, loss = 21.79 (11.8 examples/sec; 5.402 sec/batch)
2016-04-29 15:25:23.552840: step 1491, loss = 22.31 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 15:25:29.594266: step 1492, loss = 23.16 (10.6 examples/sec; 6.041 sec/batch)
2016-04-29 15:25:35.140388: step 1493, loss = 39.46 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 15:25:40.645054: step 1494, loss = 89.52 (11.6 examples/sec; 5.505 sec/batch)
2016-04-29 15:25:46.376362: step 1495, loss = 1204.87 (11.2 examples/sec; 5.731 sec/batch)
2016-04-29 15:25:51.825346: step 1496, loss = 52.05 (11.7 examples/sec; 5.449 sec/batch)
2016-04-29 15:25:57.081578: step 1497, loss = 22.14 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 15:26:03.527044: step 1498, loss = 22.25 (9.9 examples/sec; 6.445 sec/batch)
2016-04-29 15:26:08.852467: step 1499, loss = 22.18 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 15:26:14.211512: step 1500, loss = 22.08 (11.9 examples/sec; 5.359 sec/batch)
2016-04-29 15:26:26.903769: step 1501, loss = 22.09 (12.7 examples/sec; 5.028 sec/batch)
2016-04-29 15:26:32.994421: step 1502, loss = 22.14 (10.5 examples/sec; 6.091 sec/batch)
2016-04-29 15:26:38.551118: step 1503, loss = 22.22 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 15:26:44.111366: step 1504, loss = 22.11 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 15:26:49.656333: step 1505, loss = 22.11 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 15:26:55.151623: step 1506, loss = 22.12 (11.6 examples/sec; 5.495 sec/batch)
2016-04-29 15:27:00.417918: step 1507, loss = 22.24 (12.2 examples/sec; 5.266 sec/batch)
2016-04-29 15:27:06.539985: step 1508, loss = 22.03 (10.5 examples/sec; 6.122 sec/batch)
2016-04-29 15:27:11.978541: step 1509, loss = 21.95 (11.8 examples/sec; 5.438 sec/batch)
2016-04-29 15:27:17.332862: step 1510, loss = 22.10 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 15:27:30.653161: step 1511, loss = 21.91 (12.7 examples/sec; 5.047 sec/batch)
2016-04-29 15:27:35.940036: step 1512, loss = 21.96 (12.1 examples/sec; 5.287 sec/batch)
2016-04-29 15:27:42.173780: step 1513, loss = 21.99 (10.3 examples/sec; 6.234 sec/batch)
2016-04-29 15:27:47.675533: step 1514, loss = 21.81 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 15:27:53.129005: step 1515, loss = 21.98 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 15:27:58.493836: step 1516, loss = 22.03 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 15:28:03.890884: step 1517, loss = 21.88 (11.9 examples/sec; 5.397 sec/batch)
2016-04-29 15:28:09.153790: step 1518, loss = 21.81 (12.2 examples/sec; 5.263 sec/batch)
2016-04-29 15:28:15.439081: step 1519, loss = 21.81 (10.2 examples/sec; 6.285 sec/batch)
2016-04-29 15:28:20.874677: step 1520, loss = 21.88 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 15:28:33.800765: step 1521, loss = 21.79 (12.0 examples/sec; 5.326 sec/batch)
2016-04-29 15:28:39.050647: step 1522, loss = 21.81 (12.2 examples/sec; 5.250 sec/batch)
2016-04-29 15:28:45.157033: step 1523, loss = 21.81 (10.5 examples/sec; 6.106 sec/batch)
2016-04-29 15:28:50.477604: step 1524, loss = 21.72 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 15:28:56.035091: step 1525, loss = 21.67 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 15:29:01.762265: step 1526, loss = 21.65 (11.2 examples/sec; 5.727 sec/batch)
2016-04-29 15:29:07.190341: step 1527, loss = 21.72 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 15:29:12.465658: step 1528, loss = 21.77 (12.1 examples/sec; 5.275 sec/batch)
2016-04-29 15:29:18.704698: step 1529, loss = 21.57 (10.3 examples/sec; 6.239 sec/batch)
2016-04-29 15:29:23.829996: step 1530, loss = 21.75 (12.5 examples/sec; 5.125 sec/batch)
2016-04-29 15:29:36.402198: step 1531, loss = 21.67 (12.4 examples/sec; 5.148 sec/batch)
2016-04-29 15:29:41.774822: step 1532, loss = 21.72 (11.9 examples/sec; 5.373 sec/batch)
2016-04-29 15:29:47.242231: step 1533, loss = 21.63 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 15:29:53.340395: step 1534, loss = 21.49 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 15:29:58.785830: step 1535, loss = 21.74 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 15:30:04.368958: step 1536, loss = 21.49 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 15:30:09.678753: step 1537, loss = 21.64 (12.1 examples/sec; 5.310 sec/batch)
2016-04-29 15:30:14.943944: step 1538, loss = 21.66 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 15:30:20.614390: step 1539, loss = 21.61 (11.3 examples/sec; 5.670 sec/batch)
2016-04-29 15:30:26.900446: step 1540, loss = 21.50 (10.2 examples/sec; 6.286 sec/batch)
2016-04-29 15:30:39.839113: step 1541, loss = 21.57 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 15:30:44.890385: step 1542, loss = 21.52 (12.7 examples/sec; 5.051 sec/batch)
2016-04-29 15:30:50.379159: step 1543, loss = 21.39 (11.7 examples/sec; 5.489 sec/batch)
2016-04-29 15:30:56.769937: step 1544, loss = 21.33 (10.0 examples/sec; 6.391 sec/batch)
2016-04-29 15:31:02.456479: step 1545, loss = 21.33 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 15:31:08.043237: step 1546, loss = 21.22 (11.5 examples/sec; 5.587 sec/batch)
2016-04-29 15:31:13.573125: step 1547, loss = 21.24 (11.6 examples/sec; 5.530 sec/batch)
2016-04-29 15:31:19.259261: step 1548, loss = 21.57 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 15:31:24.767051: step 1549, loss = 21.40 (11.6 examples/sec; 5.508 sec/batch)
2016-04-29 15:31:30.954369: step 1550, loss = 21.52 (10.3 examples/sec; 6.187 sec/batch)
2016-04-29 15:31:43.467050: step 1551, loss = 21.16 (12.1 examples/sec; 5.283 sec/batch)
2016-04-29 15:31:49.225272: step 1552, loss = 21.29 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 15:31:54.693027: step 1553, loss = 21.31 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 15:31:59.828437: step 1554, loss = 21.10 (12.5 examples/sec; 5.135 sec/batch)
2016-04-29 15:32:06.047246: step 1555, loss = 21.14 (10.3 examples/sec; 6.219 sec/batch)
2016-04-29 15:32:11.331922: step 1556, loss = 21.40 (12.1 examples/sec; 5.285 sec/batch)
2016-04-29 15:32:16.924236: step 1557, loss = 21.17 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 15:32:22.321169: step 1558, loss = 21.27 (11.9 examples/sec; 5.397 sec/batch)
2016-04-29 15:32:27.642605: step 1559, loss = 21.21 (12.0 examples/sec; 5.321 sec/batch)
2016-04-29 15:32:33.076047: step 1560, loss = 21.21 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 15:32:46.743246: step 1561, loss = 21.02 (11.5 examples/sec; 5.588 sec/batch)
2016-04-29 15:32:52.337971: step 1562, loss = 21.12 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 15:32:57.778363: step 1563, loss = 21.26 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 15:33:03.165521: step 1564, loss = 21.24 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 15:33:09.145722: step 1565, loss = 21.22 (10.7 examples/sec; 5.980 sec/batch)
2016-04-29 15:33:14.565211: step 1566, loss = 21.00 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 15:33:19.936436: step 1567, loss = 20.97 (11.9 examples/sec; 5.371 sec/batch)
2016-04-29 15:33:25.230905: step 1568, loss = 20.99 (12.1 examples/sec; 5.294 sec/batch)
2016-04-29 15:33:30.647284: step 1569, loss = 21.10 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 15:33:35.710433: step 1570, loss = 21.09 (12.6 examples/sec; 5.063 sec/batch)
2016-04-29 15:33:49.105325: step 1571, loss = 21.05 (12.2 examples/sec; 5.248 sec/batch)
2016-04-29 15:33:54.257497: step 1572, loss = 21.02 (12.4 examples/sec; 5.152 sec/batch)
2016-04-29 15:33:59.708473: step 1573, loss = 20.97 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 15:34:05.050731: step 1574, loss = 21.01 (12.0 examples/sec; 5.342 sec/batch)
2016-04-29 15:34:10.606806: step 1575, loss = 20.79 (11.5 examples/sec; 5.556 sec/batch)
2016-04-29 15:34:16.569269: step 1576, loss = 21.02 (10.7 examples/sec; 5.962 sec/batch)
2016-04-29 15:34:22.081736: step 1577, loss = 20.98 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 15:34:27.433328: step 1578, loss = 20.99 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 15:34:32.834354: step 1579, loss = 20.92 (11.8 examples/sec; 5.401 sec/batch)
2016-04-29 15:34:38.380370: step 1580, loss = 20.84 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 15:34:51.888290: step 1581, loss = 20.92 (10.9 examples/sec; 5.891 sec/batch)
2016-04-29 15:34:57.179346: step 1582, loss = 20.70 (12.1 examples/sec; 5.291 sec/batch)
2016-04-29 15:35:03.277262: step 1583, loss = 20.63 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 15:35:08.801704: step 1584, loss = 20.84 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 15:35:14.222853: step 1585, loss = 20.81 (11.8 examples/sec; 5.421 sec/batch)
2016-04-29 15:35:20.660236: step 1586, loss = 20.65 (9.9 examples/sec; 6.437 sec/batch)
2016-04-29 15:35:26.284543: step 1587, loss = 20.79 (11.4 examples/sec; 5.624 sec/batch)
2016-04-29 15:35:31.837372: step 1588, loss = 20.67 (11.5 examples/sec; 5.553 sec/batch)
2016-04-29 15:35:37.419679: step 1589, loss = 20.63 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 15:35:42.852886: step 1590, loss = 20.84 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 15:35:56.083099: step 1591, loss = 20.66 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 15:36:01.782211: step 1592, loss = 20.54 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 15:36:07.306315: step 1593, loss = 20.73 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 15:36:12.973290: step 1594, loss = 20.76 (11.3 examples/sec; 5.667 sec/batch)
2016-04-29 15:36:18.185110: step 1595, loss = 20.82 (12.3 examples/sec; 5.212 sec/batch)
2016-04-29 15:36:23.707561: step 1596, loss = 20.64 (11.6 examples/sec; 5.522 sec/batch)
2016-04-29 15:36:29.814774: step 1597, loss = 20.78 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 15:36:35.090801: step 1598, loss = 20.54 (12.1 examples/sec; 5.276 sec/batch)
2016-04-29 15:36:40.489732: step 1599, loss = 20.71 (11.9 examples/sec; 5.399 sec/batch)
2016-04-29 15:36:46.167323: step 1600, loss = 20.61 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 15:36:59.355406: step 1601, loss = 20.47 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 15:37:05.036460: step 1602, loss = 20.69 (11.3 examples/sec; 5.681 sec/batch)
2016-04-29 15:37:10.552848: step 1603, loss = 20.45 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 15:37:15.944720: step 1604, loss = 20.64 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 15:37:21.174585: step 1605, loss = 20.45 (12.2 examples/sec; 5.230 sec/batch)
2016-04-29 15:37:26.852399: step 1606, loss = 20.43 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 15:37:32.746157: step 1607, loss = 20.28 (10.9 examples/sec; 5.894 sec/batch)
2016-04-29 15:37:38.238910: step 1608, loss = 20.39 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 15:37:43.747371: step 1609, loss = 20.29 (11.6 examples/sec; 5.508 sec/batch)
2016-04-29 15:37:49.277678: step 1610, loss = 20.30 (11.6 examples/sec; 5.530 sec/batch)
2016-04-29 15:38:02.071157: step 1611, loss = 20.40 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 15:38:08.105992: step 1612, loss = 20.63 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 15:38:13.620395: step 1613, loss = 20.34 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 15:38:19.215320: step 1614, loss = 20.37 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 15:38:24.491384: step 1615, loss = 20.31 (12.1 examples/sec; 5.276 sec/batch)
2016-04-29 15:38:29.794280: step 1616, loss = 20.37 (12.1 examples/sec; 5.303 sec/batch)
2016-04-29 15:38:35.144976: step 1617, loss = 20.21 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 15:38:41.119918: step 1618, loss = 20.28 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 15:38:46.602331: step 1619, loss = 20.15 (11.7 examples/sec; 5.482 sec/batch)
2016-04-29 15:38:51.988494: step 1620, loss = 20.37 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 15:39:04.738970: step 1621, loss = 20.29 (12.1 examples/sec; 5.305 sec/batch)
2016-04-29 15:39:10.359121: step 1622, loss = 20.30 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 15:39:16.358741: step 1623, loss = 20.24 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 15:39:21.854833: step 1624, loss = 20.22 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 15:39:27.106512: step 1625, loss = 19.99 (12.2 examples/sec; 5.252 sec/batch)
2016-04-29 15:39:32.466773: step 1626, loss = 20.12 (11.9 examples/sec; 5.360 sec/batch)
2016-04-29 15:39:37.930960: step 1627, loss = 20.18 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 15:39:44.014204: step 1628, loss = 20.32 (10.5 examples/sec; 6.083 sec/batch)
2016-04-29 15:39:49.887634: step 1629, loss = 20.21 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 15:39:55.295855: step 1630, loss = 20.10 (11.8 examples/sec; 5.408 sec/batch)
2016-04-29 15:40:08.049818: step 1631, loss = 20.22 (12.1 examples/sec; 5.298 sec/batch)
2016-04-29 15:40:13.555491: step 1632, loss = 20.13 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 15:40:19.568233: step 1633, loss = 20.00 (10.6 examples/sec; 6.013 sec/batch)
2016-04-29 15:40:24.936577: step 1634, loss = 20.16 (11.9 examples/sec; 5.368 sec/batch)
2016-04-29 15:40:29.974077: step 1635, loss = 19.93 (12.7 examples/sec; 5.037 sec/batch)
2016-04-29 15:40:35.373044: step 1636, loss = 20.06 (11.9 examples/sec; 5.399 sec/batch)
2016-04-29 15:40:40.825432: step 1637, loss = 20.11 (11.7 examples/sec; 5.452 sec/batch)
2016-04-29 15:40:46.434871: step 1638, loss = 19.97 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 15:40:52.592407: step 1639, loss = 20.04 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 15:40:58.129940: step 1640, loss = 19.90 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 15:41:10.703152: step 1641, loss = 19.85 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 15:41:16.151679: step 1642, loss = 20.12 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 15:41:21.632487: step 1643, loss = 19.88 (11.7 examples/sec; 5.481 sec/batch)
2016-04-29 15:41:27.603361: step 1644, loss = 19.94 (10.7 examples/sec; 5.971 sec/batch)
2016-04-29 15:41:32.968058: step 1645, loss = 19.78 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 15:41:38.270589: step 1646, loss = 19.87 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 15:41:43.947749: step 1647, loss = 19.79 (11.3 examples/sec; 5.677 sec/batch)
2016-04-29 15:41:49.524975: step 1648, loss = 19.85 (11.5 examples/sec; 5.577 sec/batch)
2016-04-29 15:41:54.824675: step 1649, loss = 19.77 (12.1 examples/sec; 5.300 sec/batch)
2016-04-29 15:42:01.620188: step 1650, loss = 19.89 (9.4 examples/sec; 6.795 sec/batch)
2016-04-29 15:42:14.426328: step 1651, loss = 19.65 (11.9 examples/sec; 5.377 sec/batch)
2016-04-29 15:42:20.033058: step 1652, loss = 19.72 (11.4 examples/sec; 5.607 sec/batch)
2016-04-29 15:42:25.556844: step 1653, loss = 19.86 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 15:42:31.705172: step 1654, loss = 19.78 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 15:42:37.268121: step 1655, loss = 19.59 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 15:42:42.746105: step 1656, loss = 19.58 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 15:42:48.098643: step 1657, loss = 19.71 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 15:42:53.784630: step 1658, loss = 19.78 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 15:42:59.341953: step 1659, loss = 19.77 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 15:43:05.682058: step 1660, loss = 19.71 (10.1 examples/sec; 6.340 sec/batch)
2016-04-29 15:43:18.291400: step 1661, loss = 19.67 (12.5 examples/sec; 5.132 sec/batch)
2016-04-29 15:43:23.785464: step 1662, loss = 19.71 (11.6 examples/sec; 5.494 sec/batch)
2016-04-29 15:43:29.261775: step 1663, loss = 19.51 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 15:43:35.415513: step 1664, loss = 19.67 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 15:43:41.021292: step 1665, loss = 19.44 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 15:43:46.611502: step 1666, loss = 19.30 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 15:43:52.173745: step 1667, loss = 19.49 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 15:43:57.373685: step 1668, loss = 19.49 (12.3 examples/sec; 5.200 sec/batch)
2016-04-29 15:44:03.204726: step 1669, loss = 19.50 (11.0 examples/sec; 5.831 sec/batch)
2016-04-29 15:44:09.242594: step 1670, loss = 19.47 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 15:44:22.227584: step 1671, loss = 19.56 (12.0 examples/sec; 5.336 sec/batch)
2016-04-29 15:44:27.416076: step 1672, loss = 19.53 (12.3 examples/sec; 5.188 sec/batch)
2016-04-29 15:44:32.997508: step 1673, loss = 19.67 (11.5 examples/sec; 5.581 sec/batch)
2016-04-29 15:44:38.583026: step 1674, loss = 19.32 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 15:44:44.756406: step 1675, loss = 19.48 (10.4 examples/sec; 6.173 sec/batch)
2016-04-29 15:44:50.244341: step 1676, loss = 19.54 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 15:44:55.551326: step 1677, loss = 19.62 (12.1 examples/sec; 5.307 sec/batch)
2016-04-29 15:45:01.371625: step 1678, loss = 19.43 (11.0 examples/sec; 5.820 sec/batch)
2016-04-29 15:45:06.641258: step 1679, loss = 19.34 (12.1 examples/sec; 5.270 sec/batch)
2016-04-29 15:45:12.066916: step 1680, loss = 19.53 (11.8 examples/sec; 5.426 sec/batch)
2016-04-29 15:45:25.587062: step 1681, loss = 19.31 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 15:45:31.191316: step 1682, loss = 19.35 (11.4 examples/sec; 5.604 sec/batch)
2016-04-29 15:45:36.432447: step 1683, loss = 19.48 (12.2 examples/sec; 5.241 sec/batch)
2016-04-29 15:45:41.794715: step 1684, loss = 19.46 (11.9 examples/sec; 5.362 sec/batch)
2016-04-29 15:45:48.327194: step 1685, loss = 19.34 (9.8 examples/sec; 6.532 sec/batch)
2016-04-29 15:45:53.818300: step 1686, loss = 19.27 (11.7 examples/sec; 5.491 sec/batch)
2016-04-29 15:45:59.233306: step 1687, loss = 19.25 (11.8 examples/sec; 5.415 sec/batch)
2016-04-29 15:46:05.028923: step 1688, loss = 19.24 (11.0 examples/sec; 5.796 sec/batch)
2016-04-29 15:46:10.468148: step 1689, loss = 18.99 (11.8 examples/sec; 5.439 sec/batch)
2016-04-29 15:46:15.552440: step 1690, loss = 19.22 (12.6 examples/sec; 5.084 sec/batch)
2016-04-29 15:46:28.972103: step 1691, loss = 19.24 (12.1 examples/sec; 5.284 sec/batch)
2016-04-29 15:46:34.208639: step 1692, loss = 19.15 (12.2 examples/sec; 5.236 sec/batch)
2016-04-29 15:46:39.418075: step 1693, loss = 19.08 (12.3 examples/sec; 5.209 sec/batch)
2016-04-29 15:46:44.930723: step 1694, loss = 19.18 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 15:46:50.254103: step 1695, loss = 19.15 (12.0 examples/sec; 5.323 sec/batch)
2016-04-29 15:46:56.331707: step 1696, loss = 19.09 (10.5 examples/sec; 6.078 sec/batch)
2016-04-29 15:47:02.005483: step 1697, loss = 19.23 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 15:47:07.403505: step 1698, loss = 19.14 (11.9 examples/sec; 5.398 sec/batch)
2016-04-29 15:47:12.833781: step 1699, loss = 19.10 (11.8 examples/sec; 5.430 sec/batch)
2016-04-29 15:47:18.338140: step 1700, loss = 19.02 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 15:47:32.093145: step 1701, loss = 18.82 (12.1 examples/sec; 5.294 sec/batch)
2016-04-29 15:47:37.670992: step 1702, loss = 19.28 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 15:47:43.159689: step 1703, loss = 19.10 (11.7 examples/sec; 5.489 sec/batch)
2016-04-29 15:47:48.508101: step 1704, loss = 19.13 (12.0 examples/sec; 5.348 sec/batch)
2016-04-29 15:47:54.009756: step 1705, loss = 19.10 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 15:47:59.985044: step 1706, loss = 19.04 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 15:48:05.615487: step 1707, loss = 19.04 (11.4 examples/sec; 5.630 sec/batch)
2016-04-29 15:48:11.220783: step 1708, loss = 18.94 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 15:48:16.910793: step 1709, loss = 19.06 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 15:48:22.466151: step 1710, loss = 18.79 (11.5 examples/sec; 5.555 sec/batch)
2016-04-29 15:48:35.740166: step 1711, loss = 18.96 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 15:48:41.271589: step 1712, loss = 18.96 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 15:48:46.884507: step 1713, loss = 19.02 (11.4 examples/sec; 5.613 sec/batch)
2016-04-29 15:48:52.317473: step 1714, loss = 18.99 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 15:48:57.595763: step 1715, loss = 18.85 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 15:49:03.254422: step 1716, loss = 18.92 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 15:49:09.174367: step 1717, loss = 18.95 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 15:49:14.768797: step 1718, loss = 18.81 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 15:49:20.192868: step 1719, loss = 18.85 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 15:49:25.582302: step 1720, loss = 18.73 (11.9 examples/sec; 5.389 sec/batch)
2016-04-29 15:49:38.911696: step 1721, loss = 18.78 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 15:49:44.460006: step 1722, loss = 18.69 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 15:49:49.928759: step 1723, loss = 18.75 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 15:49:55.404996: step 1724, loss = 18.92 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 15:50:00.656368: step 1725, loss = 18.78 (12.2 examples/sec; 5.251 sec/batch)
2016-04-29 15:50:06.286360: step 1726, loss = 18.77 (11.4 examples/sec; 5.630 sec/batch)
2016-04-29 15:50:12.328344: step 1727, loss = 18.80 (10.6 examples/sec; 6.042 sec/batch)
2016-04-29 15:50:17.961623: step 1728, loss = 18.68 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 15:50:23.468341: step 1729, loss = 18.72 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 15:50:29.077421: step 1730, loss = 18.75 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 15:50:41.796573: step 1731, loss = 18.57 (12.1 examples/sec; 5.307 sec/batch)
2016-04-29 15:50:47.909246: step 1732, loss = 18.66 (10.5 examples/sec; 6.112 sec/batch)
2016-04-29 15:50:53.455263: step 1733, loss = 18.66 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 15:50:58.809967: step 1734, loss = 18.45 (12.0 examples/sec; 5.355 sec/batch)
2016-04-29 15:51:04.596151: step 1735, loss = 18.60 (11.1 examples/sec; 5.786 sec/batch)
2016-04-29 15:51:10.059617: step 1736, loss = 18.53 (11.7 examples/sec; 5.463 sec/batch)
2016-04-29 15:51:15.318934: step 1737, loss = 18.49 (12.2 examples/sec; 5.259 sec/batch)
2016-04-29 15:51:21.491866: step 1738, loss = 18.62 (10.4 examples/sec; 6.173 sec/batch)
2016-04-29 15:51:26.971221: step 1739, loss = 18.67 (11.7 examples/sec; 5.479 sec/batch)
2016-04-29 15:51:32.608331: step 1740, loss = 18.42 (11.4 examples/sec; 5.637 sec/batch)
2016-04-29 15:51:45.074151: step 1741, loss = 18.59 (12.3 examples/sec; 5.186 sec/batch)
2016-04-29 15:51:51.217687: step 1742, loss = 18.57 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 15:51:56.735655: step 1743, loss = 18.62 (11.6 examples/sec; 5.518 sec/batch)
2016-04-29 15:52:02.376789: step 1744, loss = 18.43 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 15:52:08.211598: step 1745, loss = 18.46 (11.0 examples/sec; 5.835 sec/batch)
2016-04-29 15:52:13.604781: step 1746, loss = 18.61 (11.9 examples/sec; 5.393 sec/batch)
2016-04-29 15:52:19.055753: step 1747, loss = 18.31 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 15:52:25.193143: step 1748, loss = 18.40 (10.4 examples/sec; 6.137 sec/batch)
2016-04-29 15:52:30.320676: step 1749, loss = 18.39 (12.5 examples/sec; 5.127 sec/batch)
2016-04-29 15:52:35.761482: step 1750, loss = 18.47 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 15:52:48.202253: step 1751, loss = 18.44 (12.3 examples/sec; 5.183 sec/batch)
2016-04-29 15:52:53.595949: step 1752, loss = 18.27 (11.9 examples/sec; 5.393 sec/batch)
2016-04-29 15:52:59.590985: step 1753, loss = 18.49 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 15:53:05.037943: step 1754, loss = 18.45 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 15:53:10.371232: step 1755, loss = 18.41 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 15:53:15.581723: step 1756, loss = 18.31 (12.3 examples/sec; 5.210 sec/batch)
2016-04-29 15:53:20.908429: step 1757, loss = 18.33 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 15:53:26.302687: step 1758, loss = 18.40 (11.9 examples/sec; 5.394 sec/batch)
2016-04-29 15:53:33.323907: step 1759, loss = 18.36 (9.1 examples/sec; 7.021 sec/batch)
2016-04-29 15:53:39.302265: step 1760, loss = 18.25 (10.7 examples/sec; 5.978 sec/batch)
2016-04-29 15:53:52.477488: step 1761, loss = 18.24 (11.6 examples/sec; 5.529 sec/batch)
2016-04-29 15:53:59.002100: step 1762, loss = 18.16 (9.8 examples/sec; 6.525 sec/batch)
2016-04-29 15:54:06.986867: step 1763, loss = 18.25 (8.0 examples/sec; 7.985 sec/batch)
2016-04-29 15:54:15.168337: step 1764, loss = 18.11 (7.8 examples/sec; 8.179 sec/batch)
2016-04-29 15:54:22.158938: step 1765, loss = 18.19 (9.2 examples/sec; 6.990 sec/batch)
2016-04-29 15:54:28.578212: step 1766, loss = 18.37 (10.0 examples/sec; 6.419 sec/batch)
2016-04-29 15:54:35.086457: step 1767, loss = 18.22 (9.8 examples/sec; 6.506 sec/batch)
2016-04-29 15:54:40.686747: step 1768, loss = 18.17 (11.4 examples/sec; 5.600 sec/batch)
2016-04-29 15:54:45.698730: step 1769, loss = 18.12 (12.8 examples/sec; 5.012 sec/batch)
2016-04-29 15:54:50.823020: step 1770, loss = 18.20 (12.5 examples/sec; 5.124 sec/batch)
2016-04-29 15:55:04.698428: step 1771, loss = 18.23 (10.3 examples/sec; 6.240 sec/batch)
2016-04-29 15:55:11.062154: step 1772, loss = 18.00 (10.1 examples/sec; 6.364 sec/batch)
2016-04-29 15:55:16.571703: step 1773, loss = 18.08 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 15:55:22.164706: step 1774, loss = 18.14 (11.4 examples/sec; 5.593 sec/batch)
2016-04-29 15:55:27.356545: step 1775, loss = 18.02 (12.3 examples/sec; 5.192 sec/batch)
2016-04-29 15:55:32.612788: step 1776, loss = 18.04 (12.2 examples/sec; 5.256 sec/batch)
2016-04-29 15:55:37.910621: step 1777, loss = 17.88 (12.1 examples/sec; 5.298 sec/batch)
2016-04-29 15:55:43.451393: step 1778, loss = 18.15 (11.6 examples/sec; 5.541 sec/batch)
2016-04-29 15:55:48.334235: step 1779, loss = 18.06 (13.1 examples/sec; 4.883 sec/batch)
2016-04-29 15:55:53.372739: step 1780, loss = 17.79 (12.7 examples/sec; 5.038 sec/batch)
2016-04-29 15:56:05.236922: step 1781, loss = 17.96 (12.7 examples/sec; 5.040 sec/batch)
2016-04-29 15:56:10.315758: step 1782, loss = 17.88 (12.6 examples/sec; 5.079 sec/batch)
2016-04-29 15:56:16.161890: step 1783, loss = 17.93 (10.9 examples/sec; 5.846 sec/batch)
2016-04-29 15:56:21.256764: step 1784, loss = 17.96 (12.6 examples/sec; 5.095 sec/batch)
2016-04-29 15:56:26.989596: step 1785, loss = 18.03 (11.2 examples/sec; 5.733 sec/batch)
2016-04-29 15:56:32.619954: step 1786, loss = 17.94 (11.4 examples/sec; 5.630 sec/batch)
2016-04-29 15:56:38.828738: step 1787, loss = 17.89 (10.3 examples/sec; 6.209 sec/batch)
2016-04-29 15:56:45.654604: step 1788, loss = 17.83 (9.4 examples/sec; 6.826 sec/batch)
2016-04-29 15:56:51.424135: step 1789, loss = 17.94 (11.1 examples/sec; 5.769 sec/batch)
2016-04-29 15:56:56.929919: step 1790, loss = 17.71 (11.6 examples/sec; 5.506 sec/batch)
2016-04-29 15:57:12.316533: step 1791, loss = 17.69 (9.2 examples/sec; 6.969 sec/batch)
2016-04-29 15:57:23.204478: step 1792, loss = 17.70 (5.9 examples/sec; 10.888 sec/batch)
2016-04-29 15:57:32.504726: step 1793, loss = 17.87 (6.9 examples/sec; 9.300 sec/batch)
2016-04-29 15:57:39.577818: step 1794, loss = 17.92 (9.0 examples/sec; 7.073 sec/batch)
2016-04-29 15:57:46.588695: step 1795, loss = 17.77 (9.1 examples/sec; 7.011 sec/batch)
2016-04-29 15:57:53.693812: step 1796, loss = 17.83 (9.0 examples/sec; 7.105 sec/batch)
2016-04-29 15:58:01.635006: step 1797, loss = 17.82 (8.1 examples/sec; 7.941 sec/batch)
2016-04-29 15:58:07.378950: step 1798, loss = 17.68 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 15:58:12.682888: step 1799, loss = 17.70 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 15:58:18.608339: step 1800, loss = 17.88 (10.8 examples/sec; 5.925 sec/batch)
2016-04-29 15:58:33.281850: step 1801, loss = 17.71 (11.1 examples/sec; 5.745 sec/batch)
2016-04-29 15:58:38.813775: step 1802, loss = 17.69 (11.6 examples/sec; 5.532 sec/batch)
2016-04-29 15:58:44.391778: step 1803, loss = 17.80 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 15:58:50.110216: step 1804, loss = 17.62 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 15:58:55.355339: step 1805, loss = 17.79 (12.2 examples/sec; 5.245 sec/batch)
2016-04-29 15:59:00.897778: step 1806, loss = 17.66 (11.5 examples/sec; 5.542 sec/batch)
2016-04-29 15:59:05.894360: step 1807, loss = 17.65 (12.8 examples/sec; 4.997 sec/batch)
2016-04-29 15:59:11.051109: step 1808, loss = 17.52 (12.4 examples/sec; 5.157 sec/batch)
2016-04-29 15:59:16.247915: step 1809, loss = 17.78 (12.3 examples/sec; 5.197 sec/batch)
2016-04-29 15:59:21.087583: step 1810, loss = 17.72 (13.2 examples/sec; 4.840 sec/batch)
2016-04-29 15:59:33.649292: step 1811, loss = 17.73 (11.8 examples/sec; 5.443 sec/batch)
2016-04-29 15:59:38.732618: step 1812, loss = 17.72 (12.6 examples/sec; 5.083 sec/batch)
2016-04-29 15:59:45.250988: step 1813, loss = 17.63 (9.8 examples/sec; 6.518 sec/batch)
2016-04-29 15:59:54.556050: step 1814, loss = 17.69 (6.9 examples/sec; 9.305 sec/batch)
2016-04-29 16:00:03.012913: step 1815, loss = 17.56 (7.6 examples/sec; 8.457 sec/batch)
2016-04-29 16:00:11.248483: step 1816, loss = 17.55 (7.8 examples/sec; 8.235 sec/batch)
2016-04-29 16:00:16.953863: step 1817, loss = 17.48 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 16:00:22.686312: step 1818, loss = 17.60 (11.2 examples/sec; 5.732 sec/batch)
2016-04-29 16:00:28.399540: step 1819, loss = 17.41 (11.2 examples/sec; 5.713 sec/batch)
2016-04-29 16:00:34.581968: step 1820, loss = 17.34 (10.4 examples/sec; 6.182 sec/batch)
2016-04-29 16:00:48.369723: step 1821, loss = 17.48 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 16:00:54.264343: step 1822, loss = 17.41 (10.9 examples/sec; 5.894 sec/batch)
2016-04-29 16:00:59.897051: step 1823, loss = 17.43 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 16:01:05.615643: step 1824, loss = 17.37 (11.2 examples/sec; 5.719 sec/batch)
2016-04-29 16:01:12.182684: step 1825, loss = 17.34 (9.7 examples/sec; 6.567 sec/batch)
2016-04-29 16:01:17.831367: step 1826, loss = 17.30 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 16:01:23.608485: step 1827, loss = 17.39 (11.1 examples/sec; 5.777 sec/batch)
2016-04-29 16:01:29.169915: step 1828, loss = 17.33 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 16:01:34.690452: step 1829, loss = 17.34 (11.6 examples/sec; 5.520 sec/batch)
2016-04-29 16:01:40.063416: step 1830, loss = 17.37 (11.9 examples/sec; 5.373 sec/batch)
2016-04-29 16:01:54.225481: step 1831, loss = 17.29 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 16:01:59.902150: step 1832, loss = 17.27 (11.3 examples/sec; 5.677 sec/batch)
2016-04-29 16:02:07.210238: step 1833, loss = 17.23 (8.8 examples/sec; 7.308 sec/batch)
2016-04-29 16:02:13.648331: step 1834, loss = 17.31 (9.9 examples/sec; 6.438 sec/batch)
2016-04-29 16:02:20.353394: step 1835, loss = 17.31 (9.5 examples/sec; 6.705 sec/batch)
2016-04-29 16:02:26.803738: step 1836, loss = 17.38 (9.9 examples/sec; 6.450 sec/batch)
2016-04-29 16:02:33.669241: step 1837, loss = 17.12 (9.3 examples/sec; 6.865 sec/batch)
2016-04-29 16:02:39.583374: step 1838, loss = 16.99 (10.8 examples/sec; 5.914 sec/batch)
2016-04-29 16:02:46.417582: step 1839, loss = 17.32 (9.4 examples/sec; 6.834 sec/batch)
2016-04-29 16:02:52.647953: step 1840, loss = 17.30 (10.3 examples/sec; 6.230 sec/batch)
2016-04-29 16:03:06.496600: step 1841, loss = 16.96 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 16:03:12.775810: step 1842, loss = 17.19 (10.2 examples/sec; 6.279 sec/batch)
2016-04-29 16:03:19.335094: step 1843, loss = 17.22 (9.8 examples/sec; 6.559 sec/batch)
2016-04-29 16:03:26.285640: step 1844, loss = 17.12 (9.2 examples/sec; 6.950 sec/batch)
2016-04-29 16:03:31.965332: step 1845, loss = 17.04 (11.3 examples/sec; 5.680 sec/batch)
2016-04-29 16:03:37.616457: step 1846, loss = 17.22 (11.3 examples/sec; 5.651 sec/batch)
2016-04-29 16:03:43.766413: step 1847, loss = 17.20 (10.4 examples/sec; 6.150 sec/batch)
2016-04-29 16:03:49.299318: step 1848, loss = 17.24 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 16:03:55.793983: step 1849, loss = 17.15 (9.9 examples/sec; 6.495 sec/batch)
2016-04-29 16:04:03.196934: step 1850, loss = 16.94 (8.6 examples/sec; 7.403 sec/batch)
2016-04-29 16:04:16.939980: step 1851, loss = 17.22 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 16:04:23.478405: step 1852, loss = 17.10 (9.8 examples/sec; 6.538 sec/batch)
2016-04-29 16:04:30.213096: step 1853, loss = 16.96 (9.5 examples/sec; 6.735 sec/batch)
2016-04-29 16:04:37.216503: step 1854, loss = 17.00 (9.1 examples/sec; 7.003 sec/batch)
2016-04-29 16:04:43.562641: step 1855, loss = 16.88 (10.1 examples/sec; 6.346 sec/batch)
2016-04-29 16:04:50.004463: step 1856, loss = 17.11 (9.9 examples/sec; 6.442 sec/batch)
2016-04-29 16:04:56.413066: step 1857, loss = 16.99 (10.0 examples/sec; 6.409 sec/batch)
2016-04-29 16:05:03.156849: step 1858, loss = 16.83 (9.5 examples/sec; 6.744 sec/batch)
2016-04-29 16:05:09.258088: step 1859, loss = 17.08 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 16:05:15.201841: step 1860, loss = 16.92 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 16:05:27.872600: step 1861, loss = 16.84 (12.5 examples/sec; 5.137 sec/batch)
2016-04-29 16:05:33.397533: step 1862, loss = 16.96 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 16:05:39.766603: step 1863, loss = 16.81 (10.0 examples/sec; 6.369 sec/batch)
2016-04-29 16:05:45.188607: step 1864, loss = 16.72 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 16:05:50.697277: step 1865, loss = 16.91 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 16:05:57.002957: step 1866, loss = 17.13 (10.1 examples/sec; 6.306 sec/batch)
2016-04-29 16:06:03.499074: step 1867, loss = 16.87 (9.9 examples/sec; 6.496 sec/batch)
2016-04-29 16:06:09.826056: step 1868, loss = 16.68 (10.1 examples/sec; 6.327 sec/batch)
2016-04-29 16:06:16.489644: step 1869, loss = 16.77 (9.6 examples/sec; 6.664 sec/batch)
2016-04-29 16:06:22.848035: step 1870, loss = 16.98 (10.1 examples/sec; 6.358 sec/batch)
2016-04-29 16:06:36.527826: step 1871, loss = 16.77 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 16:06:42.383098: step 1872, loss = 16.83 (10.9 examples/sec; 5.855 sec/batch)
2016-04-29 16:06:48.586380: step 1873, loss = 16.65 (10.3 examples/sec; 6.203 sec/batch)
2016-04-29 16:06:54.027112: step 1874, loss = 16.88 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 16:06:59.768078: step 1875, loss = 16.89 (11.1 examples/sec; 5.741 sec/batch)
2016-04-29 16:07:05.397770: step 1876, loss = 16.91 (11.4 examples/sec; 5.630 sec/batch)
2016-04-29 16:07:10.805296: step 1877, loss = 16.70 (11.8 examples/sec; 5.407 sec/batch)
2016-04-29 16:07:16.227158: step 1878, loss = 16.50 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 16:07:24.404463: step 1879, loss = 16.80 (7.8 examples/sec; 8.177 sec/batch)
2016-04-29 16:07:31.761914: step 1880, loss = 16.73 (8.7 examples/sec; 7.357 sec/batch)
2016-04-29 16:07:46.125434: step 1881, loss = 17.06 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 16:07:51.365787: step 1882, loss = 16.61 (12.2 examples/sec; 5.240 sec/batch)
2016-04-29 16:07:57.508434: step 1883, loss = 16.73 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 16:08:02.798168: step 1884, loss = 16.67 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 16:08:07.845291: step 1885, loss = 16.76 (12.7 examples/sec; 5.047 sec/batch)
2016-04-29 16:08:12.721049: step 1886, loss = 16.72 (13.1 examples/sec; 4.876 sec/batch)
2016-04-29 16:08:17.891466: step 1887, loss = 16.58 (12.4 examples/sec; 5.170 sec/batch)
2016-04-29 16:08:23.234837: step 1888, loss = 16.42 (12.0 examples/sec; 5.343 sec/batch)
2016-04-29 16:08:28.995536: step 1889, loss = 16.51 (11.1 examples/sec; 5.761 sec/batch)
2016-04-29 16:08:33.805783: step 1890, loss = 16.52 (13.3 examples/sec; 4.810 sec/batch)
2016-04-29 16:08:45.488063: step 1891, loss = 16.69 (13.5 examples/sec; 4.729 sec/batch)
2016-04-29 16:08:50.678932: step 1892, loss = 16.56 (12.3 examples/sec; 5.191 sec/batch)
2016-04-29 16:08:55.727509: step 1893, loss = 16.46 (12.7 examples/sec; 5.048 sec/batch)
2016-04-29 16:09:01.176448: step 1894, loss = 16.51 (11.7 examples/sec; 5.449 sec/batch)
2016-04-29 16:09:06.182221: step 1895, loss = 16.39 (12.8 examples/sec; 5.006 sec/batch)
2016-04-29 16:09:11.222674: step 1896, loss = 16.39 (12.7 examples/sec; 5.040 sec/batch)
2016-04-29 16:09:16.206184: step 1897, loss = 16.67 (12.8 examples/sec; 4.983 sec/batch)
2016-04-29 16:09:21.154360: step 1898, loss = 16.49 (12.9 examples/sec; 4.948 sec/batch)
2016-04-29 16:09:26.144491: step 1899, loss = 16.37 (12.8 examples/sec; 4.990 sec/batch)
2016-04-29 16:09:31.273441: step 1900, loss = 16.48 (12.5 examples/sec; 5.129 sec/batch)
2016-04-29 16:09:43.642919: step 1901, loss = 16.44 (12.7 examples/sec; 5.033 sec/batch)
2016-04-29 16:09:48.598986: step 1902, loss = 16.51 (12.9 examples/sec; 4.956 sec/batch)
2016-04-29 16:09:53.692442: step 1903, loss = 16.40 (12.6 examples/sec; 5.093 sec/batch)
2016-04-29 16:09:58.791551: step 1904, loss = 16.42 (12.6 examples/sec; 5.099 sec/batch)
2016-04-29 16:10:04.006473: step 1905, loss = 16.52 (12.3 examples/sec; 5.215 sec/batch)
2016-04-29 16:10:09.947550: step 1906, loss = 16.55 (10.8 examples/sec; 5.941 sec/batch)
2016-04-29 16:10:14.932467: step 1907, loss = 16.45 (12.8 examples/sec; 4.985 sec/batch)
2016-04-29 16:10:19.978888: step 1908, loss = 16.22 (12.7 examples/sec; 5.046 sec/batch)
2016-04-29 16:10:25.029865: step 1909, loss = 16.32 (12.7 examples/sec; 5.051 sec/batch)
2016-04-29 16:10:30.034594: step 1910, loss = 16.18 (12.8 examples/sec; 5.005 sec/batch)
2016-04-29 16:10:42.552388: step 1911, loss = 16.24 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 16:10:47.599638: step 1912, loss = 16.50 (12.7 examples/sec; 5.047 sec/batch)
2016-04-29 16:10:52.782748: step 1913, loss = 16.22 (12.3 examples/sec; 5.183 sec/batch)
2016-04-29 16:10:57.779784: step 1914, loss = 16.33 (12.8 examples/sec; 4.997 sec/batch)
2016-04-29 16:11:02.862594: step 1915, loss = 16.38 (12.6 examples/sec; 5.083 sec/batch)
2016-04-29 16:11:08.075316: step 1916, loss = 16.32 (12.3 examples/sec; 5.213 sec/batch)
2016-04-29 16:11:13.888097: step 1917, loss = 16.32 (11.0 examples/sec; 5.813 sec/batch)
2016-04-29 16:11:18.756473: step 1918, loss = 16.26 (13.1 examples/sec; 4.868 sec/batch)
2016-04-29 16:11:23.861091: step 1919, loss = 16.15 (12.5 examples/sec; 5.105 sec/batch)
2016-04-29 16:11:29.135783: step 1920, loss = 16.36 (12.1 examples/sec; 5.275 sec/batch)
2016-04-29 16:11:41.075056: step 1921, loss = 16.10 (12.6 examples/sec; 5.083 sec/batch)
2016-04-29 16:11:46.664866: step 1922, loss = 16.34 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 16:11:51.572949: step 1923, loss = 16.26 (13.0 examples/sec; 4.908 sec/batch)
2016-04-29 16:11:56.694005: step 1924, loss = 16.15 (12.5 examples/sec; 5.121 sec/batch)
2016-04-29 16:12:01.770166: step 1925, loss = 16.13 (12.6 examples/sec; 5.076 sec/batch)
2016-04-29 16:12:06.636346: step 1926, loss = 16.04 (13.2 examples/sec; 4.866 sec/batch)
2016-04-29 16:12:11.768627: step 1927, loss = 16.16 (12.5 examples/sec; 5.132 sec/batch)
2016-04-29 16:12:17.312658: step 1928, loss = 16.16 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 16:12:22.418183: step 1929, loss = 16.15 (12.5 examples/sec; 5.105 sec/batch)
2016-04-29 16:12:27.455417: step 1930, loss = 16.10 (12.7 examples/sec; 5.037 sec/batch)
2016-04-29 16:12:39.259040: step 1931, loss = 16.11 (13.3 examples/sec; 4.816 sec/batch)
2016-04-29 16:12:47.081780: step 1932, loss = 16.07 (8.2 examples/sec; 7.823 sec/batch)
2016-04-29 16:12:56.154387: step 1933, loss = 16.14 (7.1 examples/sec; 9.073 sec/batch)
2016-04-29 16:13:03.395412: step 1934, loss = 16.00 (8.8 examples/sec; 7.241 sec/batch)
2016-04-29 16:13:09.224768: step 1935, loss = 16.22 (11.0 examples/sec; 5.829 sec/batch)
2016-04-29 16:13:14.926972: step 1936, loss = 15.97 (11.2 examples/sec; 5.702 sec/batch)
2016-04-29 16:13:20.758380: step 1937, loss = 16.23 (11.0 examples/sec; 5.831 sec/batch)
2016-04-29 16:13:26.988588: step 1938, loss = 16.11 (10.3 examples/sec; 6.230 sec/batch)
2016-04-29 16:13:32.455391: step 1939, loss = 15.99 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 16:13:38.093435: step 1940, loss = 16.03 (11.4 examples/sec; 5.638 sec/batch)
2016-04-29 16:13:50.851681: step 1941, loss = 15.83 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 16:13:56.632411: step 1942, loss = 16.01 (11.1 examples/sec; 5.781 sec/batch)
2016-04-29 16:14:02.748084: step 1943, loss = 16.01 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 16:14:08.499403: step 1944, loss = 16.01 (11.1 examples/sec; 5.751 sec/batch)
2016-04-29 16:14:14.013361: step 1945, loss = 16.09 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 16:14:19.553599: step 1946, loss = 15.74 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 16:14:25.022420: step 1947, loss = 15.93 (11.7 examples/sec; 5.469 sec/batch)
2016-04-29 16:14:30.661758: step 1948, loss = 15.67 (11.3 examples/sec; 5.639 sec/batch)
2016-04-29 16:14:36.792464: step 1949, loss = 15.92 (10.4 examples/sec; 6.131 sec/batch)
2016-04-29 16:14:42.162867: step 1950, loss = 15.89 (11.9 examples/sec; 5.370 sec/batch)
2016-04-29 16:14:54.896650: step 1951, loss = 16.02 (12.5 examples/sec; 5.125 sec/batch)
2016-04-29 16:15:00.400412: step 1952, loss = 15.92 (11.6 examples/sec; 5.504 sec/batch)
2016-04-29 16:15:06.709473: step 1953, loss = 15.79 (10.1 examples/sec; 6.309 sec/batch)
2016-04-29 16:15:12.265269: step 1954, loss = 15.78 (11.5 examples/sec; 5.556 sec/batch)
2016-04-29 16:15:17.914884: step 1955, loss = 15.93 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 16:15:23.531532: step 1956, loss = 15.70 (11.4 examples/sec; 5.617 sec/batch)
2016-04-29 16:15:29.155076: step 1957, loss = 15.90 (11.4 examples/sec; 5.623 sec/batch)
2016-04-29 16:15:34.627809: step 1958, loss = 15.80 (11.7 examples/sec; 5.473 sec/batch)
2016-04-29 16:15:40.718240: step 1959, loss = 15.85 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 16:15:46.269980: step 1960, loss = 15.90 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 16:15:59.060456: step 1961, loss = 15.74 (11.9 examples/sec; 5.384 sec/batch)
2016-04-29 16:16:04.785937: step 1962, loss = 15.66 (11.2 examples/sec; 5.725 sec/batch)
2016-04-29 16:16:10.020198: step 1963, loss = 15.91 (12.2 examples/sec; 5.234 sec/batch)
2016-04-29 16:16:16.551967: step 1964, loss = 15.74 (9.8 examples/sec; 6.532 sec/batch)
2016-04-29 16:16:21.855842: step 1965, loss = 15.50 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 16:16:27.497622: step 1966, loss = 15.67 (11.3 examples/sec; 5.642 sec/batch)
2016-04-29 16:16:32.968127: step 1967, loss = 15.76 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 16:16:38.524334: step 1968, loss = 15.51 (11.5 examples/sec; 5.556 sec/batch)
2016-04-29 16:16:44.539150: step 1969, loss = 15.53 (10.6 examples/sec; 6.015 sec/batch)
2016-04-29 16:16:50.309929: step 1970, loss = 15.58 (11.1 examples/sec; 5.771 sec/batch)
2016-04-29 16:17:03.199612: step 1971, loss = 15.85 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 16:17:09.029389: step 1972, loss = 15.70 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 16:17:14.548199: step 1973, loss = 15.54 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 16:17:20.704975: step 1974, loss = 15.61 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 16:17:26.297120: step 1975, loss = 15.44 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 16:17:31.843684: step 1976, loss = 15.63 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 16:17:37.094260: step 1977, loss = 15.56 (12.2 examples/sec; 5.250 sec/batch)
2016-04-29 16:17:42.443853: step 1978, loss = 15.48 (12.0 examples/sec; 5.350 sec/batch)
2016-04-29 16:17:48.125810: step 1979, loss = 15.56 (11.3 examples/sec; 5.682 sec/batch)
2016-04-29 16:17:54.167342: step 1980, loss = 15.59 (10.6 examples/sec; 6.041 sec/batch)
2016-04-29 16:18:06.854943: step 1981, loss = 15.68 (12.7 examples/sec; 5.050 sec/batch)
2016-04-29 16:18:12.455081: step 1982, loss = 15.54 (11.4 examples/sec; 5.600 sec/batch)
2016-04-29 16:18:18.165808: step 1983, loss = 15.31 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 16:18:24.435107: step 1984, loss = 15.36 (10.2 examples/sec; 6.269 sec/batch)
2016-04-29 16:18:30.058183: step 1985, loss = 15.54 (11.4 examples/sec; 5.623 sec/batch)
2016-04-29 16:18:35.732880: step 1986, loss = 15.42 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 16:18:41.262034: step 1987, loss = 15.42 (11.6 examples/sec; 5.529 sec/batch)
2016-04-29 16:18:46.927276: step 1988, loss = 15.45 (11.3 examples/sec; 5.665 sec/batch)
2016-04-29 16:18:52.256002: step 1989, loss = 15.30 (12.0 examples/sec; 5.329 sec/batch)
2016-04-29 16:18:58.579048: step 1990, loss = 15.34 (10.1 examples/sec; 6.323 sec/batch)
2016-04-29 16:19:11.541057: step 1991, loss = 15.46 (11.4 examples/sec; 5.618 sec/batch)
2016-04-29 16:19:16.882078: step 1992, loss = 15.41 (12.0 examples/sec; 5.341 sec/batch)
2016-04-29 16:19:22.193682: step 1993, loss = 15.40 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 16:19:27.615529: step 1994, loss = 15.41 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 16:19:33.646303: step 1995, loss = 15.42 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 16:19:39.310544: step 1996, loss = 15.45 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 16:19:44.692924: step 1997, loss = 15.33 (11.9 examples/sec; 5.382 sec/batch)
2016-04-29 16:19:50.296930: step 1998, loss = 15.37 (11.4 examples/sec; 5.604 sec/batch)
2016-04-29 16:19:55.865140: step 1999, loss = 15.19 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 16:20:01.209986: step 2000, loss = 15.42 (12.0 examples/sec; 5.345 sec/batch)
2016-04-29 16:20:14.695349: step 2001, loss = 15.07 (12.0 examples/sec; 5.349 sec/batch)
2016-04-29 16:20:20.280457: step 2002, loss = 15.33 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 16:20:25.795847: step 2003, loss = 15.28 (11.6 examples/sec; 5.515 sec/batch)
2016-04-29 16:20:31.046558: step 2004, loss = 15.27 (12.2 examples/sec; 5.251 sec/batch)
2016-04-29 16:20:37.048153: step 2005, loss = 15.30 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 16:20:42.660947: step 2006, loss = 15.20 (11.4 examples/sec; 5.613 sec/batch)
2016-04-29 16:20:48.253038: step 2007, loss = 15.33 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 16:20:53.852266: step 2008, loss = 14.98 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 16:20:59.460670: step 2009, loss = 15.10 (11.4 examples/sec; 5.608 sec/batch)
2016-04-29 16:21:05.308019: step 2010, loss = 15.17 (10.9 examples/sec; 5.847 sec/batch)
2016-04-29 16:21:18.626376: step 2011, loss = 15.23 (12.1 examples/sec; 5.304 sec/batch)
2016-04-29 16:21:24.389734: step 2012, loss = 15.12 (11.1 examples/sec; 5.763 sec/batch)
2016-04-29 16:21:29.816813: step 2013, loss = 15.27 (11.8 examples/sec; 5.427 sec/batch)
2016-04-29 16:21:35.474572: step 2014, loss = 15.21 (11.3 examples/sec; 5.658 sec/batch)
2016-04-29 16:21:40.948855: step 2015, loss = 15.31 (11.7 examples/sec; 5.474 sec/batch)
2016-04-29 16:21:46.911452: step 2016, loss = 15.08 (10.7 examples/sec; 5.963 sec/batch)
2016-04-29 16:21:52.237940: step 2017, loss = 15.25 (12.0 examples/sec; 5.326 sec/batch)
2016-04-29 16:21:57.846999: step 2018, loss = 14.95 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 16:22:03.411061: step 2019, loss = 15.20 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 16:22:09.139974: step 2020, loss = 14.84 (11.2 examples/sec; 5.729 sec/batch)
2016-04-29 16:22:22.602861: step 2021, loss = 15.00 (12.0 examples/sec; 5.343 sec/batch)
2016-04-29 16:22:28.076996: step 2022, loss = 15.06 (11.7 examples/sec; 5.474 sec/batch)
2016-04-29 16:22:33.560589: step 2023, loss = 15.01 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 16:22:39.191999: step 2024, loss = 14.99 (11.4 examples/sec; 5.631 sec/batch)
2016-04-29 16:22:44.932486: step 2025, loss = 15.01 (11.1 examples/sec; 5.740 sec/batch)
2016-04-29 16:22:51.085212: step 2026, loss = 15.04 (10.4 examples/sec; 6.153 sec/batch)
2016-04-29 16:22:56.664445: step 2027, loss = 15.09 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 16:23:02.406875: step 2028, loss = 15.10 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 16:23:08.031312: step 2029, loss = 14.93 (11.4 examples/sec; 5.624 sec/batch)
2016-04-29 16:23:13.379579: step 2030, loss = 14.99 (12.0 examples/sec; 5.348 sec/batch)
2016-04-29 16:23:26.864469: step 2031, loss = 14.90 (11.8 examples/sec; 5.418 sec/batch)
2016-04-29 16:23:32.399233: step 2032, loss = 14.90 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 16:23:37.887233: step 2033, loss = 14.96 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 16:23:43.188536: step 2034, loss = 14.81 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 16:23:48.623340: step 2035, loss = 14.87 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 16:23:54.734679: step 2036, loss = 14.86 (10.5 examples/sec; 6.111 sec/batch)
2016-04-29 16:24:00.235581: step 2037, loss = 15.00 (11.6 examples/sec; 5.501 sec/batch)
2016-04-29 16:24:05.864344: step 2038, loss = 14.92 (11.4 examples/sec; 5.629 sec/batch)
2016-04-29 16:24:12.546437: step 2039, loss = 14.94 (9.6 examples/sec; 6.682 sec/batch)
2016-04-29 16:24:18.712078: step 2040, loss = 14.69 (10.4 examples/sec; 6.166 sec/batch)
2016-04-29 16:24:32.645183: step 2041, loss = 14.87 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 16:24:38.320859: step 2042, loss = 14.97 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 16:24:43.968444: step 2043, loss = 14.65 (11.3 examples/sec; 5.647 sec/batch)
2016-04-29 16:24:49.374060: step 2044, loss = 14.94 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 16:24:54.795656: step 2045, loss = 14.70 (11.8 examples/sec; 5.422 sec/batch)
2016-04-29 16:25:01.305846: step 2046, loss = 14.95 (9.8 examples/sec; 6.510 sec/batch)
2016-04-29 16:25:06.569993: step 2047, loss = 14.90 (12.2 examples/sec; 5.264 sec/batch)
2016-04-29 16:25:12.210895: step 2048, loss = 14.71 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 16:25:17.947745: step 2049, loss = 14.81 (11.2 examples/sec; 5.737 sec/batch)
2016-04-29 16:25:23.601229: step 2050, loss = 14.51 (11.3 examples/sec; 5.653 sec/batch)
2016-04-29 16:25:36.874421: step 2051, loss = 14.75 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 16:25:42.311353: step 2052, loss = 14.81 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 16:25:48.032429: step 2053, loss = 14.68 (11.2 examples/sec; 5.721 sec/batch)
2016-04-29 16:25:53.693041: step 2054, loss = 14.71 (11.3 examples/sec; 5.661 sec/batch)
2016-04-29 16:25:59.220619: step 2055, loss = 14.67 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 16:26:05.054593: step 2056, loss = 14.41 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 16:26:11.100473: step 2057, loss = 14.75 (10.6 examples/sec; 6.046 sec/batch)
2016-04-29 16:26:16.412854: step 2058, loss = 14.84 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 16:26:21.752714: step 2059, loss = 14.70 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 16:26:27.202969: step 2060, loss = 14.65 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 16:26:40.811571: step 2061, loss = 14.66 (11.0 examples/sec; 5.813 sec/batch)
2016-04-29 16:26:46.224585: step 2062, loss = 14.66 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 16:26:51.612111: step 2063, loss = 14.67 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 16:26:56.997794: step 2064, loss = 14.57 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 16:27:02.537662: step 2065, loss = 14.63 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 16:27:08.184254: step 2066, loss = 14.75 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 16:27:14.348128: step 2067, loss = 14.54 (10.4 examples/sec; 6.164 sec/batch)
2016-04-29 16:27:19.913137: step 2068, loss = 14.80 (11.5 examples/sec; 5.565 sec/batch)
2016-04-29 16:27:25.119123: step 2069, loss = 14.65 (12.3 examples/sec; 5.206 sec/batch)
2016-04-29 16:27:30.626659: step 2070, loss = 14.47 (11.6 examples/sec; 5.507 sec/batch)
2016-04-29 16:27:44.011705: step 2071, loss = 14.45 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 16:27:50.054435: step 2072, loss = 14.54 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 16:27:55.502877: step 2073, loss = 14.37 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 16:28:00.953375: step 2074, loss = 14.62 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 16:28:06.790479: step 2075, loss = 14.59 (11.0 examples/sec; 5.837 sec/batch)
2016-04-29 16:28:12.157258: step 2076, loss = 14.41 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 16:28:18.324092: step 2077, loss = 14.35 (10.4 examples/sec; 6.167 sec/batch)
2016-04-29 16:28:23.791915: step 2078, loss = 14.60 (11.7 examples/sec; 5.468 sec/batch)
2016-04-29 16:28:29.483265: step 2079, loss = 14.49 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 16:28:35.129561: step 2080, loss = 14.57 (11.3 examples/sec; 5.646 sec/batch)
2016-04-29 16:28:47.801894: step 2081, loss = 14.49 (11.6 examples/sec; 5.497 sec/batch)
2016-04-29 16:28:54.114187: step 2082, loss = 14.26 (10.1 examples/sec; 6.312 sec/batch)
2016-04-29 16:28:59.774138: step 2083, loss = 14.40 (11.3 examples/sec; 5.660 sec/batch)
2016-04-29 16:29:05.493385: step 2084, loss = 14.59 (11.2 examples/sec; 5.719 sec/batch)
2016-04-29 16:29:11.087796: step 2085, loss = 14.44 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 16:29:16.419153: step 2086, loss = 14.60 (12.0 examples/sec; 5.331 sec/batch)
2016-04-29 16:29:21.943765: step 2087, loss = 14.41 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 16:29:28.167561: step 2088, loss = 14.49 (10.3 examples/sec; 6.224 sec/batch)
2016-04-29 16:29:33.534165: step 2089, loss = 14.45 (11.9 examples/sec; 5.367 sec/batch)
2016-04-29 16:29:39.150070: step 2090, loss = 14.29 (11.4 examples/sec; 5.616 sec/batch)
2016-04-29 16:29:51.820015: step 2091, loss = 14.23 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 16:29:57.975953: step 2092, loss = 14.20 (10.4 examples/sec; 6.156 sec/batch)
2016-04-29 16:30:03.452438: step 2093, loss = 14.37 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 16:30:08.961137: step 2094, loss = 14.29 (11.6 examples/sec; 5.509 sec/batch)
2016-04-29 16:30:14.498796: step 2095, loss = 14.31 (11.6 examples/sec; 5.538 sec/batch)
2016-04-29 16:30:20.139507: step 2096, loss = 14.32 (11.3 examples/sec; 5.641 sec/batch)
2016-04-29 16:30:25.861104: step 2097, loss = 14.24 (11.2 examples/sec; 5.721 sec/batch)
2016-04-29 16:30:32.107022: step 2098, loss = 14.20 (10.2 examples/sec; 6.246 sec/batch)
2016-04-29 16:30:37.522750: step 2099, loss = 14.22 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 16:30:42.951029: step 2100, loss = 14.15 (11.8 examples/sec; 5.427 sec/batch)
2016-04-29 16:30:55.958775: step 2101, loss = 14.28 (11.7 examples/sec; 5.452 sec/batch)
2016-04-29 16:31:01.564225: step 2102, loss = 14.17 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 16:31:07.602373: step 2103, loss = 14.25 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 16:31:12.959803: step 2104, loss = 13.93 (11.9 examples/sec; 5.357 sec/batch)
2016-04-29 16:31:18.322591: step 2105, loss = 14.17 (11.9 examples/sec; 5.363 sec/batch)
2016-04-29 16:31:23.912855: step 2106, loss = 14.34 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 16:31:29.327007: step 2107, loss = 14.19 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 16:31:35.180252: step 2108, loss = 14.20 (10.9 examples/sec; 5.853 sec/batch)
2016-04-29 16:31:41.171562: step 2109, loss = 14.04 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 16:31:46.466977: step 2110, loss = 14.01 (12.1 examples/sec; 5.295 sec/batch)
2016-04-29 16:31:59.410663: step 2111, loss = 13.98 (11.8 examples/sec; 5.424 sec/batch)
2016-04-29 16:32:05.061083: step 2112, loss = 14.14 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 16:32:11.046907: step 2113, loss = 14.08 (10.7 examples/sec; 5.986 sec/batch)
2016-04-29 16:32:16.450355: step 2114, loss = 14.08 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 16:32:21.678622: step 2115, loss = 14.12 (12.2 examples/sec; 5.228 sec/batch)
2016-04-29 16:32:27.421833: step 2116, loss = 14.01 (11.1 examples/sec; 5.743 sec/batch)
2016-04-29 16:32:33.008163: step 2117, loss = 14.03 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 16:32:38.714463: step 2118, loss = 13.86 (11.2 examples/sec; 5.706 sec/batch)
2016-04-29 16:32:44.983887: step 2119, loss = 14.20 (10.2 examples/sec; 6.269 sec/batch)
2016-04-29 16:32:50.656431: step 2120, loss = 14.09 (11.3 examples/sec; 5.672 sec/batch)
2016-04-29 16:33:03.514224: step 2121, loss = 13.94 (11.6 examples/sec; 5.537 sec/batch)
2016-04-29 16:33:08.951649: step 2122, loss = 14.00 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 16:33:14.992483: step 2123, loss = 14.06 (10.6 examples/sec; 6.041 sec/batch)
2016-04-29 16:33:20.440222: step 2124, loss = 13.85 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 16:33:25.986703: step 2125, loss = 14.02 (11.5 examples/sec; 5.546 sec/batch)
2016-04-29 16:33:31.264426: step 2126, loss = 14.12 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 16:33:36.616604: step 2127, loss = 13.93 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 16:33:42.053666: step 2128, loss = 14.08 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 16:33:48.271898: step 2129, loss = 13.94 (10.3 examples/sec; 6.218 sec/batch)
2016-04-29 16:33:53.842861: step 2130, loss = 13.87 (11.5 examples/sec; 5.571 sec/batch)
2016-04-29 16:34:06.718354: step 2131, loss = 13.77 (11.6 examples/sec; 5.521 sec/batch)
2016-04-29 16:34:12.155329: step 2132, loss = 13.82 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 16:34:17.506274: step 2133, loss = 13.86 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 16:34:23.584921: step 2134, loss = 14.03 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 16:34:29.328830: step 2135, loss = 13.84 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 16:34:34.728357: step 2136, loss = 13.76 (11.9 examples/sec; 5.399 sec/batch)
2016-04-29 16:34:40.002979: step 2137, loss = 13.98 (12.1 examples/sec; 5.275 sec/batch)
2016-04-29 16:34:45.619807: step 2138, loss = 13.73 (11.4 examples/sec; 5.617 sec/batch)
2016-04-29 16:34:51.148636: step 2139, loss = 13.63 (11.6 examples/sec; 5.529 sec/batch)
2016-04-29 16:34:57.187280: step 2140, loss = 13.81 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 16:35:09.924402: step 2141, loss = 13.70 (12.6 examples/sec; 5.091 sec/batch)
2016-04-29 16:35:15.795195: step 2142, loss = 13.88 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 16:35:21.575544: step 2143, loss = 13.81 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 16:35:27.815859: step 2144, loss = 13.92 (10.3 examples/sec; 6.240 sec/batch)
2016-04-29 16:35:33.181418: step 2145, loss = 13.73 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 16:35:38.696706: step 2146, loss = 13.61 (11.6 examples/sec; 5.515 sec/batch)
2016-04-29 16:35:44.476290: step 2147, loss = 13.83 (11.1 examples/sec; 5.779 sec/batch)
2016-04-29 16:35:49.828135: step 2148, loss = 13.86 (12.0 examples/sec; 5.352 sec/batch)
2016-04-29 16:35:55.210869: step 2149, loss = 13.84 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 16:36:01.975396: step 2150, loss = 13.67 (9.5 examples/sec; 6.764 sec/batch)
2016-04-29 16:36:15.486330: step 2151, loss = 13.74 (11.5 examples/sec; 5.551 sec/batch)
2016-04-29 16:36:21.167163: step 2152, loss = 13.74 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 16:36:26.949097: step 2153, loss = 13.64 (11.1 examples/sec; 5.782 sec/batch)
2016-04-29 16:36:32.948379: step 2154, loss = 13.57 (10.7 examples/sec; 5.999 sec/batch)
2016-04-29 16:36:39.035295: step 2155, loss = 13.69 (10.5 examples/sec; 6.087 sec/batch)
2016-04-29 16:36:44.527372: step 2156, loss = 13.82 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 16:36:50.040427: step 2157, loss = 13.65 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 16:36:55.329985: step 2158, loss = 13.62 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 16:37:00.966122: step 2159, loss = 13.72 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 16:37:07.056954: step 2160, loss = 13.64 (10.5 examples/sec; 6.091 sec/batch)
2016-04-29 16:37:19.706113: step 2161, loss = 13.65 (12.4 examples/sec; 5.166 sec/batch)
2016-04-29 16:37:25.713429: step 2162, loss = 13.56 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 16:37:31.072223: step 2163, loss = 13.93 (11.9 examples/sec; 5.359 sec/batch)
2016-04-29 16:37:36.664468: step 2164, loss = 13.58 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 16:37:42.910294: step 2165, loss = 13.62 (10.2 examples/sec; 6.246 sec/batch)
2016-04-29 16:37:48.504471: step 2166, loss = 13.76 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 16:37:54.024030: step 2167, loss = 13.67 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 16:37:59.692377: step 2168, loss = 13.61 (11.3 examples/sec; 5.668 sec/batch)
2016-04-29 16:38:05.216548: step 2169, loss = 13.67 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 16:38:10.849448: step 2170, loss = 13.67 (11.4 examples/sec; 5.633 sec/batch)
2016-04-29 16:38:24.631750: step 2171, loss = 13.54 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 16:38:30.099277: step 2172, loss = 13.40 (11.7 examples/sec; 5.467 sec/batch)
2016-04-29 16:38:35.717789: step 2173, loss = 13.48 (11.4 examples/sec; 5.618 sec/batch)
2016-04-29 16:38:41.043004: step 2174, loss = 13.56 (12.0 examples/sec; 5.325 sec/batch)
2016-04-29 16:38:47.194154: step 2175, loss = 13.37 (10.4 examples/sec; 6.151 sec/batch)
2016-04-29 16:38:52.557217: step 2176, loss = 13.53 (11.9 examples/sec; 5.363 sec/batch)
2016-04-29 16:38:57.976462: step 2177, loss = 13.43 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 16:39:04.072458: step 2178, loss = 13.40 (10.5 examples/sec; 6.096 sec/batch)
2016-04-29 16:39:09.447015: step 2179, loss = 13.40 (11.9 examples/sec; 5.374 sec/batch)
2016-04-29 16:39:14.910725: step 2180, loss = 13.40 (11.7 examples/sec; 5.464 sec/batch)
2016-04-29 16:39:28.158841: step 2181, loss = 13.38 (12.1 examples/sec; 5.281 sec/batch)
2016-04-29 16:39:33.524458: step 2182, loss = 13.59 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 16:39:39.077341: step 2183, loss = 13.32 (11.5 examples/sec; 5.553 sec/batch)
2016-04-29 16:39:44.779363: step 2184, loss = 13.36 (11.2 examples/sec; 5.702 sec/batch)
2016-04-29 16:39:50.553321: step 2185, loss = 13.38 (11.1 examples/sec; 5.774 sec/batch)
2016-04-29 16:39:56.499822: step 2186, loss = 13.32 (10.8 examples/sec; 5.946 sec/batch)
2016-04-29 16:40:02.381692: step 2187, loss = 13.30 (10.9 examples/sec; 5.882 sec/batch)
2016-04-29 16:40:07.891536: step 2188, loss = 13.32 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 16:40:13.351482: step 2189, loss = 13.41 (11.7 examples/sec; 5.460 sec/batch)
2016-04-29 16:40:18.949944: step 2190, loss = 13.46 (11.4 examples/sec; 5.598 sec/batch)
2016-04-29 16:40:32.380902: step 2191, loss = 13.37 (12.0 examples/sec; 5.328 sec/batch)
2016-04-29 16:40:37.809578: step 2192, loss = 13.25 (11.8 examples/sec; 5.429 sec/batch)
2016-04-29 16:40:43.092092: step 2193, loss = 13.42 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 16:40:48.806377: step 2194, loss = 13.32 (11.2 examples/sec; 5.714 sec/batch)
2016-04-29 16:40:54.246771: step 2195, loss = 13.37 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 16:41:00.651908: step 2196, loss = 13.28 (10.0 examples/sec; 6.405 sec/batch)
2016-04-29 16:41:06.237580: step 2197, loss = 13.40 (11.5 examples/sec; 5.584 sec/batch)
2016-04-29 16:41:11.724741: step 2198, loss = 13.16 (11.7 examples/sec; 5.487 sec/batch)
2016-04-29 16:41:17.181286: step 2199, loss = 13.29 (11.7 examples/sec; 5.456 sec/batch)
2016-04-29 16:41:22.674578: step 2200, loss = 13.34 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 16:41:36.337155: step 2201, loss = 13.31 (11.8 examples/sec; 5.408 sec/batch)
2016-04-29 16:41:41.884176: step 2202, loss = 13.37 (11.5 examples/sec; 5.547 sec/batch)
2016-04-29 16:41:47.504163: step 2203, loss = 13.27 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 16:41:52.946240: step 2204, loss = 13.09 (11.8 examples/sec; 5.442 sec/batch)
2016-04-29 16:41:58.374229: step 2205, loss = 13.34 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 16:42:04.711438: step 2206, loss = 13.31 (10.1 examples/sec; 6.337 sec/batch)
2016-04-29 16:42:10.097771: step 2207, loss = 13.09 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 16:42:15.796816: step 2208, loss = 13.25 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 16:42:21.683748: step 2209, loss = 13.08 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 16:42:27.249724: step 2210, loss = 13.38 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 16:42:40.802890: step 2211, loss = 12.97 (10.2 examples/sec; 6.244 sec/batch)
2016-04-29 16:42:46.259581: step 2212, loss = 13.03 (11.7 examples/sec; 5.457 sec/batch)
2016-04-29 16:42:51.823413: step 2213, loss = 13.02 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 16:42:57.420890: step 2214, loss = 13.21 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 16:43:03.385394: step 2215, loss = 13.30 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 16:43:09.745614: step 2216, loss = 13.08 (10.1 examples/sec; 6.360 sec/batch)
2016-04-29 16:43:15.272328: step 2217, loss = 13.16 (11.6 examples/sec; 5.527 sec/batch)
2016-04-29 16:43:20.707934: step 2218, loss = 13.10 (11.8 examples/sec; 5.436 sec/batch)
2016-04-29 16:43:26.385990: step 2219, loss = 12.92 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 16:43:32.014393: step 2220, loss = 13.05 (11.4 examples/sec; 5.628 sec/batch)
2016-04-29 16:43:45.511449: step 2221, loss = 13.05 (10.6 examples/sec; 6.023 sec/batch)
2016-04-29 16:43:51.205010: step 2222, loss = 13.06 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 16:43:56.799091: step 2223, loss = 13.12 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 16:44:02.701026: step 2224, loss = 13.12 (10.8 examples/sec; 5.902 sec/batch)
2016-04-29 16:44:08.500125: step 2225, loss = 12.95 (11.0 examples/sec; 5.799 sec/batch)
2016-04-29 16:44:14.143565: step 2226, loss = 12.97 (11.3 examples/sec; 5.643 sec/batch)
2016-04-29 16:44:20.274793: step 2227, loss = 13.01 (10.4 examples/sec; 6.131 sec/batch)
2016-04-29 16:44:25.835447: step 2228, loss = 13.02 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 16:44:31.420859: step 2229, loss = 13.01 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 16:44:37.018338: step 2230, loss = 12.89 (11.4 examples/sec; 5.597 sec/batch)
2016-04-29 16:44:50.935723: step 2231, loss = 13.06 (10.3 examples/sec; 6.217 sec/batch)
2016-04-29 16:44:56.575617: step 2232, loss = 12.93 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 16:45:02.250302: step 2233, loss = 12.89 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 16:45:07.531430: step 2234, loss = 12.91 (12.1 examples/sec; 5.281 sec/batch)
2016-04-29 16:45:13.165576: step 2235, loss = 12.89 (11.4 examples/sec; 5.634 sec/batch)
2016-04-29 16:45:18.704259: step 2236, loss = 12.84 (11.6 examples/sec; 5.539 sec/batch)
2016-04-29 16:45:25.051431: step 2237, loss = 12.93 (10.1 examples/sec; 6.347 sec/batch)
2016-04-29 16:45:30.508784: step 2238, loss = 12.88 (11.7 examples/sec; 5.457 sec/batch)
2016-04-29 16:45:35.980912: step 2239, loss = 12.93 (11.7 examples/sec; 5.472 sec/batch)
2016-04-29 16:45:41.674816: step 2240, loss = 12.80 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 16:45:55.460815: step 2241, loss = 12.87 (10.4 examples/sec; 6.125 sec/batch)
2016-04-29 16:46:01.240644: step 2242, loss = 12.85 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 16:46:06.809531: step 2243, loss = 12.90 (11.5 examples/sec; 5.569 sec/batch)
2016-04-29 16:46:12.424700: step 2244, loss = 12.92 (11.4 examples/sec; 5.615 sec/batch)
2016-04-29 16:46:18.046539: step 2245, loss = 12.85 (11.4 examples/sec; 5.622 sec/batch)
2016-04-29 16:46:23.914934: step 2246, loss = 12.77 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 16:46:30.186256: step 2247, loss = 12.91 (10.2 examples/sec; 6.271 sec/batch)
2016-04-29 16:46:35.824859: step 2248, loss = 12.63 (11.4 examples/sec; 5.639 sec/batch)
2016-04-29 16:46:41.458979: step 2249, loss = 12.82 (11.4 examples/sec; 5.634 sec/batch)
2016-04-29 16:46:46.798961: step 2250, loss = 12.68 (12.0 examples/sec; 5.340 sec/batch)
2016-04-29 16:47:00.322678: step 2251, loss = 12.80 (10.7 examples/sec; 5.987 sec/batch)
2016-04-29 16:47:06.145924: step 2252, loss = 12.80 (11.0 examples/sec; 5.823 sec/batch)
2016-04-29 16:47:13.601615: step 2253, loss = 12.81 (8.6 examples/sec; 7.456 sec/batch)
2016-04-29 16:47:20.381367: step 2254, loss = 12.65 (9.4 examples/sec; 6.780 sec/batch)
2016-04-29 16:47:27.330179: step 2255, loss = 12.82 (9.2 examples/sec; 6.949 sec/batch)
2016-04-29 16:47:35.565771: step 2256, loss = 12.68 (7.8 examples/sec; 8.235 sec/batch)
2016-04-29 16:47:43.224777: step 2257, loss = 12.59 (8.4 examples/sec; 7.659 sec/batch)
2016-04-29 16:47:48.962404: step 2258, loss = 12.80 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 16:47:56.060140: step 2259, loss = 12.72 (9.0 examples/sec; 7.098 sec/batch)
2016-04-29 16:48:01.523628: step 2260, loss = 12.79 (11.7 examples/sec; 5.463 sec/batch)
2016-04-29 16:48:17.220065: step 2261, loss = 12.68 (10.9 examples/sec; 5.891 sec/batch)
2016-04-29 16:48:23.849830: step 2262, loss = 12.68 (9.7 examples/sec; 6.630 sec/batch)
2016-04-29 16:48:30.940519: step 2263, loss = 12.70 (9.0 examples/sec; 7.091 sec/batch)
2016-04-29 16:48:37.432206: step 2264, loss = 12.72 (9.9 examples/sec; 6.492 sec/batch)
2016-04-29 16:48:43.644171: step 2265, loss = 12.53 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 16:48:49.122453: step 2266, loss = 12.55 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 16:48:55.654958: step 2267, loss = 12.63 (9.8 examples/sec; 6.532 sec/batch)
2016-04-29 16:49:02.208808: step 2268, loss = 12.57 (9.8 examples/sec; 6.554 sec/batch)
2016-04-29 16:49:08.118088: step 2269, loss = 12.63 (10.8 examples/sec; 5.909 sec/batch)
2016-04-29 16:49:14.581152: step 2270, loss = 12.70 (9.9 examples/sec; 6.463 sec/batch)
2016-04-29 16:49:28.185090: step 2271, loss = 12.51 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 16:49:34.240096: step 2272, loss = 12.56 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 16:49:39.999887: step 2273, loss = 12.55 (11.1 examples/sec; 5.760 sec/batch)
2016-04-29 16:49:46.518911: step 2274, loss = 12.52 (9.8 examples/sec; 6.519 sec/batch)
2016-04-29 16:49:51.727882: step 2275, loss = 12.66 (12.3 examples/sec; 5.209 sec/batch)
2016-04-29 16:49:56.907737: step 2276, loss = 12.53 (12.4 examples/sec; 5.180 sec/batch)
2016-04-29 16:50:02.106195: step 2277, loss = 12.51 (12.3 examples/sec; 5.198 sec/batch)
2016-04-29 16:50:07.028037: step 2278, loss = 12.61 (13.0 examples/sec; 4.922 sec/batch)
2016-04-29 16:50:12.958949: step 2279, loss = 12.41 (10.8 examples/sec; 5.931 sec/batch)
2016-04-29 16:50:19.116291: step 2280, loss = 12.51 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 16:50:31.709373: step 2281, loss = 12.48 (12.4 examples/sec; 5.159 sec/batch)
2016-04-29 16:50:37.936000: step 2282, loss = 12.52 (10.3 examples/sec; 6.224 sec/batch)
2016-04-29 16:50:43.218508: step 2283, loss = 12.50 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 16:50:49.248498: step 2284, loss = 12.32 (10.6 examples/sec; 6.030 sec/batch)
2016-04-29 16:50:55.674443: step 2285, loss = 12.56 (10.0 examples/sec; 6.426 sec/batch)
2016-04-29 16:51:01.225872: step 2286, loss = 12.43 (11.5 examples/sec; 5.549 sec/batch)
2016-04-29 16:51:06.897509: step 2287, loss = 12.28 (11.3 examples/sec; 5.672 sec/batch)
2016-04-29 16:51:12.755765: step 2288, loss = 12.34 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 16:51:18.513860: step 2289, loss = 12.58 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 16:51:25.077494: step 2290, loss = 12.35 (9.8 examples/sec; 6.564 sec/batch)
2016-04-29 16:51:39.297952: step 2291, loss = 12.62 (10.9 examples/sec; 5.849 sec/batch)
2016-04-29 16:51:44.986273: step 2292, loss = 12.40 (11.3 examples/sec; 5.688 sec/batch)
2016-04-29 16:51:50.486636: step 2293, loss = 12.45 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 16:51:56.001499: step 2294, loss = 12.23 (11.6 examples/sec; 5.515 sec/batch)
2016-04-29 16:52:01.800224: step 2295, loss = 12.38 (11.0 examples/sec; 5.799 sec/batch)
2016-04-29 16:52:07.284873: step 2296, loss = 12.68 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 16:52:13.581826: step 2297, loss = 12.32 (10.2 examples/sec; 6.297 sec/batch)
2016-04-29 16:52:20.497195: step 2298, loss = 12.37 (9.3 examples/sec; 6.915 sec/batch)
2016-04-29 16:52:29.942204: step 2299, loss = 12.42 (6.8 examples/sec; 9.445 sec/batch)
2016-04-29 16:52:38.323806: step 2300, loss = 12.31 (7.6 examples/sec; 8.382 sec/batch)
2016-04-29 16:52:54.007169: step 2301, loss = 12.23 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 16:53:01.002856: step 2302, loss = 12.15 (9.1 examples/sec; 6.995 sec/batch)
2016-04-29 16:53:09.286545: step 2303, loss = 12.28 (7.7 examples/sec; 8.284 sec/batch)
2016-04-29 16:53:16.085059: step 2304, loss = 12.41 (9.4 examples/sec; 6.798 sec/batch)
2016-04-29 16:53:23.034808: step 2305, loss = 12.39 (9.2 examples/sec; 6.950 sec/batch)
2016-04-29 16:53:30.125696: step 2306, loss = 12.27 (9.0 examples/sec; 7.091 sec/batch)
2016-04-29 16:53:36.917737: step 2307, loss = 12.38 (9.4 examples/sec; 6.789 sec/batch)
2016-04-29 16:53:44.888445: step 2308, loss = 12.39 (8.0 examples/sec; 7.970 sec/batch)
2016-04-29 16:53:50.581127: step 2309, loss = 12.04 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 16:53:56.310954: step 2310, loss = 12.26 (11.2 examples/sec; 5.730 sec/batch)
2016-04-29 16:54:09.256315: step 2311, loss = 12.18 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 16:54:15.488856: step 2312, loss = 12.08 (10.3 examples/sec; 6.232 sec/batch)
2016-04-29 16:54:21.180142: step 2313, loss = 12.19 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 16:54:26.977529: step 2314, loss = 12.11 (11.0 examples/sec; 5.797 sec/batch)
2016-04-29 16:54:32.454250: step 2315, loss = 12.03 (11.7 examples/sec; 5.477 sec/batch)
2016-04-29 16:54:37.935856: step 2316, loss = 12.06 (11.7 examples/sec; 5.482 sec/batch)
2016-04-29 16:54:43.382366: step 2317, loss = 12.21 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 16:54:49.521809: step 2318, loss = 12.23 (10.4 examples/sec; 6.139 sec/batch)
2016-04-29 16:54:54.910403: step 2319, loss = 12.19 (11.9 examples/sec; 5.389 sec/batch)
2016-04-29 16:55:00.631964: step 2320, loss = 12.13 (11.2 examples/sec; 5.721 sec/batch)
2016-04-29 16:55:13.641139: step 2321, loss = 12.11 (11.6 examples/sec; 5.515 sec/batch)
2016-04-29 16:55:20.198295: step 2322, loss = 12.03 (9.8 examples/sec; 6.557 sec/batch)
2016-04-29 16:55:25.612331: step 2323, loss = 12.07 (11.8 examples/sec; 5.414 sec/batch)
2016-04-29 16:55:31.337133: step 2324, loss = 12.15 (11.2 examples/sec; 5.725 sec/batch)
2016-04-29 16:55:36.738045: step 2325, loss = 12.28 (11.9 examples/sec; 5.401 sec/batch)
2016-04-29 16:55:42.331887: step 2326, loss = 12.04 (11.4 examples/sec; 5.594 sec/batch)
2016-04-29 16:55:47.833297: step 2327, loss = 11.96 (11.6 examples/sec; 5.501 sec/batch)
2016-04-29 16:55:54.019344: step 2328, loss = 12.06 (10.3 examples/sec; 6.186 sec/batch)
2016-04-29 16:55:59.511568: step 2329, loss = 12.09 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 16:56:05.137174: step 2330, loss = 12.13 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 16:56:18.030344: step 2331, loss = 12.03 (11.9 examples/sec; 5.376 sec/batch)
2016-04-29 16:56:24.035922: step 2332, loss = 12.00 (10.7 examples/sec; 6.005 sec/batch)
2016-04-29 16:56:29.801323: step 2333, loss = 12.13 (11.1 examples/sec; 5.765 sec/batch)
2016-04-29 16:56:35.301131: step 2334, loss = 12.15 (11.6 examples/sec; 5.500 sec/batch)
2016-04-29 16:56:40.565233: step 2335, loss = 11.94 (12.2 examples/sec; 5.264 sec/batch)
2016-04-29 16:56:46.075945: step 2336, loss = 12.17 (11.6 examples/sec; 5.511 sec/batch)
2016-04-29 16:56:51.597771: step 2337, loss = 11.97 (11.6 examples/sec; 5.522 sec/batch)
2016-04-29 16:56:57.769433: step 2338, loss = 11.94 (10.4 examples/sec; 6.172 sec/batch)
2016-04-29 16:57:03.767991: step 2339, loss = 12.10 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 16:57:09.325499: step 2340, loss = 11.96 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 16:57:22.166239: step 2341, loss = 11.89 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 16:57:27.954747: step 2342, loss = 11.81 (11.1 examples/sec; 5.788 sec/batch)
2016-04-29 16:57:34.098607: step 2343, loss = 11.89 (10.4 examples/sec; 6.144 sec/batch)
2016-04-29 16:57:39.417468: step 2344, loss = 11.99 (12.0 examples/sec; 5.319 sec/batch)
2016-04-29 16:57:44.862236: step 2345, loss = 11.88 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 16:57:50.478476: step 2346, loss = 11.92 (11.4 examples/sec; 5.616 sec/batch)
2016-04-29 16:57:56.132618: step 2347, loss = 11.98 (11.3 examples/sec; 5.654 sec/batch)
2016-04-29 16:58:01.544438: step 2348, loss = 11.95 (11.8 examples/sec; 5.412 sec/batch)
2016-04-29 16:58:07.624973: step 2349, loss = 11.95 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 16:58:13.272503: step 2350, loss = 11.83 (11.3 examples/sec; 5.647 sec/batch)
2016-04-29 16:58:26.347493: step 2351, loss = 12.00 (11.9 examples/sec; 5.400 sec/batch)
2016-04-29 16:58:32.891633: step 2352, loss = 11.62 (9.8 examples/sec; 6.544 sec/batch)
2016-04-29 16:58:39.528714: step 2353, loss = 11.80 (9.6 examples/sec; 6.637 sec/batch)
2016-04-29 16:58:45.484913: step 2354, loss = 11.84 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 16:58:52.135437: step 2355, loss = 11.93 (9.6 examples/sec; 6.650 sec/batch)
2016-04-29 16:58:58.090022: step 2356, loss = 12.00 (10.7 examples/sec; 5.954 sec/batch)
2016-04-29 16:59:04.276030: step 2357, loss = 11.94 (10.3 examples/sec; 6.186 sec/batch)
2016-04-29 16:59:11.349760: step 2358, loss = 11.63 (9.0 examples/sec; 7.074 sec/batch)
2016-04-29 16:59:17.355617: step 2359, loss = 12.04 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 16:59:23.619457: step 2360, loss = 11.85 (10.2 examples/sec; 6.264 sec/batch)
2016-04-29 16:59:37.436469: step 2361, loss = 11.67 (11.1 examples/sec; 5.772 sec/batch)
2016-04-29 16:59:44.079767: step 2362, loss = 11.92 (9.6 examples/sec; 6.643 sec/batch)
2016-04-29 16:59:49.797129: step 2363, loss = 11.89 (11.2 examples/sec; 5.717 sec/batch)
2016-04-29 16:59:55.734906: step 2364, loss = 12.02 (10.8 examples/sec; 5.938 sec/batch)
2016-04-29 17:00:02.098217: step 2365, loss = 11.75 (10.1 examples/sec; 6.363 sec/batch)
2016-04-29 17:00:08.149143: step 2366, loss = 11.90 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 17:00:14.173234: step 2367, loss = 11.82 (10.6 examples/sec; 6.024 sec/batch)
2016-04-29 17:00:20.749836: step 2368, loss = 11.94 (9.7 examples/sec; 6.577 sec/batch)
2016-04-29 17:00:26.811615: step 2369, loss = 11.63 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 17:00:32.698397: step 2370, loss = 11.69 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 17:00:46.465841: step 2371, loss = 11.87 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 17:00:53.247168: step 2372, loss = 11.61 (9.4 examples/sec; 6.781 sec/batch)
2016-04-29 17:00:59.371296: step 2373, loss = 11.76 (10.5 examples/sec; 6.124 sec/batch)
2016-04-29 17:01:05.319349: step 2374, loss = 11.65 (10.8 examples/sec; 5.948 sec/batch)
2016-04-29 17:01:11.376470: step 2375, loss = 11.86 (10.6 examples/sec; 6.057 sec/batch)
2016-04-29 17:01:17.146257: step 2376, loss = 11.68 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 17:01:23.782748: step 2377, loss = 11.48 (9.6 examples/sec; 6.636 sec/batch)
2016-04-29 17:01:29.871873: step 2378, loss = 11.59 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 17:01:35.862604: step 2379, loss = 11.55 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 17:01:41.971344: step 2380, loss = 11.57 (10.5 examples/sec; 6.109 sec/batch)
2016-04-29 17:01:56.440037: step 2381, loss = 11.61 (9.7 examples/sec; 6.607 sec/batch)
2016-04-29 17:02:02.548706: step 2382, loss = 11.66 (10.5 examples/sec; 6.109 sec/batch)
2016-04-29 17:02:08.833919: step 2383, loss = 11.83 (10.2 examples/sec; 6.285 sec/batch)
2016-04-29 17:02:14.687634: step 2384, loss = 11.67 (10.9 examples/sec; 5.854 sec/batch)
2016-04-29 17:02:20.786972: step 2385, loss = 11.58 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 17:02:27.164057: step 2386, loss = 11.49 (10.0 examples/sec; 6.377 sec/batch)
2016-04-29 17:02:33.539128: step 2387, loss = 11.64 (10.0 examples/sec; 6.375 sec/batch)
2016-04-29 17:02:39.475720: step 2388, loss = 11.63 (10.8 examples/sec; 5.936 sec/batch)
2016-04-29 17:02:45.538561: step 2389, loss = 11.66 (10.6 examples/sec; 6.063 sec/batch)
2016-04-29 17:02:51.779901: step 2390, loss = 11.40 (10.3 examples/sec; 6.241 sec/batch)
2016-04-29 17:03:06.550770: step 2391, loss = 11.58 (11.1 examples/sec; 5.746 sec/batch)
2016-04-29 17:03:12.594825: step 2392, loss = 11.27 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 17:03:18.531961: step 2393, loss = 11.48 (10.8 examples/sec; 5.937 sec/batch)
2016-04-29 17:03:24.688958: step 2394, loss = 11.44 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 17:03:31.352258: step 2395, loss = 11.61 (9.6 examples/sec; 6.663 sec/batch)
2016-04-29 17:03:40.803098: step 2396, loss = 11.38 (6.8 examples/sec; 9.451 sec/batch)
2016-04-29 17:03:49.419761: step 2397, loss = 11.49 (7.4 examples/sec; 8.617 sec/batch)
2016-04-29 17:03:58.138383: step 2398, loss = 11.42 (7.3 examples/sec; 8.719 sec/batch)
2016-04-29 17:04:06.904697: step 2399, loss = 11.39 (7.3 examples/sec; 8.766 sec/batch)
2016-04-29 17:04:16.023030: step 2400, loss = 11.50 (7.0 examples/sec; 9.118 sec/batch)
2016-04-29 17:04:36.372060: step 2401, loss = 11.52 (7.3 examples/sec; 8.816 sec/batch)
2016-04-29 17:04:45.936882: step 2402, loss = 11.55 (6.7 examples/sec; 9.565 sec/batch)
2016-04-29 17:04:54.655570: step 2403, loss = 11.49 (7.3 examples/sec; 8.719 sec/batch)
2016-04-29 17:05:03.479611: step 2404, loss = 11.25 (7.3 examples/sec; 8.824 sec/batch)
2016-04-29 17:05:12.557077: step 2405, loss = 11.44 (7.1 examples/sec; 9.077 sec/batch)
2016-04-29 17:05:21.927774: step 2406, loss = 11.50 (6.8 examples/sec; 9.371 sec/batch)
2016-04-29 17:05:30.364937: step 2407, loss = 11.31 (7.6 examples/sec; 8.437 sec/batch)
2016-04-29 17:05:36.052464: step 2408, loss = 11.44 (11.3 examples/sec; 5.687 sec/batch)
2016-04-29 17:05:41.255564: step 2409, loss = 11.38 (12.3 examples/sec; 5.203 sec/batch)
2016-04-29 17:05:46.311811: step 2410, loss = 11.43 (12.7 examples/sec; 5.056 sec/batch)
2016-04-29 17:05:58.891246: step 2411, loss = 11.52 (13.4 examples/sec; 4.789 sec/batch)
2016-04-29 17:06:04.142942: step 2412, loss = 11.28 (12.2 examples/sec; 5.252 sec/batch)
2016-04-29 17:06:09.577808: step 2413, loss = 11.47 (11.8 examples/sec; 5.435 sec/batch)
2016-04-29 17:06:14.980526: step 2414, loss = 11.37 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 17:06:19.983815: step 2415, loss = 11.51 (12.8 examples/sec; 5.003 sec/batch)
2016-04-29 17:06:25.871464: step 2416, loss = 11.37 (10.9 examples/sec; 5.888 sec/batch)
2016-04-29 17:06:31.163534: step 2417, loss = 11.40 (12.1 examples/sec; 5.292 sec/batch)
2016-04-29 17:06:36.430357: step 2418, loss = 11.38 (12.2 examples/sec; 5.267 sec/batch)
2016-04-29 17:06:41.647299: step 2419, loss = 11.26 (12.3 examples/sec; 5.217 sec/batch)
2016-04-29 17:06:46.765088: step 2420, loss = 11.25 (12.5 examples/sec; 5.118 sec/batch)
2016-04-29 17:06:59.349216: step 2421, loss = 11.27 (12.8 examples/sec; 4.990 sec/batch)
2016-04-29 17:07:04.348365: step 2422, loss = 11.44 (12.8 examples/sec; 4.999 sec/batch)
2016-04-29 17:07:09.678310: step 2423, loss = 11.30 (12.0 examples/sec; 5.330 sec/batch)
2016-04-29 17:07:14.973272: step 2424, loss = 11.30 (12.1 examples/sec; 5.295 sec/batch)
2016-04-29 17:07:19.845549: step 2425, loss = 11.18 (13.1 examples/sec; 4.872 sec/batch)
2016-04-29 17:07:25.077326: step 2426, loss = 11.22 (12.2 examples/sec; 5.232 sec/batch)
2016-04-29 17:07:31.133928: step 2427, loss = 11.39 (10.6 examples/sec; 6.057 sec/batch)
2016-04-29 17:07:36.424784: step 2428, loss = 11.18 (12.1 examples/sec; 5.291 sec/batch)
2016-04-29 17:07:41.673819: step 2429, loss = 11.29 (12.2 examples/sec; 5.249 sec/batch)
2016-04-29 17:07:46.849781: step 2430, loss = 11.22 (12.4 examples/sec; 5.176 sec/batch)
2016-04-29 17:07:58.833339: step 2431, loss = 11.26 (13.4 examples/sec; 4.759 sec/batch)
2016-04-29 17:08:04.808190: step 2432, loss = 11.22 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 17:08:09.977946: step 2433, loss = 11.32 (12.4 examples/sec; 5.170 sec/batch)
2016-04-29 17:08:15.295038: step 2434, loss = 11.19 (12.0 examples/sec; 5.317 sec/batch)
2016-04-29 17:08:20.663388: step 2435, loss = 11.08 (11.9 examples/sec; 5.368 sec/batch)
2016-04-29 17:08:25.659823: step 2436, loss = 11.22 (12.8 examples/sec; 4.996 sec/batch)
2016-04-29 17:08:30.961734: step 2437, loss = 11.13 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 17:08:36.818175: step 2438, loss = 11.16 (10.9 examples/sec; 5.856 sec/batch)
2016-04-29 17:08:42.118775: step 2439, loss = 11.18 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 17:08:47.243410: step 2440, loss = 11.11 (12.5 examples/sec; 5.125 sec/batch)
2016-04-29 17:08:59.802550: step 2441, loss = 11.32 (12.6 examples/sec; 5.083 sec/batch)
2016-04-29 17:09:05.073334: step 2442, loss = 11.26 (12.1 examples/sec; 5.271 sec/batch)
2016-04-29 17:09:10.926965: step 2443, loss = 11.19 (10.9 examples/sec; 5.854 sec/batch)
2016-04-29 17:09:16.109386: step 2444, loss = 11.14 (12.3 examples/sec; 5.182 sec/batch)
2016-04-29 17:09:21.356977: step 2445, loss = 11.17 (12.2 examples/sec; 5.248 sec/batch)
2016-04-29 17:09:26.594171: step 2446, loss = 11.27 (12.2 examples/sec; 5.237 sec/batch)
2016-04-29 17:09:31.540408: step 2447, loss = 11.22 (12.9 examples/sec; 4.946 sec/batch)
2016-04-29 17:09:36.615338: step 2448, loss = 11.00 (12.6 examples/sec; 5.075 sec/batch)
2016-04-29 17:09:42.404349: step 2449, loss = 11.15 (11.1 examples/sec; 5.789 sec/batch)
2016-04-29 17:09:47.693048: step 2450, loss = 11.15 (12.1 examples/sec; 5.289 sec/batch)
2016-04-29 17:09:59.564033: step 2451, loss = 10.92 (12.7 examples/sec; 5.024 sec/batch)
2016-04-29 17:10:04.637390: step 2452, loss = 11.16 (12.6 examples/sec; 5.073 sec/batch)
2016-04-29 17:10:09.934516: step 2453, loss = 11.05 (12.1 examples/sec; 5.297 sec/batch)
2016-04-29 17:10:15.826309: step 2454, loss = 11.10 (10.9 examples/sec; 5.892 sec/batch)
2016-04-29 17:10:21.210350: step 2455, loss = 11.00 (11.9 examples/sec; 5.384 sec/batch)
2016-04-29 17:10:26.274489: step 2456, loss = 10.97 (12.6 examples/sec; 5.064 sec/batch)
2016-04-29 17:10:31.385011: step 2457, loss = 11.04 (12.5 examples/sec; 5.110 sec/batch)
2016-04-29 17:10:36.646588: step 2458, loss = 10.96 (12.2 examples/sec; 5.261 sec/batch)
2016-04-29 17:10:41.904366: step 2459, loss = 10.81 (12.2 examples/sec; 5.258 sec/batch)
2016-04-29 17:10:47.694466: step 2460, loss = 11.14 (11.1 examples/sec; 5.790 sec/batch)
2016-04-29 17:10:59.358285: step 2461, loss = 11.09 (13.2 examples/sec; 4.838 sec/batch)
2016-04-29 17:11:04.488413: step 2462, loss = 11.11 (12.5 examples/sec; 5.130 sec/batch)
2016-04-29 17:11:09.861032: step 2463, loss = 10.99 (11.9 examples/sec; 5.372 sec/batch)
2016-04-29 17:11:15.129657: step 2464, loss = 11.05 (12.1 examples/sec; 5.269 sec/batch)
2016-04-29 17:11:20.912142: step 2465, loss = 10.87 (11.1 examples/sec; 5.782 sec/batch)
2016-04-29 17:11:25.931480: step 2466, loss = 10.94 (12.8 examples/sec; 5.019 sec/batch)
2016-04-29 17:11:31.170452: step 2467, loss = 10.82 (12.2 examples/sec; 5.239 sec/batch)
2016-04-29 17:11:36.482371: step 2468, loss = 10.86 (12.0 examples/sec; 5.312 sec/batch)
2016-04-29 17:11:41.761317: step 2469, loss = 11.03 (12.1 examples/sec; 5.279 sec/batch)
2016-04-29 17:11:46.721225: step 2470, loss = 10.86 (12.9 examples/sec; 4.960 sec/batch)
2016-04-29 17:11:59.754043: step 2471, loss = 10.94 (12.4 examples/sec; 5.157 sec/batch)
2016-04-29 17:12:04.900500: step 2472, loss = 10.89 (12.4 examples/sec; 5.146 sec/batch)
2016-04-29 17:12:10.453358: step 2473, loss = 10.96 (11.5 examples/sec; 5.553 sec/batch)
2016-04-29 17:12:15.924434: step 2474, loss = 10.97 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 17:12:21.262004: step 2475, loss = 10.99 (12.0 examples/sec; 5.337 sec/batch)
2016-04-29 17:12:29.634880: step 2476, loss = 10.74 (7.6 examples/sec; 8.373 sec/batch)
2016-04-29 17:12:35.170630: step 2477, loss = 10.93 (11.6 examples/sec; 5.536 sec/batch)
2016-04-29 17:12:40.597522: step 2478, loss = 10.77 (11.8 examples/sec; 5.427 sec/batch)
2016-04-29 17:12:46.067973: step 2479, loss = 10.86 (11.7 examples/sec; 5.470 sec/batch)
2016-04-29 17:12:51.646316: step 2480, loss = 10.78 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 17:13:05.071645: step 2481, loss = 10.61 (12.1 examples/sec; 5.282 sec/batch)
2016-04-29 17:13:10.404098: step 2482, loss = 10.93 (12.0 examples/sec; 5.332 sec/batch)
2016-04-29 17:13:15.849922: step 2483, loss = 10.63 (11.8 examples/sec; 5.446 sec/batch)
2016-04-29 17:13:21.435675: step 2484, loss = 10.77 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 17:13:27.135138: step 2485, loss = 10.64 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 17:13:33.711033: step 2486, loss = 10.83 (9.7 examples/sec; 6.576 sec/batch)
2016-04-29 17:13:39.985591: step 2487, loss = 10.90 (10.2 examples/sec; 6.274 sec/batch)
2016-04-29 17:13:45.726061: step 2488, loss = 10.78 (11.1 examples/sec; 5.740 sec/batch)
2016-04-29 17:13:51.433869: step 2489, loss = 10.66 (11.2 examples/sec; 5.708 sec/batch)
2016-04-29 17:13:57.395589: step 2490, loss = 10.63 (10.7 examples/sec; 5.962 sec/batch)
2016-04-29 17:14:10.201216: step 2491, loss = 10.78 (12.8 examples/sec; 5.012 sec/batch)
2016-04-29 17:14:15.308640: step 2492, loss = 11.00 (12.5 examples/sec; 5.107 sec/batch)
2016-04-29 17:14:20.464032: step 2493, loss = 10.94 (12.4 examples/sec; 5.155 sec/batch)
2016-04-29 17:14:25.361082: step 2494, loss = 10.60 (13.1 examples/sec; 4.897 sec/batch)
2016-04-29 17:14:30.505338: step 2495, loss = 10.81 (12.4 examples/sec; 5.144 sec/batch)
2016-04-29 17:14:35.555003: step 2496, loss = 10.72 (12.7 examples/sec; 5.050 sec/batch)
2016-04-29 17:14:42.014574: step 2497, loss = 10.69 (9.9 examples/sec; 6.459 sec/batch)
2016-04-29 17:14:48.189598: step 2498, loss = 10.73 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 17:14:53.823642: step 2499, loss = 10.86 (11.4 examples/sec; 5.634 sec/batch)
2016-04-29 17:14:58.860474: step 2500, loss = 10.70 (12.7 examples/sec; 5.037 sec/batch)
2016-04-29 17:15:11.699543: step 2501, loss = 10.77 (11.4 examples/sec; 5.637 sec/batch)
2016-04-29 17:15:16.532469: step 2502, loss = 10.68 (13.2 examples/sec; 4.833 sec/batch)
2016-04-29 17:15:21.917294: step 2503, loss = 10.63 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 17:15:27.018892: step 2504, loss = 10.63 (12.5 examples/sec; 5.101 sec/batch)
2016-04-29 17:15:31.915802: step 2505, loss = 10.65 (13.1 examples/sec; 4.897 sec/batch)
2016-04-29 17:15:36.868086: step 2506, loss = 10.60 (12.9 examples/sec; 4.952 sec/batch)
2016-04-29 17:15:41.882261: step 2507, loss = 10.48 (12.8 examples/sec; 5.014 sec/batch)
2016-04-29 17:15:47.127459: step 2508, loss = 10.60 (12.2 examples/sec; 5.245 sec/batch)
2016-04-29 17:15:52.276709: step 2509, loss = 10.46 (12.4 examples/sec; 5.149 sec/batch)
2016-04-29 17:15:57.417742: step 2510, loss = 10.72 (12.4 examples/sec; 5.141 sec/batch)
2016-04-29 17:16:09.261750: step 2511, loss = 10.56 (12.7 examples/sec; 5.025 sec/batch)
2016-04-29 17:16:14.217842: step 2512, loss = 10.61 (12.9 examples/sec; 4.956 sec/batch)
2016-04-29 17:16:21.577499: step 2513, loss = 10.63 (8.7 examples/sec; 7.360 sec/batch)
2016-04-29 17:16:28.906383: step 2514, loss = 10.69 (8.7 examples/sec; 7.329 sec/batch)
2016-04-29 17:16:34.104497: step 2515, loss = 10.65 (12.3 examples/sec; 5.198 sec/batch)
2016-04-29 17:16:39.522287: step 2516, loss = 10.70 (11.8 examples/sec; 5.418 sec/batch)
2016-04-29 17:16:44.724162: step 2517, loss = 10.56 (12.3 examples/sec; 5.202 sec/batch)
2016-04-29 17:16:50.924132: step 2518, loss = 10.58 (10.3 examples/sec; 6.200 sec/batch)
2016-04-29 17:16:56.871046: step 2519, loss = 10.42 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 17:17:02.038208: step 2520, loss = 10.63 (12.4 examples/sec; 5.165 sec/batch)
2016-04-29 17:17:15.153560: step 2521, loss = 10.58 (11.6 examples/sec; 5.520 sec/batch)
2016-04-29 17:17:20.455268: step 2522, loss = 10.60 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 17:17:26.119623: step 2523, loss = 10.48 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 17:17:31.491844: step 2524, loss = 10.66 (11.9 examples/sec; 5.372 sec/batch)
2016-04-29 17:17:36.688446: step 2525, loss = 10.44 (12.3 examples/sec; 5.197 sec/batch)
2016-04-29 17:17:42.039129: step 2526, loss = 10.52 (12.0 examples/sec; 5.351 sec/batch)
2016-04-29 17:17:47.466823: step 2527, loss = 10.51 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 17:17:53.111395: step 2528, loss = 10.57 (11.3 examples/sec; 5.644 sec/batch)
2016-04-29 17:17:58.810024: step 2529, loss = 10.28 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 17:18:04.240789: step 2530, loss = 10.64 (11.8 examples/sec; 5.431 sec/batch)
2016-04-29 17:18:16.576407: step 2531, loss = 10.45 (12.7 examples/sec; 5.035 sec/batch)
2016-04-29 17:18:21.942522: step 2532, loss = 10.45 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 17:18:27.717450: step 2533, loss = 10.57 (11.1 examples/sec; 5.775 sec/batch)
2016-04-29 17:18:36.130197: step 2534, loss = 10.41 (7.6 examples/sec; 8.413 sec/batch)
2016-04-29 17:18:45.713393: step 2535, loss = 10.41 (6.7 examples/sec; 9.583 sec/batch)
2016-04-29 17:18:54.061807: step 2536, loss = 10.42 (7.7 examples/sec; 8.348 sec/batch)
2016-04-29 17:19:02.030812: step 2537, loss = 10.45 (8.0 examples/sec; 7.968 sec/batch)
2016-04-29 17:19:07.888442: step 2538, loss = 10.24 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 17:19:13.658203: step 2539, loss = 10.37 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 17:19:20.360192: step 2540, loss = 10.44 (9.5 examples/sec; 6.702 sec/batch)
2016-04-29 17:19:38.583170: step 2541, loss = 10.46 (9.7 examples/sec; 6.599 sec/batch)
2016-04-29 17:19:44.862834: step 2542, loss = 10.45 (10.2 examples/sec; 6.280 sec/batch)
2016-04-29 17:19:50.689170: step 2543, loss = 10.33 (11.0 examples/sec; 5.826 sec/batch)
2016-04-29 17:19:56.769734: step 2544, loss = 10.19 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 17:20:02.738090: step 2545, loss = 10.55 (10.7 examples/sec; 5.968 sec/batch)
2016-04-29 17:20:09.149060: step 2546, loss = 10.41 (10.0 examples/sec; 6.411 sec/batch)
2016-04-29 17:20:14.680474: step 2547, loss = 10.28 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 17:20:20.086625: step 2548, loss = 10.24 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 17:20:25.731186: step 2549, loss = 10.26 (11.3 examples/sec; 5.644 sec/batch)
2016-04-29 17:20:31.542815: step 2550, loss = 10.32 (11.0 examples/sec; 5.812 sec/batch)
2016-04-29 17:20:46.057776: step 2551, loss = 10.36 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 17:20:51.865565: step 2552, loss = 10.22 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 17:20:58.555429: step 2553, loss = 10.20 (9.6 examples/sec; 6.690 sec/batch)
2016-04-29 17:21:04.890226: step 2554, loss = 10.45 (10.1 examples/sec; 6.335 sec/batch)
2016-04-29 17:21:13.936117: step 2555, loss = 10.23 (7.1 examples/sec; 9.046 sec/batch)
2016-04-29 17:21:20.483063: step 2556, loss = 10.36 (9.8 examples/sec; 6.547 sec/batch)
2016-04-29 17:21:26.195606: step 2557, loss = 10.20 (11.2 examples/sec; 5.712 sec/batch)
2016-04-29 17:21:33.413465: step 2558, loss = 10.25 (8.9 examples/sec; 7.218 sec/batch)
2016-04-29 17:21:40.790918: step 2559, loss = 10.25 (8.7 examples/sec; 7.377 sec/batch)
2016-04-29 17:21:47.408571: step 2560, loss = 10.42 (9.7 examples/sec; 6.618 sec/batch)
2016-04-29 17:22:00.803142: step 2561, loss = 10.29 (11.7 examples/sec; 5.452 sec/batch)
2016-04-29 17:22:06.052504: step 2562, loss = 10.14 (12.2 examples/sec; 5.249 sec/batch)
2016-04-29 17:22:11.885007: step 2563, loss = 10.20 (11.0 examples/sec; 5.832 sec/batch)
2016-04-29 17:22:16.867770: step 2564, loss = 10.27 (12.8 examples/sec; 4.983 sec/batch)
2016-04-29 17:22:23.283075: step 2565, loss = 10.18 (10.0 examples/sec; 6.415 sec/batch)
2016-04-29 17:22:28.699618: step 2566, loss = 10.19 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 17:22:34.406643: step 2567, loss = 10.30 (11.2 examples/sec; 5.707 sec/batch)
2016-04-29 17:22:39.460810: step 2568, loss = 10.06 (12.7 examples/sec; 5.054 sec/batch)
2016-04-29 17:22:44.713279: step 2569, loss = 10.22 (12.2 examples/sec; 5.252 sec/batch)
2016-04-29 17:22:49.801995: step 2570, loss = 10.17 (12.6 examples/sec; 5.089 sec/batch)
2016-04-29 17:23:03.072665: step 2571, loss = 10.06 (12.5 examples/sec; 5.136 sec/batch)
2016-04-29 17:23:08.948192: step 2572, loss = 10.36 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 17:23:14.004924: step 2573, loss = 10.19 (12.7 examples/sec; 5.057 sec/batch)
2016-04-29 17:23:19.570646: step 2574, loss = 10.10 (11.5 examples/sec; 5.566 sec/batch)
2016-04-29 17:23:25.482094: step 2575, loss = 10.14 (10.8 examples/sec; 5.911 sec/batch)
2016-04-29 17:23:30.695896: step 2576, loss = 10.25 (12.3 examples/sec; 5.214 sec/batch)
2016-04-29 17:23:35.910644: step 2577, loss = 9.99 (12.3 examples/sec; 5.215 sec/batch)
2016-04-29 17:23:41.019912: step 2578, loss = 10.15 (12.5 examples/sec; 5.109 sec/batch)
2016-04-29 17:23:46.551262: step 2579, loss = 10.31 (11.6 examples/sec; 5.531 sec/batch)
2016-04-29 17:23:52.553470: step 2580, loss = 10.11 (10.7 examples/sec; 6.002 sec/batch)
2016-04-29 17:24:06.825848: step 2581, loss = 10.15 (11.9 examples/sec; 5.401 sec/batch)
2016-04-29 17:24:13.244195: step 2582, loss = 10.08 (10.0 examples/sec; 6.415 sec/batch)
2016-04-29 17:24:18.932976: step 2583, loss = 9.78 (11.3 examples/sec; 5.689 sec/batch)
2016-04-29 17:24:24.907579: step 2584, loss = 10.26 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 17:24:31.137364: step 2585, loss = 10.07 (10.3 examples/sec; 6.230 sec/batch)
2016-04-29 17:24:36.499140: step 2586, loss = 10.11 (11.9 examples/sec; 5.362 sec/batch)
2016-04-29 17:24:42.082580: step 2587, loss = 9.98 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 17:24:48.461187: step 2588, loss = 10.05 (10.0 examples/sec; 6.378 sec/batch)
2016-04-29 17:24:54.667244: step 2589, loss = 10.08 (10.3 examples/sec; 6.206 sec/batch)
2016-04-29 17:24:59.957158: step 2590, loss = 10.17 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 17:25:13.517178: step 2591, loss = 10.19 (12.3 examples/sec; 5.191 sec/batch)
2016-04-29 17:25:18.939826: step 2592, loss = 10.00 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 17:25:24.275963: step 2593, loss = 9.98 (12.0 examples/sec; 5.336 sec/batch)
2016-04-29 17:25:29.629941: step 2594, loss = 9.77 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 17:25:34.648513: step 2595, loss = 9.93 (12.8 examples/sec; 5.018 sec/batch)
2016-04-29 17:25:40.339187: step 2596, loss = 9.94 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 17:25:45.640021: step 2597, loss = 10.07 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 17:25:51.085062: step 2598, loss = 9.85 (11.8 examples/sec; 5.445 sec/batch)
2016-04-29 17:25:56.065223: step 2599, loss = 10.10 (12.9 examples/sec; 4.980 sec/batch)
2016-04-29 17:26:01.346573: step 2600, loss = 9.99 (12.1 examples/sec; 5.281 sec/batch)
2016-04-29 17:26:14.024456: step 2601, loss = 9.87 (11.2 examples/sec; 5.708 sec/batch)
2016-04-29 17:26:19.193713: step 2602, loss = 10.08 (12.4 examples/sec; 5.169 sec/batch)
2016-04-29 17:26:26.424097: step 2603, loss = 9.93 (8.9 examples/sec; 7.230 sec/batch)
2016-04-29 17:26:34.594701: step 2604, loss = 9.94 (7.8 examples/sec; 8.170 sec/batch)
2016-04-29 17:26:43.342325: step 2605, loss = 9.96 (7.3 examples/sec; 8.748 sec/batch)
2016-04-29 17:26:50.016995: step 2606, loss = 9.92 (9.6 examples/sec; 6.675 sec/batch)
2016-04-29 17:26:57.655035: step 2607, loss = 9.90 (8.4 examples/sec; 7.638 sec/batch)
2016-04-29 17:27:05.206166: step 2608, loss = 10.01 (8.5 examples/sec; 7.551 sec/batch)
2016-04-29 17:27:11.196375: step 2609, loss = 9.85 (10.7 examples/sec; 5.990 sec/batch)
2016-04-29 17:27:17.892937: step 2610, loss = 9.98 (9.6 examples/sec; 6.696 sec/batch)
2016-04-29 17:27:32.296371: step 2611, loss = 10.08 (11.0 examples/sec; 5.832 sec/batch)
2016-04-29 17:27:38.300583: step 2612, loss = 9.92 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 17:27:44.230513: step 2613, loss = 10.11 (10.8 examples/sec; 5.930 sec/batch)
2016-04-29 17:27:50.926948: step 2614, loss = 9.85 (9.6 examples/sec; 6.696 sec/batch)
2016-04-29 17:27:58.667794: step 2615, loss = 9.96 (8.3 examples/sec; 7.741 sec/batch)
2016-04-29 17:28:06.066878: step 2616, loss = 9.92 (8.6 examples/sec; 7.399 sec/batch)
2016-04-29 17:28:11.483456: step 2617, loss = 9.77 (11.8 examples/sec; 5.416 sec/batch)
2016-04-29 17:28:17.273114: step 2618, loss = 9.80 (11.1 examples/sec; 5.790 sec/batch)
2016-04-29 17:28:24.072885: step 2619, loss = 9.80 (9.4 examples/sec; 6.800 sec/batch)
2016-04-29 17:28:29.896857: step 2620, loss = 10.02 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 17:28:43.950583: step 2621, loss = 9.73 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 17:28:50.804566: step 2622, loss = 9.77 (9.3 examples/sec; 6.854 sec/batch)
2016-04-29 17:28:57.842392: step 2623, loss = 9.89 (9.1 examples/sec; 7.038 sec/batch)
2016-04-29 17:29:04.007887: step 2624, loss = 10.04 (10.4 examples/sec; 6.165 sec/batch)
2016-04-29 17:29:10.290341: step 2625, loss = 9.68 (10.2 examples/sec; 6.282 sec/batch)
2016-04-29 17:29:16.353077: step 2626, loss = 9.66 (10.6 examples/sec; 6.063 sec/batch)
2016-04-29 17:29:22.340554: step 2627, loss = 9.88 (10.7 examples/sec; 5.987 sec/batch)
2016-04-29 17:29:30.234521: step 2628, loss = 9.73 (8.1 examples/sec; 7.894 sec/batch)
2016-04-29 17:29:35.972414: step 2629, loss = 9.77 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 17:29:41.772797: step 2630, loss = 9.70 (11.0 examples/sec; 5.800 sec/batch)
2016-04-29 17:29:55.289611: step 2631, loss = 9.81 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 17:30:01.145189: step 2632, loss = 9.74 (10.9 examples/sec; 5.855 sec/batch)
2016-04-29 17:30:07.889415: step 2633, loss = 9.74 (9.5 examples/sec; 6.744 sec/batch)
2016-04-29 17:30:13.927763: step 2634, loss = 9.81 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 17:30:19.727716: step 2635, loss = 9.41 (11.0 examples/sec; 5.800 sec/batch)
2016-04-29 17:30:25.554982: step 2636, loss = 9.56 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 17:30:31.961368: step 2637, loss = 9.74 (10.0 examples/sec; 6.406 sec/batch)
2016-04-29 17:30:38.931240: step 2638, loss = 9.59 (9.2 examples/sec; 6.969 sec/batch)
2016-04-29 17:30:45.172943: step 2639, loss = 9.76 (10.3 examples/sec; 6.242 sec/batch)
2016-04-29 17:30:51.415297: step 2640, loss = 9.59 (10.3 examples/sec; 6.242 sec/batch)
2016-04-29 17:31:07.157245: step 2641, loss = 9.69 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 17:31:14.930529: step 2642, loss = 9.71 (8.2 examples/sec; 7.773 sec/batch)
2016-04-29 17:31:22.987088: step 2643, loss = 9.62 (7.9 examples/sec; 8.056 sec/batch)
2016-04-29 17:31:30.819282: step 2644, loss = 9.56 (8.2 examples/sec; 7.832 sec/batch)
2016-04-29 17:31:38.903761: step 2645, loss = 9.69 (7.9 examples/sec; 8.084 sec/batch)
2016-04-29 17:31:45.743431: step 2646, loss = 9.70 (9.4 examples/sec; 6.840 sec/batch)
2016-04-29 17:31:52.264486: step 2647, loss = 9.71 (9.8 examples/sec; 6.521 sec/batch)
2016-04-29 17:31:58.726308: step 2648, loss = 9.71 (9.9 examples/sec; 6.462 sec/batch)
2016-04-29 17:32:05.217494: step 2649, loss = 9.59 (9.9 examples/sec; 6.491 sec/batch)
2016-04-29 17:32:11.164726: step 2650, loss = 9.59 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 17:32:28.799740: step 2651, loss = 9.62 (9.8 examples/sec; 6.554 sec/batch)
2016-04-29 17:32:34.697811: step 2652, loss = 9.74 (10.9 examples/sec; 5.898 sec/batch)
2016-04-29 17:32:40.496042: step 2653, loss = 9.78 (11.0 examples/sec; 5.798 sec/batch)
2016-04-29 17:32:46.224472: step 2654, loss = 9.70 (11.2 examples/sec; 5.728 sec/batch)
2016-04-29 17:32:52.735541: step 2655, loss = 9.68 (9.8 examples/sec; 6.511 sec/batch)
2016-04-29 17:32:58.740424: step 2656, loss = 9.58 (10.7 examples/sec; 6.005 sec/batch)
2016-04-29 17:33:04.704663: step 2657, loss = 9.68 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 17:33:10.390865: step 2658, loss = 9.55 (11.3 examples/sec; 5.686 sec/batch)
2016-04-29 17:33:16.147454: step 2659, loss = 9.40 (11.1 examples/sec; 5.757 sec/batch)
2016-04-29 17:33:22.656024: step 2660, loss = 9.75 (9.8 examples/sec; 6.508 sec/batch)
2016-04-29 17:33:35.751265: step 2661, loss = 9.68 (12.1 examples/sec; 5.306 sec/batch)
2016-04-29 17:33:41.513497: step 2662, loss = 9.38 (11.1 examples/sec; 5.762 sec/batch)
2016-04-29 17:33:47.237050: step 2663, loss = 9.53 (11.2 examples/sec; 5.723 sec/batch)
2016-04-29 17:33:52.921126: step 2664, loss = 9.69 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 17:33:59.496349: step 2665, loss = 9.56 (9.7 examples/sec; 6.575 sec/batch)
2016-04-29 17:34:06.740600: step 2666, loss = 9.54 (8.8 examples/sec; 7.244 sec/batch)
2016-04-29 17:34:12.677020: step 2667, loss = 9.41 (10.8 examples/sec; 5.936 sec/batch)
2016-04-29 17:34:19.404335: step 2668, loss = 9.50 (9.5 examples/sec; 6.727 sec/batch)
2016-04-29 17:34:25.820569: step 2669, loss = 9.56 (10.0 examples/sec; 6.416 sec/batch)
2016-04-29 17:34:32.327643: step 2670, loss = 9.48 (9.8 examples/sec; 6.507 sec/batch)
2016-04-29 17:34:45.941234: step 2671, loss = 9.46 (11.2 examples/sec; 5.693 sec/batch)
2016-04-29 17:34:51.798296: step 2672, loss = 9.45 (10.9 examples/sec; 5.857 sec/batch)
2016-04-29 17:34:58.759674: step 2673, loss = 9.62 (9.2 examples/sec; 6.961 sec/batch)
2016-04-29 17:35:05.624171: step 2674, loss = 9.42 (9.3 examples/sec; 6.864 sec/batch)
2016-04-29 17:35:11.610690: step 2675, loss = 9.54 (10.7 examples/sec; 5.986 sec/batch)
2016-04-29 17:35:19.640003: step 2676, loss = 9.58 (8.0 examples/sec; 8.029 sec/batch)
2016-04-29 17:35:26.332432: step 2677, loss = 9.27 (9.6 examples/sec; 6.692 sec/batch)
2016-04-29 17:35:32.195460: step 2678, loss = 9.42 (10.9 examples/sec; 5.863 sec/batch)
2016-04-29 17:35:38.842715: step 2679, loss = 9.39 (9.6 examples/sec; 6.647 sec/batch)
2016-04-29 17:35:44.615606: step 2680, loss = 9.45 (11.1 examples/sec; 5.773 sec/batch)
2016-04-29 17:35:57.712148: step 2681, loss = 9.49 (12.5 examples/sec; 5.117 sec/batch)
2016-04-29 17:36:03.456453: step 2682, loss = 9.61 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 17:36:09.588986: step 2683, loss = 9.50 (10.4 examples/sec; 6.132 sec/batch)
2016-04-29 17:36:15.103310: step 2684, loss = 9.44 (11.6 examples/sec; 5.514 sec/batch)
2016-04-29 17:36:20.170476: step 2685, loss = 9.34 (12.6 examples/sec; 5.067 sec/batch)
2016-04-29 17:36:25.373512: step 2686, loss = 9.50 (12.3 examples/sec; 5.203 sec/batch)
2016-04-29 17:36:30.653955: step 2687, loss = 9.47 (12.1 examples/sec; 5.280 sec/batch)
2016-04-29 17:36:35.702330: step 2688, loss = 9.34 (12.7 examples/sec; 5.048 sec/batch)
2016-04-29 17:36:41.194980: step 2689, loss = 9.44 (11.7 examples/sec; 5.493 sec/batch)
2016-04-29 17:36:46.541411: step 2690, loss = 9.32 (12.0 examples/sec; 5.346 sec/batch)
2016-04-29 17:36:58.963957: step 2691, loss = 9.54 (11.7 examples/sec; 5.484 sec/batch)
2016-04-29 17:37:05.373592: step 2692, loss = 9.31 (10.0 examples/sec; 6.410 sec/batch)
2016-04-29 17:37:10.511094: step 2693, loss = 9.25 (12.5 examples/sec; 5.137 sec/batch)
2016-04-29 17:37:16.098931: step 2694, loss = 9.61 (11.5 examples/sec; 5.588 sec/batch)
2016-04-29 17:37:21.227248: step 2695, loss = 9.50 (12.5 examples/sec; 5.128 sec/batch)
2016-04-29 17:37:26.347172: step 2696, loss = 9.31 (12.5 examples/sec; 5.120 sec/batch)
2016-04-29 17:37:31.325689: step 2697, loss = 9.25 (12.9 examples/sec; 4.978 sec/batch)
2016-04-29 17:37:36.301853: step 2698, loss = 9.38 (12.9 examples/sec; 4.976 sec/batch)
2016-04-29 17:37:41.302054: step 2699, loss = 9.33 (12.8 examples/sec; 5.000 sec/batch)
2016-04-29 17:37:46.859109: step 2700, loss = 9.37 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 17:37:58.570514: step 2701, loss = 9.47 (13.3 examples/sec; 4.800 sec/batch)
2016-04-29 17:38:03.792427: step 2702, loss = 9.33 (12.3 examples/sec; 5.222 sec/batch)
2016-04-29 17:38:08.725302: step 2703, loss = 9.26 (13.0 examples/sec; 4.933 sec/batch)
2016-04-29 17:38:13.894104: step 2704, loss = 9.46 (12.4 examples/sec; 5.169 sec/batch)
2016-04-29 17:38:19.515460: step 2705, loss = 9.09 (11.4 examples/sec; 5.621 sec/batch)
2016-04-29 17:38:24.658830: step 2706, loss = 9.35 (12.4 examples/sec; 5.143 sec/batch)
2016-04-29 17:38:29.539135: step 2707, loss = 9.21 (13.1 examples/sec; 4.880 sec/batch)
2016-04-29 17:38:34.616441: step 2708, loss = 9.22 (12.6 examples/sec; 5.077 sec/batch)
2016-04-29 17:38:39.806809: step 2709, loss = 9.16 (12.3 examples/sec; 5.190 sec/batch)
2016-04-29 17:38:44.989937: step 2710, loss = 9.25 (12.3 examples/sec; 5.183 sec/batch)
2016-04-29 17:38:57.523921: step 2711, loss = 9.22 (12.7 examples/sec; 5.031 sec/batch)
2016-04-29 17:39:02.497726: step 2712, loss = 9.21 (12.9 examples/sec; 4.974 sec/batch)
2016-04-29 17:39:07.403115: step 2713, loss = 8.98 (13.0 examples/sec; 4.905 sec/batch)
2016-04-29 17:39:12.415942: step 2714, loss = 9.33 (12.8 examples/sec; 5.013 sec/batch)
2016-04-29 17:39:17.238962: step 2715, loss = 9.20 (13.3 examples/sec; 4.823 sec/batch)
2016-04-29 17:39:22.375128: step 2716, loss = 9.16 (12.5 examples/sec; 5.136 sec/batch)
2016-04-29 17:39:27.940471: step 2717, loss = 9.12 (11.5 examples/sec; 5.565 sec/batch)
2016-04-29 17:39:32.972496: step 2718, loss = 9.21 (12.7 examples/sec; 5.032 sec/batch)
2016-04-29 17:39:37.823429: step 2719, loss = 9.13 (13.2 examples/sec; 4.851 sec/batch)
2016-04-29 17:39:42.972185: step 2720, loss = 9.14 (12.4 examples/sec; 5.149 sec/batch)
2016-04-29 17:39:55.203516: step 2721, loss = 9.23 (12.5 examples/sec; 5.122 sec/batch)
2016-04-29 17:40:00.738853: step 2722, loss = 9.09 (11.6 examples/sec; 5.535 sec/batch)
2016-04-29 17:40:05.636522: step 2723, loss = 9.25 (13.1 examples/sec; 4.898 sec/batch)
2016-04-29 17:40:10.648547: step 2724, loss = 9.06 (12.8 examples/sec; 5.012 sec/batch)
2016-04-29 17:40:15.785372: step 2725, loss = 8.93 (12.5 examples/sec; 5.137 sec/batch)
2016-04-29 17:40:20.929976: step 2726, loss = 9.17 (12.4 examples/sec; 5.145 sec/batch)
2016-04-29 17:40:25.776848: step 2727, loss = 9.00 (13.2 examples/sec; 4.847 sec/batch)
2016-04-29 17:40:31.579255: step 2728, loss = 9.23 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 17:40:39.537853: step 2729, loss = 9.23 (8.0 examples/sec; 7.959 sec/batch)
2016-04-29 17:40:47.542016: step 2730, loss = 9.21 (8.0 examples/sec; 8.004 sec/batch)
2016-04-29 17:41:02.540086: step 2731, loss = 9.20 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 17:41:08.777428: step 2732, loss = 9.09 (10.3 examples/sec; 6.237 sec/batch)
2016-04-29 17:41:14.340045: step 2733, loss = 9.22 (11.5 examples/sec; 5.563 sec/batch)
2016-04-29 17:41:20.033858: step 2734, loss = 9.13 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 17:41:25.711725: step 2735, loss = 9.10 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 17:41:31.294773: step 2736, loss = 9.15 (11.5 examples/sec; 5.583 sec/batch)
2016-04-29 17:41:37.960878: step 2737, loss = 9.17 (9.6 examples/sec; 6.666 sec/batch)
2016-04-29 17:41:43.913037: step 2738, loss = 8.89 (10.8 examples/sec; 5.952 sec/batch)
2016-04-29 17:41:49.438647: step 2739, loss = 8.94 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 17:41:55.201779: step 2740, loss = 9.02 (11.1 examples/sec; 5.763 sec/batch)
2016-04-29 17:42:07.946247: step 2741, loss = 9.11 (11.9 examples/sec; 5.374 sec/batch)
2016-04-29 17:42:14.125708: step 2742, loss = 9.09 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 17:42:19.572834: step 2743, loss = 8.87 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 17:42:25.375490: step 2744, loss = 9.03 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 17:42:30.908146: step 2745, loss = 8.94 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 17:42:36.583474: step 2746, loss = 8.98 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 17:42:41.970930: step 2747, loss = 9.02 (11.9 examples/sec; 5.387 sec/batch)
2016-04-29 17:42:48.087496: step 2748, loss = 9.14 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 17:42:53.500837: step 2749, loss = 8.89 (11.8 examples/sec; 5.413 sec/batch)
2016-04-29 17:42:59.065337: step 2750, loss = 9.20 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 17:43:12.241770: step 2751, loss = 9.10 (11.6 examples/sec; 5.528 sec/batch)
2016-04-29 17:43:18.340263: step 2752, loss = 8.75 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 17:43:23.718428: step 2753, loss = 8.73 (11.9 examples/sec; 5.378 sec/batch)
2016-04-29 17:43:29.151463: step 2754, loss = 8.86 (11.8 examples/sec; 5.433 sec/batch)
2016-04-29 17:43:34.730942: step 2755, loss = 8.94 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 17:43:40.282556: step 2756, loss = 8.93 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 17:43:45.987176: step 2757, loss = 8.99 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 17:43:52.295274: step 2758, loss = 8.85 (10.1 examples/sec; 6.308 sec/batch)
2016-04-29 17:43:57.857465: step 2759, loss = 9.04 (11.5 examples/sec; 5.562 sec/batch)
2016-04-29 17:44:03.659963: step 2760, loss = 8.90 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 17:44:16.398116: step 2761, loss = 8.81 (11.9 examples/sec; 5.386 sec/batch)
2016-04-29 17:44:22.689882: step 2762, loss = 8.88 (10.2 examples/sec; 6.292 sec/batch)
2016-04-29 17:44:28.484149: step 2763, loss = 8.82 (11.0 examples/sec; 5.794 sec/batch)
2016-04-29 17:44:34.759400: step 2764, loss = 9.07 (10.2 examples/sec; 6.275 sec/batch)
2016-04-29 17:44:40.427860: step 2765, loss = 8.98 (11.3 examples/sec; 5.668 sec/batch)
2016-04-29 17:44:46.262223: step 2766, loss = 9.09 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 17:44:52.072653: step 2767, loss = 9.07 (11.0 examples/sec; 5.810 sec/batch)
2016-04-29 17:44:58.188424: step 2768, loss = 8.89 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 17:45:04.087414: step 2769, loss = 8.92 (10.8 examples/sec; 5.899 sec/batch)
2016-04-29 17:45:09.739977: step 2770, loss = 8.82 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 17:45:22.605404: step 2771, loss = 8.70 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 17:45:28.664631: step 2772, loss = 8.74 (10.6 examples/sec; 6.059 sec/batch)
2016-04-29 17:45:34.177307: step 2773, loss = 8.91 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 17:45:39.932412: step 2774, loss = 8.82 (11.1 examples/sec; 5.755 sec/batch)
2016-04-29 17:45:45.516494: step 2775, loss = 8.90 (11.5 examples/sec; 5.584 sec/batch)
2016-04-29 17:45:50.976599: step 2776, loss = 8.80 (11.7 examples/sec; 5.460 sec/batch)
2016-04-29 17:45:56.750555: step 2777, loss = 8.73 (11.1 examples/sec; 5.774 sec/batch)
2016-04-29 17:46:02.845902: step 2778, loss = 8.83 (10.5 examples/sec; 6.095 sec/batch)
2016-04-29 17:46:08.335984: step 2779, loss = 8.97 (11.7 examples/sec; 5.490 sec/batch)
2016-04-29 17:46:14.127963: step 2780, loss = 8.67 (11.0 examples/sec; 5.792 sec/batch)
2016-04-29 17:46:27.085004: step 2781, loss = 8.76 (12.0 examples/sec; 5.329 sec/batch)
2016-04-29 17:46:32.598261: step 2782, loss = 8.80 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 17:46:38.897923: step 2783, loss = 8.93 (10.2 examples/sec; 6.300 sec/batch)
2016-04-29 17:46:44.244356: step 2784, loss = 8.68 (12.0 examples/sec; 5.346 sec/batch)
2016-04-29 17:46:49.663475: step 2785, loss = 8.80 (11.8 examples/sec; 5.419 sec/batch)
2016-04-29 17:46:55.407387: step 2786, loss = 8.83 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 17:47:01.260960: step 2787, loss = 8.71 (10.9 examples/sec; 5.853 sec/batch)
2016-04-29 17:47:07.442596: step 2788, loss = 8.89 (10.4 examples/sec; 6.182 sec/batch)
2016-04-29 17:47:13.117112: step 2789, loss = 8.84 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 17:47:18.649875: step 2790, loss = 8.59 (11.6 examples/sec; 5.533 sec/batch)
2016-04-29 17:47:31.671399: step 2791, loss = 8.78 (11.2 examples/sec; 5.720 sec/batch)
2016-04-29 17:47:37.509915: step 2792, loss = 8.82 (11.0 examples/sec; 5.838 sec/batch)
2016-04-29 17:47:43.618527: step 2793, loss = 8.71 (10.5 examples/sec; 6.108 sec/batch)
2016-04-29 17:47:49.153053: step 2794, loss = 8.69 (11.6 examples/sec; 5.534 sec/batch)
2016-04-29 17:47:54.837176: step 2795, loss = 8.59 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 17:48:00.485929: step 2796, loss = 8.85 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 17:48:05.964959: step 2797, loss = 8.64 (11.7 examples/sec; 5.479 sec/batch)
2016-04-29 17:48:11.588481: step 2798, loss = 8.74 (11.4 examples/sec; 5.623 sec/batch)
2016-04-29 17:48:17.978948: step 2799, loss = 8.76 (10.0 examples/sec; 6.390 sec/batch)
2016-04-29 17:48:23.347251: step 2800, loss = 8.77 (11.9 examples/sec; 5.368 sec/batch)
2016-04-29 17:48:36.206355: step 2801, loss = 8.55 (12.1 examples/sec; 5.276 sec/batch)
2016-04-29 17:48:41.526588: step 2802, loss = 8.70 (12.0 examples/sec; 5.320 sec/batch)
2016-04-29 17:48:47.649372: step 2803, loss = 8.79 (10.5 examples/sec; 6.123 sec/batch)
2016-04-29 17:48:53.405064: step 2804, loss = 8.58 (11.1 examples/sec; 5.756 sec/batch)
2016-04-29 17:48:58.843193: step 2805, loss = 8.68 (11.8 examples/sec; 5.438 sec/batch)
2016-04-29 17:49:04.383328: step 2806, loss = 8.63 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 17:49:10.079847: step 2807, loss = 8.63 (11.2 examples/sec; 5.696 sec/batch)
2016-04-29 17:49:15.716421: step 2808, loss = 8.64 (11.4 examples/sec; 5.636 sec/batch)
2016-04-29 17:49:21.808928: step 2809, loss = 8.55 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 17:49:27.562671: step 2810, loss = 8.59 (11.1 examples/sec; 5.754 sec/batch)
2016-04-29 17:49:40.391539: step 2811, loss = 8.61 (12.2 examples/sec; 5.245 sec/batch)
2016-04-29 17:49:46.031602: step 2812, loss = 8.52 (11.3 examples/sec; 5.640 sec/batch)
2016-04-29 17:49:52.146664: step 2813, loss = 8.71 (10.5 examples/sec; 6.115 sec/batch)
2016-04-29 17:49:57.949359: step 2814, loss = 8.64 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 17:50:03.723012: step 2815, loss = 8.58 (11.1 examples/sec; 5.774 sec/batch)
2016-04-29 17:50:09.405007: step 2816, loss = 8.64 (11.3 examples/sec; 5.682 sec/batch)
2016-04-29 17:50:15.006279: step 2817, loss = 8.72 (11.4 examples/sec; 5.601 sec/batch)
2016-04-29 17:50:20.869917: step 2818, loss = 8.57 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 17:50:27.101288: step 2819, loss = 8.59 (10.3 examples/sec; 6.231 sec/batch)
2016-04-29 17:50:32.402869: step 2820, loss = 8.52 (12.1 examples/sec; 5.301 sec/batch)
2016-04-29 17:50:45.238772: step 2821, loss = 8.61 (12.3 examples/sec; 5.212 sec/batch)
2016-04-29 17:50:50.843704: step 2822, loss = 8.49 (11.4 examples/sec; 5.605 sec/batch)
2016-04-29 17:50:56.240202: step 2823, loss = 8.43 (11.9 examples/sec; 5.396 sec/batch)
2016-04-29 17:51:02.319295: step 2824, loss = 8.63 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 17:51:07.866941: step 2825, loss = 8.47 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 17:51:13.407382: step 2826, loss = 8.46 (11.6 examples/sec; 5.540 sec/batch)
2016-04-29 17:51:19.185733: step 2827, loss = 8.54 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 17:51:24.472371: step 2828, loss = 8.59 (12.1 examples/sec; 5.287 sec/batch)
2016-04-29 17:51:29.711618: step 2829, loss = 8.72 (12.2 examples/sec; 5.239 sec/batch)
2016-04-29 17:51:35.881672: step 2830, loss = 8.73 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 17:51:48.860289: step 2831, loss = 8.46 (11.9 examples/sec; 5.397 sec/batch)
2016-04-29 17:51:54.270773: step 2832, loss = 8.49 (11.8 examples/sec; 5.410 sec/batch)
2016-04-29 17:51:59.717916: step 2833, loss = 8.45 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 17:52:05.944265: step 2834, loss = 8.47 (10.3 examples/sec; 6.226 sec/batch)
2016-04-29 17:52:11.511947: step 2835, loss = 8.27 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 17:52:17.278897: step 2836, loss = 8.47 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 17:52:22.854310: step 2837, loss = 8.34 (11.5 examples/sec; 5.575 sec/batch)
2016-04-29 17:52:28.410635: step 2838, loss = 8.50 (11.5 examples/sec; 5.556 sec/batch)
2016-04-29 17:52:34.173011: step 2839, loss = 8.49 (11.1 examples/sec; 5.762 sec/batch)
2016-04-29 17:52:40.629799: step 2840, loss = 8.57 (9.9 examples/sec; 6.457 sec/batch)
2016-04-29 17:52:53.696447: step 2841, loss = 8.50 (12.6 examples/sec; 5.063 sec/batch)
2016-04-29 17:52:59.167869: step 2842, loss = 8.37 (11.7 examples/sec; 5.471 sec/batch)
2016-04-29 17:53:04.920562: step 2843, loss = 8.49 (11.1 examples/sec; 5.753 sec/batch)
2016-04-29 17:53:11.089373: step 2844, loss = 8.61 (10.4 examples/sec; 6.169 sec/batch)
2016-04-29 17:53:16.798564: step 2845, loss = 8.55 (11.2 examples/sec; 5.709 sec/batch)
2016-04-29 17:53:22.505197: step 2846, loss = 8.44 (11.2 examples/sec; 5.707 sec/batch)
2016-04-29 17:53:28.289277: step 2847, loss = 8.41 (11.1 examples/sec; 5.784 sec/batch)
2016-04-29 17:53:33.834236: step 2848, loss = 8.46 (11.5 examples/sec; 5.545 sec/batch)
2016-04-29 17:53:39.151184: step 2849, loss = 8.41 (12.0 examples/sec; 5.317 sec/batch)
2016-04-29 17:53:45.398287: step 2850, loss = 8.39 (10.2 examples/sec; 6.247 sec/batch)
2016-04-29 17:53:58.005704: step 2851, loss = 8.42 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 17:54:03.737088: step 2852, loss = 8.28 (11.2 examples/sec; 5.731 sec/batch)
2016-04-29 17:54:09.637410: step 2853, loss = 8.56 (10.8 examples/sec; 5.900 sec/batch)
2016-04-29 17:54:17.573208: step 2854, loss = 8.45 (8.1 examples/sec; 7.936 sec/batch)
2016-04-29 17:54:23.860075: step 2855, loss = 8.54 (10.2 examples/sec; 6.287 sec/batch)
2016-04-29 17:54:29.920993: step 2856, loss = 8.37 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 17:54:35.761977: step 2857, loss = 8.35 (11.0 examples/sec; 5.841 sec/batch)
2016-04-29 17:54:41.859564: step 2858, loss = 8.46 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 17:54:47.784536: step 2859, loss = 8.31 (10.8 examples/sec; 5.925 sec/batch)
2016-04-29 17:54:54.958891: step 2860, loss = 8.40 (8.9 examples/sec; 7.174 sec/batch)
2016-04-29 17:55:09.950100: step 2861, loss = 8.30 (9.8 examples/sec; 6.508 sec/batch)
2016-04-29 17:55:16.290603: step 2862, loss = 8.46 (10.1 examples/sec; 6.340 sec/batch)
2016-04-29 17:55:22.677615: step 2863, loss = 8.39 (10.0 examples/sec; 6.387 sec/batch)
2016-04-29 17:55:28.562125: step 2864, loss = 8.38 (10.9 examples/sec; 5.884 sec/batch)
2016-04-29 17:55:34.253148: step 2865, loss = 8.28 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 17:55:40.189119: step 2866, loss = 8.39 (10.8 examples/sec; 5.936 sec/batch)
2016-04-29 17:55:45.976142: step 2867, loss = 8.31 (11.1 examples/sec; 5.787 sec/batch)
2016-04-29 17:55:51.458608: step 2868, loss = 8.07 (11.7 examples/sec; 5.482 sec/batch)
2016-04-29 17:55:57.652477: step 2869, loss = 8.39 (10.3 examples/sec; 6.194 sec/batch)
2016-04-29 17:56:03.229427: step 2870, loss = 8.34 (11.5 examples/sec; 5.577 sec/batch)
2016-04-29 17:56:16.304058: step 2871, loss = 8.26 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 17:56:22.039252: step 2872, loss = 8.23 (11.2 examples/sec; 5.735 sec/batch)
2016-04-29 17:56:28.008771: step 2873, loss = 8.21 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 17:56:33.885034: step 2874, loss = 8.28 (10.9 examples/sec; 5.876 sec/batch)
2016-04-29 17:56:39.315827: step 2875, loss = 8.33 (11.8 examples/sec; 5.431 sec/batch)
2016-04-29 17:56:44.532474: step 2876, loss = 8.40 (12.3 examples/sec; 5.217 sec/batch)
2016-04-29 17:56:50.340684: step 2877, loss = 8.31 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 17:56:55.805609: step 2878, loss = 8.24 (11.7 examples/sec; 5.465 sec/batch)
2016-04-29 17:57:02.085853: step 2879, loss = 8.17 (10.2 examples/sec; 6.280 sec/batch)
2016-04-29 17:57:07.658833: step 2880, loss = 8.34 (11.5 examples/sec; 5.573 sec/batch)
2016-04-29 17:57:20.804156: step 2881, loss = 8.33 (12.4 examples/sec; 5.145 sec/batch)
2016-04-29 17:57:26.417687: step 2882, loss = 8.35 (11.4 examples/sec; 5.613 sec/batch)
2016-04-29 17:57:31.941639: step 2883, loss = 8.32 (11.6 examples/sec; 5.524 sec/batch)
2016-04-29 17:57:38.173189: step 2884, loss = 8.23 (10.3 examples/sec; 6.231 sec/batch)
2016-04-29 17:57:43.833913: step 2885, loss = 8.18 (11.3 examples/sec; 5.661 sec/batch)
2016-04-29 17:57:49.395375: step 2886, loss = 8.26 (11.5 examples/sec; 5.561 sec/batch)
2016-04-29 17:57:54.936349: step 2887, loss = 8.28 (11.6 examples/sec; 5.541 sec/batch)
2016-04-29 17:58:00.600453: step 2888, loss = 8.14 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 17:58:06.579698: step 2889, loss = 8.21 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 17:58:12.601860: step 2890, loss = 8.21 (10.6 examples/sec; 6.022 sec/batch)
2016-04-29 17:58:25.614251: step 2891, loss = 8.17 (11.8 examples/sec; 5.437 sec/batch)
2016-04-29 17:58:31.327102: step 2892, loss = 8.09 (11.2 examples/sec; 5.713 sec/batch)
2016-04-29 17:58:36.925821: step 2893, loss = 8.09 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 17:58:42.984977: step 2894, loss = 8.16 (10.6 examples/sec; 6.059 sec/batch)
2016-04-29 17:58:48.311703: step 2895, loss = 8.26 (12.0 examples/sec; 5.327 sec/batch)
2016-04-29 17:58:53.576646: step 2896, loss = 8.17 (12.2 examples/sec; 5.265 sec/batch)
2016-04-29 17:58:59.126457: step 2897, loss = 8.05 (11.5 examples/sec; 5.550 sec/batch)
2016-04-29 17:59:04.861902: step 2898, loss = 8.23 (11.2 examples/sec; 5.735 sec/batch)
2016-04-29 17:59:10.442218: step 2899, loss = 8.17 (11.5 examples/sec; 5.580 sec/batch)
2016-04-29 17:59:16.511817: step 2900, loss = 8.34 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 17:59:29.097709: step 2901, loss = 8.06 (12.1 examples/sec; 5.293 sec/batch)
2016-04-29 17:59:34.827541: step 2902, loss = 8.20 (11.2 examples/sec; 5.730 sec/batch)
2016-04-29 17:59:40.647907: step 2903, loss = 8.13 (11.0 examples/sec; 5.820 sec/batch)
2016-04-29 17:59:46.842257: step 2904, loss = 8.14 (10.3 examples/sec; 6.194 sec/batch)
2016-04-29 17:59:52.552325: step 2905, loss = 8.11 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 17:59:58.194334: step 2906, loss = 8.00 (11.3 examples/sec; 5.642 sec/batch)
2016-04-29 18:00:03.793213: step 2907, loss = 8.14 (11.4 examples/sec; 5.599 sec/batch)
2016-04-29 18:00:09.528295: step 2908, loss = 8.14 (11.2 examples/sec; 5.735 sec/batch)
2016-04-29 18:00:15.400585: step 2909, loss = 8.14 (10.9 examples/sec; 5.872 sec/batch)
2016-04-29 18:00:21.579486: step 2910, loss = 7.93 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 18:00:34.402969: step 2911, loss = 8.03 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 18:00:40.055740: step 2912, loss = 8.30 (11.3 examples/sec; 5.653 sec/batch)
2016-04-29 18:00:45.547530: step 2913, loss = 7.99 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 18:00:51.287445: step 2914, loss = 8.10 (11.2 examples/sec; 5.740 sec/batch)
2016-04-29 18:00:57.612798: step 2915, loss = 8.08 (10.1 examples/sec; 6.325 sec/batch)
2016-04-29 18:01:03.198904: step 2916, loss = 8.11 (11.5 examples/sec; 5.586 sec/batch)
2016-04-29 18:01:08.499035: step 2917, loss = 7.92 (12.1 examples/sec; 5.300 sec/batch)
2016-04-29 18:01:14.021736: step 2918, loss = 8.01 (11.6 examples/sec; 5.523 sec/batch)
2016-04-29 18:01:19.638240: step 2919, loss = 8.06 (11.4 examples/sec; 5.616 sec/batch)
2016-04-29 18:01:25.774763: step 2920, loss = 8.01 (10.4 examples/sec; 6.136 sec/batch)
2016-04-29 18:01:38.499302: step 2921, loss = 8.09 (12.6 examples/sec; 5.069 sec/batch)
2016-04-29 18:01:44.214229: step 2922, loss = 7.95 (11.2 examples/sec; 5.715 sec/batch)
2016-04-29 18:01:49.690606: step 2923, loss = 8.11 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 18:01:55.495157: step 2924, loss = 7.95 (11.0 examples/sec; 5.804 sec/batch)
2016-04-29 18:02:01.885151: step 2925, loss = 7.87 (10.0 examples/sec; 6.390 sec/batch)
2016-04-29 18:02:07.476460: step 2926, loss = 7.87 (11.4 examples/sec; 5.591 sec/batch)
2016-04-29 18:02:13.121163: step 2927, loss = 7.87 (11.3 examples/sec; 5.645 sec/batch)
2016-04-29 18:02:18.987234: step 2928, loss = 7.96 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 18:02:24.698029: step 2929, loss = 7.94 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 18:02:30.210505: step 2930, loss = 7.85 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 18:02:43.635843: step 2931, loss = 8.02 (11.9 examples/sec; 5.356 sec/batch)
2016-04-29 18:02:49.380135: step 2932, loss = 8.07 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 18:02:55.205415: step 2933, loss = 8.02 (11.0 examples/sec; 5.825 sec/batch)
2016-04-29 18:03:01.011853: step 2934, loss = 7.91 (11.0 examples/sec; 5.806 sec/batch)
2016-04-29 18:03:07.002476: step 2935, loss = 8.09 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 18:03:12.292197: step 2936, loss = 7.87 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 18:03:17.808961: step 2937, loss = 7.87 (11.6 examples/sec; 5.517 sec/batch)
2016-04-29 18:03:23.466010: step 2938, loss = 8.09 (11.3 examples/sec; 5.657 sec/batch)
2016-04-29 18:03:29.000500: step 2939, loss = 7.64 (11.6 examples/sec; 5.534 sec/batch)
2016-04-29 18:03:35.060220: step 2940, loss = 7.92 (10.6 examples/sec; 6.060 sec/batch)
2016-04-29 18:03:49.147797: step 2941, loss = 7.81 (11.6 examples/sec; 5.522 sec/batch)
2016-04-29 18:03:54.649905: step 2942, loss = 7.92 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 18:04:00.311250: step 2943, loss = 7.94 (11.3 examples/sec; 5.661 sec/batch)
2016-04-29 18:04:05.854460: step 2944, loss = 7.97 (11.5 examples/sec; 5.543 sec/batch)
2016-04-29 18:04:11.875910: step 2945, loss = 7.90 (10.6 examples/sec; 6.021 sec/batch)
2016-04-29 18:04:17.478158: step 2946, loss = 7.76 (11.4 examples/sec; 5.602 sec/batch)
2016-04-29 18:04:23.225282: step 2947, loss = 7.97 (11.1 examples/sec; 5.747 sec/batch)
2016-04-29 18:04:28.814518: step 2948, loss = 7.96 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 18:04:34.683702: step 2949, loss = 8.06 (10.9 examples/sec; 5.869 sec/batch)
2016-04-29 18:04:40.365577: step 2950, loss = 7.96 (11.3 examples/sec; 5.682 sec/batch)
2016-04-29 18:04:53.750714: step 2951, loss = 7.74 (12.7 examples/sec; 5.054 sec/batch)
2016-04-29 18:04:59.607716: step 2952, loss = 7.81 (10.9 examples/sec; 5.857 sec/batch)
2016-04-29 18:05:05.708305: step 2953, loss = 7.69 (10.5 examples/sec; 6.100 sec/batch)
2016-04-29 18:05:11.256205: step 2954, loss = 7.89 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 18:05:17.597196: step 2955, loss = 7.78 (10.1 examples/sec; 6.341 sec/batch)
2016-04-29 18:05:23.316516: step 2956, loss = 7.89 (11.2 examples/sec; 5.719 sec/batch)
2016-04-29 18:05:29.812804: step 2957, loss = 7.73 (9.9 examples/sec; 6.496 sec/batch)
2016-04-29 18:05:36.726751: step 2958, loss = 8.01 (9.3 examples/sec; 6.914 sec/batch)
2016-04-29 18:05:42.274410: step 2959, loss = 8.02 (11.5 examples/sec; 5.548 sec/batch)
2016-04-29 18:05:47.800125: step 2960, loss = 7.62 (11.6 examples/sec; 5.526 sec/batch)
2016-04-29 18:06:01.633406: step 2961, loss = 7.79 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 18:06:07.606289: step 2962, loss = 8.02 (10.7 examples/sec; 5.973 sec/batch)
2016-04-29 18:06:13.793417: step 2963, loss = 7.68 (10.3 examples/sec; 6.187 sec/batch)
2016-04-29 18:06:20.143856: step 2964, loss = 7.87 (10.1 examples/sec; 6.350 sec/batch)
2016-04-29 18:06:29.725158: step 2965, loss = 7.67 (6.7 examples/sec; 9.581 sec/batch)
2016-04-29 18:06:37.929326: step 2966, loss = 7.72 (7.8 examples/sec; 8.204 sec/batch)
2016-04-29 18:06:47.382883: step 2967, loss = 7.84 (6.8 examples/sec; 9.453 sec/batch)
2016-04-29 18:06:53.708748: step 2968, loss = 7.70 (10.1 examples/sec; 6.326 sec/batch)
2016-04-29 18:07:00.818992: step 2969, loss = 7.79 (9.0 examples/sec; 7.110 sec/batch)
2016-04-29 18:07:07.495866: step 2970, loss = 7.70 (9.6 examples/sec; 6.677 sec/batch)
2016-04-29 18:07:22.995357: step 2971, loss = 7.66 (10.2 examples/sec; 6.267 sec/batch)
2016-04-29 18:07:29.563165: step 2972, loss = 7.78 (9.7 examples/sec; 6.568 sec/batch)
2016-04-29 18:07:37.094951: step 2973, loss = 7.73 (8.5 examples/sec; 7.532 sec/batch)
2016-04-29 18:07:43.689167: step 2974, loss = 7.68 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 18:07:50.483478: step 2975, loss = 7.69 (9.4 examples/sec; 6.794 sec/batch)
2016-04-29 18:07:57.000005: step 2976, loss = 7.62 (9.8 examples/sec; 6.516 sec/batch)
2016-04-29 18:08:02.953251: step 2977, loss = 7.77 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 18:08:10.342582: step 2978, loss = 7.64 (8.7 examples/sec; 7.389 sec/batch)
2016-04-29 18:08:17.262040: step 2979, loss = 7.81 (9.2 examples/sec; 6.919 sec/batch)
2016-04-29 18:08:23.376719: step 2980, loss = 7.65 (10.5 examples/sec; 6.115 sec/batch)
2016-04-29 18:08:39.000405: step 2981, loss = 7.70 (8.8 examples/sec; 7.254 sec/batch)
2016-04-29 18:08:45.598841: step 2982, loss = 7.60 (9.7 examples/sec; 6.598 sec/batch)
2016-04-29 18:08:52.459619: step 2983, loss = 7.82 (9.3 examples/sec; 6.861 sec/batch)
2016-04-29 18:08:58.800429: step 2984, loss = 7.70 (10.1 examples/sec; 6.341 sec/batch)
2016-04-29 18:09:07.174185: step 2985, loss = 7.67 (7.6 examples/sec; 8.374 sec/batch)
2016-04-29 18:09:14.673547: step 2986, loss = 7.66 (8.5 examples/sec; 7.499 sec/batch)
2016-04-29 18:09:20.724434: step 2987, loss = 7.89 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 18:09:26.597540: step 2988, loss = 7.69 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 18:09:32.711173: step 2989, loss = 7.65 (10.5 examples/sec; 6.114 sec/batch)
2016-04-29 18:09:38.744456: step 2990, loss = 7.57 (10.6 examples/sec; 6.033 sec/batch)
2016-04-29 18:09:53.419860: step 2991, loss = 7.72 (10.9 examples/sec; 5.877 sec/batch)
2016-04-29 18:10:01.450207: step 2992, loss = 7.65 (8.0 examples/sec; 8.030 sec/batch)
2016-04-29 18:10:08.102180: step 2993, loss = 7.67 (9.6 examples/sec; 6.652 sec/batch)
2016-04-29 18:10:14.306066: step 2994, loss = 7.73 (10.3 examples/sec; 6.204 sec/batch)
2016-04-29 18:10:21.280992: step 2995, loss = 7.65 (9.2 examples/sec; 6.975 sec/batch)
2016-04-29 18:10:27.376020: step 2996, loss = 7.51 (10.5 examples/sec; 6.095 sec/batch)
2016-04-29 18:10:33.441664: step 2997, loss = 7.65 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 18:10:39.473700: step 2998, loss = 7.33 (10.6 examples/sec; 6.032 sec/batch)
2016-04-29 18:10:45.384225: step 2999, loss = 7.54 (10.8 examples/sec; 5.910 sec/batch)
2016-04-29 18:10:52.158570: step 3000, loss = 7.60 (9.4 examples/sec; 6.774 sec/batch)
2016-04-29 18:11:06.963867: step 3001, loss = 7.59 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 18:11:12.980761: step 3002, loss = 7.74 (10.6 examples/sec; 6.017 sec/batch)
2016-04-29 18:11:19.118955: step 3003, loss = 7.68 (10.4 examples/sec; 6.138 sec/batch)
2016-04-29 18:11:25.961156: step 3004, loss = 7.54 (9.4 examples/sec; 6.842 sec/batch)
2016-04-29 18:11:32.586671: step 3005, loss = 7.66 (9.7 examples/sec; 6.625 sec/batch)
2016-04-29 18:11:38.813682: step 3006, loss = 7.55 (10.3 examples/sec; 6.227 sec/batch)
2016-04-29 18:11:45.025683: step 3007, loss = 7.50 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 18:11:51.024002: step 3008, loss = 7.46 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 18:11:57.966944: step 3009, loss = 7.61 (9.2 examples/sec; 6.943 sec/batch)
2016-04-29 18:12:04.469984: step 3010, loss = 7.73 (9.8 examples/sec; 6.503 sec/batch)
2016-04-29 18:12:19.026704: step 3011, loss = 7.66 (10.6 examples/sec; 6.029 sec/batch)
2016-04-29 18:12:25.146215: step 3012, loss = 7.45 (10.5 examples/sec; 6.119 sec/batch)
2016-04-29 18:12:31.908776: step 3013, loss = 7.64 (9.5 examples/sec; 6.762 sec/batch)
2016-04-29 18:12:38.174026: step 3014, loss = 7.48 (10.2 examples/sec; 6.265 sec/batch)
2016-04-29 18:12:44.439967: step 3015, loss = 7.59 (10.2 examples/sec; 6.266 sec/batch)
2016-04-29 18:12:50.640492: step 3016, loss = 7.60 (10.3 examples/sec; 6.200 sec/batch)
2016-04-29 18:12:56.794881: step 3017, loss = 7.43 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 18:13:03.920187: step 3018, loss = 7.51 (9.0 examples/sec; 7.125 sec/batch)
2016-04-29 18:13:10.279628: step 3019, loss = 7.42 (10.1 examples/sec; 6.359 sec/batch)
2016-04-29 18:13:16.740339: step 3020, loss = 7.55 (9.9 examples/sec; 6.461 sec/batch)
2016-04-29 18:13:31.352116: step 3021, loss = 7.32 (10.5 examples/sec; 6.088 sec/batch)
2016-04-29 18:13:38.307937: step 3022, loss = 7.74 (9.2 examples/sec; 6.955 sec/batch)
2016-04-29 18:13:44.463978: step 3023, loss = 7.59 (10.4 examples/sec; 6.156 sec/batch)
2016-04-29 18:13:50.581088: step 3024, loss = 7.56 (10.5 examples/sec; 6.117 sec/batch)
2016-04-29 18:13:56.769274: step 3025, loss = 7.48 (10.3 examples/sec; 6.188 sec/batch)
2016-04-29 18:14:03.052410: step 3026, loss = 7.50 (10.2 examples/sec; 6.283 sec/batch)
2016-04-29 18:14:10.670448: step 3027, loss = 7.41 (8.4 examples/sec; 7.618 sec/batch)
2016-04-29 18:14:16.639451: step 3028, loss = 7.58 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 18:14:23.175917: step 3029, loss = 7.34 (9.8 examples/sec; 6.536 sec/batch)
2016-04-29 18:14:30.434344: step 3030, loss = 7.45 (8.8 examples/sec; 7.258 sec/batch)
2016-04-29 18:14:46.507788: step 3031, loss = 7.50 (8.6 examples/sec; 7.443 sec/batch)
2016-04-29 18:14:53.727553: step 3032, loss = 7.31 (8.9 examples/sec; 7.220 sec/batch)
2016-04-29 18:15:00.124409: step 3033, loss = 7.45 (10.0 examples/sec; 6.397 sec/batch)
2016-04-29 18:15:06.737884: step 3034, loss = 7.36 (9.7 examples/sec; 6.613 sec/batch)
2016-04-29 18:15:13.094524: step 3035, loss = 7.32 (10.1 examples/sec; 6.357 sec/batch)
2016-04-29 18:15:19.949886: step 3036, loss = 7.55 (9.3 examples/sec; 6.855 sec/batch)
2016-04-29 18:15:26.323230: step 3037, loss = 7.46 (10.0 examples/sec; 6.373 sec/batch)
2016-04-29 18:15:32.668206: step 3038, loss = 7.63 (10.1 examples/sec; 6.345 sec/batch)
2016-04-29 18:15:38.796687: step 3039, loss = 7.37 (10.4 examples/sec; 6.128 sec/batch)
2016-04-29 18:15:44.759858: step 3040, loss = 7.47 (10.7 examples/sec; 5.963 sec/batch)
2016-04-29 18:16:00.081393: step 3041, loss = 7.32 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 18:16:06.587051: step 3042, loss = 7.47 (9.8 examples/sec; 6.506 sec/batch)
2016-04-29 18:16:13.375029: step 3043, loss = 7.56 (9.4 examples/sec; 6.788 sec/batch)
2016-04-29 18:16:20.898024: step 3044, loss = 7.38 (8.5 examples/sec; 7.523 sec/batch)
2016-04-29 18:16:26.702041: step 3045, loss = 7.28 (11.0 examples/sec; 5.804 sec/batch)
2016-04-29 18:16:32.893565: step 3046, loss = 7.35 (10.3 examples/sec; 6.191 sec/batch)
2016-04-29 18:16:39.915114: step 3047, loss = 7.50 (9.1 examples/sec; 7.021 sec/batch)
2016-04-29 18:16:46.095942: step 3048, loss = 7.28 (10.4 examples/sec; 6.181 sec/batch)
2016-04-29 18:16:52.706797: step 3049, loss = 7.47 (9.7 examples/sec; 6.611 sec/batch)
2016-04-29 18:16:59.155478: step 3050, loss = 7.19 (9.9 examples/sec; 6.449 sec/batch)
2016-04-29 18:17:13.948003: step 3051, loss = 7.16 (10.6 examples/sec; 6.028 sec/batch)
2016-04-29 18:17:20.251857: step 3052, loss = 7.58 (10.2 examples/sec; 6.304 sec/batch)
2016-04-29 18:17:27.218236: step 3053, loss = 7.43 (9.2 examples/sec; 6.966 sec/batch)
2016-04-29 18:17:33.579872: step 3054, loss = 7.34 (10.1 examples/sec; 6.362 sec/batch)
2016-04-29 18:17:39.641720: step 3055, loss = 7.27 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 18:17:45.878102: step 3056, loss = 7.22 (10.3 examples/sec; 6.236 sec/batch)
2016-04-29 18:17:52.055743: step 3057, loss = 7.19 (10.4 examples/sec; 6.178 sec/batch)
2016-04-29 18:17:58.267884: step 3058, loss = 7.40 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 18:18:04.891662: step 3059, loss = 7.34 (9.7 examples/sec; 6.624 sec/batch)
2016-04-29 18:18:10.866833: step 3060, loss = 7.36 (10.7 examples/sec; 5.975 sec/batch)
2016-04-29 18:18:25.290320: step 3061, loss = 7.36 (10.6 examples/sec; 6.029 sec/batch)
2016-04-29 18:18:31.865216: step 3062, loss = 7.31 (9.7 examples/sec; 6.575 sec/batch)
2016-04-29 18:18:38.470123: step 3063, loss = 7.23 (9.7 examples/sec; 6.605 sec/batch)
2016-04-29 18:18:44.551061: step 3064, loss = 7.34 (10.5 examples/sec; 6.081 sec/batch)
2016-04-29 18:18:50.704934: step 3065, loss = 7.45 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 18:18:56.681492: step 3066, loss = 7.30 (10.7 examples/sec; 5.976 sec/batch)
2016-04-29 18:19:03.104399: step 3067, loss = 7.29 (10.0 examples/sec; 6.423 sec/batch)
2016-04-29 18:19:10.025317: step 3068, loss = 7.25 (9.2 examples/sec; 6.919 sec/batch)
2016-04-29 18:19:16.125950: step 3069, loss = 7.29 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 18:19:22.175986: step 3070, loss = 7.27 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 18:19:35.886833: step 3071, loss = 7.39 (10.9 examples/sec; 5.847 sec/batch)
2016-04-29 18:19:43.059497: step 3072, loss = 7.30 (8.9 examples/sec; 7.173 sec/batch)
2016-04-29 18:19:49.227237: step 3073, loss = 7.28 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 18:19:55.322786: step 3074, loss = 7.26 (10.5 examples/sec; 6.095 sec/batch)
2016-04-29 18:20:01.545903: step 3075, loss = 7.14 (10.3 examples/sec; 6.223 sec/batch)
2016-04-29 18:20:07.853909: step 3076, loss = 7.36 (10.1 examples/sec; 6.308 sec/batch)
2016-04-29 18:20:14.721265: step 3077, loss = 7.18 (9.3 examples/sec; 6.867 sec/batch)
2016-04-29 18:20:20.786634: step 3078, loss = 7.14 (10.6 examples/sec; 6.065 sec/batch)
2016-04-29 18:20:26.757034: step 3079, loss = 7.19 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 18:20:32.792521: step 3080, loss = 7.16 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 18:20:47.878094: step 3081, loss = 7.24 (9.6 examples/sec; 6.662 sec/batch)
2016-04-29 18:20:53.977475: step 3082, loss = 7.11 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 18:21:00.951182: step 3083, loss = 7.09 (9.2 examples/sec; 6.974 sec/batch)
2016-04-29 18:21:07.256094: step 3084, loss = 7.32 (10.2 examples/sec; 6.305 sec/batch)
2016-04-29 18:21:13.177937: step 3085, loss = 7.09 (10.8 examples/sec; 5.922 sec/batch)
2016-04-29 18:21:20.127927: step 3086, loss = 7.10 (9.2 examples/sec; 6.950 sec/batch)
2016-04-29 18:21:26.364895: step 3087, loss = 7.10 (10.3 examples/sec; 6.237 sec/batch)
2016-04-29 18:21:32.505336: step 3088, loss = 7.35 (10.4 examples/sec; 6.140 sec/batch)
2016-04-29 18:21:38.675824: step 3089, loss = 7.00 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 18:21:44.525326: step 3090, loss = 7.12 (10.9 examples/sec; 5.849 sec/batch)
2016-04-29 18:21:59.156213: step 3091, loss = 7.29 (10.9 examples/sec; 5.891 sec/batch)
2016-04-29 18:22:05.474725: step 3092, loss = 7.10 (10.1 examples/sec; 6.318 sec/batch)
2016-04-29 18:22:11.544948: step 3093, loss = 7.10 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 18:22:17.784448: step 3094, loss = 7.30 (10.3 examples/sec; 6.239 sec/batch)
2016-04-29 18:22:24.533214: step 3095, loss = 7.16 (9.5 examples/sec; 6.749 sec/batch)
2016-04-29 18:22:30.423565: step 3096, loss = 7.22 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 18:22:36.368500: step 3097, loss = 7.12 (10.8 examples/sec; 5.945 sec/batch)
2016-04-29 18:22:42.174099: step 3098, loss = 7.10 (11.0 examples/sec; 5.805 sec/batch)
2016-04-29 18:22:48.228821: step 3099, loss = 7.11 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 18:22:54.376877: step 3100, loss = 7.02 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 18:23:08.974325: step 3101, loss = 7.17 (11.2 examples/sec; 5.722 sec/batch)
2016-04-29 18:23:15.066113: step 3102, loss = 7.01 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 18:23:21.183262: step 3103, loss = 7.13 (10.5 examples/sec; 6.117 sec/batch)
2016-04-29 18:23:28.423191: step 3104, loss = 6.91 (8.8 examples/sec; 7.240 sec/batch)
2016-04-29 18:23:35.433382: step 3105, loss = 7.10 (9.1 examples/sec; 7.010 sec/batch)
2016-04-29 18:23:41.614288: step 3106, loss = 7.09 (10.4 examples/sec; 6.181 sec/batch)
2016-04-29 18:23:47.791285: step 3107, loss = 7.06 (10.4 examples/sec; 6.177 sec/batch)
2016-04-29 18:23:54.141207: step 3108, loss = 6.96 (10.1 examples/sec; 6.350 sec/batch)
2016-04-29 18:24:00.471034: step 3109, loss = 6.89 (10.1 examples/sec; 6.330 sec/batch)
2016-04-29 18:24:07.745419: step 3110, loss = 7.03 (8.8 examples/sec; 7.274 sec/batch)
2016-04-29 18:24:28.847149: step 3111, loss = 7.16 (8.1 examples/sec; 7.879 sec/batch)
2016-04-29 18:24:35.935782: step 3112, loss = 7.08 (9.0 examples/sec; 7.088 sec/batch)
2016-04-29 18:24:42.231892: step 3113, loss = 7.01 (10.2 examples/sec; 6.296 sec/batch)
2016-04-29 18:24:48.540541: step 3114, loss = 6.97 (10.1 examples/sec; 6.309 sec/batch)
2016-04-29 18:24:54.056468: step 3115, loss = 6.96 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 18:24:59.541712: step 3116, loss = 7.07 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 18:25:05.144994: step 3117, loss = 6.92 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 18:25:11.215480: step 3118, loss = 7.05 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 18:25:17.484654: step 3119, loss = 7.03 (10.2 examples/sec; 6.269 sec/batch)
2016-04-29 18:25:23.181469: step 3120, loss = 7.13 (11.2 examples/sec; 5.697 sec/batch)
2016-04-29 18:25:36.370997: step 3121, loss = 7.05 (12.0 examples/sec; 5.354 sec/batch)
2016-04-29 18:25:42.005938: step 3122, loss = 6.94 (11.4 examples/sec; 5.635 sec/batch)
2016-04-29 18:25:48.257556: step 3123, loss = 7.01 (10.2 examples/sec; 6.251 sec/batch)
2016-04-29 18:25:53.863957: step 3124, loss = 7.07 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 18:25:59.484053: step 3125, loss = 6.90 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 18:26:05.188827: step 3126, loss = 7.06 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 18:26:10.701325: step 3127, loss = 7.04 (11.6 examples/sec; 5.512 sec/batch)
2016-04-29 18:26:16.988567: step 3128, loss = 6.95 (10.2 examples/sec; 6.287 sec/batch)
2016-04-29 18:26:22.638471: step 3129, loss = 7.08 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 18:26:28.198048: step 3130, loss = 7.08 (11.5 examples/sec; 5.559 sec/batch)
2016-04-29 18:26:41.138092: step 3131, loss = 6.98 (11.7 examples/sec; 5.450 sec/batch)
2016-04-29 18:26:46.677033: step 3132, loss = 6.98 (11.6 examples/sec; 5.539 sec/batch)
2016-04-29 18:26:52.774620: step 3133, loss = 7.09 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 18:26:58.365123: step 3134, loss = 7.02 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 18:27:03.748145: step 3135, loss = 7.00 (11.9 examples/sec; 5.383 sec/batch)
2016-04-29 18:27:08.955652: step 3136, loss = 6.90 (12.3 examples/sec; 5.207 sec/batch)
2016-04-29 18:27:14.414488: step 3137, loss = 6.89 (11.7 examples/sec; 5.459 sec/batch)
2016-04-29 18:27:20.108084: step 3138, loss = 7.17 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 18:27:26.069468: step 3139, loss = 7.01 (10.7 examples/sec; 5.961 sec/batch)
2016-04-29 18:27:31.517568: step 3140, loss = 7.00 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 18:27:44.533124: step 3141, loss = 6.76 (11.2 examples/sec; 5.706 sec/batch)
2016-04-29 18:27:50.305852: step 3142, loss = 6.88 (11.1 examples/sec; 5.773 sec/batch)
2016-04-29 18:27:57.377401: step 3143, loss = 6.88 (9.1 examples/sec; 7.071 sec/batch)
2016-04-29 18:28:03.618955: step 3144, loss = 7.06 (10.3 examples/sec; 6.241 sec/batch)
2016-04-29 18:28:08.830100: step 3145, loss = 7.07 (12.3 examples/sec; 5.211 sec/batch)
2016-04-29 18:28:14.022351: step 3146, loss = 6.72 (12.3 examples/sec; 5.192 sec/batch)
2016-04-29 18:28:19.452532: step 3147, loss = 6.92 (11.8 examples/sec; 5.430 sec/batch)
2016-04-29 18:28:24.685617: step 3148, loss = 6.83 (12.2 examples/sec; 5.233 sec/batch)
2016-04-29 18:28:31.030515: step 3149, loss = 6.91 (10.1 examples/sec; 6.345 sec/batch)
2016-04-29 18:28:36.555916: step 3150, loss = 6.83 (11.6 examples/sec; 5.525 sec/batch)
2016-04-29 18:28:49.377140: step 3151, loss = 6.86 (12.4 examples/sec; 5.141 sec/batch)
2016-04-29 18:28:54.546680: step 3152, loss = 7.03 (12.4 examples/sec; 5.169 sec/batch)
2016-04-29 18:28:59.938355: step 3153, loss = 7.05 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 18:29:05.588528: step 3154, loss = 6.88 (11.3 examples/sec; 5.650 sec/batch)
2016-04-29 18:29:10.682110: step 3155, loss = 6.87 (12.6 examples/sec; 5.094 sec/batch)
2016-04-29 18:29:15.873650: step 3156, loss = 6.90 (12.3 examples/sec; 5.191 sec/batch)
2016-04-29 18:29:20.829383: step 3157, loss = 6.91 (12.9 examples/sec; 4.956 sec/batch)
2016-04-29 18:29:26.007616: step 3158, loss = 6.71 (12.4 examples/sec; 5.178 sec/batch)
2016-04-29 18:29:31.345942: step 3159, loss = 6.67 (12.0 examples/sec; 5.338 sec/batch)
2016-04-29 18:29:37.296414: step 3160, loss = 6.69 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 18:29:52.399693: step 3161, loss = 6.84 (7.9 examples/sec; 8.073 sec/batch)
2016-04-29 18:30:01.628653: step 3162, loss = 6.74 (6.9 examples/sec; 9.229 sec/batch)
2016-04-29 18:30:10.293947: step 3163, loss = 6.75 (7.4 examples/sec; 8.665 sec/batch)
2016-04-29 18:30:16.890510: step 3164, loss = 6.82 (9.7 examples/sec; 6.595 sec/batch)
2016-04-29 18:30:23.505290: step 3165, loss = 6.92 (9.7 examples/sec; 6.615 sec/batch)
2016-04-29 18:30:30.649620: step 3166, loss = 6.96 (9.0 examples/sec; 7.144 sec/batch)
2016-04-29 18:30:37.395812: step 3167, loss = 6.82 (9.5 examples/sec; 6.746 sec/batch)
2016-04-29 18:30:44.527204: step 3168, loss = 6.68 (9.0 examples/sec; 7.131 sec/batch)
2016-04-29 18:30:50.746268: step 3169, loss = 6.90 (10.3 examples/sec; 6.219 sec/batch)
2016-04-29 18:30:57.327851: step 3170, loss = 6.66 (9.7 examples/sec; 6.581 sec/batch)
2016-04-29 18:31:13.118509: step 3171, loss = 6.78 (9.8 examples/sec; 6.533 sec/batch)
2016-04-29 18:31:20.120540: step 3172, loss = 6.90 (9.1 examples/sec; 7.002 sec/batch)
2016-04-29 18:31:26.622689: step 3173, loss = 6.80 (9.8 examples/sec; 6.502 sec/batch)
2016-04-29 18:31:33.643892: step 3174, loss = 6.72 (9.1 examples/sec; 7.021 sec/batch)
2016-04-29 18:31:40.777779: step 3175, loss = 6.52 (9.0 examples/sec; 7.134 sec/batch)
2016-04-29 18:31:49.632321: step 3176, loss = 6.79 (7.2 examples/sec; 8.854 sec/batch)
2016-04-29 18:31:56.179832: step 3177, loss = 6.95 (9.8 examples/sec; 6.547 sec/batch)
2016-04-29 18:32:03.104967: step 3178, loss = 6.86 (9.2 examples/sec; 6.925 sec/batch)
2016-04-29 18:32:10.144101: step 3179, loss = 6.88 (9.1 examples/sec; 7.039 sec/batch)
2016-04-29 18:32:16.795169: step 3180, loss = 6.88 (9.6 examples/sec; 6.651 sec/batch)
2016-04-29 18:32:37.023979: step 3181, loss = 6.71 (6.5 examples/sec; 9.798 sec/batch)
2016-04-29 18:32:43.663469: step 3182, loss = 6.73 (9.6 examples/sec; 6.639 sec/batch)
2016-04-29 18:32:50.358158: step 3183, loss = 6.81 (9.6 examples/sec; 6.695 sec/batch)
2016-04-29 18:32:58.070399: step 3184, loss = 6.78 (8.3 examples/sec; 7.712 sec/batch)
2016-04-29 18:33:05.031534: step 3185, loss = 6.61 (9.2 examples/sec; 6.961 sec/batch)
2016-04-29 18:33:11.815498: step 3186, loss = 6.66 (9.4 examples/sec; 6.784 sec/batch)
2016-04-29 18:33:18.389480: step 3187, loss = 6.78 (9.7 examples/sec; 6.574 sec/batch)
2016-04-29 18:33:25.596646: step 3188, loss = 6.61 (8.9 examples/sec; 7.207 sec/batch)
2016-04-29 18:33:33.221535: step 3189, loss = 6.47 (8.4 examples/sec; 7.625 sec/batch)
2016-04-29 18:33:40.163432: step 3190, loss = 6.69 (9.2 examples/sec; 6.942 sec/batch)
2016-04-29 18:34:00.163223: step 3191, loss = 6.68 (7.1 examples/sec; 9.021 sec/batch)
2016-04-29 18:34:08.337948: step 3192, loss = 6.62 (7.8 examples/sec; 8.175 sec/batch)
2016-04-29 18:34:15.470805: step 3193, loss = 6.74 (9.0 examples/sec; 7.133 sec/batch)
2016-04-29 18:34:22.826673: step 3194, loss = 6.75 (8.7 examples/sec; 7.356 sec/batch)
2016-04-29 18:34:29.317479: step 3195, loss = 6.72 (9.9 examples/sec; 6.491 sec/batch)
2016-04-29 18:34:37.783556: step 3196, loss = 6.49 (7.6 examples/sec; 8.466 sec/batch)
2016-04-29 18:34:44.682437: step 3197, loss = 6.71 (9.3 examples/sec; 6.899 sec/batch)
2016-04-29 18:34:51.406510: step 3198, loss = 6.60 (9.5 examples/sec; 6.724 sec/batch)
2016-04-29 18:34:58.805256: step 3199, loss = 6.46 (8.7 examples/sec; 7.399 sec/batch)
2016-04-29 18:35:06.711464: step 3200, loss = 6.68 (8.1 examples/sec; 7.906 sec/batch)
2016-04-29 18:35:24.961447: step 3201, loss = 6.71 (8.7 examples/sec; 7.374 sec/batch)
2016-04-29 18:35:32.677944: step 3202, loss = 6.66 (8.3 examples/sec; 7.716 sec/batch)
2016-04-29 18:35:40.727155: step 3203, loss = 6.61 (8.0 examples/sec; 8.049 sec/batch)
2016-04-29 18:35:49.131009: step 3204, loss = 6.54 (7.6 examples/sec; 8.404 sec/batch)
2016-04-29 18:35:55.879911: step 3205, loss = 6.60 (9.5 examples/sec; 6.749 sec/batch)
2016-04-29 18:36:03.422184: step 3206, loss = 6.72 (8.5 examples/sec; 7.542 sec/batch)
2016-04-29 18:36:11.056171: step 3207, loss = 6.46 (8.4 examples/sec; 7.634 sec/batch)
2016-04-29 18:36:19.596437: step 3208, loss = 6.59 (7.5 examples/sec; 8.540 sec/batch)
2016-04-29 18:36:26.944690: step 3209, loss = 6.60 (8.7 examples/sec; 7.348 sec/batch)
2016-04-29 18:36:34.867310: step 3210, loss = 6.65 (8.1 examples/sec; 7.923 sec/batch)
2016-04-29 18:36:54.017367: step 3211, loss = 6.72 (7.2 examples/sec; 8.924 sec/batch)
2016-04-29 18:37:02.626832: step 3212, loss = 6.70 (7.4 examples/sec; 8.609 sec/batch)
2016-04-29 18:37:11.130175: step 3213, loss = 6.51 (7.5 examples/sec; 8.503 sec/batch)
2016-04-29 18:37:18.959814: step 3214, loss = 6.68 (8.2 examples/sec; 7.830 sec/batch)
2016-04-29 18:37:28.259354: step 3215, loss = 6.58 (6.9 examples/sec; 9.299 sec/batch)
2016-04-29 18:37:36.891252: step 3216, loss = 6.47 (7.4 examples/sec; 8.632 sec/batch)
2016-04-29 18:37:45.860910: step 3217, loss = 6.66 (7.1 examples/sec; 8.970 sec/batch)
2016-04-29 18:37:54.121716: step 3218, loss = 6.62 (7.7 examples/sec; 8.261 sec/batch)
2016-04-29 18:38:01.441476: step 3219, loss = 6.61 (8.7 examples/sec; 7.320 sec/batch)
2016-04-29 18:38:08.410412: step 3220, loss = 6.44 (9.2 examples/sec; 6.969 sec/batch)
2016-04-29 18:38:24.439561: step 3221, loss = 6.41 (9.3 examples/sec; 6.918 sec/batch)
2016-04-29 18:38:32.070197: step 3222, loss = 6.53 (8.4 examples/sec; 7.631 sec/batch)
2016-04-29 18:38:38.496936: step 3223, loss = 6.47 (10.0 examples/sec; 6.427 sec/batch)
2016-04-29 18:38:46.625648: step 3224, loss = 6.69 (7.9 examples/sec; 8.129 sec/batch)
2016-04-29 18:38:53.877903: step 3225, loss = 6.76 (8.8 examples/sec; 7.252 sec/batch)
2016-04-29 18:39:01.478001: step 3226, loss = 6.59 (8.4 examples/sec; 7.600 sec/batch)
2016-04-29 18:39:08.052188: step 3227, loss = 6.67 (9.7 examples/sec; 6.574 sec/batch)
2016-04-29 18:39:14.594032: step 3228, loss = 6.56 (9.8 examples/sec; 6.542 sec/batch)
2016-04-29 18:39:21.128446: step 3229, loss = 6.43 (9.8 examples/sec; 6.534 sec/batch)
2016-04-29 18:39:27.508131: step 3230, loss = 6.56 (10.0 examples/sec; 6.380 sec/batch)
2016-04-29 18:39:43.348434: step 3231, loss = 6.68 (9.7 examples/sec; 6.576 sec/batch)
2016-04-29 18:39:49.596141: step 3232, loss = 6.47 (10.2 examples/sec; 6.248 sec/batch)
2016-04-29 18:39:55.997858: step 3233, loss = 6.50 (10.0 examples/sec; 6.402 sec/batch)
2016-04-29 18:40:02.180552: step 3234, loss = 6.67 (10.4 examples/sec; 6.183 sec/batch)
2016-04-29 18:40:09.503359: step 3235, loss = 6.60 (8.7 examples/sec; 7.323 sec/batch)
2016-04-29 18:40:16.275440: step 3236, loss = 6.47 (9.5 examples/sec; 6.772 sec/batch)
2016-04-29 18:40:22.783151: step 3237, loss = 6.69 (9.8 examples/sec; 6.508 sec/batch)
2016-04-29 18:40:29.305700: step 3238, loss = 6.49 (9.8 examples/sec; 6.522 sec/batch)
2016-04-29 18:40:35.735521: step 3239, loss = 6.45 (10.0 examples/sec; 6.430 sec/batch)
2016-04-29 18:40:43.503795: step 3240, loss = 6.46 (8.2 examples/sec; 7.768 sec/batch)
2016-04-29 18:40:58.665064: step 3241, loss = 6.71 (9.8 examples/sec; 6.502 sec/batch)
2016-04-29 18:41:05.144901: step 3242, loss = 6.47 (9.9 examples/sec; 6.480 sec/batch)
2016-04-29 18:41:12.475365: step 3243, loss = 6.33 (8.7 examples/sec; 7.330 sec/batch)
2016-04-29 18:41:18.928368: step 3244, loss = 6.65 (9.9 examples/sec; 6.453 sec/batch)
2016-04-29 18:41:25.565808: step 3245, loss = 6.58 (9.6 examples/sec; 6.637 sec/batch)
2016-04-29 18:41:31.944411: step 3246, loss = 6.53 (10.0 examples/sec; 6.379 sec/batch)
2016-04-29 18:41:38.351260: step 3247, loss = 6.62 (10.0 examples/sec; 6.407 sec/batch)
2016-04-29 18:41:45.716818: step 3248, loss = 6.46 (8.7 examples/sec; 7.365 sec/batch)
2016-04-29 18:41:52.181937: step 3249, loss = 6.62 (9.9 examples/sec; 6.465 sec/batch)
2016-04-29 18:41:58.644728: step 3250, loss = 6.34 (9.9 examples/sec; 6.463 sec/batch)
2016-04-29 18:42:14.138906: step 3251, loss = 6.38 (10.1 examples/sec; 6.337 sec/batch)
2016-04-29 18:42:21.238870: step 3252, loss = 6.26 (9.0 examples/sec; 7.100 sec/batch)
2016-04-29 18:42:27.625976: step 3253, loss = 6.47 (10.0 examples/sec; 6.387 sec/batch)
2016-04-29 18:42:34.306440: step 3254, loss = 6.52 (9.6 examples/sec; 6.680 sec/batch)
2016-04-29 18:42:40.788859: step 3255, loss = 6.51 (9.9 examples/sec; 6.482 sec/batch)
2016-04-29 18:42:46.973291: step 3256, loss = 6.39 (10.3 examples/sec; 6.184 sec/batch)
2016-04-29 18:42:53.931759: step 3257, loss = 6.53 (9.2 examples/sec; 6.958 sec/batch)
2016-04-29 18:43:00.151443: step 3258, loss = 6.46 (10.3 examples/sec; 6.220 sec/batch)
2016-04-29 18:43:07.160938: step 3259, loss = 6.37 (9.1 examples/sec; 7.009 sec/batch)
2016-04-29 18:43:14.646506: step 3260, loss = 6.50 (8.5 examples/sec; 7.485 sec/batch)
2016-04-29 18:43:32.029227: step 3261, loss = 6.33 (9.1 examples/sec; 7.063 sec/batch)
2016-04-29 18:43:39.173850: step 3262, loss = 6.46 (9.0 examples/sec; 7.145 sec/batch)
2016-04-29 18:43:46.268568: step 3263, loss = 6.58 (9.0 examples/sec; 7.095 sec/batch)
2016-04-29 18:43:53.078317: step 3264, loss = 6.37 (9.4 examples/sec; 6.810 sec/batch)
2016-04-29 18:44:00.731700: step 3265, loss = 6.30 (8.4 examples/sec; 7.653 sec/batch)
2016-04-29 18:44:07.840008: step 3266, loss = 6.56 (9.0 examples/sec; 7.108 sec/batch)
2016-04-29 18:44:14.722276: step 3267, loss = 6.48 (9.3 examples/sec; 6.882 sec/batch)
2016-04-29 18:44:22.031146: step 3268, loss = 6.50 (8.8 examples/sec; 7.309 sec/batch)
2016-04-29 18:44:29.621749: step 3269, loss = 6.25 (8.4 examples/sec; 7.591 sec/batch)
2016-04-29 18:44:36.762048: step 3270, loss = 6.37 (9.0 examples/sec; 7.140 sec/batch)
2016-04-29 18:44:53.815226: step 3271, loss = 6.24 (9.1 examples/sec; 7.021 sec/batch)
2016-04-29 18:45:01.301620: step 3272, loss = 6.35 (8.5 examples/sec; 7.486 sec/batch)
2016-04-29 18:45:09.173415: step 3273, loss = 6.55 (8.1 examples/sec; 7.872 sec/batch)
2016-04-29 18:45:16.985817: step 3274, loss = 6.28 (8.2 examples/sec; 7.812 sec/batch)
2016-04-29 18:45:26.746439: step 3275, loss = 6.48 (6.6 examples/sec; 9.761 sec/batch)
2016-04-29 18:45:34.775815: step 3276, loss = 6.30 (8.0 examples/sec; 8.029 sec/batch)
2016-04-29 18:45:41.034473: step 3277, loss = 6.48 (10.2 examples/sec; 6.259 sec/batch)
2016-04-29 18:45:47.072162: step 3278, loss = 6.33 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 18:45:53.000011: step 3279, loss = 6.19 (10.8 examples/sec; 5.928 sec/batch)
2016-04-29 18:45:58.874125: step 3280, loss = 6.20 (10.9 examples/sec; 5.874 sec/batch)
2016-04-29 18:46:12.300436: step 3281, loss = 6.26 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 18:46:17.882397: step 3282, loss = 6.17 (11.5 examples/sec; 5.582 sec/batch)
2016-04-29 18:46:23.360168: step 3283, loss = 6.49 (11.7 examples/sec; 5.478 sec/batch)
2016-04-29 18:46:28.912411: step 3284, loss = 6.40 (11.5 examples/sec; 5.552 sec/batch)
2016-04-29 18:46:34.601292: step 3285, loss = 6.38 (11.3 examples/sec; 5.689 sec/batch)
2016-04-29 18:46:40.011385: step 3286, loss = 6.36 (11.8 examples/sec; 5.410 sec/batch)
2016-04-29 18:46:46.010136: step 3287, loss = 6.33 (10.7 examples/sec; 5.999 sec/batch)
2016-04-29 18:46:51.226308: step 3288, loss = 6.43 (12.3 examples/sec; 5.216 sec/batch)
2016-04-29 18:46:56.679690: step 3289, loss = 6.44 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 18:47:02.126663: step 3290, loss = 6.25 (11.7 examples/sec; 5.447 sec/batch)
2016-04-29 18:47:15.311803: step 3291, loss = 6.27 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 18:47:20.741150: step 3292, loss = 6.24 (11.8 examples/sec; 5.429 sec/batch)
2016-04-29 18:47:26.243485: step 3293, loss = 6.15 (11.6 examples/sec; 5.502 sec/batch)
2016-04-29 18:47:31.792816: step 3294, loss = 6.25 (11.5 examples/sec; 5.549 sec/batch)
2016-04-29 18:47:37.198972: step 3295, loss = 6.06 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 18:47:42.456745: step 3296, loss = 6.12 (12.2 examples/sec; 5.258 sec/batch)
2016-04-29 18:47:48.488814: step 3297, loss = 6.20 (10.6 examples/sec; 6.032 sec/batch)
2016-04-29 18:47:53.854720: step 3298, loss = 6.23 (11.9 examples/sec; 5.366 sec/batch)
2016-04-29 18:47:59.316900: step 3299, loss = 6.37 (11.7 examples/sec; 5.462 sec/batch)
2016-04-29 18:48:04.859268: step 3300, loss = 6.27 (11.5 examples/sec; 5.542 sec/batch)
2016-04-29 18:48:17.712413: step 3301, loss = 6.24 (11.4 examples/sec; 5.611 sec/batch)
2016-04-29 18:48:23.809275: step 3302, loss = 6.10 (10.5 examples/sec; 6.095 sec/batch)
2016-04-29 18:48:29.294512: step 3303, loss = 6.29 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 18:48:34.807995: step 3304, loss = 6.27 (11.6 examples/sec; 5.513 sec/batch)
2016-04-29 18:48:40.166455: step 3305, loss = 6.21 (11.9 examples/sec; 5.358 sec/batch)
2016-04-29 18:48:45.456258: step 3306, loss = 6.37 (12.1 examples/sec; 5.290 sec/batch)
2016-04-29 18:48:50.902944: step 3307, loss = 6.30 (11.8 examples/sec; 5.447 sec/batch)
2016-04-29 18:48:57.048324: step 3308, loss = 6.23 (10.4 examples/sec; 6.145 sec/batch)
2016-04-29 18:49:02.544183: step 3309, loss = 6.22 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 18:49:08.133978: step 3310, loss = 6.17 (11.4 examples/sec; 5.590 sec/batch)
2016-04-29 18:49:20.856764: step 3311, loss = 6.14 (11.7 examples/sec; 5.461 sec/batch)
2016-04-29 18:49:26.967807: step 3312, loss = 6.30 (10.5 examples/sec; 6.109 sec/batch)
2016-04-29 18:49:32.332623: step 3313, loss = 6.35 (11.9 examples/sec; 5.365 sec/batch)
2016-04-29 18:49:37.830009: step 3314, loss = 6.32 (11.6 examples/sec; 5.497 sec/batch)
2016-04-29 18:49:43.238739: step 3315, loss = 6.25 (11.8 examples/sec; 5.409 sec/batch)
2016-04-29 18:49:48.642075: step 3316, loss = 6.26 (11.8 examples/sec; 5.403 sec/batch)
2016-04-29 18:49:54.068892: step 3317, loss = 6.13 (11.8 examples/sec; 5.427 sec/batch)
2016-04-29 18:50:00.099880: step 3318, loss = 6.10 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 18:50:05.597886: step 3319, loss = 6.07 (11.6 examples/sec; 5.498 sec/batch)
2016-04-29 18:50:11.037884: step 3320, loss = 6.34 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 18:50:23.764429: step 3321, loss = 6.19 (11.9 examples/sec; 5.360 sec/batch)
2016-04-29 18:50:29.249306: step 3322, loss = 6.19 (11.7 examples/sec; 5.485 sec/batch)
2016-04-29 18:50:35.386710: step 3323, loss = 6.24 (10.4 examples/sec; 6.137 sec/batch)
2016-04-29 18:50:40.765991: step 3324, loss = 6.20 (11.9 examples/sec; 5.379 sec/batch)
2016-04-29 18:50:46.049445: step 3325, loss = 6.09 (12.1 examples/sec; 5.283 sec/batch)
2016-04-29 18:50:54.537993: step 3326, loss = 6.25 (7.5 examples/sec; 8.488 sec/batch)
2016-04-29 18:51:03.454741: step 3327, loss = 6.28 (7.2 examples/sec; 8.917 sec/batch)
2016-04-29 18:51:12.275544: step 3328, loss = 6.11 (7.3 examples/sec; 8.821 sec/batch)
2016-04-29 18:51:18.451672: step 3329, loss = 6.11 (10.4 examples/sec; 6.176 sec/batch)
2016-04-29 18:51:24.662603: step 3330, loss = 6.21 (10.3 examples/sec; 6.211 sec/batch)
2016-04-29 18:51:39.297209: step 3331, loss = 6.11 (9.8 examples/sec; 6.502 sec/batch)
2016-04-29 18:51:45.270021: step 3332, loss = 6.16 (10.7 examples/sec; 5.973 sec/batch)
2016-04-29 18:51:51.048811: step 3333, loss = 5.88 (11.1 examples/sec; 5.779 sec/batch)
2016-04-29 18:51:57.688765: step 3334, loss = 6.26 (9.6 examples/sec; 6.640 sec/batch)
2016-04-29 18:52:04.886111: step 3335, loss = 6.19 (8.9 examples/sec; 7.197 sec/batch)
2016-04-29 18:52:12.497253: step 3336, loss = 6.02 (8.4 examples/sec; 7.611 sec/batch)
2016-04-29 18:52:19.264803: step 3337, loss = 6.17 (9.5 examples/sec; 6.767 sec/batch)
2016-04-29 18:52:25.068463: step 3338, loss = 6.21 (11.0 examples/sec; 5.804 sec/batch)
2016-04-29 18:52:31.264499: step 3339, loss = 6.12 (10.3 examples/sec; 6.196 sec/batch)
2016-04-29 18:52:36.873200: step 3340, loss = 6.05 (11.4 examples/sec; 5.609 sec/batch)
2016-04-29 18:52:51.138143: step 3341, loss = 5.98 (11.2 examples/sec; 5.734 sec/batch)
2016-04-29 18:52:57.071806: step 3342, loss = 6.12 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 18:53:02.996256: step 3343, loss = 6.10 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 18:53:08.741993: step 3344, loss = 6.04 (11.1 examples/sec; 5.746 sec/batch)
2016-04-29 18:53:14.622124: step 3345, loss = 5.96 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 18:53:21.114889: step 3346, loss = 5.93 (9.9 examples/sec; 6.493 sec/batch)
2016-04-29 18:53:26.904478: step 3347, loss = 6.19 (11.1 examples/sec; 5.789 sec/batch)
2016-04-29 18:53:32.620302: step 3348, loss = 6.24 (11.2 examples/sec; 5.716 sec/batch)
2016-04-29 18:53:38.388899: step 3349, loss = 6.06 (11.1 examples/sec; 5.768 sec/batch)
2016-04-29 18:53:44.216489: step 3350, loss = 6.12 (11.0 examples/sec; 5.828 sec/batch)
2016-04-29 18:53:58.126441: step 3351, loss = 6.04 (12.1 examples/sec; 5.302 sec/batch)
2016-04-29 18:54:03.960220: step 3352, loss = 6.15 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 18:54:09.580666: step 3353, loss = 6.12 (11.4 examples/sec; 5.620 sec/batch)
2016-04-29 18:54:15.358078: step 3354, loss = 6.20 (11.1 examples/sec; 5.777 sec/batch)
2016-04-29 18:54:21.460305: step 3355, loss = 6.17 (10.5 examples/sec; 6.102 sec/batch)
2016-04-29 18:54:28.129806: step 3356, loss = 6.08 (9.6 examples/sec; 6.669 sec/batch)
2016-04-29 18:54:33.953549: step 3357, loss = 6.15 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 18:54:39.722377: step 3358, loss = 6.13 (11.1 examples/sec; 5.769 sec/batch)
2016-04-29 18:54:45.642556: step 3359, loss = 6.01 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 18:54:51.274150: step 3360, loss = 6.11 (11.4 examples/sec; 5.632 sec/batch)
2016-04-29 18:55:05.500435: step 3361, loss = 5.99 (11.0 examples/sec; 5.804 sec/batch)
2016-04-29 18:55:11.470094: step 3362, loss = 5.95 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 18:55:17.210233: step 3363, loss = 6.08 (11.1 examples/sec; 5.740 sec/batch)
2016-04-29 18:55:22.930289: step 3364, loss = 6.18 (11.2 examples/sec; 5.720 sec/batch)
2016-04-29 18:55:29.163333: step 3365, loss = 6.19 (10.3 examples/sec; 6.233 sec/batch)
2016-04-29 18:55:35.176959: step 3366, loss = 6.20 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 18:55:41.051915: step 3367, loss = 5.99 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 18:55:46.942169: step 3368, loss = 6.04 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 18:55:52.794679: step 3369, loss = 6.16 (10.9 examples/sec; 5.852 sec/batch)
2016-04-29 18:55:58.705749: step 3370, loss = 5.84 (10.8 examples/sec; 5.911 sec/batch)
2016-04-29 18:56:12.918274: step 3371, loss = 6.05 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 18:56:18.584501: step 3372, loss = 6.01 (11.3 examples/sec; 5.666 sec/batch)
2016-04-29 18:56:24.474933: step 3373, loss = 6.10 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 18:56:30.354913: step 3374, loss = 5.99 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 18:56:36.693527: step 3375, loss = 6.09 (10.1 examples/sec; 6.339 sec/batch)
2016-04-29 18:56:42.357718: step 3376, loss = 6.03 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 18:56:47.983787: step 3377, loss = 6.09 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 18:56:53.670953: step 3378, loss = 5.98 (11.3 examples/sec; 5.687 sec/batch)
2016-04-29 18:56:59.391310: step 3379, loss = 6.04 (11.2 examples/sec; 5.720 sec/batch)
2016-04-29 18:57:05.230183: step 3380, loss = 6.03 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 18:57:18.902557: step 3381, loss = 5.95 (12.1 examples/sec; 5.292 sec/batch)
2016-04-29 18:57:24.504559: step 3382, loss = 5.93 (11.4 examples/sec; 5.602 sec/batch)
2016-04-29 18:57:30.214459: step 3383, loss = 6.26 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 18:57:36.025251: step 3384, loss = 6.12 (11.0 examples/sec; 5.811 sec/batch)
2016-04-29 18:57:42.447741: step 3385, loss = 5.94 (10.0 examples/sec; 6.422 sec/batch)
2016-04-29 18:57:47.900489: step 3386, loss = 5.91 (11.7 examples/sec; 5.453 sec/batch)
2016-04-29 18:57:53.573722: step 3387, loss = 5.97 (11.3 examples/sec; 5.673 sec/batch)
2016-04-29 18:57:59.365809: step 3388, loss = 5.97 (11.0 examples/sec; 5.792 sec/batch)
2016-04-29 18:58:05.329869: step 3389, loss = 5.97 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 18:58:10.958645: step 3390, loss = 5.88 (11.4 examples/sec; 5.629 sec/batch)
2016-04-29 18:58:24.724467: step 3391, loss = 5.94 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 18:58:30.405474: step 3392, loss = 5.86 (11.3 examples/sec; 5.681 sec/batch)
2016-04-29 18:58:36.263735: step 3393, loss = 5.93 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 18:58:42.166066: step 3394, loss = 5.92 (10.8 examples/sec; 5.902 sec/batch)
2016-04-29 18:58:48.655436: step 3395, loss = 5.90 (9.9 examples/sec; 6.489 sec/batch)
2016-04-29 18:58:54.331007: step 3396, loss = 5.96 (11.3 examples/sec; 5.675 sec/batch)
2016-04-29 18:59:00.158422: step 3397, loss = 5.76 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 18:59:05.957329: step 3398, loss = 5.77 (11.0 examples/sec; 5.799 sec/batch)
2016-04-29 18:59:11.764890: step 3399, loss = 6.00 (11.0 examples/sec; 5.807 sec/batch)
2016-04-29 18:59:17.516050: step 3400, loss = 5.87 (11.1 examples/sec; 5.751 sec/batch)
2016-04-29 18:59:31.692643: step 3401, loss = 5.94 (11.2 examples/sec; 5.727 sec/batch)
2016-04-29 18:59:37.589611: step 3402, loss = 5.92 (10.9 examples/sec; 5.897 sec/batch)
2016-04-29 18:59:43.390307: step 3403, loss = 5.87 (11.0 examples/sec; 5.801 sec/batch)
2016-04-29 18:59:49.277148: step 3404, loss = 5.91 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 18:59:55.846792: step 3405, loss = 5.89 (9.7 examples/sec; 6.570 sec/batch)
2016-04-29 19:00:01.659657: step 3406, loss = 5.79 (11.0 examples/sec; 5.813 sec/batch)
2016-04-29 19:00:07.152133: step 3407, loss = 5.86 (11.7 examples/sec; 5.492 sec/batch)
2016-04-29 19:00:12.862547: step 3408, loss = 5.89 (11.2 examples/sec; 5.710 sec/batch)
2016-04-29 19:00:18.745526: step 3409, loss = 5.73 (10.9 examples/sec; 5.883 sec/batch)
2016-04-29 19:00:24.489528: step 3410, loss = 5.82 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 19:00:38.634928: step 3411, loss = 5.85 (11.4 examples/sec; 5.621 sec/batch)
2016-04-29 19:00:44.695781: step 3412, loss = 5.70 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 19:00:50.592329: step 3413, loss = 5.92 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 19:00:56.453433: step 3414, loss = 5.81 (10.9 examples/sec; 5.861 sec/batch)
2016-04-29 19:01:03.007159: step 3415, loss = 5.82 (9.8 examples/sec; 6.554 sec/batch)
2016-04-29 19:01:08.785313: step 3416, loss = 5.80 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 19:01:14.407431: step 3417, loss = 5.82 (11.4 examples/sec; 5.622 sec/batch)
2016-04-29 19:01:20.174810: step 3418, loss = 5.80 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 19:01:25.912817: step 3419, loss = 5.79 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 19:01:31.429308: step 3420, loss = 5.66 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 19:01:45.072417: step 3421, loss = 5.85 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 19:01:50.996464: step 3422, loss = 5.84 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 19:01:56.762907: step 3423, loss = 5.82 (11.1 examples/sec; 5.766 sec/batch)
2016-04-29 19:02:02.580337: step 3424, loss = 5.98 (11.0 examples/sec; 5.817 sec/batch)
2016-04-29 19:02:08.884449: step 3425, loss = 5.64 (10.2 examples/sec; 6.304 sec/batch)
2016-04-29 19:02:14.946337: step 3426, loss = 5.82 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 19:02:20.434786: step 3427, loss = 6.00 (11.7 examples/sec; 5.488 sec/batch)
2016-04-29 19:02:26.174691: step 3428, loss = 5.70 (11.2 examples/sec; 5.740 sec/batch)
2016-04-29 19:02:31.917045: step 3429, loss = 5.89 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 19:02:37.606533: step 3430, loss = 5.85 (11.2 examples/sec; 5.689 sec/batch)
2016-04-29 19:02:51.623189: step 3431, loss = 5.64 (11.1 examples/sec; 5.771 sec/batch)
2016-04-29 19:02:57.348447: step 3432, loss = 5.81 (11.2 examples/sec; 5.725 sec/batch)
2016-04-29 19:03:03.110168: step 3433, loss = 5.76 (11.1 examples/sec; 5.762 sec/batch)
2016-04-29 19:03:08.847394: step 3434, loss = 5.71 (11.2 examples/sec; 5.737 sec/batch)
2016-04-29 19:03:15.081203: step 3435, loss = 5.72 (10.3 examples/sec; 6.234 sec/batch)
2016-04-29 19:03:20.687948: step 3436, loss = 5.76 (11.4 examples/sec; 5.607 sec/batch)
2016-04-29 19:03:26.542478: step 3437, loss = 5.82 (10.9 examples/sec; 5.854 sec/batch)
2016-04-29 19:03:32.417479: step 3438, loss = 5.62 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 19:03:38.269599: step 3439, loss = 5.81 (10.9 examples/sec; 5.852 sec/batch)
2016-04-29 19:03:44.986965: step 3440, loss = 5.62 (9.5 examples/sec; 6.717 sec/batch)
2016-04-29 19:03:58.944531: step 3441, loss = 5.81 (11.0 examples/sec; 5.840 sec/batch)
2016-04-29 19:04:05.365830: step 3442, loss = 5.84 (10.0 examples/sec; 6.421 sec/batch)
2016-04-29 19:04:11.298831: step 3443, loss = 5.61 (10.8 examples/sec; 5.933 sec/batch)
2016-04-29 19:04:18.203032: step 3444, loss = 5.61 (9.3 examples/sec; 6.904 sec/batch)
2016-04-29 19:04:24.079078: step 3445, loss = 5.78 (10.9 examples/sec; 5.876 sec/batch)
2016-04-29 19:04:30.082918: step 3446, loss = 5.65 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 19:04:36.100742: step 3447, loss = 5.68 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 19:04:42.019634: step 3448, loss = 5.58 (10.8 examples/sec; 5.919 sec/batch)
2016-04-29 19:04:48.072477: step 3449, loss = 5.75 (10.6 examples/sec; 6.053 sec/batch)
2016-04-29 19:04:54.607080: step 3450, loss = 5.75 (9.8 examples/sec; 6.534 sec/batch)
2016-04-29 19:05:08.756062: step 3451, loss = 5.62 (10.9 examples/sec; 5.861 sec/batch)
2016-04-29 19:05:14.626158: step 3452, loss = 5.71 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 19:05:20.453675: step 3453, loss = 5.57 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 19:05:27.124183: step 3454, loss = 5.65 (9.6 examples/sec; 6.670 sec/batch)
2016-04-29 19:05:33.041691: step 3455, loss = 5.76 (10.8 examples/sec; 5.917 sec/batch)
2016-04-29 19:05:39.131396: step 3456, loss = 5.52 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 19:05:45.137163: step 3457, loss = 5.71 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 19:05:51.116619: step 3458, loss = 5.54 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 19:05:57.819679: step 3459, loss = 5.58 (9.5 examples/sec; 6.703 sec/batch)
2016-04-29 19:06:04.007565: step 3460, loss = 5.60 (10.3 examples/sec; 6.188 sec/batch)
2016-04-29 19:06:18.000002: step 3461, loss = 5.85 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 19:06:23.978003: step 3462, loss = 5.54 (10.7 examples/sec; 5.978 sec/batch)
2016-04-29 19:06:30.420450: step 3463, loss = 5.60 (9.9 examples/sec; 6.442 sec/batch)
2016-04-29 19:06:36.185927: step 3464, loss = 5.64 (11.1 examples/sec; 5.765 sec/batch)
2016-04-29 19:06:42.309160: step 3465, loss = 5.70 (10.5 examples/sec; 6.123 sec/batch)
2016-04-29 19:06:48.252584: step 3466, loss = 5.58 (10.8 examples/sec; 5.943 sec/batch)
2016-04-29 19:06:54.413354: step 3467, loss = 5.76 (10.4 examples/sec; 6.161 sec/batch)
2016-04-29 19:07:00.690775: step 3468, loss = 5.64 (10.2 examples/sec; 6.277 sec/batch)
2016-04-29 19:07:07.506211: step 3469, loss = 5.69 (9.4 examples/sec; 6.815 sec/batch)
2016-04-29 19:07:13.344921: step 3470, loss = 5.66 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 19:07:27.148888: step 3471, loss = 5.59 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 19:07:33.294694: step 3472, loss = 5.77 (10.4 examples/sec; 6.146 sec/batch)
2016-04-29 19:07:39.931858: step 3473, loss = 5.73 (9.6 examples/sec; 6.637 sec/batch)
2016-04-29 19:07:45.846684: step 3474, loss = 5.72 (10.8 examples/sec; 5.915 sec/batch)
2016-04-29 19:07:51.876569: step 3475, loss = 5.69 (10.6 examples/sec; 6.030 sec/batch)
2016-04-29 19:07:57.664451: step 3476, loss = 5.68 (11.1 examples/sec; 5.788 sec/batch)
2016-04-29 19:08:03.775171: step 3477, loss = 5.86 (10.5 examples/sec; 6.111 sec/batch)
2016-04-29 19:08:10.613436: step 3478, loss = 5.64 (9.4 examples/sec; 6.838 sec/batch)
2016-04-29 19:08:17.059456: step 3479, loss = 5.65 (9.9 examples/sec; 6.446 sec/batch)
2016-04-29 19:08:23.472062: step 3480, loss = 5.56 (10.0 examples/sec; 6.413 sec/batch)
2016-04-29 19:08:37.915071: step 3481, loss = 5.47 (10.7 examples/sec; 5.997 sec/batch)
2016-04-29 19:08:44.560010: step 3482, loss = 5.61 (9.6 examples/sec; 6.645 sec/batch)
2016-04-29 19:08:50.650217: step 3483, loss = 5.76 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 19:08:56.637940: step 3484, loss = 5.73 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 19:09:02.736351: step 3485, loss = 5.60 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 19:09:09.078246: step 3486, loss = 5.55 (10.1 examples/sec; 6.342 sec/batch)
2016-04-29 19:09:15.810737: step 3487, loss = 5.71 (9.5 examples/sec; 6.732 sec/batch)
2016-04-29 19:09:21.734281: step 3488, loss = 5.55 (10.8 examples/sec; 5.923 sec/batch)
2016-04-29 19:09:27.634892: step 3489, loss = 5.58 (10.8 examples/sec; 5.901 sec/batch)
2016-04-29 19:09:33.762207: step 3490, loss = 5.68 (10.4 examples/sec; 6.127 sec/batch)
2016-04-29 19:09:48.589887: step 3491, loss = 5.67 (10.0 examples/sec; 6.408 sec/batch)
2016-04-29 19:09:54.506294: step 3492, loss = 5.59 (10.8 examples/sec; 5.916 sec/batch)
2016-04-29 19:10:00.735144: step 3493, loss = 5.56 (10.3 examples/sec; 6.229 sec/batch)
2016-04-29 19:10:06.342429: step 3494, loss = 5.47 (11.4 examples/sec; 5.607 sec/batch)
2016-04-29 19:10:12.274355: step 3495, loss = 5.67 (10.8 examples/sec; 5.932 sec/batch)
2016-04-29 19:10:18.439853: step 3496, loss = 5.49 (10.4 examples/sec; 6.165 sec/batch)
2016-04-29 19:10:25.488457: step 3497, loss = 5.41 (9.1 examples/sec; 7.049 sec/batch)
2016-04-29 19:10:31.284470: step 3498, loss = 5.53 (11.0 examples/sec; 5.796 sec/batch)
2016-04-29 19:10:37.369453: step 3499, loss = 5.73 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 19:10:43.360966: step 3500, loss = 5.57 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 19:10:58.014155: step 3501, loss = 5.70 (9.9 examples/sec; 6.442 sec/batch)
2016-04-29 19:11:04.459986: step 3502, loss = 5.40 (9.9 examples/sec; 6.446 sec/batch)
2016-04-29 19:11:10.447267: step 3503, loss = 5.40 (10.7 examples/sec; 5.987 sec/batch)
2016-04-29 19:11:16.640302: step 3504, loss = 5.53 (10.3 examples/sec; 6.193 sec/batch)
2016-04-29 19:11:22.652531: step 3505, loss = 5.52 (10.6 examples/sec; 6.012 sec/batch)
2016-04-29 19:11:29.372482: step 3506, loss = 5.47 (9.5 examples/sec; 6.720 sec/batch)
2016-04-29 19:11:35.258129: step 3507, loss = 5.57 (10.9 examples/sec; 5.886 sec/batch)
2016-04-29 19:11:41.128465: step 3508, loss = 5.58 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 19:11:47.515623: step 3509, loss = 5.52 (10.0 examples/sec; 6.387 sec/batch)
2016-04-29 19:11:55.354202: step 3510, loss = 5.58 (8.2 examples/sec; 7.838 sec/batch)
2016-04-29 19:12:11.562615: step 3511, loss = 5.57 (10.9 examples/sec; 5.851 sec/batch)
2016-04-29 19:12:17.994662: step 3512, loss = 5.35 (10.0 examples/sec; 6.432 sec/batch)
2016-04-29 19:12:23.879934: step 3513, loss = 5.48 (10.9 examples/sec; 5.885 sec/batch)
2016-04-29 19:12:30.394440: step 3514, loss = 5.39 (9.8 examples/sec; 6.514 sec/batch)
2016-04-29 19:12:36.744744: step 3515, loss = 5.38 (10.1 examples/sec; 6.350 sec/batch)
2016-04-29 19:12:42.625773: step 3516, loss = 5.45 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 19:12:47.759803: step 3517, loss = 5.40 (12.5 examples/sec; 5.134 sec/batch)
2016-04-29 19:12:52.922396: step 3518, loss = 5.22 (12.4 examples/sec; 5.162 sec/batch)
2016-04-29 19:12:57.983652: step 3519, loss = 5.59 (12.6 examples/sec; 5.061 sec/batch)
2016-04-29 19:13:03.316939: step 3520, loss = 5.44 (12.0 examples/sec; 5.333 sec/batch)
2016-04-29 19:13:15.822782: step 3521, loss = 5.48 (12.9 examples/sec; 4.978 sec/batch)
2016-04-29 19:13:20.826000: step 3522, loss = 5.42 (12.8 examples/sec; 5.003 sec/batch)
2016-04-29 19:13:26.053149: step 3523, loss = 5.49 (12.2 examples/sec; 5.227 sec/batch)
2016-04-29 19:13:31.747476: step 3524, loss = 5.53 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 19:13:37.153265: step 3525, loss = 5.33 (11.8 examples/sec; 5.406 sec/batch)
2016-04-29 19:13:43.122741: step 3526, loss = 5.46 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 19:13:48.400780: step 3527, loss = 5.44 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 19:13:53.826326: step 3528, loss = 5.40 (11.8 examples/sec; 5.425 sec/batch)
2016-04-29 19:13:59.390336: step 3529, loss = 5.36 (11.5 examples/sec; 5.564 sec/batch)
2016-04-29 19:14:04.941258: step 3530, loss = 5.47 (11.5 examples/sec; 5.551 sec/batch)
2016-04-29 19:14:18.129654: step 3531, loss = 5.31 (10.8 examples/sec; 5.921 sec/batch)
2016-04-29 19:14:23.626006: step 3532, loss = 5.33 (11.6 examples/sec; 5.496 sec/batch)
2016-04-29 19:14:29.014332: step 3533, loss = 5.45 (11.9 examples/sec; 5.388 sec/batch)
2016-04-29 19:14:34.454061: step 3534, loss = 5.46 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 19:14:39.732561: step 3535, loss = 5.40 (12.1 examples/sec; 5.278 sec/batch)
2016-04-29 19:14:45.316451: step 3536, loss = 5.36 (11.5 examples/sec; 5.584 sec/batch)
2016-04-29 19:14:51.190570: step 3537, loss = 5.50 (10.9 examples/sec; 5.874 sec/batch)
2016-04-29 19:14:56.735130: step 3538, loss = 5.50 (11.5 examples/sec; 5.544 sec/batch)
2016-04-29 19:15:02.269230: step 3539, loss = 5.55 (11.6 examples/sec; 5.534 sec/batch)
2016-04-29 19:15:07.875233: step 3540, loss = 5.53 (11.4 examples/sec; 5.606 sec/batch)
2016-04-29 19:15:21.133092: step 3541, loss = 5.30 (10.8 examples/sec; 5.943 sec/batch)
2016-04-29 19:15:26.736729: step 3542, loss = 5.52 (11.4 examples/sec; 5.604 sec/batch)
2016-04-29 19:15:32.141722: step 3543, loss = 5.32 (11.8 examples/sec; 5.405 sec/batch)
2016-04-29 19:15:37.709666: step 3544, loss = 5.44 (11.5 examples/sec; 5.568 sec/batch)
2016-04-29 19:15:43.109705: step 3545, loss = 5.45 (11.9 examples/sec; 5.400 sec/batch)
2016-04-29 19:15:48.537393: step 3546, loss = 5.56 (11.8 examples/sec; 5.428 sec/batch)
2016-04-29 19:15:54.675731: step 3547, loss = 5.40 (10.4 examples/sec; 6.138 sec/batch)
2016-04-29 19:16:00.151418: step 3548, loss = 5.42 (11.7 examples/sec; 5.476 sec/batch)
2016-04-29 19:16:05.574154: step 3549, loss = 5.65 (11.8 examples/sec; 5.423 sec/batch)
2016-04-29 19:16:11.022225: step 3550, loss = 5.41 (11.7 examples/sec; 5.448 sec/batch)
2016-04-29 19:16:23.751494: step 3551, loss = 5.34 (11.4 examples/sec; 5.595 sec/batch)
2016-04-29 19:16:29.754217: step 3552, loss = 5.47 (10.7 examples/sec; 6.003 sec/batch)
2016-04-29 19:16:35.406734: step 3553, loss = 5.32 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 19:16:40.763443: step 3554, loss = 5.55 (11.9 examples/sec; 5.357 sec/batch)
2016-04-29 19:16:45.995085: step 3555, loss = 5.46 (12.2 examples/sec; 5.232 sec/batch)
2016-04-29 19:16:51.686938: step 3556, loss = 5.37 (11.2 examples/sec; 5.692 sec/batch)
2016-04-29 19:16:57.203498: step 3557, loss = 5.33 (11.6 examples/sec; 5.516 sec/batch)
2016-04-29 19:17:03.426377: step 3558, loss = 5.44 (10.3 examples/sec; 6.223 sec/batch)
2016-04-29 19:17:08.936467: step 3559, loss = 5.21 (11.6 examples/sec; 5.510 sec/batch)
2016-04-29 19:17:14.321890: step 3560, loss = 5.26 (11.9 examples/sec; 5.385 sec/batch)
2016-04-29 19:17:27.139155: step 3561, loss = 5.31 (11.8 examples/sec; 5.441 sec/batch)
2016-04-29 19:17:33.150595: step 3562, loss = 5.22 (10.6 examples/sec; 6.011 sec/batch)
2016-04-29 19:17:38.833991: step 3563, loss = 5.25 (11.3 examples/sec; 5.683 sec/batch)
2016-04-29 19:17:44.539366: step 3564, loss = 5.31 (11.2 examples/sec; 5.705 sec/batch)
2016-04-29 19:17:50.014358: step 3565, loss = 5.29 (11.7 examples/sec; 5.475 sec/batch)
2016-04-29 19:17:55.205774: step 3566, loss = 5.22 (12.3 examples/sec; 5.191 sec/batch)
2016-04-29 19:18:00.784660: step 3567, loss = 5.11 (11.5 examples/sec; 5.579 sec/batch)
2016-04-29 19:18:07.048663: step 3568, loss = 5.25 (10.2 examples/sec; 6.264 sec/batch)
2016-04-29 19:18:12.627021: step 3569, loss = 5.23 (11.5 examples/sec; 5.578 sec/batch)
2016-04-29 19:18:18.019110: step 3570, loss = 5.39 (11.9 examples/sec; 5.392 sec/batch)
2016-04-29 19:18:32.222489: step 3571, loss = 5.29 (9.1 examples/sec; 7.038 sec/batch)
2016-04-29 19:18:42.057519: step 3572, loss = 5.25 (6.5 examples/sec; 9.835 sec/batch)
2016-04-29 19:18:50.850973: step 3573, loss = 5.31 (7.3 examples/sec; 8.793 sec/batch)
2016-04-29 19:18:58.038369: step 3574, loss = 5.39 (8.9 examples/sec; 7.187 sec/batch)
2016-04-29 19:19:05.343978: step 3575, loss = 5.26 (8.8 examples/sec; 7.305 sec/batch)
2016-04-29 19:19:12.780614: step 3576, loss = 5.09 (8.6 examples/sec; 7.437 sec/batch)
2016-04-29 19:19:20.589363: step 3577, loss = 5.46 (8.2 examples/sec; 7.809 sec/batch)
2016-04-29 19:19:27.497810: step 3578, loss = 5.47 (9.3 examples/sec; 6.908 sec/batch)
2016-04-29 19:19:33.787884: step 3579, loss = 5.24 (10.2 examples/sec; 6.290 sec/batch)
2016-04-29 19:19:39.872417: step 3580, loss = 5.41 (10.5 examples/sec; 6.084 sec/batch)
2016-04-29 19:19:54.535184: step 3581, loss = 5.14 (10.9 examples/sec; 5.851 sec/batch)
2016-04-29 19:20:00.658804: step 3582, loss = 5.25 (10.5 examples/sec; 6.124 sec/batch)
2016-04-29 19:20:06.538597: step 3583, loss = 5.15 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 19:20:12.476358: step 3584, loss = 5.32 (10.8 examples/sec; 5.938 sec/batch)
2016-04-29 19:20:19.001062: step 3585, loss = 5.38 (9.8 examples/sec; 6.525 sec/batch)
2016-04-29 19:20:25.081893: step 3586, loss = 5.34 (10.5 examples/sec; 6.081 sec/batch)
2016-04-29 19:20:31.041678: step 3587, loss = 5.36 (10.7 examples/sec; 5.960 sec/batch)
2016-04-29 19:20:37.013828: step 3588, loss = 5.16 (10.7 examples/sec; 5.972 sec/batch)
2016-04-29 19:20:43.154061: step 3589, loss = 5.15 (10.4 examples/sec; 6.140 sec/batch)
2016-04-29 19:20:49.136470: step 3590, loss = 5.22 (10.7 examples/sec; 5.982 sec/batch)
2016-04-29 19:21:03.960078: step 3591, loss = 5.19 (10.7 examples/sec; 5.996 sec/batch)
2016-04-29 19:21:10.089077: step 3592, loss = 5.24 (10.4 examples/sec; 6.129 sec/batch)
2016-04-29 19:21:16.145307: step 3593, loss = 5.29 (10.6 examples/sec; 6.056 sec/batch)
2016-04-29 19:21:22.031982: step 3594, loss = 5.15 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 19:21:28.932469: step 3595, loss = 5.30 (9.3 examples/sec; 6.900 sec/batch)
2016-04-29 19:21:35.080947: step 3596, loss = 5.23 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 19:21:41.050718: step 3597, loss = 5.05 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 19:21:47.263986: step 3598, loss = 5.21 (10.3 examples/sec; 6.213 sec/batch)
2016-04-29 19:21:53.462941: step 3599, loss = 5.04 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 19:22:00.056999: step 3600, loss = 5.05 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 19:22:13.834503: step 3601, loss = 5.01 (11.3 examples/sec; 5.659 sec/batch)
2016-04-29 19:22:19.768898: step 3602, loss = 5.29 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 19:22:25.548468: step 3603, loss = 5.17 (11.1 examples/sec; 5.779 sec/batch)
2016-04-29 19:22:32.307325: step 3604, loss = 5.20 (9.5 examples/sec; 6.759 sec/batch)
2016-04-29 19:22:38.329535: step 3605, loss = 5.32 (10.6 examples/sec; 6.022 sec/batch)
2016-04-29 19:22:44.268907: step 3606, loss = 5.10 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 19:22:50.226075: step 3607, loss = 5.29 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 19:22:56.086576: step 3608, loss = 5.26 (10.9 examples/sec; 5.860 sec/batch)
2016-04-29 19:23:02.503173: step 3609, loss = 5.22 (10.0 examples/sec; 6.417 sec/batch)
2016-04-29 19:23:08.863870: step 3610, loss = 5.17 (10.1 examples/sec; 6.361 sec/batch)
2016-04-29 19:23:22.602532: step 3611, loss = 5.22 (11.1 examples/sec; 5.783 sec/batch)
2016-04-29 19:23:28.683000: step 3612, loss = 5.30 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 19:23:35.387787: step 3613, loss = 4.91 (9.5 examples/sec; 6.705 sec/batch)
2016-04-29 19:23:41.870925: step 3614, loss = 5.29 (9.9 examples/sec; 6.483 sec/batch)
2016-04-29 19:23:48.198329: step 3615, loss = 5.13 (10.1 examples/sec; 6.327 sec/batch)
2016-04-29 19:23:54.602965: step 3616, loss = 5.28 (10.0 examples/sec; 6.405 sec/batch)
2016-04-29 19:24:00.918379: step 3617, loss = 5.07 (10.1 examples/sec; 6.313 sec/batch)
2016-04-29 19:24:06.862125: step 3618, loss = 5.17 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 19:24:15.205131: step 3619, loss = 5.06 (7.7 examples/sec; 8.343 sec/batch)
2016-04-29 19:24:21.661686: step 3620, loss = 5.12 (9.9 examples/sec; 6.456 sec/batch)
2016-04-29 19:24:36.092465: step 3621, loss = 5.12 (10.4 examples/sec; 6.162 sec/batch)
2016-04-29 19:24:43.022801: step 3622, loss = 5.21 (9.2 examples/sec; 6.930 sec/batch)
2016-04-29 19:24:48.924802: step 3623, loss = 5.31 (10.8 examples/sec; 5.902 sec/batch)
2016-04-29 19:24:54.989766: step 3624, loss = 5.20 (10.6 examples/sec; 6.065 sec/batch)
2016-04-29 19:25:01.358100: step 3625, loss = 5.10 (10.0 examples/sec; 6.368 sec/batch)
2016-04-29 19:25:07.537978: step 3626, loss = 5.10 (10.4 examples/sec; 6.180 sec/batch)
2016-04-29 19:25:13.275825: step 3627, loss = 5.12 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 19:25:20.048575: step 3628, loss = 5.22 (9.4 examples/sec; 6.773 sec/batch)
2016-04-29 19:25:25.805521: step 3629, loss = 5.07 (11.1 examples/sec; 5.757 sec/batch)
2016-04-29 19:25:31.547953: step 3630, loss = 5.21 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 19:25:45.281852: step 3631, loss = 5.04 (11.0 examples/sec; 5.819 sec/batch)
2016-04-29 19:25:51.989716: step 3632, loss = 5.24 (9.5 examples/sec; 6.708 sec/batch)
2016-04-29 19:25:58.048188: step 3633, loss = 5.02 (10.6 examples/sec; 6.058 sec/batch)
2016-04-29 19:26:04.086420: step 3634, loss = 5.14 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 19:26:10.063790: step 3635, loss = 5.12 (10.7 examples/sec; 5.977 sec/batch)
2016-04-29 19:26:15.988250: step 3636, loss = 5.28 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 19:26:22.652248: step 3637, loss = 5.13 (9.6 examples/sec; 6.664 sec/batch)
2016-04-29 19:26:28.511373: step 3638, loss = 5.11 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 19:26:34.499078: step 3639, loss = 5.12 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 19:26:40.629031: step 3640, loss = 5.21 (10.4 examples/sec; 6.130 sec/batch)
2016-04-29 19:26:54.932472: step 3641, loss = 5.05 (10.1 examples/sec; 6.332 sec/batch)
2016-04-29 19:27:01.344486: step 3642, loss = 5.10 (10.0 examples/sec; 6.412 sec/batch)
2016-04-29 19:27:07.035793: step 3643, loss = 5.02 (11.2 examples/sec; 5.690 sec/batch)
2016-04-29 19:27:13.042872: step 3644, loss = 5.03 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 19:27:19.048507: step 3645, loss = 5.07 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 19:27:25.018567: step 3646, loss = 5.26 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 19:27:31.920442: step 3647, loss = 4.98 (9.3 examples/sec; 6.902 sec/batch)
2016-04-29 19:27:37.782100: step 3648, loss = 5.17 (10.9 examples/sec; 5.862 sec/batch)
2016-04-29 19:27:43.626572: step 3649, loss = 4.98 (11.0 examples/sec; 5.844 sec/batch)
2016-04-29 19:27:49.712584: step 3650, loss = 5.10 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 19:28:04.104797: step 3651, loss = 5.07 (10.0 examples/sec; 6.416 sec/batch)
2016-04-29 19:28:10.215273: step 3652, loss = 5.24 (10.5 examples/sec; 6.110 sec/batch)
2016-04-29 19:28:16.210088: step 3653, loss = 5.12 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 19:28:22.251926: step 3654, loss = 5.09 (10.6 examples/sec; 6.042 sec/batch)
2016-04-29 19:28:28.229133: step 3655, loss = 5.17 (10.7 examples/sec; 5.977 sec/batch)
2016-04-29 19:28:34.601814: step 3656, loss = 4.91 (10.0 examples/sec; 6.373 sec/batch)
2016-04-29 19:28:40.698669: step 3657, loss = 5.18 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 19:28:46.579519: step 3658, loss = 5.02 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 19:28:52.539621: step 3659, loss = 4.98 (10.7 examples/sec; 5.960 sec/batch)
2016-04-29 19:28:58.559379: step 3660, loss = 4.91 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 19:29:13.155512: step 3661, loss = 5.05 (11.0 examples/sec; 5.797 sec/batch)
2016-04-29 19:29:19.200225: step 3662, loss = 4.98 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 19:29:25.315494: step 3663, loss = 5.11 (10.5 examples/sec; 6.115 sec/batch)
2016-04-29 19:29:31.260378: step 3664, loss = 4.91 (10.8 examples/sec; 5.945 sec/batch)
2016-04-29 19:29:37.463703: step 3665, loss = 5.17 (10.3 examples/sec; 6.203 sec/batch)
2016-04-29 19:29:44.124344: step 3666, loss = 4.96 (9.6 examples/sec; 6.661 sec/batch)
2016-04-29 19:29:49.922371: step 3667, loss = 4.94 (11.0 examples/sec; 5.798 sec/batch)
2016-04-29 19:29:55.776309: step 3668, loss = 4.93 (10.9 examples/sec; 5.854 sec/batch)
2016-04-29 19:30:02.120917: step 3669, loss = 5.23 (10.1 examples/sec; 6.345 sec/batch)
2016-04-29 19:30:07.955134: step 3670, loss = 4.91 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 19:30:22.601886: step 3671, loss = 5.00 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 19:30:28.398392: step 3672, loss = 5.11 (11.0 examples/sec; 5.796 sec/batch)
2016-04-29 19:30:34.417767: step 3673, loss = 4.78 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 19:30:40.478727: step 3674, loss = 5.01 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 19:30:47.145750: step 3675, loss = 4.96 (9.6 examples/sec; 6.667 sec/batch)
2016-04-29 19:30:53.401019: step 3676, loss = 5.10 (10.2 examples/sec; 6.255 sec/batch)
2016-04-29 19:30:59.420472: step 3677, loss = 4.97 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 19:31:05.675971: step 3678, loss = 5.02 (10.2 examples/sec; 6.255 sec/batch)
2016-04-29 19:31:11.411020: step 3679, loss = 5.00 (11.2 examples/sec; 5.735 sec/batch)
2016-04-29 19:31:17.296801: step 3680, loss = 4.81 (10.9 examples/sec; 5.886 sec/batch)
2016-04-29 19:31:31.400827: step 3681, loss = 5.05 (11.5 examples/sec; 5.585 sec/batch)
2016-04-29 19:31:37.449802: step 3682, loss = 5.00 (10.6 examples/sec; 6.049 sec/batch)
2016-04-29 19:31:43.257728: step 3683, loss = 4.90 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 19:31:49.193740: step 3684, loss = 4.90 (10.8 examples/sec; 5.936 sec/batch)
2016-04-29 19:31:55.808554: step 3685, loss = 4.65 (9.7 examples/sec; 6.615 sec/batch)
2016-04-29 19:32:01.601156: step 3686, loss = 4.92 (11.0 examples/sec; 5.793 sec/batch)
2016-04-29 19:32:07.278755: step 3687, loss = 5.09 (11.3 examples/sec; 5.678 sec/batch)
2016-04-29 19:32:13.191024: step 3688, loss = 5.03 (10.8 examples/sec; 5.912 sec/batch)
2016-04-29 19:32:19.219973: step 3689, loss = 4.82 (10.6 examples/sec; 6.029 sec/batch)
2016-04-29 19:32:25.118325: step 3690, loss = 4.89 (10.9 examples/sec; 5.898 sec/batch)
2016-04-29 19:32:39.854927: step 3691, loss = 4.86 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 19:32:45.605155: step 3692, loss = 5.02 (11.1 examples/sec; 5.750 sec/batch)
2016-04-29 19:32:51.470438: step 3693, loss = 4.94 (10.9 examples/sec; 5.865 sec/batch)
2016-04-29 19:32:57.309938: step 3694, loss = 4.82 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 19:33:04.088630: step 3695, loss = 4.96 (9.4 examples/sec; 6.779 sec/batch)
2016-04-29 19:33:09.829558: step 3696, loss = 4.96 (11.1 examples/sec; 5.741 sec/batch)
2016-04-29 19:33:15.729141: step 3697, loss = 5.10 (10.8 examples/sec; 5.899 sec/batch)
2016-04-29 19:33:21.487430: step 3698, loss = 4.99 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 19:33:27.358417: step 3699, loss = 4.90 (10.9 examples/sec; 5.871 sec/batch)
2016-04-29 19:33:33.808140: step 3700, loss = 4.85 (9.9 examples/sec; 6.450 sec/batch)
2016-04-29 19:33:47.725329: step 3701, loss = 4.99 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 19:33:53.511421: step 3702, loss = 4.94 (11.1 examples/sec; 5.786 sec/batch)
2016-04-29 19:33:59.418347: step 3703, loss = 4.78 (10.8 examples/sec; 5.907 sec/batch)
2016-04-29 19:34:05.949387: step 3704, loss = 4.76 (9.8 examples/sec; 6.531 sec/batch)
2016-04-29 19:34:11.869141: step 3705, loss = 4.91 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 19:34:18.038909: step 3706, loss = 4.83 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 19:34:24.202606: step 3707, loss = 4.86 (10.4 examples/sec; 6.164 sec/batch)
2016-04-29 19:34:30.279428: step 3708, loss = 4.91 (10.5 examples/sec; 6.077 sec/batch)
2016-04-29 19:34:36.277077: step 3709, loss = 4.82 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 19:34:42.935965: step 3710, loss = 4.79 (9.6 examples/sec; 6.659 sec/batch)
2016-04-29 19:34:56.669422: step 3711, loss = 4.77 (11.3 examples/sec; 5.644 sec/batch)
2016-04-29 19:35:02.658869: step 3712, loss = 4.98 (10.7 examples/sec; 5.989 sec/batch)
2016-04-29 19:35:08.560203: step 3713, loss = 5.01 (10.8 examples/sec; 5.901 sec/batch)
2016-04-29 19:35:15.175192: step 3714, loss = 4.72 (9.7 examples/sec; 6.615 sec/batch)
2016-04-29 19:35:21.050313: step 3715, loss = 4.97 (10.9 examples/sec; 5.875 sec/batch)
2016-04-29 19:35:27.107263: step 3716, loss = 4.84 (10.6 examples/sec; 6.057 sec/batch)
2016-04-29 19:35:33.126384: step 3717, loss = 5.14 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 19:35:39.137579: step 3718, loss = 4.95 (10.6 examples/sec; 6.011 sec/batch)
2016-04-29 19:35:45.619156: step 3719, loss = 5.02 (9.9 examples/sec; 6.481 sec/batch)
2016-04-29 19:35:51.605994: step 3720, loss = 4.82 (10.7 examples/sec; 5.987 sec/batch)
2016-04-29 19:36:05.495954: step 3721, loss = 4.96 (10.9 examples/sec; 5.846 sec/batch)
2016-04-29 19:36:11.590884: step 3722, loss = 4.97 (10.5 examples/sec; 6.095 sec/batch)
2016-04-29 19:36:17.531242: step 3723, loss = 4.84 (10.8 examples/sec; 5.940 sec/batch)
2016-04-29 19:36:24.146873: step 3724, loss = 4.74 (9.7 examples/sec; 6.616 sec/batch)
2016-04-29 19:36:30.130012: step 3725, loss = 4.86 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 19:36:36.329327: step 3726, loss = 4.67 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 19:36:42.225278: step 3727, loss = 4.65 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 19:36:48.338942: step 3728, loss = 4.86 (10.5 examples/sec; 6.114 sec/batch)
2016-04-29 19:36:54.819700: step 3729, loss = 4.92 (9.9 examples/sec; 6.481 sec/batch)
2016-04-29 19:37:00.978614: step 3730, loss = 4.95 (10.4 examples/sec; 6.159 sec/batch)
2016-04-29 19:37:16.969795: step 3731, loss = 4.82 (11.0 examples/sec; 5.816 sec/batch)
2016-04-29 19:37:23.727760: step 3732, loss = 4.78 (9.5 examples/sec; 6.757 sec/batch)
2016-04-29 19:37:30.093890: step 3733, loss = 4.79 (10.1 examples/sec; 6.366 sec/batch)
2016-04-29 19:37:36.269513: step 3734, loss = 4.90 (10.4 examples/sec; 6.176 sec/batch)
2016-04-29 19:37:42.295596: step 3735, loss = 4.82 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 19:37:48.039988: step 3736, loss = 4.89 (11.1 examples/sec; 5.744 sec/batch)
2016-04-29 19:37:53.830863: step 3737, loss = 4.88 (11.1 examples/sec; 5.791 sec/batch)
2016-04-29 19:38:00.577441: step 3738, loss = 4.86 (9.5 examples/sec; 6.746 sec/batch)
2016-04-29 19:38:06.422351: step 3739, loss = 4.72 (10.9 examples/sec; 5.845 sec/batch)
2016-04-29 19:38:12.455561: step 3740, loss = 4.84 (10.6 examples/sec; 6.033 sec/batch)
2016-04-29 19:38:26.241563: step 3741, loss = 4.84 (11.3 examples/sec; 5.664 sec/batch)
2016-04-29 19:38:32.808469: step 3742, loss = 4.72 (9.7 examples/sec; 6.567 sec/batch)
2016-04-29 19:38:38.912481: step 3743, loss = 4.64 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 19:38:44.974044: step 3744, loss = 4.85 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 19:38:50.794314: step 3745, loss = 4.71 (11.0 examples/sec; 5.820 sec/batch)
2016-04-29 19:38:56.658346: step 3746, loss = 4.65 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 19:39:02.991636: step 3747, loss = 4.87 (10.1 examples/sec; 6.333 sec/batch)
2016-04-29 19:39:09.393789: step 3748, loss = 4.80 (10.0 examples/sec; 6.402 sec/batch)
2016-04-29 19:39:15.373607: step 3749, loss = 4.97 (10.7 examples/sec; 5.978 sec/batch)
2016-04-29 19:39:21.330798: step 3750, loss = 4.65 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 19:39:34.820964: step 3751, loss = 4.72 (11.8 examples/sec; 5.440 sec/batch)
2016-04-29 19:39:41.741056: step 3752, loss = 4.67 (9.2 examples/sec; 6.920 sec/batch)
2016-04-29 19:39:47.620957: step 3753, loss = 4.77 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 19:39:53.395943: step 3754, loss = 4.67 (11.1 examples/sec; 5.775 sec/batch)
2016-04-29 19:39:59.373331: step 3755, loss = 4.88 (10.7 examples/sec; 5.977 sec/batch)
2016-04-29 19:40:05.322842: step 3756, loss = 4.68 (10.8 examples/sec; 5.949 sec/batch)
2016-04-29 19:40:12.078082: step 3757, loss = 4.77 (9.5 examples/sec; 6.755 sec/batch)
2016-04-29 19:40:18.079597: step 3758, loss = 4.87 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 19:40:24.101872: step 3759, loss = 4.74 (10.6 examples/sec; 6.022 sec/batch)
2016-04-29 19:40:30.048929: step 3760, loss = 4.53 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 19:40:44.514396: step 3761, loss = 4.61 (9.9 examples/sec; 6.470 sec/batch)
2016-04-29 19:40:50.415310: step 3762, loss = 4.67 (10.8 examples/sec; 5.901 sec/batch)
2016-04-29 19:40:56.295775: step 3763, loss = 4.88 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 19:41:02.530090: step 3764, loss = 4.44 (10.3 examples/sec; 6.234 sec/batch)
2016-04-29 19:41:08.390462: step 3765, loss = 4.68 (10.9 examples/sec; 5.860 sec/batch)
2016-04-29 19:41:14.132243: step 3766, loss = 4.86 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 19:41:20.963503: step 3767, loss = 5.02 (9.4 examples/sec; 6.831 sec/batch)
2016-04-29 19:41:26.903335: step 3768, loss = 4.95 (10.8 examples/sec; 5.940 sec/batch)
2016-04-29 19:41:32.810095: step 3769, loss = 4.76 (10.8 examples/sec; 5.907 sec/batch)
2016-04-29 19:41:38.886247: step 3770, loss = 4.79 (10.5 examples/sec; 6.076 sec/batch)
2016-04-29 19:41:53.862577: step 3771, loss = 4.75 (9.8 examples/sec; 6.560 sec/batch)
2016-04-29 19:41:59.802559: step 3772, loss = 4.69 (10.8 examples/sec; 5.940 sec/batch)
2016-04-29 19:42:05.887869: step 3773, loss = 4.78 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 19:42:11.894242: step 3774, loss = 4.57 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 19:42:17.924582: step 3775, loss = 4.69 (10.6 examples/sec; 6.030 sec/batch)
2016-04-29 19:42:24.613289: step 3776, loss = 4.61 (9.6 examples/sec; 6.689 sec/batch)
2016-04-29 19:42:30.593243: step 3777, loss = 4.72 (10.7 examples/sec; 5.980 sec/batch)
2016-04-29 19:42:36.517929: step 3778, loss = 4.78 (10.8 examples/sec; 5.925 sec/batch)
2016-04-29 19:42:42.578829: step 3779, loss = 4.60 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 19:42:48.503353: step 3780, loss = 4.71 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 19:43:03.476080: step 3781, loss = 4.71 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 19:43:09.245028: step 3782, loss = 4.84 (11.1 examples/sec; 5.769 sec/batch)
2016-04-29 19:43:15.301272: step 3783, loss = 4.73 (10.6 examples/sec; 6.056 sec/batch)
2016-04-29 19:43:21.233198: step 3784, loss = 4.77 (10.8 examples/sec; 5.932 sec/batch)
2016-04-29 19:43:27.437945: step 3785, loss = 4.84 (10.3 examples/sec; 6.205 sec/batch)
2016-04-29 19:43:33.715355: step 3786, loss = 4.71 (10.2 examples/sec; 6.277 sec/batch)
2016-04-29 19:43:39.663678: step 3787, loss = 4.70 (10.8 examples/sec; 5.948 sec/batch)
2016-04-29 19:43:45.628451: step 3788, loss = 4.65 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 19:43:52.144538: step 3789, loss = 4.60 (9.8 examples/sec; 6.516 sec/batch)
2016-04-29 19:43:58.148339: step 3790, loss = 4.68 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 19:44:13.103309: step 3791, loss = 4.60 (10.9 examples/sec; 5.852 sec/batch)
2016-04-29 19:44:19.129981: step 3792, loss = 4.70 (10.6 examples/sec; 6.027 sec/batch)
2016-04-29 19:44:25.072752: step 3793, loss = 5.00 (10.8 examples/sec; 5.943 sec/batch)
2016-04-29 19:44:31.227085: step 3794, loss = 4.68 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 19:44:38.061144: step 3795, loss = 4.72 (9.4 examples/sec; 6.834 sec/batch)
2016-04-29 19:44:43.956309: step 3796, loss = 4.67 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 19:44:49.921602: step 3797, loss = 4.56 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 19:44:55.792781: step 3798, loss = 4.51 (10.9 examples/sec; 5.871 sec/batch)
2016-04-29 19:45:01.908912: step 3799, loss = 4.44 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 19:45:08.467166: step 3800, loss = 4.75 (9.8 examples/sec; 6.558 sec/batch)
2016-04-29 19:45:22.414328: step 3801, loss = 4.69 (10.6 examples/sec; 6.060 sec/batch)
2016-04-29 19:45:28.405620: step 3802, loss = 4.60 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 19:45:34.374586: step 3803, loss = 4.74 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 19:45:41.145901: step 3804, loss = 4.61 (9.5 examples/sec; 6.771 sec/batch)
2016-04-29 19:45:46.706432: step 3805, loss = 4.71 (11.5 examples/sec; 5.560 sec/batch)
2016-04-29 19:45:52.632195: step 3806, loss = 4.56 (10.8 examples/sec; 5.926 sec/batch)
2016-04-29 19:45:58.532458: step 3807, loss = 4.68 (10.8 examples/sec; 5.900 sec/batch)
2016-04-29 19:46:04.771901: step 3808, loss = 4.63 (10.3 examples/sec; 6.239 sec/batch)
2016-04-29 19:46:10.629770: step 3809, loss = 4.59 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 19:46:17.356782: step 3810, loss = 4.61 (9.5 examples/sec; 6.727 sec/batch)
2016-04-29 19:46:30.849609: step 3811, loss = 4.56 (11.2 examples/sec; 5.721 sec/batch)
2016-04-29 19:46:36.931082: step 3812, loss = 4.53 (10.5 examples/sec; 6.081 sec/batch)
2016-04-29 19:46:43.031203: step 3813, loss = 4.49 (10.5 examples/sec; 6.100 sec/batch)
2016-04-29 19:46:49.658650: step 3814, loss = 4.56 (9.7 examples/sec; 6.627 sec/batch)
2016-04-29 19:46:55.567013: step 3815, loss = 4.76 (10.8 examples/sec; 5.908 sec/batch)
2016-04-29 19:47:01.714243: step 3816, loss = 4.55 (10.4 examples/sec; 6.147 sec/batch)
2016-04-29 19:47:07.525926: step 3817, loss = 4.56 (11.0 examples/sec; 5.812 sec/batch)
2016-04-29 19:47:13.445029: step 3818, loss = 4.62 (10.8 examples/sec; 5.919 sec/batch)
2016-04-29 19:47:20.180915: step 3819, loss = 4.63 (9.5 examples/sec; 6.736 sec/batch)
2016-04-29 19:47:26.114565: step 3820, loss = 4.58 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 19:47:39.940083: step 3821, loss = 4.58 (11.2 examples/sec; 5.722 sec/batch)
2016-04-29 19:47:45.910274: step 3822, loss = 4.54 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 19:47:52.559462: step 3823, loss = 4.60 (9.6 examples/sec; 6.649 sec/batch)
2016-04-29 19:47:58.375693: step 3824, loss = 4.48 (11.0 examples/sec; 5.816 sec/batch)
2016-04-29 19:48:04.600430: step 3825, loss = 4.63 (10.3 examples/sec; 6.225 sec/batch)
2016-04-29 19:48:10.591316: step 3826, loss = 4.56 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 19:48:16.527115: step 3827, loss = 4.69 (10.8 examples/sec; 5.936 sec/batch)
2016-04-29 19:48:22.187559: step 3828, loss = 4.59 (11.3 examples/sec; 5.660 sec/batch)
2016-04-29 19:48:28.779753: step 3829, loss = 4.52 (9.7 examples/sec; 6.592 sec/batch)
2016-04-29 19:48:34.677216: step 3830, loss = 4.62 (10.9 examples/sec; 5.897 sec/batch)
2016-04-29 19:48:48.653438: step 3831, loss = 4.74 (11.0 examples/sec; 5.835 sec/batch)
2016-04-29 19:48:54.650237: step 3832, loss = 4.39 (10.7 examples/sec; 5.997 sec/batch)
2016-04-29 19:49:01.378190: step 3833, loss = 4.55 (9.5 examples/sec; 6.728 sec/batch)
2016-04-29 19:49:07.240974: step 3834, loss = 4.63 (10.9 examples/sec; 5.863 sec/batch)
2016-04-29 19:49:13.507669: step 3835, loss = 4.52 (10.2 examples/sec; 6.267 sec/batch)
2016-04-29 19:49:20.126635: step 3836, loss = 4.44 (9.7 examples/sec; 6.619 sec/batch)
2016-04-29 19:49:26.903261: step 3837, loss = 4.49 (9.4 examples/sec; 6.777 sec/batch)
2016-04-29 19:49:33.602144: step 3838, loss = 4.49 (9.6 examples/sec; 6.699 sec/batch)
2016-04-29 19:49:39.722077: step 3839, loss = 4.45 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 19:49:45.738410: step 3840, loss = 4.53 (10.6 examples/sec; 6.016 sec/batch)
2016-04-29 19:49:59.632230: step 3841, loss = 4.42 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 19:50:06.368243: step 3842, loss = 4.59 (9.5 examples/sec; 6.736 sec/batch)
2016-04-29 19:50:12.318793: step 3843, loss = 4.53 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 19:50:18.619852: step 3844, loss = 4.54 (10.2 examples/sec; 6.301 sec/batch)
2016-04-29 19:50:24.734308: step 3845, loss = 4.56 (10.5 examples/sec; 6.114 sec/batch)
2016-04-29 19:50:30.836360: step 3846, loss = 4.46 (10.5 examples/sec; 6.102 sec/batch)
2016-04-29 19:50:37.381578: step 3847, loss = 4.49 (9.8 examples/sec; 6.545 sec/batch)
2016-04-29 19:50:43.529898: step 3848, loss = 4.52 (10.4 examples/sec; 6.148 sec/batch)
2016-04-29 19:50:49.522952: step 3849, loss = 4.58 (10.7 examples/sec; 5.993 sec/batch)
2016-04-29 19:50:55.347296: step 3850, loss = 4.40 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 19:51:09.903260: step 3851, loss = 4.60 (10.0 examples/sec; 6.382 sec/batch)
2016-04-29 19:51:15.750877: step 3852, loss = 4.39 (10.9 examples/sec; 5.848 sec/batch)
2016-04-29 19:51:21.644436: step 3853, loss = 4.52 (10.9 examples/sec; 5.893 sec/batch)
2016-04-29 19:51:27.629316: step 3854, loss = 4.36 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 19:51:33.907689: step 3855, loss = 4.25 (10.2 examples/sec; 6.278 sec/batch)
2016-04-29 19:51:39.939621: step 3856, loss = 4.50 (10.6 examples/sec; 6.032 sec/batch)
2016-04-29 19:51:46.481140: step 3857, loss = 4.43 (9.8 examples/sec; 6.541 sec/batch)
2016-04-29 19:51:52.277433: step 3858, loss = 4.48 (11.0 examples/sec; 5.796 sec/batch)
2016-04-29 19:51:58.123448: step 3859, loss = 4.34 (10.9 examples/sec; 5.846 sec/batch)
2016-04-29 19:52:04.263982: step 3860, loss = 4.63 (10.4 examples/sec; 6.140 sec/batch)
2016-04-29 19:52:18.918663: step 3861, loss = 4.43 (9.8 examples/sec; 6.512 sec/batch)
2016-04-29 19:52:24.934240: step 3862, loss = 4.37 (10.6 examples/sec; 6.015 sec/batch)
2016-04-29 19:52:30.922725: step 3863, loss = 4.53 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 19:52:37.122125: step 3864, loss = 4.61 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 19:52:43.187763: step 3865, loss = 4.50 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 19:52:49.990122: step 3866, loss = 4.62 (9.4 examples/sec; 6.802 sec/batch)
2016-04-29 19:52:55.745423: step 3867, loss = 4.60 (11.1 examples/sec; 5.755 sec/batch)
2016-04-29 19:53:01.756350: step 3868, loss = 4.52 (10.6 examples/sec; 6.011 sec/batch)
2016-04-29 19:53:07.519089: step 3869, loss = 4.45 (11.1 examples/sec; 5.763 sec/batch)
2016-04-29 19:53:13.471280: step 3870, loss = 4.47 (10.8 examples/sec; 5.952 sec/batch)
2016-04-29 19:53:28.081777: step 3871, loss = 4.47 (10.7 examples/sec; 5.963 sec/batch)
2016-04-29 19:53:34.222793: step 3872, loss = 4.43 (10.4 examples/sec; 6.141 sec/batch)
2016-04-29 19:53:40.206568: step 3873, loss = 4.57 (10.7 examples/sec; 5.984 sec/batch)
2016-04-29 19:53:46.233064: step 3874, loss = 4.51 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 19:53:52.057112: step 3875, loss = 4.23 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 19:53:58.653501: step 3876, loss = 4.51 (9.7 examples/sec; 6.596 sec/batch)
2016-04-29 19:54:04.704840: step 3877, loss = 4.40 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 19:54:10.733179: step 3878, loss = 4.37 (10.6 examples/sec; 6.028 sec/batch)
2016-04-29 19:54:16.677412: step 3879, loss = 4.24 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 19:54:22.750561: step 3880, loss = 4.34 (10.5 examples/sec; 6.073 sec/batch)
2016-04-29 19:54:37.106055: step 3881, loss = 4.26 (11.3 examples/sec; 5.666 sec/batch)
2016-04-29 19:54:43.107313: step 3882, loss = 4.50 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 19:54:49.182631: step 3883, loss = 4.39 (10.5 examples/sec; 6.075 sec/batch)
2016-04-29 19:54:55.203736: step 3884, loss = 4.44 (10.6 examples/sec; 6.021 sec/batch)
2016-04-29 19:55:01.868170: step 3885, loss = 4.42 (9.6 examples/sec; 6.664 sec/batch)
2016-04-29 19:55:07.387739: step 3886, loss = 4.52 (11.6 examples/sec; 5.519 sec/batch)
2016-04-29 19:55:13.308508: step 3887, loss = 4.51 (10.8 examples/sec; 5.921 sec/batch)
2016-04-29 19:55:19.232902: step 3888, loss = 4.35 (10.8 examples/sec; 5.924 sec/batch)
2016-04-29 19:55:25.308476: step 3889, loss = 4.46 (10.5 examples/sec; 6.075 sec/batch)
2016-04-29 19:55:31.077432: step 3890, loss = 4.30 (11.1 examples/sec; 5.769 sec/batch)
2016-04-29 19:55:45.954393: step 3891, loss = 4.34 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 19:55:51.898941: step 3892, loss = 4.49 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 19:55:57.916096: step 3893, loss = 4.30 (10.6 examples/sec; 6.017 sec/batch)
2016-04-29 19:56:04.023366: step 3894, loss = 4.38 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 19:56:10.479888: step 3895, loss = 4.40 (9.9 examples/sec; 6.456 sec/batch)
2016-04-29 19:56:16.380969: step 3896, loss = 4.34 (10.8 examples/sec; 5.901 sec/batch)
2016-04-29 19:56:22.322181: step 3897, loss = 4.37 (10.8 examples/sec; 5.941 sec/batch)
2016-04-29 19:56:28.528226: step 3898, loss = 4.47 (10.3 examples/sec; 6.206 sec/batch)
2016-04-29 19:56:34.605645: step 3899, loss = 4.39 (10.5 examples/sec; 6.077 sec/batch)
2016-04-29 19:56:41.557603: step 3900, loss = 4.21 (9.2 examples/sec; 6.952 sec/batch)
2016-04-29 19:56:55.122945: step 3901, loss = 4.46 (11.2 examples/sec; 5.733 sec/batch)
2016-04-29 19:57:01.374492: step 3902, loss = 4.34 (10.2 examples/sec; 6.251 sec/batch)
2016-04-29 19:57:07.398305: step 3903, loss = 4.50 (10.6 examples/sec; 6.024 sec/batch)
2016-04-29 19:57:13.989334: step 3904, loss = 4.45 (9.7 examples/sec; 6.591 sec/batch)
2016-04-29 19:57:19.953697: step 3905, loss = 4.14 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 19:57:26.251803: step 3906, loss = 4.30 (10.2 examples/sec; 6.298 sec/batch)
2016-04-29 19:57:32.062042: step 3907, loss = 4.53 (11.0 examples/sec; 5.810 sec/batch)
2016-04-29 19:57:38.092895: step 3908, loss = 4.46 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 19:57:43.834560: step 3909, loss = 4.40 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 19:57:50.623957: step 3910, loss = 4.36 (9.4 examples/sec; 6.789 sec/batch)
2016-04-29 19:58:04.603830: step 3911, loss = 4.36 (10.4 examples/sec; 6.142 sec/batch)
2016-04-29 19:58:10.489513: step 3912, loss = 4.43 (10.9 examples/sec; 5.886 sec/batch)
2016-04-29 19:58:16.587098: step 3913, loss = 4.40 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 19:58:23.678861: step 3914, loss = 4.18 (9.0 examples/sec; 7.092 sec/batch)
2016-04-29 19:58:29.715728: step 3915, loss = 4.31 (10.6 examples/sec; 6.037 sec/batch)
2016-04-29 19:58:35.653958: step 3916, loss = 4.34 (10.8 examples/sec; 5.938 sec/batch)
2016-04-29 19:58:41.693126: step 3917, loss = 4.25 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 19:58:47.387445: step 3918, loss = 4.23 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 19:58:53.935155: step 3919, loss = 4.23 (9.8 examples/sec; 6.548 sec/batch)
2016-04-29 19:58:59.919159: step 3920, loss = 4.31 (10.7 examples/sec; 5.984 sec/batch)
2016-04-29 19:59:13.732487: step 3921, loss = 4.33 (11.0 examples/sec; 5.840 sec/batch)
2016-04-29 19:59:19.624820: step 3922, loss = 4.34 (10.9 examples/sec; 5.892 sec/batch)
2016-04-29 19:59:26.629982: step 3923, loss = 4.33 (9.1 examples/sec; 7.005 sec/batch)
2016-04-29 19:59:32.601440: step 3924, loss = 4.50 (10.7 examples/sec; 5.971 sec/batch)
2016-04-29 19:59:38.662562: step 3925, loss = 4.29 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 19:59:44.752625: step 3926, loss = 4.35 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 19:59:50.753716: step 3927, loss = 4.20 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 19:59:56.526431: step 3928, loss = 4.31 (11.1 examples/sec; 5.773 sec/batch)
2016-04-29 20:00:03.224456: step 3929, loss = 4.23 (9.6 examples/sec; 6.698 sec/batch)
2016-04-29 20:00:09.308401: step 3930, loss = 4.21 (10.5 examples/sec; 6.084 sec/batch)
2016-04-29 20:00:22.776892: step 3931, loss = 4.43 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 20:00:28.951270: step 3932, loss = 4.40 (10.4 examples/sec; 6.174 sec/batch)
2016-04-29 20:00:35.718901: step 3933, loss = 4.27 (9.5 examples/sec; 6.768 sec/batch)
2016-04-29 20:00:41.791348: step 3934, loss = 4.21 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:00:47.826969: step 3935, loss = 4.28 (10.6 examples/sec; 6.036 sec/batch)
2016-04-29 20:00:53.832803: step 3936, loss = 4.27 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 20:00:59.549422: step 3937, loss = 4.36 (11.2 examples/sec; 5.717 sec/batch)
2016-04-29 20:01:06.112755: step 3938, loss = 4.37 (9.8 examples/sec; 6.563 sec/batch)
2016-04-29 20:01:11.979540: step 3939, loss = 4.32 (10.9 examples/sec; 5.867 sec/batch)
2016-04-29 20:01:18.051759: step 3940, loss = 4.29 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:01:31.609113: step 3941, loss = 4.37 (11.1 examples/sec; 5.784 sec/batch)
2016-04-29 20:01:38.187027: step 3942, loss = 4.21 (9.7 examples/sec; 6.577 sec/batch)
2016-04-29 20:01:43.979426: step 3943, loss = 4.23 (11.0 examples/sec; 5.792 sec/batch)
2016-04-29 20:01:49.935951: step 3944, loss = 4.20 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 20:01:55.724491: step 3945, loss = 4.07 (11.1 examples/sec; 5.788 sec/batch)
2016-04-29 20:02:01.897929: step 3946, loss = 4.26 (10.4 examples/sec; 6.173 sec/batch)
2016-04-29 20:02:07.769853: step 3947, loss = 4.28 (10.9 examples/sec; 5.872 sec/batch)
2016-04-29 20:02:14.733193: step 3948, loss = 4.29 (9.2 examples/sec; 6.963 sec/batch)
2016-04-29 20:02:20.573887: step 3949, loss = 4.31 (11.0 examples/sec; 5.841 sec/batch)
2016-04-29 20:02:26.587293: step 3950, loss = 4.29 (10.6 examples/sec; 6.013 sec/batch)
2016-04-29 20:02:40.272573: step 3951, loss = 4.34 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 20:02:46.849170: step 3952, loss = 4.02 (9.7 examples/sec; 6.577 sec/batch)
2016-04-29 20:02:53.222428: step 3953, loss = 4.28 (10.0 examples/sec; 6.373 sec/batch)
2016-04-29 20:02:58.977095: step 3954, loss = 4.30 (11.1 examples/sec; 5.755 sec/batch)
2016-04-29 20:03:04.913949: step 3955, loss = 4.25 (10.8 examples/sec; 5.937 sec/batch)
2016-04-29 20:03:10.927523: step 3956, loss = 4.32 (10.6 examples/sec; 6.013 sec/batch)
2016-04-29 20:03:17.498111: step 3957, loss = 4.19 (9.7 examples/sec; 6.570 sec/batch)
2016-04-29 20:03:23.800630: step 3958, loss = 4.32 (10.2 examples/sec; 6.302 sec/batch)
2016-04-29 20:03:29.638090: step 3959, loss = 4.35 (11.0 examples/sec; 5.837 sec/batch)
2016-04-29 20:03:36.233998: step 3960, loss = 4.27 (9.7 examples/sec; 6.596 sec/batch)
2016-04-29 20:03:50.941153: step 3961, loss = 4.31 (9.6 examples/sec; 6.643 sec/batch)
2016-04-29 20:03:56.952447: step 3962, loss = 4.30 (10.6 examples/sec; 6.011 sec/batch)
2016-04-29 20:04:03.022225: step 3963, loss = 4.32 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 20:04:09.124650: step 3964, loss = 4.16 (10.5 examples/sec; 6.102 sec/batch)
2016-04-29 20:04:15.231534: step 3965, loss = 4.30 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 20:04:21.831106: step 3966, loss = 4.25 (9.7 examples/sec; 6.599 sec/batch)
2016-04-29 20:04:27.960588: step 3967, loss = 4.20 (10.4 examples/sec; 6.129 sec/batch)
2016-04-29 20:04:34.085406: step 3968, loss = 4.10 (10.4 examples/sec; 6.125 sec/batch)
2016-04-29 20:04:40.169604: step 3969, loss = 4.36 (10.5 examples/sec; 6.084 sec/batch)
2016-04-29 20:04:46.138800: step 3970, loss = 4.44 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 20:05:01.352945: step 3971, loss = 4.30 (10.5 examples/sec; 6.100 sec/batch)
2016-04-29 20:05:07.324291: step 3972, loss = 4.24 (10.7 examples/sec; 5.971 sec/batch)
2016-04-29 20:05:13.096739: step 3973, loss = 4.33 (11.1 examples/sec; 5.772 sec/batch)
2016-04-29 20:05:19.066438: step 3974, loss = 4.35 (10.7 examples/sec; 5.970 sec/batch)
2016-04-29 20:05:25.023939: step 3975, loss = 4.17 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 20:05:31.566952: step 3976, loss = 4.18 (9.8 examples/sec; 6.543 sec/batch)
2016-04-29 20:05:37.643900: step 3977, loss = 4.09 (10.5 examples/sec; 6.077 sec/batch)
2016-04-29 20:05:43.702636: step 3978, loss = 4.10 (10.6 examples/sec; 6.059 sec/batch)
2016-04-29 20:05:49.774785: step 3979, loss = 4.18 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:05:55.814673: step 3980, loss = 4.25 (10.6 examples/sec; 6.040 sec/batch)
2016-04-29 20:06:10.647992: step 3981, loss = 4.04 (10.8 examples/sec; 5.930 sec/batch)
2016-04-29 20:06:16.689619: step 3982, loss = 4.18 (10.6 examples/sec; 6.042 sec/batch)
2016-04-29 20:06:22.636151: step 3983, loss = 4.20 (10.8 examples/sec; 5.946 sec/batch)
2016-04-29 20:06:28.656551: step 3984, loss = 4.23 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 20:06:34.928895: step 3985, loss = 4.21 (10.2 examples/sec; 6.272 sec/batch)
2016-04-29 20:06:41.010894: step 3986, loss = 4.16 (10.5 examples/sec; 6.082 sec/batch)
2016-04-29 20:06:46.806059: step 3987, loss = 4.08 (11.0 examples/sec; 5.795 sec/batch)
2016-04-29 20:06:52.829871: step 3988, loss = 4.20 (10.6 examples/sec; 6.024 sec/batch)
2016-04-29 20:06:58.580423: step 3989, loss = 4.15 (11.1 examples/sec; 5.750 sec/batch)
2016-04-29 20:07:05.054745: step 3990, loss = 4.25 (9.9 examples/sec; 6.474 sec/batch)
2016-04-29 20:07:19.544452: step 3991, loss = 4.26 (11.1 examples/sec; 5.747 sec/batch)
2016-04-29 20:07:25.620057: step 3992, loss = 4.16 (10.5 examples/sec; 6.076 sec/batch)
2016-04-29 20:07:31.597188: step 3993, loss = 4.08 (10.7 examples/sec; 5.977 sec/batch)
2016-04-29 20:07:37.733383: step 3994, loss = 4.14 (10.4 examples/sec; 6.136 sec/batch)
2016-04-29 20:07:44.707218: step 3995, loss = 4.20 (9.2 examples/sec; 6.974 sec/batch)
2016-04-29 20:07:50.522432: step 3996, loss = 4.18 (11.0 examples/sec; 5.815 sec/batch)
2016-04-29 20:07:56.735465: step 3997, loss = 4.21 (10.3 examples/sec; 6.213 sec/batch)
2016-04-29 20:08:02.890046: step 3998, loss = 4.12 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 20:08:08.709506: step 3999, loss = 4.08 (11.0 examples/sec; 5.819 sec/batch)
2016-04-29 20:08:15.303731: step 4000, loss = 4.08 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 20:08:28.762342: step 4001, loss = 4.15 (11.4 examples/sec; 5.592 sec/batch)
2016-04-29 20:08:34.784807: step 4002, loss = 4.20 (10.6 examples/sec; 6.022 sec/batch)
2016-04-29 20:08:40.828425: step 4003, loss = 4.15 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 20:08:47.417103: step 4004, loss = 4.23 (9.7 examples/sec; 6.589 sec/batch)
2016-04-29 20:08:53.585482: step 4005, loss = 4.10 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 20:08:59.447994: step 4006, loss = 4.22 (10.9 examples/sec; 5.862 sec/batch)
2016-04-29 20:09:05.591738: step 4007, loss = 4.27 (10.4 examples/sec; 6.144 sec/batch)
2016-04-29 20:09:11.345946: step 4008, loss = 4.10 (11.1 examples/sec; 5.754 sec/batch)
2016-04-29 20:09:17.106502: step 4009, loss = 4.06 (11.1 examples/sec; 5.760 sec/batch)
2016-04-29 20:09:24.004948: step 4010, loss = 4.34 (9.3 examples/sec; 6.898 sec/batch)
2016-04-29 20:09:37.723825: step 4011, loss = 4.23 (10.9 examples/sec; 5.869 sec/batch)
2016-04-29 20:09:43.859489: step 4012, loss = 4.12 (10.4 examples/sec; 6.135 sec/batch)
2016-04-29 20:09:49.797770: step 4013, loss = 4.23 (10.8 examples/sec; 5.938 sec/batch)
2016-04-29 20:09:56.471829: step 4014, loss = 4.04 (9.6 examples/sec; 6.674 sec/batch)
2016-04-29 20:10:02.370448: step 4015, loss = 4.21 (10.9 examples/sec; 5.898 sec/batch)
2016-04-29 20:10:08.442096: step 4016, loss = 4.13 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:10:14.180832: step 4017, loss = 4.16 (11.2 examples/sec; 5.739 sec/batch)
2016-04-29 20:10:20.220221: step 4018, loss = 4.17 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 20:10:27.191983: step 4019, loss = 4.04 (9.2 examples/sec; 6.972 sec/batch)
2016-04-29 20:10:33.196473: step 4020, loss = 4.08 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 20:10:46.746340: step 4021, loss = 4.11 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 20:10:52.730556: step 4022, loss = 4.29 (10.7 examples/sec; 5.984 sec/batch)
2016-04-29 20:10:59.757394: step 4023, loss = 4.09 (9.1 examples/sec; 7.027 sec/batch)
2016-04-29 20:11:06.086647: step 4024, loss = 4.03 (10.1 examples/sec; 6.329 sec/batch)
2016-04-29 20:11:11.905729: step 4025, loss = 4.04 (11.0 examples/sec; 5.819 sec/batch)
2016-04-29 20:11:17.758858: step 4026, loss = 4.15 (10.9 examples/sec; 5.853 sec/batch)
2016-04-29 20:11:23.458444: step 4027, loss = 4.14 (11.2 examples/sec; 5.699 sec/batch)
2016-04-29 20:11:30.033741: step 4028, loss = 4.04 (9.7 examples/sec; 6.575 sec/batch)
2016-04-29 20:11:36.822470: step 4029, loss = 4.05 (9.4 examples/sec; 6.789 sec/batch)
2016-04-29 20:11:42.967818: step 4030, loss = 3.99 (10.4 examples/sec; 6.145 sec/batch)
2016-04-29 20:11:57.145132: step 4031, loss = 4.33 (11.1 examples/sec; 5.764 sec/batch)
2016-04-29 20:12:03.804805: step 4032, loss = 4.27 (9.6 examples/sec; 6.660 sec/batch)
2016-04-29 20:12:10.160706: step 4033, loss = 4.01 (10.1 examples/sec; 6.356 sec/batch)
2016-04-29 20:12:16.267564: step 4034, loss = 4.14 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 20:12:22.140242: step 4035, loss = 4.22 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 20:12:28.246630: step 4036, loss = 4.21 (10.5 examples/sec; 6.106 sec/batch)
2016-04-29 20:12:34.220853: step 4037, loss = 4.12 (10.7 examples/sec; 5.974 sec/batch)
2016-04-29 20:12:40.861992: step 4038, loss = 4.04 (9.6 examples/sec; 6.641 sec/batch)
2016-04-29 20:12:46.820803: step 4039, loss = 4.07 (10.7 examples/sec; 5.959 sec/batch)
2016-04-29 20:12:52.652664: step 4040, loss = 3.86 (11.0 examples/sec; 5.832 sec/batch)
2016-04-29 20:13:06.803633: step 4041, loss = 3.98 (10.8 examples/sec; 5.927 sec/batch)
2016-04-29 20:13:13.300846: step 4042, loss = 4.18 (9.9 examples/sec; 6.497 sec/batch)
2016-04-29 20:13:19.299827: step 4043, loss = 3.99 (10.7 examples/sec; 5.999 sec/batch)
2016-04-29 20:13:25.249268: step 4044, loss = 3.94 (10.8 examples/sec; 5.949 sec/batch)
2016-04-29 20:13:31.461491: step 4045, loss = 4.01 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 20:13:37.351389: step 4046, loss = 4.00 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 20:13:43.946539: step 4047, loss = 4.11 (9.7 examples/sec; 6.595 sec/batch)
2016-04-29 20:13:49.866314: step 4048, loss = 4.03 (10.8 examples/sec; 5.920 sec/batch)
2016-04-29 20:13:55.845814: step 4049, loss = 4.01 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 20:14:02.050146: step 4050, loss = 4.00 (10.3 examples/sec; 6.204 sec/batch)
2016-04-29 20:14:16.499344: step 4051, loss = 4.14 (9.9 examples/sec; 6.475 sec/batch)
2016-04-29 20:14:22.396327: step 4052, loss = 4.03 (10.9 examples/sec; 5.897 sec/batch)
2016-04-29 20:14:28.353073: step 4053, loss = 4.03 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 20:14:34.281375: step 4054, loss = 4.15 (10.8 examples/sec; 5.928 sec/batch)
2016-04-29 20:14:40.261456: step 4055, loss = 4.12 (10.7 examples/sec; 5.980 sec/batch)
2016-04-29 20:14:46.274273: step 4056, loss = 4.08 (10.6 examples/sec; 6.013 sec/batch)
2016-04-29 20:14:52.861471: step 4057, loss = 4.07 (9.7 examples/sec; 6.587 sec/batch)
2016-04-29 20:14:58.990079: step 4058, loss = 3.83 (10.4 examples/sec; 6.129 sec/batch)
2016-04-29 20:15:05.224300: step 4059, loss = 3.81 (10.3 examples/sec; 6.234 sec/batch)
2016-04-29 20:15:11.512094: step 4060, loss = 4.15 (10.2 examples/sec; 6.288 sec/batch)
2016-04-29 20:15:26.063810: step 4061, loss = 3.93 (10.2 examples/sec; 6.289 sec/batch)
2016-04-29 20:15:31.838502: step 4062, loss = 4.12 (11.1 examples/sec; 5.775 sec/batch)
2016-04-29 20:15:37.881552: step 4063, loss = 3.95 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 20:15:43.947515: step 4064, loss = 3.90 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 20:15:49.805421: step 4065, loss = 3.91 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 20:15:56.307397: step 4066, loss = 3.99 (9.8 examples/sec; 6.502 sec/batch)
2016-04-29 20:16:02.188553: step 4067, loss = 3.95 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 20:16:08.373296: step 4068, loss = 4.10 (10.3 examples/sec; 6.185 sec/batch)
2016-04-29 20:16:14.255206: step 4069, loss = 4.04 (10.9 examples/sec; 5.882 sec/batch)
2016-04-29 20:16:20.140595: step 4070, loss = 3.90 (10.9 examples/sec; 5.885 sec/batch)
2016-04-29 20:16:34.686244: step 4071, loss = 4.15 (11.1 examples/sec; 5.764 sec/batch)
2016-04-29 20:16:40.581789: step 4072, loss = 4.14 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 20:16:46.577613: step 4073, loss = 4.04 (10.7 examples/sec; 5.996 sec/batch)
2016-04-29 20:16:52.470554: step 4074, loss = 4.17 (10.9 examples/sec; 5.893 sec/batch)
2016-04-29 20:16:58.669091: step 4075, loss = 3.93 (10.3 examples/sec; 6.198 sec/batch)
2016-04-29 20:17:05.391080: step 4076, loss = 3.89 (9.5 examples/sec; 6.722 sec/batch)
2016-04-29 20:17:11.271792: step 4077, loss = 4.05 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 20:17:17.316443: step 4078, loss = 3.97 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 20:17:23.219720: step 4079, loss = 4.15 (10.8 examples/sec; 5.903 sec/batch)
2016-04-29 20:17:29.109789: step 4080, loss = 4.00 (10.9 examples/sec; 5.890 sec/batch)
2016-04-29 20:17:43.673323: step 4081, loss = 4.05 (11.2 examples/sec; 5.709 sec/batch)
2016-04-29 20:17:49.828954: step 4082, loss = 4.02 (10.4 examples/sec; 6.156 sec/batch)
2016-04-29 20:17:55.881258: step 4083, loss = 3.87 (10.6 examples/sec; 6.052 sec/batch)
2016-04-29 20:18:02.035255: step 4084, loss = 4.08 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 20:18:08.411414: step 4085, loss = 4.11 (10.0 examples/sec; 6.376 sec/batch)
2016-04-29 20:18:14.425436: step 4086, loss = 4.02 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 20:18:20.376925: step 4087, loss = 3.90 (10.8 examples/sec; 5.951 sec/batch)
2016-04-29 20:18:26.414317: step 4088, loss = 3.96 (10.6 examples/sec; 6.037 sec/batch)
2016-04-29 20:18:32.110012: step 4089, loss = 3.99 (11.2 examples/sec; 5.696 sec/batch)
2016-04-29 20:18:38.054120: step 4090, loss = 3.72 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 20:18:52.499719: step 4091, loss = 3.82 (11.0 examples/sec; 5.797 sec/batch)
2016-04-29 20:18:58.752192: step 4092, loss = 3.84 (10.2 examples/sec; 6.252 sec/batch)
2016-04-29 20:19:05.146166: step 4093, loss = 3.96 (10.0 examples/sec; 6.394 sec/batch)
2016-04-29 20:19:11.086136: step 4094, loss = 4.03 (10.8 examples/sec; 5.940 sec/batch)
2016-04-29 20:19:17.737137: step 4095, loss = 3.88 (9.6 examples/sec; 6.651 sec/batch)
2016-04-29 20:19:23.757117: step 4096, loss = 3.84 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 20:19:29.785849: step 4097, loss = 4.01 (10.6 examples/sec; 6.029 sec/batch)
2016-04-29 20:19:35.761566: step 4098, loss = 3.90 (10.7 examples/sec; 5.976 sec/batch)
2016-04-29 20:19:41.759162: step 4099, loss = 3.93 (10.7 examples/sec; 5.998 sec/batch)
2016-04-29 20:19:47.690780: step 4100, loss = 4.07 (10.8 examples/sec; 5.932 sec/batch)
2016-04-29 20:20:02.411688: step 4101, loss = 4.10 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 20:20:08.556460: step 4102, loss = 3.91 (10.4 examples/sec; 6.145 sec/batch)
2016-04-29 20:20:14.389686: step 4103, loss = 3.91 (11.0 examples/sec; 5.833 sec/batch)
2016-04-29 20:20:20.244300: step 4104, loss = 3.91 (10.9 examples/sec; 5.854 sec/batch)
2016-04-29 20:20:26.950627: step 4105, loss = 3.75 (9.5 examples/sec; 6.706 sec/batch)
2016-04-29 20:20:33.009327: step 4106, loss = 3.93 (10.6 examples/sec; 6.059 sec/batch)
2016-04-29 20:20:39.245183: step 4107, loss = 4.00 (10.3 examples/sec; 6.236 sec/batch)
2016-04-29 20:20:45.075792: step 4108, loss = 3.95 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 20:20:50.850301: step 4109, loss = 4.08 (11.1 examples/sec; 5.774 sec/batch)
2016-04-29 20:20:57.450240: step 4110, loss = 3.98 (9.7 examples/sec; 6.600 sec/batch)
2016-04-29 20:21:11.062650: step 4111, loss = 3.90 (11.5 examples/sec; 5.574 sec/batch)
2016-04-29 20:21:16.871375: step 4112, loss = 3.96 (11.0 examples/sec; 5.809 sec/batch)
2016-04-29 20:21:22.936635: step 4113, loss = 3.93 (10.6 examples/sec; 6.065 sec/batch)
2016-04-29 20:21:29.659353: step 4114, loss = 3.99 (9.5 examples/sec; 6.723 sec/batch)
2016-04-29 20:21:35.876502: step 4115, loss = 3.97 (10.3 examples/sec; 6.217 sec/batch)
2016-04-29 20:21:41.865106: step 4116, loss = 3.88 (10.7 examples/sec; 5.989 sec/batch)
2016-04-29 20:21:47.421816: step 4117, loss = 3.82 (11.5 examples/sec; 5.557 sec/batch)
2016-04-29 20:21:53.300238: step 4118, loss = 3.91 (10.9 examples/sec; 5.878 sec/batch)
2016-04-29 20:21:59.153741: step 4119, loss = 3.90 (10.9 examples/sec; 5.853 sec/batch)
2016-04-29 20:22:06.107693: step 4120, loss = 3.97 (9.2 examples/sec; 6.954 sec/batch)
2016-04-29 20:22:19.649143: step 4121, loss = 3.84 (11.4 examples/sec; 5.621 sec/batch)
2016-04-29 20:22:25.692349: step 4122, loss = 3.84 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 20:22:31.615285: step 4123, loss = 3.97 (10.8 examples/sec; 5.923 sec/batch)
2016-04-29 20:22:38.166711: step 4124, loss = 3.99 (9.8 examples/sec; 6.551 sec/batch)
2016-04-29 20:22:44.352447: step 4125, loss = 3.87 (10.3 examples/sec; 6.186 sec/batch)
2016-04-29 20:22:50.292631: step 4126, loss = 3.82 (10.8 examples/sec; 5.940 sec/batch)
2016-04-29 20:22:56.343518: step 4127, loss = 3.84 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 20:23:02.650912: step 4128, loss = 3.91 (10.1 examples/sec; 6.307 sec/batch)
2016-04-29 20:23:09.325810: step 4129, loss = 3.99 (9.6 examples/sec; 6.675 sec/batch)
2016-04-29 20:23:15.455793: step 4130, loss = 3.82 (10.4 examples/sec; 6.130 sec/batch)
2016-04-29 20:23:29.669731: step 4131, loss = 3.90 (10.7 examples/sec; 5.955 sec/batch)
2016-04-29 20:23:35.742602: step 4132, loss = 3.86 (10.5 examples/sec; 6.073 sec/batch)
2016-04-29 20:23:42.571189: step 4133, loss = 3.86 (9.4 examples/sec; 6.828 sec/batch)
2016-04-29 20:23:48.643752: step 4134, loss = 3.74 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:23:54.483471: step 4135, loss = 4.02 (11.0 examples/sec; 5.840 sec/batch)
2016-04-29 20:24:00.679975: step 4136, loss = 3.82 (10.3 examples/sec; 6.196 sec/batch)
2016-04-29 20:24:06.759469: step 4137, loss = 3.89 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 20:24:14.417131: step 4138, loss = 4.03 (8.4 examples/sec; 7.658 sec/batch)
2016-04-29 20:24:20.505231: step 4139, loss = 3.87 (10.5 examples/sec; 6.088 sec/batch)
2016-04-29 20:24:26.453954: step 4140, loss = 3.94 (10.8 examples/sec; 5.949 sec/batch)
2016-04-29 20:24:40.369468: step 4141, loss = 3.84 (11.1 examples/sec; 5.757 sec/batch)
2016-04-29 20:24:47.051872: step 4142, loss = 3.91 (9.6 examples/sec; 6.682 sec/batch)
2016-04-29 20:24:53.137940: step 4143, loss = 3.87 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 20:24:58.976606: step 4144, loss = 3.80 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 20:25:05.083245: step 4145, loss = 3.81 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 20:25:11.009917: step 4146, loss = 3.93 (10.8 examples/sec; 5.927 sec/batch)
2016-04-29 20:25:16.724620: step 4147, loss = 3.91 (11.2 examples/sec; 5.715 sec/batch)
2016-04-29 20:25:23.275157: step 4148, loss = 3.91 (9.8 examples/sec; 6.550 sec/batch)
2016-04-29 20:25:29.247934: step 4149, loss = 3.83 (10.7 examples/sec; 5.973 sec/batch)
2016-04-29 20:25:35.072474: step 4150, loss = 3.87 (11.0 examples/sec; 5.824 sec/batch)
2016-04-29 20:25:49.068325: step 4151, loss = 3.90 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 20:25:55.585042: step 4152, loss = 3.93 (9.8 examples/sec; 6.517 sec/batch)
2016-04-29 20:26:01.862132: step 4153, loss = 3.87 (10.2 examples/sec; 6.277 sec/batch)
2016-04-29 20:26:07.801407: step 4154, loss = 3.96 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 20:26:13.665927: step 4155, loss = 3.67 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 20:26:19.543546: step 4156, loss = 3.79 (10.9 examples/sec; 5.878 sec/batch)
2016-04-29 20:26:26.114693: step 4157, loss = 3.73 (9.7 examples/sec; 6.571 sec/batch)
2016-04-29 20:26:32.001307: step 4158, loss = 3.79 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 20:26:37.795861: step 4159, loss = 3.54 (11.0 examples/sec; 5.794 sec/batch)
2016-04-29 20:26:43.813877: step 4160, loss = 3.78 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 20:26:58.503769: step 4161, loss = 3.74 (9.9 examples/sec; 6.458 sec/batch)
2016-04-29 20:27:04.511868: step 4162, loss = 3.70 (10.7 examples/sec; 6.008 sec/batch)
2016-04-29 20:27:10.300979: step 4163, loss = 3.72 (11.1 examples/sec; 5.789 sec/batch)
2016-04-29 20:27:16.337483: step 4164, loss = 3.78 (10.6 examples/sec; 6.036 sec/batch)
2016-04-29 20:27:22.150900: step 4165, loss = 3.77 (11.0 examples/sec; 5.813 sec/batch)
2016-04-29 20:27:28.051189: step 4166, loss = 3.97 (10.8 examples/sec; 5.900 sec/batch)
2016-04-29 20:27:34.559685: step 4167, loss = 3.86 (9.8 examples/sec; 6.508 sec/batch)
2016-04-29 20:27:40.426805: step 4168, loss = 3.75 (10.9 examples/sec; 5.867 sec/batch)
2016-04-29 20:27:46.314833: step 4169, loss = 3.68 (10.9 examples/sec; 5.888 sec/batch)
2016-04-29 20:27:52.327263: step 4170, loss = 3.77 (10.6 examples/sec; 6.012 sec/batch)
2016-04-29 20:28:06.798902: step 4171, loss = 3.80 (9.9 examples/sec; 6.470 sec/batch)
2016-04-29 20:28:12.528003: step 4172, loss = 3.72 (11.2 examples/sec; 5.729 sec/batch)
2016-04-29 20:28:18.510737: step 4173, loss = 3.71 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 20:28:24.350005: step 4174, loss = 3.81 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 20:28:30.358750: step 4175, loss = 3.88 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 20:28:36.659212: step 4176, loss = 3.67 (10.2 examples/sec; 6.300 sec/batch)
2016-04-29 20:28:43.057038: step 4177, loss = 3.99 (10.0 examples/sec; 6.398 sec/batch)
2016-04-29 20:28:48.978043: step 4178, loss = 3.68 (10.8 examples/sec; 5.921 sec/batch)
2016-04-29 20:28:55.179767: step 4179, loss = 3.62 (10.3 examples/sec; 6.202 sec/batch)
2016-04-29 20:29:01.269590: step 4180, loss = 3.76 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 20:29:15.613140: step 4181, loss = 3.73 (10.0 examples/sec; 6.376 sec/batch)
2016-04-29 20:29:21.540485: step 4182, loss = 3.67 (10.8 examples/sec; 5.927 sec/batch)
2016-04-29 20:29:27.320889: step 4183, loss = 3.79 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 20:29:33.149720: step 4184, loss = 3.77 (11.0 examples/sec; 5.829 sec/batch)
2016-04-29 20:29:39.092804: step 4185, loss = 3.83 (10.8 examples/sec; 5.943 sec/batch)
2016-04-29 20:29:45.694460: step 4186, loss = 3.67 (9.7 examples/sec; 6.602 sec/batch)
2016-04-29 20:29:51.567389: step 4187, loss = 3.78 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 20:29:57.500517: step 4188, loss = 3.66 (10.8 examples/sec; 5.933 sec/batch)
2016-04-29 20:30:03.633944: step 4189, loss = 3.70 (10.4 examples/sec; 6.133 sec/batch)
2016-04-29 20:30:09.479716: step 4190, loss = 3.74 (10.9 examples/sec; 5.846 sec/batch)
2016-04-29 20:30:24.099275: step 4191, loss = 3.40 (11.4 examples/sec; 5.618 sec/batch)
2016-04-29 20:30:29.934230: step 4192, loss = 3.74 (11.0 examples/sec; 5.835 sec/batch)
2016-04-29 20:30:35.773127: step 4193, loss = 3.95 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 20:30:41.642958: step 4194, loss = 3.86 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 20:30:47.744008: step 4195, loss = 3.74 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 20:30:54.468291: step 4196, loss = 3.83 (9.5 examples/sec; 6.724 sec/batch)
2016-04-29 20:31:00.588166: step 4197, loss = 3.91 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 20:31:06.747668: step 4198, loss = 3.82 (10.4 examples/sec; 6.159 sec/batch)
2016-04-29 20:31:12.561629: step 4199, loss = 3.79 (11.0 examples/sec; 5.814 sec/batch)
2016-04-29 20:31:18.458152: step 4200, loss = 3.77 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 20:31:32.652163: step 4201, loss = 3.85 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 20:31:38.640338: step 4202, loss = 3.85 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 20:31:45.130985: step 4203, loss = 3.76 (9.9 examples/sec; 6.491 sec/batch)
2016-04-29 20:31:51.153009: step 4204, loss = 3.55 (10.6 examples/sec; 6.022 sec/batch)
2016-04-29 20:31:57.955963: step 4205, loss = 3.74 (9.4 examples/sec; 6.803 sec/batch)
2016-04-29 20:32:04.198383: step 4206, loss = 3.53 (10.3 examples/sec; 6.242 sec/batch)
2016-04-29 20:32:10.332507: step 4207, loss = 3.86 (10.4 examples/sec; 6.134 sec/batch)
2016-04-29 20:32:16.372455: step 4208, loss = 3.68 (10.6 examples/sec; 6.040 sec/batch)
2016-04-29 20:32:22.341095: step 4209, loss = 3.63 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 20:32:28.762453: step 4210, loss = 3.77 (10.0 examples/sec; 6.421 sec/batch)
2016-04-29 20:32:43.324580: step 4211, loss = 3.73 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 20:32:49.387627: step 4212, loss = 3.97 (10.6 examples/sec; 6.063 sec/batch)
2016-04-29 20:32:55.515353: step 4213, loss = 3.69 (10.4 examples/sec; 6.128 sec/batch)
2016-04-29 20:33:02.220456: step 4214, loss = 3.64 (9.5 examples/sec; 6.705 sec/batch)
2016-04-29 20:33:08.229301: step 4215, loss = 3.73 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 20:33:13.995065: step 4216, loss = 3.60 (11.1 examples/sec; 5.766 sec/batch)
2016-04-29 20:33:19.911434: step 4217, loss = 3.75 (10.8 examples/sec; 5.916 sec/batch)
2016-04-29 20:33:25.732180: step 4218, loss = 3.73 (11.0 examples/sec; 5.821 sec/batch)
2016-04-29 20:33:31.541404: step 4219, loss = 3.80 (11.0 examples/sec; 5.809 sec/batch)
2016-04-29 20:33:38.253264: step 4220, loss = 3.52 (9.5 examples/sec; 6.712 sec/batch)
2016-04-29 20:33:51.764157: step 4221, loss = 3.78 (11.3 examples/sec; 5.688 sec/batch)
2016-04-29 20:33:57.466050: step 4222, loss = 3.68 (11.2 examples/sec; 5.702 sec/batch)
2016-04-29 20:34:03.564474: step 4223, loss = 3.68 (10.5 examples/sec; 6.098 sec/batch)
2016-04-29 20:34:10.195348: step 4224, loss = 3.80 (9.7 examples/sec; 6.631 sec/batch)
2016-04-29 20:34:16.137078: step 4225, loss = 3.59 (10.8 examples/sec; 5.942 sec/batch)
2016-04-29 20:34:21.987932: step 4226, loss = 3.69 (10.9 examples/sec; 5.851 sec/batch)
2016-04-29 20:34:27.989287: step 4227, loss = 3.77 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 20:34:33.855842: step 4228, loss = 3.55 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 20:34:40.031092: step 4229, loss = 3.78 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 20:34:46.185241: step 4230, loss = 3.83 (10.4 examples/sec; 6.154 sec/batch)
2016-04-29 20:35:00.015112: step 4231, loss = 3.58 (11.2 examples/sec; 5.696 sec/batch)
2016-04-29 20:35:06.131978: step 4232, loss = 3.75 (10.5 examples/sec; 6.117 sec/batch)
2016-04-29 20:35:11.871397: step 4233, loss = 3.58 (11.2 examples/sec; 5.739 sec/batch)
2016-04-29 20:35:18.453227: step 4234, loss = 3.73 (9.7 examples/sec; 6.582 sec/batch)
2016-04-29 20:35:24.399175: step 4235, loss = 3.49 (10.8 examples/sec; 5.946 sec/batch)
2016-04-29 20:35:30.057431: step 4236, loss = 3.61 (11.3 examples/sec; 5.658 sec/batch)
2016-04-29 20:35:35.893097: step 4237, loss = 3.80 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 20:35:41.908259: step 4238, loss = 3.63 (10.6 examples/sec; 6.015 sec/batch)
2016-04-29 20:35:48.719597: step 4239, loss = 3.69 (9.4 examples/sec; 6.811 sec/batch)
2016-04-29 20:35:54.635475: step 4240, loss = 3.72 (10.8 examples/sec; 5.916 sec/batch)
2016-04-29 20:36:08.444926: step 4241, loss = 3.79 (11.4 examples/sec; 5.603 sec/batch)
2016-04-29 20:36:14.223829: step 4242, loss = 3.67 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 20:36:21.014617: step 4243, loss = 3.60 (9.4 examples/sec; 6.791 sec/batch)
2016-04-29 20:36:26.756346: step 4244, loss = 3.70 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 20:36:32.539822: step 4245, loss = 3.68 (11.1 examples/sec; 5.783 sec/batch)
2016-04-29 20:36:38.491742: step 4246, loss = 3.51 (10.8 examples/sec; 5.952 sec/batch)
2016-04-29 20:36:44.536847: step 4247, loss = 3.67 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 20:36:50.430640: step 4248, loss = 3.62 (10.9 examples/sec; 5.894 sec/batch)
2016-04-29 20:36:57.013929: step 4249, loss = 3.56 (9.7 examples/sec; 6.583 sec/batch)
2016-04-29 20:37:03.639623: step 4250, loss = 3.54 (9.7 examples/sec; 6.626 sec/batch)
2016-04-29 20:37:19.506644: step 4251, loss = 3.66 (10.0 examples/sec; 6.427 sec/batch)
2016-04-29 20:37:26.387299: step 4252, loss = 3.63 (9.3 examples/sec; 6.881 sec/batch)
2016-04-29 20:37:32.574076: step 4253, loss = 3.51 (10.3 examples/sec; 6.187 sec/batch)
2016-04-29 20:37:38.675104: step 4254, loss = 3.70 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 20:37:44.688325: step 4255, loss = 3.51 (10.6 examples/sec; 6.013 sec/batch)
2016-04-29 20:37:50.619587: step 4256, loss = 3.70 (10.8 examples/sec; 5.931 sec/batch)
2016-04-29 20:37:56.537411: step 4257, loss = 3.56 (10.8 examples/sec; 5.918 sec/batch)
2016-04-29 20:38:03.835049: step 4258, loss = 3.52 (8.8 examples/sec; 7.298 sec/batch)
2016-04-29 20:38:10.346999: step 4259, loss = 3.63 (9.8 examples/sec; 6.512 sec/batch)
2016-04-29 20:38:16.490015: step 4260, loss = 3.71 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 20:38:30.255647: step 4261, loss = 3.70 (11.3 examples/sec; 5.649 sec/batch)
2016-04-29 20:38:37.134271: step 4262, loss = 3.48 (9.3 examples/sec; 6.879 sec/batch)
2016-04-29 20:38:43.189282: step 4263, loss = 3.68 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 20:38:49.062457: step 4264, loss = 3.68 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 20:38:55.107610: step 4265, loss = 3.56 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 20:39:01.189906: step 4266, loss = 3.69 (10.5 examples/sec; 6.082 sec/batch)
2016-04-29 20:39:07.744651: step 4267, loss = 3.70 (9.8 examples/sec; 6.555 sec/batch)
2016-04-29 20:39:13.671013: step 4268, loss = 3.66 (10.8 examples/sec; 5.926 sec/batch)
2016-04-29 20:39:19.627121: step 4269, loss = 3.71 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 20:39:25.759443: step 4270, loss = 3.60 (10.4 examples/sec; 6.132 sec/batch)
2016-04-29 20:39:40.157325: step 4271, loss = 3.58 (10.0 examples/sec; 6.408 sec/batch)
2016-04-29 20:39:46.187971: step 4272, loss = 3.43 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 20:39:52.207925: step 4273, loss = 3.52 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 20:39:58.191259: step 4274, loss = 3.50 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 20:40:04.407487: step 4275, loss = 3.65 (10.3 examples/sec; 6.216 sec/batch)
2016-04-29 20:40:11.112965: step 4276, loss = 3.56 (9.5 examples/sec; 6.705 sec/batch)
2016-04-29 20:40:17.283774: step 4277, loss = 3.60 (10.4 examples/sec; 6.171 sec/batch)
2016-04-29 20:40:23.266325: step 4278, loss = 3.54 (10.7 examples/sec; 5.982 sec/batch)
2016-04-29 20:40:29.055813: step 4279, loss = 3.56 (11.1 examples/sec; 5.789 sec/batch)
2016-04-29 20:40:35.067698: step 4280, loss = 3.46 (10.6 examples/sec; 6.012 sec/batch)
2016-04-29 20:40:49.970320: step 4281, loss = 3.69 (11.2 examples/sec; 5.703 sec/batch)
2016-04-29 20:40:56.150824: step 4282, loss = 3.59 (10.4 examples/sec; 6.180 sec/batch)
2016-04-29 20:41:02.475126: step 4283, loss = 3.57 (10.1 examples/sec; 6.324 sec/batch)
2016-04-29 20:41:08.479601: step 4284, loss = 3.48 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 20:41:14.275867: step 4285, loss = 3.60 (11.0 examples/sec; 5.796 sec/batch)
2016-04-29 20:41:21.124356: step 4286, loss = 3.65 (9.3 examples/sec; 6.848 sec/batch)
2016-04-29 20:41:27.238371: step 4287, loss = 3.52 (10.5 examples/sec; 6.114 sec/batch)
2016-04-29 20:41:33.032861: step 4288, loss = 3.53 (11.0 examples/sec; 5.794 sec/batch)
2016-04-29 20:41:39.145119: step 4289, loss = 3.60 (10.5 examples/sec; 6.112 sec/batch)
2016-04-29 20:41:45.330339: step 4290, loss = 3.50 (10.3 examples/sec; 6.185 sec/batch)
2016-04-29 20:41:59.718811: step 4291, loss = 3.51 (11.1 examples/sec; 5.779 sec/batch)
2016-04-29 20:42:05.633451: step 4292, loss = 3.50 (10.8 examples/sec; 5.915 sec/batch)
2016-04-29 20:42:11.668477: step 4293, loss = 3.54 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 20:42:17.471421: step 4294, loss = 3.52 (11.0 examples/sec; 5.803 sec/batch)
2016-04-29 20:42:24.209175: step 4295, loss = 3.42 (9.5 examples/sec; 6.738 sec/batch)
2016-04-29 20:42:30.193979: step 4296, loss = 3.59 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 20:42:36.152163: step 4297, loss = 3.62 (10.7 examples/sec; 5.958 sec/batch)
2016-04-29 20:42:42.405917: step 4298, loss = 3.75 (10.2 examples/sec; 6.254 sec/batch)
2016-04-29 20:42:48.085104: step 4299, loss = 3.36 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 20:42:54.169793: step 4300, loss = 3.53 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 20:43:08.516483: step 4301, loss = 3.54 (11.3 examples/sec; 5.687 sec/batch)
2016-04-29 20:43:14.513795: step 4302, loss = 3.38 (10.7 examples/sec; 5.997 sec/batch)
2016-04-29 20:43:20.510417: step 4303, loss = 3.51 (10.7 examples/sec; 5.997 sec/batch)
2016-04-29 20:43:26.475434: step 4304, loss = 3.40 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 20:43:33.158616: step 4305, loss = 3.56 (9.6 examples/sec; 6.683 sec/batch)
2016-04-29 20:43:39.148228: step 4306, loss = 3.56 (10.7 examples/sec; 5.990 sec/batch)
2016-04-29 20:43:44.819213: step 4307, loss = 3.50 (11.3 examples/sec; 5.671 sec/batch)
2016-04-29 20:43:50.603319: step 4308, loss = 3.48 (11.1 examples/sec; 5.784 sec/batch)
2016-04-29 20:43:56.615352: step 4309, loss = 3.58 (10.6 examples/sec; 6.012 sec/batch)
2016-04-29 20:44:03.575208: step 4310, loss = 3.47 (9.2 examples/sec; 6.960 sec/batch)
2016-04-29 20:44:17.215778: step 4311, loss = 3.62 (10.9 examples/sec; 5.876 sec/batch)
2016-04-29 20:44:23.313280: step 4312, loss = 3.70 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 20:44:29.381389: step 4313, loss = 3.53 (10.5 examples/sec; 6.068 sec/batch)
2016-04-29 20:44:35.892614: step 4314, loss = 3.51 (9.8 examples/sec; 6.511 sec/batch)
2016-04-29 20:44:41.858001: step 4315, loss = 3.48 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 20:44:47.767782: step 4316, loss = 3.58 (10.8 examples/sec; 5.910 sec/batch)
2016-04-29 20:44:53.899514: step 4317, loss = 3.50 (10.4 examples/sec; 6.132 sec/batch)
2016-04-29 20:44:59.791139: step 4318, loss = 3.49 (10.9 examples/sec; 5.892 sec/batch)
2016-04-29 20:45:05.650235: step 4319, loss = 3.42 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 20:45:12.397667: step 4320, loss = 3.32 (9.5 examples/sec; 6.747 sec/batch)
2016-04-29 20:45:26.033220: step 4321, loss = 3.46 (10.9 examples/sec; 5.870 sec/batch)
2016-04-29 20:45:32.083019: step 4322, loss = 3.52 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 20:45:37.938030: step 4323, loss = 3.59 (10.9 examples/sec; 5.855 sec/batch)
2016-04-29 20:45:44.691933: step 4324, loss = 3.39 (9.5 examples/sec; 6.754 sec/batch)
2016-04-29 20:45:50.858360: step 4325, loss = 3.47 (10.4 examples/sec; 6.166 sec/batch)
2016-04-29 20:45:56.954752: step 4326, loss = 3.34 (10.5 examples/sec; 6.096 sec/batch)
2016-04-29 20:46:02.840167: step 4327, loss = 3.56 (10.9 examples/sec; 5.885 sec/batch)
2016-04-29 20:46:08.814196: step 4328, loss = 3.62 (10.7 examples/sec; 5.974 sec/batch)
2016-04-29 20:46:15.828363: step 4329, loss = 3.35 (9.1 examples/sec; 7.014 sec/batch)
2016-04-29 20:46:21.946662: step 4330, loss = 3.55 (10.5 examples/sec; 6.118 sec/batch)
2016-04-29 20:46:35.523299: step 4331, loss = 3.53 (11.2 examples/sec; 5.734 sec/batch)
2016-04-29 20:46:41.651396: step 4332, loss = 4.11 (10.4 examples/sec; 6.128 sec/batch)
2016-04-29 20:46:48.344349: step 4333, loss = 4.08 (9.6 examples/sec; 6.693 sec/batch)
2016-04-29 20:46:54.668520: step 4334, loss = 3.51 (10.1 examples/sec; 6.324 sec/batch)
2016-04-29 20:47:00.459107: step 4335, loss = 3.52 (11.1 examples/sec; 5.790 sec/batch)
2016-04-29 20:47:06.032523: step 4336, loss = 3.48 (11.5 examples/sec; 5.573 sec/batch)
2016-04-29 20:47:12.105316: step 4337, loss = 3.51 (10.5 examples/sec; 6.073 sec/batch)
2016-04-29 20:47:17.731587: step 4338, loss = 3.83 (11.4 examples/sec; 5.626 sec/batch)
2016-04-29 20:47:24.798778: step 4339, loss = 4.75 (9.1 examples/sec; 7.067 sec/batch)
2016-04-29 20:47:30.716563: step 4340, loss = 6.73 (10.8 examples/sec; 5.918 sec/batch)
2016-04-29 20:47:44.475291: step 4341, loss = 5.59 (10.9 examples/sec; 5.851 sec/batch)
2016-04-29 20:47:50.518270: step 4342, loss = 3.59 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 20:47:57.467862: step 4343, loss = 3.55 (9.2 examples/sec; 6.949 sec/batch)
2016-04-29 20:48:03.468299: step 4344, loss = 3.45 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 20:48:09.332375: step 4345, loss = 3.33 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 20:48:15.313628: step 4346, loss = 3.39 (10.7 examples/sec; 5.981 sec/batch)
2016-04-29 20:48:21.227558: step 4347, loss = 3.48 (10.8 examples/sec; 5.914 sec/batch)
2016-04-29 20:48:27.906109: step 4348, loss = 3.47 (9.6 examples/sec; 6.678 sec/batch)
2016-04-29 20:48:33.782101: step 4349, loss = 3.48 (10.9 examples/sec; 5.876 sec/batch)
2016-04-29 20:48:39.927814: step 4350, loss = 3.44 (10.4 examples/sec; 6.146 sec/batch)
2016-04-29 20:48:53.549341: step 4351, loss = 3.35 (11.2 examples/sec; 5.711 sec/batch)
2016-04-29 20:49:00.220046: step 4352, loss = 3.51 (9.6 examples/sec; 6.670 sec/batch)
2016-04-29 20:49:06.366319: step 4353, loss = 3.44 (10.4 examples/sec; 6.146 sec/batch)
2016-04-29 20:49:11.817829: step 4354, loss = 3.44 (11.7 examples/sec; 5.451 sec/batch)
2016-04-29 20:49:17.638216: step 4355, loss = 3.49 (11.0 examples/sec; 5.820 sec/batch)
2016-04-29 20:49:23.455637: step 4356, loss = 3.53 (11.0 examples/sec; 5.817 sec/batch)
2016-04-29 20:49:29.475978: step 4357, loss = 3.66 (10.6 examples/sec; 6.020 sec/batch)
2016-04-29 20:49:36.061863: step 4358, loss = 3.45 (9.7 examples/sec; 6.586 sec/batch)
2016-04-29 20:49:42.014664: step 4359, loss = 3.47 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 20:49:47.666350: step 4360, loss = 3.61 (11.3 examples/sec; 5.652 sec/batch)
2016-04-29 20:50:01.767321: step 4361, loss = 3.43 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 20:50:08.420454: step 4362, loss = 3.43 (9.6 examples/sec; 6.653 sec/batch)
2016-04-29 20:50:14.251307: step 4363, loss = 3.49 (11.0 examples/sec; 5.831 sec/batch)
2016-04-29 20:50:20.204422: step 4364, loss = 3.45 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 20:50:26.279027: step 4365, loss = 3.47 (10.5 examples/sec; 6.074 sec/batch)
2016-04-29 20:50:32.231766: step 4366, loss = 3.39 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 20:50:38.870067: step 4367, loss = 3.57 (9.6 examples/sec; 6.638 sec/batch)
2016-04-29 20:50:44.679381: step 4368, loss = 3.45 (11.0 examples/sec; 5.809 sec/batch)
2016-04-29 20:50:50.667969: step 4369, loss = 3.47 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 20:50:56.661257: step 4370, loss = 3.46 (10.7 examples/sec; 5.993 sec/batch)
2016-04-29 20:51:11.292486: step 4371, loss = 3.32 (9.9 examples/sec; 6.480 sec/batch)
2016-04-29 20:51:17.276163: step 4372, loss = 3.71 (10.7 examples/sec; 5.984 sec/batch)
2016-04-29 20:51:23.433781: step 4373, loss = 3.43 (10.4 examples/sec; 6.157 sec/batch)
2016-04-29 20:51:29.397803: step 4374, loss = 3.36 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 20:51:35.477005: step 4375, loss = 3.52 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 20:51:41.593580: step 4376, loss = 3.48 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 20:51:48.395249: step 4377, loss = 3.40 (9.4 examples/sec; 6.802 sec/batch)
2016-04-29 20:51:54.536211: step 4378, loss = 3.47 (10.4 examples/sec; 6.141 sec/batch)
2016-04-29 20:52:00.818240: step 4379, loss = 3.46 (10.2 examples/sec; 6.282 sec/batch)
2016-04-29 20:52:06.775917: step 4380, loss = 3.52 (10.7 examples/sec; 5.958 sec/batch)
2016-04-29 20:52:21.588151: step 4381, loss = 3.14 (9.3 examples/sec; 6.897 sec/batch)
2016-04-29 20:52:27.469321: step 4382, loss = 3.25 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 20:52:33.723781: step 4383, loss = 3.43 (10.2 examples/sec; 6.254 sec/batch)
2016-04-29 20:52:39.775904: step 4384, loss = 3.33 (10.6 examples/sec; 6.052 sec/batch)
2016-04-29 20:52:45.737087: step 4385, loss = 3.45 (10.7 examples/sec; 5.961 sec/batch)
2016-04-29 20:52:52.380795: step 4386, loss = 3.25 (9.6 examples/sec; 6.644 sec/batch)
2016-04-29 20:52:58.481157: step 4387, loss = 3.43 (10.5 examples/sec; 6.100 sec/batch)
2016-04-29 20:53:04.735667: step 4388, loss = 3.50 (10.2 examples/sec; 6.254 sec/batch)
2016-04-29 20:53:10.893610: step 4389, loss = 3.42 (10.4 examples/sec; 6.158 sec/batch)
2016-04-29 20:53:16.835485: step 4390, loss = 3.48 (10.8 examples/sec; 5.942 sec/batch)
2016-04-29 20:53:31.654854: step 4391, loss = 3.34 (11.0 examples/sec; 5.836 sec/batch)
2016-04-29 20:53:37.776023: step 4392, loss = 3.31 (10.5 examples/sec; 6.121 sec/batch)
2016-04-29 20:53:43.770670: step 4393, loss = 3.25 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 20:53:49.900759: step 4394, loss = 3.54 (10.4 examples/sec; 6.130 sec/batch)
2016-04-29 20:53:56.742709: step 4395, loss = 3.38 (9.4 examples/sec; 6.842 sec/batch)
2016-04-29 20:54:02.877995: step 4396, loss = 3.41 (10.4 examples/sec; 6.135 sec/batch)
2016-04-29 20:54:08.717477: step 4397, loss = 3.26 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 20:54:14.721049: step 4398, loss = 3.19 (10.7 examples/sec; 6.003 sec/batch)
2016-04-29 20:54:20.618144: step 4399, loss = 3.22 (10.9 examples/sec; 5.897 sec/batch)
2016-04-29 20:54:26.867000: step 4400, loss = 3.37 (10.2 examples/sec; 6.249 sec/batch)
2016-04-29 20:54:40.997780: step 4401, loss = 3.41 (11.2 examples/sec; 5.694 sec/batch)
2016-04-29 20:54:46.953895: step 4402, loss = 3.34 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 20:54:52.918069: step 4403, loss = 3.33 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 20:54:58.730283: step 4404, loss = 3.40 (11.0 examples/sec; 5.812 sec/batch)
2016-04-29 20:55:05.704674: step 4405, loss = 3.38 (9.2 examples/sec; 6.974 sec/batch)
2016-04-29 20:55:11.668545: step 4406, loss = 3.26 (10.7 examples/sec; 5.964 sec/batch)
2016-04-29 20:55:17.483002: step 4407, loss = 3.42 (11.0 examples/sec; 5.814 sec/batch)
2016-04-29 20:55:23.422416: step 4408, loss = 3.42 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 20:55:29.287628: step 4409, loss = 3.38 (10.9 examples/sec; 5.865 sec/batch)
2016-04-29 20:55:35.927417: step 4410, loss = 3.45 (9.6 examples/sec; 6.640 sec/batch)
2016-04-29 20:55:49.923774: step 4411, loss = 3.49 (10.9 examples/sec; 5.891 sec/batch)
2016-04-29 20:55:55.974083: step 4412, loss = 3.42 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 20:56:02.194229: step 4413, loss = 3.36 (10.3 examples/sec; 6.220 sec/batch)
2016-04-29 20:56:08.754277: step 4414, loss = 3.40 (9.8 examples/sec; 6.560 sec/batch)
2016-04-29 20:56:14.727722: step 4415, loss = 3.40 (10.7 examples/sec; 5.973 sec/batch)
2016-04-29 20:56:20.674505: step 4416, loss = 3.46 (10.8 examples/sec; 5.947 sec/batch)
2016-04-29 20:56:26.746751: step 4417, loss = 3.26 (10.5 examples/sec; 6.072 sec/batch)
2016-04-29 20:56:32.769822: step 4418, loss = 3.18 (10.6 examples/sec; 6.023 sec/batch)
2016-04-29 20:56:38.849268: step 4419, loss = 3.39 (10.5 examples/sec; 6.079 sec/batch)
2016-04-29 20:56:45.700479: step 4420, loss = 3.39 (9.3 examples/sec; 6.851 sec/batch)
2016-04-29 20:56:59.416716: step 4421, loss = 3.46 (10.9 examples/sec; 5.888 sec/batch)
2016-04-29 20:57:05.491473: step 4422, loss = 3.18 (10.5 examples/sec; 6.074 sec/batch)
2016-04-29 20:57:11.611482: step 4423, loss = 3.33 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 20:57:18.411808: step 4424, loss = 3.30 (9.4 examples/sec; 6.800 sec/batch)
2016-04-29 20:57:24.463633: step 4425, loss = 3.34 (10.6 examples/sec; 6.052 sec/batch)
2016-04-29 20:57:30.447589: step 4426, loss = 3.39 (10.7 examples/sec; 5.984 sec/batch)
2016-04-29 20:57:36.606839: step 4427, loss = 3.32 (10.4 examples/sec; 6.159 sec/batch)
2016-04-29 20:57:42.465506: step 4428, loss = 3.15 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 20:57:49.205720: step 4429, loss = 3.30 (9.5 examples/sec; 6.740 sec/batch)
2016-04-29 20:57:55.306508: step 4430, loss = 3.31 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 20:58:09.348453: step 4431, loss = 3.33 (11.1 examples/sec; 5.754 sec/batch)
2016-04-29 20:58:15.390645: step 4432, loss = 3.51 (10.6 examples/sec; 6.042 sec/batch)
2016-04-29 20:58:22.379877: step 4433, loss = 3.39 (9.2 examples/sec; 6.989 sec/batch)
2016-04-29 20:58:28.629787: step 4434, loss = 3.36 (10.2 examples/sec; 6.250 sec/batch)
2016-04-29 20:58:34.655413: step 4435, loss = 3.21 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 20:58:40.870809: step 4436, loss = 3.28 (10.3 examples/sec; 6.215 sec/batch)
2016-04-29 20:58:46.952393: step 4437, loss = 3.43 (10.5 examples/sec; 6.081 sec/batch)
2016-04-29 20:58:53.477759: step 4438, loss = 3.27 (9.8 examples/sec; 6.525 sec/batch)
2016-04-29 20:58:59.628482: step 4439, loss = 3.24 (10.4 examples/sec; 6.151 sec/batch)
2016-04-29 20:59:05.780501: step 4440, loss = 3.42 (10.4 examples/sec; 6.152 sec/batch)
2016-04-29 20:59:19.690991: step 4441, loss = 3.32 (11.1 examples/sec; 5.770 sec/batch)
2016-04-29 20:59:26.458578: step 4442, loss = 3.28 (9.5 examples/sec; 6.767 sec/batch)
2016-04-29 20:59:32.464922: step 4443, loss = 3.31 (10.7 examples/sec; 6.006 sec/batch)
2016-04-29 20:59:38.479408: step 4444, loss = 3.40 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 20:59:44.518083: step 4445, loss = 3.26 (10.6 examples/sec; 6.039 sec/batch)
2016-04-29 20:59:50.487566: step 4446, loss = 3.29 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 20:59:56.382267: step 4447, loss = 3.27 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 21:00:03.422183: step 4448, loss = 3.36 (9.1 examples/sec; 7.040 sec/batch)
2016-04-29 21:00:09.667688: step 4449, loss = 3.28 (10.2 examples/sec; 6.245 sec/batch)
2016-04-29 21:00:15.559892: step 4450, loss = 3.37 (10.9 examples/sec; 5.892 sec/batch)
2016-04-29 21:00:29.254878: step 4451, loss = 3.22 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 21:00:35.849580: step 4452, loss = 3.10 (9.7 examples/sec; 6.595 sec/batch)
2016-04-29 21:00:41.858770: step 4453, loss = 3.25 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 21:00:48.191767: step 4454, loss = 3.25 (10.1 examples/sec; 6.333 sec/batch)
2016-04-29 21:00:53.949998: step 4455, loss = 3.21 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 21:00:59.895392: step 4456, loss = 3.33 (10.8 examples/sec; 5.945 sec/batch)
2016-04-29 21:01:06.925957: step 4457, loss = 3.12 (9.1 examples/sec; 7.030 sec/batch)
2016-04-29 21:01:12.937865: step 4458, loss = 3.27 (10.6 examples/sec; 6.012 sec/batch)
2016-04-29 21:01:19.000323: step 4459, loss = 3.22 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 21:01:25.093924: step 4460, loss = 3.14 (10.5 examples/sec; 6.094 sec/batch)
2016-04-29 21:01:39.870517: step 4461, loss = 3.33 (9.7 examples/sec; 6.585 sec/batch)
2016-04-29 21:01:45.773713: step 4462, loss = 3.26 (10.8 examples/sec; 5.903 sec/batch)
2016-04-29 21:01:52.126314: step 4463, loss = 3.25 (10.1 examples/sec; 6.353 sec/batch)
2016-04-29 21:01:58.171595: step 4464, loss = 3.15 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 21:02:04.390734: step 4465, loss = 3.44 (10.3 examples/sec; 6.219 sec/batch)
2016-04-29 21:02:11.251368: step 4466, loss = 3.09 (9.3 examples/sec; 6.861 sec/batch)
2016-04-29 21:02:17.255077: step 4467, loss = 3.12 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 21:02:23.455859: step 4468, loss = 3.31 (10.3 examples/sec; 6.201 sec/batch)
2016-04-29 21:02:29.275139: step 4469, loss = 3.21 (11.0 examples/sec; 5.819 sec/batch)
2016-04-29 21:02:35.257759: step 4470, loss = 3.45 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 21:02:49.915826: step 4471, loss = 3.21 (10.3 examples/sec; 6.238 sec/batch)
2016-04-29 21:02:55.789322: step 4472, loss = 3.17 (10.9 examples/sec; 5.873 sec/batch)
2016-04-29 21:03:02.001847: step 4473, loss = 3.28 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 21:03:08.003175: step 4474, loss = 3.22 (10.7 examples/sec; 6.001 sec/batch)
2016-04-29 21:03:14.003696: step 4475, loss = 3.13 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 21:03:20.618360: step 4476, loss = 3.47 (9.7 examples/sec; 6.615 sec/batch)
2016-04-29 21:03:26.735284: step 4477, loss = 3.37 (10.5 examples/sec; 6.117 sec/batch)
2016-04-29 21:03:32.889967: step 4478, loss = 3.24 (10.4 examples/sec; 6.155 sec/batch)
2016-04-29 21:03:38.855769: step 4479, loss = 3.33 (10.7 examples/sec; 5.966 sec/batch)
2016-04-29 21:03:44.881278: step 4480, loss = 3.19 (10.6 examples/sec; 6.025 sec/batch)
2016-04-29 21:03:59.552934: step 4481, loss = 3.35 (11.2 examples/sec; 5.738 sec/batch)
2016-04-29 21:04:05.853895: step 4482, loss = 3.25 (10.2 examples/sec; 6.301 sec/batch)
2016-04-29 21:04:11.715297: step 4483, loss = 3.15 (10.9 examples/sec; 5.861 sec/batch)
2016-04-29 21:04:17.696447: step 4484, loss = 3.42 (10.7 examples/sec; 5.981 sec/batch)
2016-04-29 21:04:24.455948: step 4485, loss = 3.28 (9.5 examples/sec; 6.759 sec/batch)
2016-04-29 21:04:30.481655: step 4486, loss = 3.10 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 21:04:36.436537: step 4487, loss = 3.15 (10.7 examples/sec; 5.955 sec/batch)
2016-04-29 21:04:42.263246: step 4488, loss = 3.37 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 21:04:48.318306: step 4489, loss = 3.24 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 21:04:54.369804: step 4490, loss = 3.31 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 21:05:08.947373: step 4491, loss = 3.42 (11.2 examples/sec; 5.691 sec/batch)
2016-04-29 21:05:14.844292: step 4492, loss = 3.22 (10.9 examples/sec; 5.897 sec/batch)
2016-04-29 21:05:21.098691: step 4493, loss = 3.16 (10.2 examples/sec; 6.254 sec/batch)
2016-04-29 21:05:27.125582: step 4494, loss = 3.27 (10.6 examples/sec; 6.027 sec/batch)
2016-04-29 21:05:33.823266: step 4495, loss = 3.37 (9.6 examples/sec; 6.698 sec/batch)
2016-04-29 21:05:39.889760: step 4496, loss = 3.33 (10.5 examples/sec; 6.066 sec/batch)
2016-04-29 21:05:45.928232: step 4497, loss = 3.12 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 21:05:51.947547: step 4498, loss = 3.24 (10.6 examples/sec; 6.019 sec/batch)
2016-04-29 21:05:57.891508: step 4499, loss = 3.11 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 21:06:04.594250: step 4500, loss = 3.30 (9.5 examples/sec; 6.703 sec/batch)
2016-04-29 21:06:18.212593: step 4501, loss = 3.27 (11.0 examples/sec; 5.792 sec/batch)
2016-04-29 21:06:24.212723: step 4502, loss = 3.21 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 21:06:30.118953: step 4503, loss = 3.17 (10.8 examples/sec; 5.906 sec/batch)
2016-04-29 21:06:37.002440: step 4504, loss = 3.20 (9.3 examples/sec; 6.883 sec/batch)
2016-04-29 21:06:42.760491: step 4505, loss = 3.30 (11.1 examples/sec; 5.758 sec/batch)
2016-04-29 21:06:48.976354: step 4506, loss = 3.25 (10.3 examples/sec; 6.216 sec/batch)
2016-04-29 21:06:54.939245: step 4507, loss = 3.26 (10.7 examples/sec; 5.963 sec/batch)
2016-04-29 21:07:01.285818: step 4508, loss = 3.20 (10.1 examples/sec; 6.346 sec/batch)
2016-04-29 21:07:07.503874: step 4509, loss = 3.17 (10.3 examples/sec; 6.218 sec/batch)
2016-04-29 21:07:14.018618: step 4510, loss = 3.32 (9.8 examples/sec; 6.515 sec/batch)
2016-04-29 21:07:28.086328: step 4511, loss = 3.17 (10.9 examples/sec; 5.859 sec/batch)
2016-04-29 21:07:34.150495: step 4512, loss = 3.14 (10.6 examples/sec; 6.064 sec/batch)
2016-04-29 21:07:40.320436: step 4513, loss = 3.13 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 21:07:46.793156: step 4514, loss = 3.13 (9.9 examples/sec; 6.473 sec/batch)
2016-04-29 21:07:52.688209: step 4515, loss = 3.04 (10.9 examples/sec; 5.895 sec/batch)
2016-04-29 21:07:58.767868: step 4516, loss = 3.36 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 21:08:04.974207: step 4517, loss = 3.33 (10.3 examples/sec; 6.206 sec/batch)
2016-04-29 21:08:11.280773: step 4518, loss = 3.23 (10.1 examples/sec; 6.306 sec/batch)
2016-04-29 21:08:18.045235: step 4519, loss = 3.18 (9.5 examples/sec; 6.764 sec/batch)
2016-04-29 21:08:24.422497: step 4520, loss = 3.25 (10.0 examples/sec; 6.377 sec/batch)
2016-04-29 21:08:38.298531: step 4521, loss = 3.07 (10.8 examples/sec; 5.913 sec/batch)
2016-04-29 21:08:44.402677: step 4522, loss = 3.21 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 21:08:51.124497: step 4523, loss = 3.11 (9.5 examples/sec; 6.722 sec/batch)
2016-04-29 21:08:57.214591: step 4524, loss = 3.23 (10.5 examples/sec; 6.090 sec/batch)
2016-04-29 21:09:03.465461: step 4525, loss = 3.19 (10.2 examples/sec; 6.251 sec/batch)
2016-04-29 21:09:09.523771: step 4526, loss = 3.18 (10.6 examples/sec; 6.058 sec/batch)
2016-04-29 21:09:15.407170: step 4527, loss = 3.27 (10.9 examples/sec; 5.883 sec/batch)
2016-04-29 21:09:22.282911: step 4528, loss = 3.07 (9.3 examples/sec; 6.876 sec/batch)
2016-04-29 21:09:28.519560: step 4529, loss = 3.15 (10.3 examples/sec; 6.237 sec/batch)
2016-04-29 21:09:34.606977: step 4530, loss = 3.15 (10.5 examples/sec; 6.087 sec/batch)
2016-04-29 21:09:48.884828: step 4531, loss = 3.09 (11.0 examples/sec; 5.827 sec/batch)
2016-04-29 21:09:55.521089: step 4532, loss = 3.06 (9.6 examples/sec; 6.636 sec/batch)
2016-04-29 21:10:01.933278: step 4533, loss = 3.19 (10.0 examples/sec; 6.412 sec/batch)
2016-04-29 21:10:07.932183: step 4534, loss = 2.95 (10.7 examples/sec; 5.999 sec/batch)
2016-04-29 21:10:13.920152: step 4535, loss = 3.24 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 21:10:20.018981: step 4536, loss = 3.09 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:10:26.771363: step 4537, loss = 3.26 (9.5 examples/sec; 6.752 sec/batch)
2016-04-29 21:10:32.867985: step 4538, loss = 3.17 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 21:10:38.898670: step 4539, loss = 3.13 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 21:10:44.785926: step 4540, loss = 3.18 (10.9 examples/sec; 5.887 sec/batch)
2016-04-29 21:10:59.825576: step 4541, loss = 3.17 (9.6 examples/sec; 6.632 sec/batch)
2016-04-29 21:11:05.851780: step 4542, loss = 3.12 (10.6 examples/sec; 6.026 sec/batch)
2016-04-29 21:11:12.001684: step 4543, loss = 3.21 (10.4 examples/sec; 6.150 sec/batch)
2016-04-29 21:11:17.966894: step 4544, loss = 3.12 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 21:11:23.916460: step 4545, loss = 3.21 (10.8 examples/sec; 5.949 sec/batch)
2016-04-29 21:11:29.820103: step 4546, loss = 3.02 (10.8 examples/sec; 5.904 sec/batch)
2016-04-29 21:11:36.911363: step 4547, loss = 3.09 (9.0 examples/sec; 7.091 sec/batch)
2016-04-29 21:11:42.909991: step 4548, loss = 3.17 (10.7 examples/sec; 5.999 sec/batch)
2016-04-29 21:11:49.274289: step 4549, loss = 3.04 (10.1 examples/sec; 6.364 sec/batch)
2016-04-29 21:11:55.274523: step 4550, loss = 3.31 (10.7 examples/sec; 6.000 sec/batch)
2016-04-29 21:12:10.155403: step 4551, loss = 3.09 (9.6 examples/sec; 6.691 sec/batch)
2016-04-29 21:12:16.151063: step 4552, loss = 3.03 (10.7 examples/sec; 5.996 sec/batch)
2016-04-29 21:12:22.216739: step 4553, loss = 3.17 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 21:12:28.458062: step 4554, loss = 3.09 (10.3 examples/sec; 6.241 sec/batch)
2016-04-29 21:12:34.576561: step 4555, loss = 3.14 (10.5 examples/sec; 6.118 sec/batch)
2016-04-29 21:12:41.243112: step 4556, loss = 3.08 (9.6 examples/sec; 6.666 sec/batch)
2016-04-29 21:12:47.331802: step 4557, loss = 3.13 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 21:12:53.352616: step 4558, loss = 3.14 (10.6 examples/sec; 6.021 sec/batch)
2016-04-29 21:12:59.497749: step 4559, loss = 3.10 (10.4 examples/sec; 6.145 sec/batch)
2016-04-29 21:13:05.805422: step 4560, loss = 3.28 (10.1 examples/sec; 6.308 sec/batch)
2016-04-29 21:13:20.371453: step 4561, loss = 3.15 (11.2 examples/sec; 5.737 sec/batch)
2016-04-29 21:13:26.353646: step 4562, loss = 2.93 (10.7 examples/sec; 5.982 sec/batch)
2016-04-29 21:13:32.508469: step 4563, loss = 3.11 (10.4 examples/sec; 6.155 sec/batch)
2016-04-29 21:13:38.503843: step 4564, loss = 3.35 (10.7 examples/sec; 5.995 sec/batch)
2016-04-29 21:13:45.141481: step 4565, loss = 3.07 (9.6 examples/sec; 6.638 sec/batch)
2016-04-29 21:13:51.262436: step 4566, loss = 3.20 (10.5 examples/sec; 6.121 sec/batch)
2016-04-29 21:13:57.203158: step 4567, loss = 3.07 (10.8 examples/sec; 5.941 sec/batch)
2016-04-29 21:14:03.292571: step 4568, loss = 3.31 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 21:14:09.408436: step 4569, loss = 3.15 (10.5 examples/sec; 6.116 sec/batch)
2016-04-29 21:14:15.343159: step 4570, loss = 3.22 (10.8 examples/sec; 5.935 sec/batch)
2016-04-29 21:14:29.991296: step 4571, loss = 3.00 (10.9 examples/sec; 5.884 sec/batch)
2016-04-29 21:14:35.882257: step 4572, loss = 3.22 (10.9 examples/sec; 5.891 sec/batch)
2016-04-29 21:14:42.045362: step 4573, loss = 3.04 (10.4 examples/sec; 6.163 sec/batch)
2016-04-29 21:14:48.107201: step 4574, loss = 3.00 (10.6 examples/sec; 6.062 sec/batch)
2016-04-29 21:14:54.706893: step 4575, loss = 3.13 (9.7 examples/sec; 6.600 sec/batch)
2016-04-29 21:15:01.000957: step 4576, loss = 3.06 (10.2 examples/sec; 6.294 sec/batch)
2016-04-29 21:15:07.193606: step 4577, loss = 3.08 (10.3 examples/sec; 6.193 sec/batch)
2016-04-29 21:15:13.593542: step 4578, loss = 3.03 (10.0 examples/sec; 6.400 sec/batch)
2016-04-29 21:15:19.761424: step 4579, loss = 3.09 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 21:15:26.465953: step 4580, loss = 3.07 (9.5 examples/sec; 6.704 sec/batch)
2016-04-29 21:15:40.510488: step 4581, loss = 2.95 (10.8 examples/sec; 5.953 sec/batch)
2016-04-29 21:15:46.743988: step 4582, loss = 2.96 (10.3 examples/sec; 6.233 sec/batch)
2016-04-29 21:15:53.028667: step 4583, loss = 3.12 (10.2 examples/sec; 6.285 sec/batch)
2016-04-29 21:15:59.551509: step 4584, loss = 3.17 (9.8 examples/sec; 6.523 sec/batch)
2016-04-29 21:16:05.642114: step 4585, loss = 2.98 (10.5 examples/sec; 6.091 sec/batch)
2016-04-29 21:16:11.851266: step 4586, loss = 3.17 (10.3 examples/sec; 6.209 sec/batch)
2016-04-29 21:16:18.126397: step 4587, loss = 3.08 (10.2 examples/sec; 6.275 sec/batch)
2016-04-29 21:16:24.302002: step 4588, loss = 2.98 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 21:16:31.192406: step 4589, loss = 3.11 (9.3 examples/sec; 6.890 sec/batch)
2016-04-29 21:16:37.056299: step 4590, loss = 3.05 (10.9 examples/sec; 5.864 sec/batch)
2016-04-29 21:16:50.742119: step 4591, loss = 3.12 (10.9 examples/sec; 5.850 sec/batch)
2016-04-29 21:16:56.806272: step 4592, loss = 2.98 (10.6 examples/sec; 6.064 sec/batch)
2016-04-29 21:17:03.972003: step 4593, loss = 3.19 (8.9 examples/sec; 7.166 sec/batch)
2016-04-29 21:17:10.239135: step 4594, loss = 3.28 (10.2 examples/sec; 6.267 sec/batch)
2016-04-29 21:17:16.271190: step 4595, loss = 3.14 (10.6 examples/sec; 6.032 sec/batch)
2016-04-29 21:17:22.239557: step 4596, loss = 2.88 (10.7 examples/sec; 5.968 sec/batch)
2016-04-29 21:17:28.342786: step 4597, loss = 3.03 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 21:17:34.347062: step 4598, loss = 3.08 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 21:17:40.826657: step 4599, loss = 3.02 (9.9 examples/sec; 6.479 sec/batch)
2016-04-29 21:17:46.954916: step 4600, loss = 3.17 (10.4 examples/sec; 6.128 sec/batch)
2016-04-29 21:18:01.107415: step 4601, loss = 3.12 (10.7 examples/sec; 6.007 sec/batch)
2016-04-29 21:18:07.206397: step 4602, loss = 3.21 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:18:13.724814: step 4603, loss = 3.00 (9.8 examples/sec; 6.518 sec/batch)
2016-04-29 21:18:19.759813: step 4604, loss = 3.02 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 21:18:25.750712: step 4605, loss = 3.06 (10.7 examples/sec; 5.991 sec/batch)
2016-04-29 21:18:31.698640: step 4606, loss = 2.96 (10.8 examples/sec; 5.948 sec/batch)
2016-04-29 21:18:37.795869: step 4607, loss = 3.05 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 21:18:44.485867: step 4608, loss = 2.96 (9.6 examples/sec; 6.690 sec/batch)
2016-04-29 21:18:50.419174: step 4609, loss = 2.96 (10.8 examples/sec; 5.933 sec/batch)
2016-04-29 21:18:56.461954: step 4610, loss = 3.13 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 21:19:10.562943: step 4611, loss = 3.07 (11.0 examples/sec; 5.838 sec/batch)
2016-04-29 21:19:17.221107: step 4612, loss = 2.88 (9.6 examples/sec; 6.658 sec/batch)
2016-04-29 21:19:23.405495: step 4613, loss = 2.90 (10.3 examples/sec; 6.184 sec/batch)
2016-04-29 21:19:29.507754: step 4614, loss = 3.11 (10.5 examples/sec; 6.102 sec/batch)
2016-04-29 21:19:35.557889: step 4615, loss = 3.07 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:19:41.506258: step 4616, loss = 3.06 (10.8 examples/sec; 5.948 sec/batch)
2016-04-29 21:19:48.100002: step 4617, loss = 3.14 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 21:19:54.075736: step 4618, loss = 3.18 (10.7 examples/sec; 5.976 sec/batch)
2016-04-29 21:20:00.278602: step 4619, loss = 3.02 (10.3 examples/sec; 6.203 sec/batch)
2016-04-29 21:20:06.722261: step 4620, loss = 3.27 (9.9 examples/sec; 6.444 sec/batch)
2016-04-29 21:20:21.327407: step 4621, loss = 2.98 (9.8 examples/sec; 6.521 sec/batch)
2016-04-29 21:20:27.348035: step 4622, loss = 3.08 (10.6 examples/sec; 6.021 sec/batch)
2016-04-29 21:20:33.381649: step 4623, loss = 3.22 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 21:20:39.417006: step 4624, loss = 3.03 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 21:20:45.585434: step 4625, loss = 3.04 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 21:20:51.711880: step 4626, loss = 2.97 (10.4 examples/sec; 6.126 sec/batch)
2016-04-29 21:20:58.353528: step 4627, loss = 2.83 (9.6 examples/sec; 6.642 sec/batch)
2016-04-29 21:21:04.565949: step 4628, loss = 3.09 (10.3 examples/sec; 6.212 sec/batch)
2016-04-29 21:21:10.499794: step 4629, loss = 3.04 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 21:21:16.483374: step 4630, loss = 2.95 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 21:21:31.045877: step 4631, loss = 3.01 (9.5 examples/sec; 6.721 sec/batch)
2016-04-29 21:21:37.092156: step 4632, loss = 3.06 (10.6 examples/sec; 6.046 sec/batch)
2016-04-29 21:21:43.046207: step 4633, loss = 3.27 (10.7 examples/sec; 5.954 sec/batch)
2016-04-29 21:21:49.344607: step 4634, loss = 3.08 (10.2 examples/sec; 6.298 sec/batch)
2016-04-29 21:21:55.222431: step 4635, loss = 3.13 (10.9 examples/sec; 5.878 sec/batch)
2016-04-29 21:22:01.975484: step 4636, loss = 3.18 (9.5 examples/sec; 6.753 sec/batch)
2016-04-29 21:22:07.871477: step 4637, loss = 2.97 (10.9 examples/sec; 5.896 sec/batch)
2016-04-29 21:22:13.968525: step 4638, loss = 3.00 (10.5 examples/sec; 6.097 sec/batch)
2016-04-29 21:22:19.962414: step 4639, loss = 2.89 (10.7 examples/sec; 5.994 sec/batch)
2016-04-29 21:22:26.083087: step 4640, loss = 3.04 (10.5 examples/sec; 6.121 sec/batch)
2016-04-29 21:22:40.694712: step 4641, loss = 2.96 (10.8 examples/sec; 5.925 sec/batch)
2016-04-29 21:22:46.781438: step 4642, loss = 3.11 (10.5 examples/sec; 6.087 sec/batch)
2016-04-29 21:22:52.707267: step 4643, loss = 3.20 (10.8 examples/sec; 5.926 sec/batch)
2016-04-29 21:22:58.968384: step 4644, loss = 3.13 (10.2 examples/sec; 6.261 sec/batch)
2016-04-29 21:23:05.768207: step 4645, loss = 3.02 (9.4 examples/sec; 6.800 sec/batch)
2016-04-29 21:23:11.857708: step 4646, loss = 2.87 (10.5 examples/sec; 6.089 sec/batch)
2016-04-29 21:23:17.670523: step 4647, loss = 3.19 (11.0 examples/sec; 5.813 sec/batch)
2016-04-29 21:23:23.648962: step 4648, loss = 2.93 (10.7 examples/sec; 5.978 sec/batch)
2016-04-29 21:23:29.823901: step 4649, loss = 3.05 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 21:23:35.773984: step 4650, loss = 3.10 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 21:23:51.262953: step 4651, loss = 2.86 (9.7 examples/sec; 6.587 sec/batch)
2016-04-29 21:23:57.413497: step 4652, loss = 2.96 (10.4 examples/sec; 6.150 sec/batch)
2016-04-29 21:24:03.595081: step 4653, loss = 2.88 (10.4 examples/sec; 6.181 sec/batch)
2016-04-29 21:24:10.675558: step 4654, loss = 2.96 (9.0 examples/sec; 7.080 sec/batch)
2016-04-29 21:24:17.422620: step 4655, loss = 3.11 (9.5 examples/sec; 6.747 sec/batch)
2016-04-29 21:24:23.472877: step 4656, loss = 2.98 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:24:29.575780: step 4657, loss = 2.87 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 21:24:35.846317: step 4658, loss = 2.96 (10.2 examples/sec; 6.270 sec/batch)
2016-04-29 21:24:42.046450: step 4659, loss = 3.18 (10.3 examples/sec; 6.200 sec/batch)
2016-04-29 21:24:48.784481: step 4660, loss = 3.00 (9.5 examples/sec; 6.738 sec/batch)
2016-04-29 21:25:03.067936: step 4661, loss = 2.90 (10.6 examples/sec; 6.049 sec/batch)
2016-04-29 21:25:08.894277: step 4662, loss = 3.02 (11.0 examples/sec; 5.826 sec/batch)
2016-04-29 21:25:14.700074: step 4663, loss = 2.92 (11.0 examples/sec; 5.806 sec/batch)
2016-04-29 21:25:21.262763: step 4664, loss = 2.98 (9.8 examples/sec; 6.563 sec/batch)
2016-04-29 21:25:27.189422: step 4665, loss = 3.10 (10.8 examples/sec; 5.927 sec/batch)
2016-04-29 21:25:33.270188: step 4666, loss = 3.00 (10.5 examples/sec; 6.081 sec/batch)
2016-04-29 21:25:39.177089: step 4667, loss = 3.02 (10.8 examples/sec; 5.907 sec/batch)
2016-04-29 21:25:45.141930: step 4668, loss = 2.81 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 21:25:51.765913: step 4669, loss = 3.07 (9.7 examples/sec; 6.624 sec/batch)
2016-04-29 21:25:57.533138: step 4670, loss = 2.94 (11.1 examples/sec; 5.767 sec/batch)
2016-04-29 21:26:11.816135: step 4671, loss = 3.01 (10.9 examples/sec; 5.865 sec/batch)
2016-04-29 21:26:17.766651: step 4672, loss = 2.96 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 21:26:24.393239: step 4673, loss = 2.95 (9.7 examples/sec; 6.627 sec/batch)
2016-04-29 21:26:30.450926: step 4674, loss = 3.08 (10.6 examples/sec; 6.058 sec/batch)
2016-04-29 21:26:36.430187: step 4675, loss = 3.04 (10.7 examples/sec; 5.979 sec/batch)
2016-04-29 21:26:42.198353: step 4676, loss = 3.18 (11.1 examples/sec; 5.768 sec/batch)
2016-04-29 21:26:48.242847: step 4677, loss = 3.08 (10.6 examples/sec; 6.044 sec/batch)
2016-04-29 21:26:54.122510: step 4678, loss = 2.88 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 21:27:01.054457: step 4679, loss = 2.89 (9.2 examples/sec; 6.932 sec/batch)
2016-04-29 21:27:07.082930: step 4680, loss = 3.04 (10.6 examples/sec; 6.028 sec/batch)
2016-04-29 21:27:20.678217: step 4681, loss = 3.01 (10.8 examples/sec; 5.929 sec/batch)
2016-04-29 21:27:26.791490: step 4682, loss = 3.21 (10.5 examples/sec; 6.113 sec/batch)
2016-04-29 21:27:33.398083: step 4683, loss = 3.05 (9.7 examples/sec; 6.607 sec/batch)
2016-04-29 21:27:39.670475: step 4684, loss = 2.86 (10.2 examples/sec; 6.272 sec/batch)
2016-04-29 21:27:45.814082: step 4685, loss = 2.92 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 21:27:51.682092: step 4686, loss = 2.81 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 21:27:57.540503: step 4687, loss = 2.96 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 21:28:04.730002: step 4688, loss = 2.85 (8.9 examples/sec; 7.189 sec/batch)
2016-04-29 21:28:10.930305: step 4689, loss = 3.01 (10.3 examples/sec; 6.200 sec/batch)
2016-04-29 21:28:17.232472: step 4690, loss = 2.91 (10.2 examples/sec; 6.302 sec/batch)
2016-04-29 21:28:31.031319: step 4691, loss = 3.04 (11.0 examples/sec; 5.814 sec/batch)
2016-04-29 21:28:37.607213: step 4692, loss = 3.00 (9.7 examples/sec; 6.576 sec/batch)
2016-04-29 21:28:43.894131: step 4693, loss = 2.82 (10.2 examples/sec; 6.287 sec/batch)
2016-04-29 21:28:49.964368: step 4694, loss = 3.07 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 21:28:55.999382: step 4695, loss = 3.06 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 21:29:02.136869: step 4696, loss = 2.92 (10.4 examples/sec; 6.137 sec/batch)
2016-04-29 21:29:08.778873: step 4697, loss = 3.08 (9.6 examples/sec; 6.642 sec/batch)
2016-04-29 21:29:14.811108: step 4698, loss = 2.99 (10.6 examples/sec; 6.032 sec/batch)
2016-04-29 21:29:20.799598: step 4699, loss = 2.95 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 21:29:26.857721: step 4700, loss = 3.04 (10.6 examples/sec; 6.058 sec/batch)
2016-04-29 21:29:41.225604: step 4701, loss = 2.89 (10.3 examples/sec; 6.207 sec/batch)
2016-04-29 21:29:47.240140: step 4702, loss = 2.99 (10.6 examples/sec; 6.014 sec/batch)
2016-04-29 21:29:53.333006: step 4703, loss = 2.93 (10.5 examples/sec; 6.093 sec/batch)
2016-04-29 21:29:59.234174: step 4704, loss = 2.90 (10.8 examples/sec; 5.901 sec/batch)
2016-04-29 21:30:05.464720: step 4705, loss = 3.01 (10.3 examples/sec; 6.230 sec/batch)
2016-04-29 21:30:11.418363: step 4706, loss = 2.98 (10.7 examples/sec; 5.954 sec/batch)
2016-04-29 21:30:18.065796: step 4707, loss = 2.99 (9.6 examples/sec; 6.647 sec/batch)
2016-04-29 21:30:24.115996: step 4708, loss = 3.01 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:30:30.101557: step 4709, loss = 2.90 (10.7 examples/sec; 5.985 sec/batch)
2016-04-29 21:30:36.067670: step 4710, loss = 2.96 (10.7 examples/sec; 5.966 sec/batch)
2016-04-29 21:30:50.841093: step 4711, loss = 2.92 (10.0 examples/sec; 6.429 sec/batch)
2016-04-29 21:30:56.795742: step 4712, loss = 2.95 (10.7 examples/sec; 5.955 sec/batch)
2016-04-29 21:31:03.110208: step 4713, loss = 3.04 (10.1 examples/sec; 6.314 sec/batch)
2016-04-29 21:31:08.975066: step 4714, loss = 2.92 (10.9 examples/sec; 5.865 sec/batch)
2016-04-29 21:31:14.741069: step 4715, loss = 3.01 (11.1 examples/sec; 5.766 sec/batch)
2016-04-29 21:31:21.331148: step 4716, loss = 2.91 (9.7 examples/sec; 6.590 sec/batch)
2016-04-29 21:31:27.267756: step 4717, loss = 2.95 (10.8 examples/sec; 5.937 sec/batch)
2016-04-29 21:31:33.371448: step 4718, loss = 2.80 (10.5 examples/sec; 6.104 sec/batch)
2016-04-29 21:31:39.570422: step 4719, loss = 2.89 (10.3 examples/sec; 6.199 sec/batch)
2016-04-29 21:31:45.403011: step 4720, loss = 2.82 (11.0 examples/sec; 5.832 sec/batch)
2016-04-29 21:31:59.965179: step 4721, loss = 2.90 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 21:32:06.146812: step 4722, loss = 2.89 (10.4 examples/sec; 6.182 sec/batch)
2016-04-29 21:32:12.182048: step 4723, loss = 2.91 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 21:32:18.281431: step 4724, loss = 2.88 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:32:24.187889: step 4725, loss = 2.97 (10.8 examples/sec; 5.906 sec/batch)
2016-04-29 21:32:30.892190: step 4726, loss = 2.84 (9.5 examples/sec; 6.704 sec/batch)
2016-04-29 21:32:36.804407: step 4727, loss = 2.87 (10.8 examples/sec; 5.912 sec/batch)
2016-04-29 21:32:42.743921: step 4728, loss = 2.98 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 21:32:48.746869: step 4729, loss = 2.90 (10.7 examples/sec; 6.003 sec/batch)
2016-04-29 21:32:54.632446: step 4730, loss = 2.92 (10.9 examples/sec; 5.885 sec/batch)
2016-04-29 21:33:09.186563: step 4731, loss = 2.72 (11.0 examples/sec; 5.822 sec/batch)
2016-04-29 21:33:15.031853: step 4732, loss = 2.88 (10.9 examples/sec; 5.845 sec/batch)
2016-04-29 21:33:21.223677: step 4733, loss = 2.89 (10.3 examples/sec; 6.192 sec/batch)
2016-04-29 21:33:27.403310: step 4734, loss = 3.01 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 21:33:33.988675: step 4735, loss = 2.87 (9.7 examples/sec; 6.585 sec/batch)
2016-04-29 21:33:39.736946: step 4736, loss = 2.96 (11.1 examples/sec; 5.748 sec/batch)
2016-04-29 21:33:45.770631: step 4737, loss = 2.78 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 21:33:51.948045: step 4738, loss = 3.07 (10.4 examples/sec; 6.177 sec/batch)
2016-04-29 21:33:57.828688: step 4739, loss = 2.73 (10.9 examples/sec; 5.881 sec/batch)
2016-04-29 21:34:03.870366: step 4740, loss = 2.97 (10.6 examples/sec; 6.042 sec/batch)
2016-04-29 21:34:17.909471: step 4741, loss = 2.75 (11.3 examples/sec; 5.674 sec/batch)
2016-04-29 21:34:23.944426: step 4742, loss = 2.93 (10.6 examples/sec; 6.035 sec/batch)
2016-04-29 21:34:29.899408: step 4743, loss = 2.86 (10.7 examples/sec; 5.955 sec/batch)
2016-04-29 21:34:36.090470: step 4744, loss = 3.02 (10.3 examples/sec; 6.191 sec/batch)
2016-04-29 21:34:42.672086: step 4745, loss = 3.01 (9.7 examples/sec; 6.582 sec/batch)
2016-04-29 21:34:48.813827: step 4746, loss = 2.87 (10.4 examples/sec; 6.142 sec/batch)
2016-04-29 21:34:54.863630: step 4747, loss = 2.78 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:35:00.990936: step 4748, loss = 2.86 (10.4 examples/sec; 6.127 sec/batch)
2016-04-29 21:35:06.909093: step 4749, loss = 2.91 (10.8 examples/sec; 5.918 sec/batch)
2016-04-29 21:35:13.709113: step 4750, loss = 3.10 (9.4 examples/sec; 6.800 sec/batch)
2016-04-29 21:35:26.987848: step 4751, loss = 2.91 (11.6 examples/sec; 5.523 sec/batch)
2016-04-29 21:35:32.925492: step 4752, loss = 2.69 (10.8 examples/sec; 5.938 sec/batch)
2016-04-29 21:35:38.934441: step 4753, loss = 2.75 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 21:35:45.536179: step 4754, loss = 2.88 (9.7 examples/sec; 6.602 sec/batch)
2016-04-29 21:35:51.642347: step 4755, loss = 2.82 (10.5 examples/sec; 6.106 sec/batch)
2016-04-29 21:35:57.290462: step 4756, loss = 2.82 (11.3 examples/sec; 5.648 sec/batch)
2016-04-29 21:36:03.609474: step 4757, loss = 2.76 (10.1 examples/sec; 6.319 sec/batch)
2016-04-29 21:36:09.553132: step 4758, loss = 2.84 (10.8 examples/sec; 5.944 sec/batch)
2016-04-29 21:36:15.643999: step 4759, loss = 2.85 (10.5 examples/sec; 6.091 sec/batch)
2016-04-29 21:36:22.373688: step 4760, loss = 2.88 (9.5 examples/sec; 6.730 sec/batch)
2016-04-29 21:36:36.012455: step 4761, loss = 2.90 (11.1 examples/sec; 5.741 sec/batch)
2016-04-29 21:36:42.219857: step 4762, loss = 2.88 (10.3 examples/sec; 6.207 sec/batch)
2016-04-29 21:36:48.299824: step 4763, loss = 2.85 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 21:36:54.877878: step 4764, loss = 2.91 (9.7 examples/sec; 6.578 sec/batch)
2016-04-29 21:37:00.887149: step 4765, loss = 2.78 (10.7 examples/sec; 6.009 sec/batch)
2016-04-29 21:37:06.816084: step 4766, loss = 2.79 (10.8 examples/sec; 5.929 sec/batch)
2016-04-29 21:37:12.574683: step 4767, loss = 2.83 (11.1 examples/sec; 5.759 sec/batch)
2016-04-29 21:37:18.693361: step 4768, loss = 3.02 (10.5 examples/sec; 6.119 sec/batch)
2016-04-29 21:37:25.547141: step 4769, loss = 2.99 (9.3 examples/sec; 6.854 sec/batch)
2016-04-29 21:37:31.355575: step 4770, loss = 2.84 (11.0 examples/sec; 5.808 sec/batch)
2016-04-29 21:37:45.194342: step 4771, loss = 2.75 (11.2 examples/sec; 5.718 sec/batch)
2016-04-29 21:37:51.159226: step 4772, loss = 2.93 (10.7 examples/sec; 5.965 sec/batch)
2016-04-29 21:37:57.840897: step 4773, loss = 2.92 (9.6 examples/sec; 6.682 sec/batch)
2016-04-29 21:38:04.301408: step 4774, loss = 2.70 (9.9 examples/sec; 6.460 sec/batch)
2016-04-29 21:38:10.787125: step 4775, loss = 3.01 (9.9 examples/sec; 6.486 sec/batch)
2016-04-29 21:38:16.838556: step 4776, loss = 2.62 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:38:22.699179: step 4777, loss = 2.81 (10.9 examples/sec; 5.861 sec/batch)
2016-04-29 21:38:28.987087: step 4778, loss = 2.97 (10.2 examples/sec; 6.288 sec/batch)
2016-04-29 21:38:35.339284: step 4779, loss = 2.72 (10.1 examples/sec; 6.352 sec/batch)
2016-04-29 21:38:41.438186: step 4780, loss = 2.78 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:38:55.884778: step 4781, loss = 2.75 (10.9 examples/sec; 5.884 sec/batch)
2016-04-29 21:39:02.577671: step 4782, loss = 2.96 (9.6 examples/sec; 6.692 sec/batch)
2016-04-29 21:39:08.898690: step 4783, loss = 2.63 (10.1 examples/sec; 6.321 sec/batch)
2016-04-29 21:39:14.829625: step 4784, loss = 3.02 (10.8 examples/sec; 5.931 sec/batch)
2016-04-29 21:39:20.988461: step 4785, loss = 2.87 (10.4 examples/sec; 6.159 sec/batch)
2016-04-29 21:39:26.864619: step 4786, loss = 2.75 (10.9 examples/sec; 5.876 sec/batch)
2016-04-29 21:39:32.965743: step 4787, loss = 2.62 (10.5 examples/sec; 6.101 sec/batch)
2016-04-29 21:39:39.543380: step 4788, loss = 3.04 (9.7 examples/sec; 6.578 sec/batch)
2016-04-29 21:39:45.499399: step 4789, loss = 2.88 (10.7 examples/sec; 5.956 sec/batch)
2016-04-29 21:39:51.291770: step 4790, loss = 2.66 (11.0 examples/sec; 5.792 sec/batch)
2016-04-29 21:40:05.714789: step 4791, loss = 2.77 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 21:40:12.223966: step 4792, loss = 2.83 (9.8 examples/sec; 6.509 sec/batch)
2016-04-29 21:40:18.285506: step 4793, loss = 3.01 (10.6 examples/sec; 6.061 sec/batch)
2016-04-29 21:40:24.195332: step 4794, loss = 2.90 (10.8 examples/sec; 5.910 sec/batch)
2016-04-29 21:40:30.175983: step 4795, loss = 2.93 (10.7 examples/sec; 5.981 sec/batch)
2016-04-29 21:40:36.084455: step 4796, loss = 2.84 (10.8 examples/sec; 5.908 sec/batch)
2016-04-29 21:40:42.540723: step 4797, loss = 2.93 (9.9 examples/sec; 6.456 sec/batch)
2016-04-29 21:40:48.691603: step 4798, loss = 2.91 (10.4 examples/sec; 6.151 sec/batch)
2016-04-29 21:40:54.609708: step 4799, loss = 2.87 (10.8 examples/sec; 5.918 sec/batch)
2016-04-29 21:41:00.954289: step 4800, loss = 2.89 (10.1 examples/sec; 6.344 sec/batch)
2016-04-29 21:41:15.558177: step 4801, loss = 2.61 (9.7 examples/sec; 6.604 sec/batch)
2016-04-29 21:41:21.475376: step 4802, loss = 2.76 (10.8 examples/sec; 5.917 sec/batch)
2016-04-29 21:41:27.580457: step 4803, loss = 2.75 (10.5 examples/sec; 6.105 sec/batch)
2016-04-29 21:41:33.665419: step 4804, loss = 2.91 (10.5 examples/sec; 6.085 sec/batch)
2016-04-29 21:41:39.608738: step 4805, loss = 2.81 (10.8 examples/sec; 5.943 sec/batch)
2016-04-29 21:41:45.700854: step 4806, loss = 2.86 (10.5 examples/sec; 6.092 sec/batch)
2016-04-29 21:41:52.294578: step 4807, loss = 3.00 (9.7 examples/sec; 6.594 sec/batch)
2016-04-29 21:41:58.093094: step 4808, loss = 2.67 (11.0 examples/sec; 5.798 sec/batch)
2016-04-29 21:42:04.456495: step 4809, loss = 2.74 (10.1 examples/sec; 6.363 sec/batch)
2016-04-29 21:42:10.542617: step 4810, loss = 2.87 (10.5 examples/sec; 6.086 sec/batch)
2016-04-29 21:42:25.256084: step 4811, loss = 2.84 (9.7 examples/sec; 6.568 sec/batch)
2016-04-29 21:42:31.215729: step 4812, loss = 2.58 (10.7 examples/sec; 5.960 sec/batch)
2016-04-29 21:42:37.182008: step 4813, loss = 2.71 (10.7 examples/sec; 5.966 sec/batch)
2016-04-29 21:42:42.962513: step 4814, loss = 3.02 (11.1 examples/sec; 5.780 sec/batch)
2016-04-29 21:42:49.455866: step 4815, loss = 2.72 (9.9 examples/sec; 6.493 sec/batch)
2016-04-29 21:42:56.228250: step 4816, loss = 2.76 (9.5 examples/sec; 6.772 sec/batch)
2016-04-29 21:43:02.599515: step 4817, loss = 2.72 (10.0 examples/sec; 6.371 sec/batch)
2016-04-29 21:43:08.742784: step 4818, loss = 2.82 (10.4 examples/sec; 6.143 sec/batch)
2016-04-29 21:43:14.764104: step 4819, loss = 2.86 (10.6 examples/sec; 6.021 sec/batch)
2016-04-29 21:43:20.767807: step 4820, loss = 2.86 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 21:43:35.522900: step 4821, loss = 2.74 (11.1 examples/sec; 5.754 sec/batch)
2016-04-29 21:43:41.500431: step 4822, loss = 2.84 (10.7 examples/sec; 5.977 sec/batch)
2016-04-29 21:43:47.656481: step 4823, loss = 2.68 (10.4 examples/sec; 6.156 sec/batch)
2016-04-29 21:43:53.707817: step 4824, loss = 2.88 (10.6 examples/sec; 6.051 sec/batch)
2016-04-29 21:44:00.531255: step 4825, loss = 2.84 (9.4 examples/sec; 6.823 sec/batch)
2016-04-29 21:44:06.349241: step 4826, loss = 2.70 (11.0 examples/sec; 5.818 sec/batch)
2016-04-29 21:44:12.490413: step 4827, loss = 2.77 (10.4 examples/sec; 6.141 sec/batch)
2016-04-29 21:44:18.657647: step 4828, loss = 2.92 (10.4 examples/sec; 6.167 sec/batch)
2016-04-29 21:44:24.619968: step 4829, loss = 2.81 (10.7 examples/sec; 5.962 sec/batch)
2016-04-29 21:44:30.459047: step 4830, loss = 2.86 (11.0 examples/sec; 5.839 sec/batch)
2016-04-29 21:44:45.128448: step 4831, loss = 2.59 (10.8 examples/sec; 5.931 sec/batch)
2016-04-29 21:44:51.162963: step 4832, loss = 2.62 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 21:44:57.311725: step 4833, loss = 2.90 (10.4 examples/sec; 6.149 sec/batch)
2016-04-29 21:45:03.524573: step 4834, loss = 2.93 (10.3 examples/sec; 6.213 sec/batch)
2016-04-29 21:45:10.245344: step 4835, loss = 2.72 (9.5 examples/sec; 6.721 sec/batch)
2016-04-29 21:45:16.228458: step 4836, loss = 2.72 (10.7 examples/sec; 5.983 sec/batch)
2016-04-29 21:45:22.018817: step 4837, loss = 2.86 (11.1 examples/sec; 5.790 sec/batch)
2016-04-29 21:45:27.885221: step 4838, loss = 2.75 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 21:45:33.812217: step 4839, loss = 2.75 (10.8 examples/sec; 5.927 sec/batch)
2016-04-29 21:45:40.438633: step 4840, loss = 2.72 (9.7 examples/sec; 6.626 sec/batch)
2016-04-29 21:45:54.185343: step 4841, loss = 2.84 (11.3 examples/sec; 5.679 sec/batch)
2016-04-29 21:46:00.142879: step 4842, loss = 2.90 (10.7 examples/sec; 5.957 sec/batch)
2016-04-29 21:46:06.283968: step 4843, loss = 2.65 (10.4 examples/sec; 6.141 sec/batch)
2016-04-29 21:46:13.227201: step 4844, loss = 2.88 (9.2 examples/sec; 6.943 sec/batch)
2016-04-29 21:46:19.402803: step 4845, loss = 2.68 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 21:46:25.295657: step 4846, loss = 2.70 (10.9 examples/sec; 5.893 sec/batch)
2016-04-29 21:46:30.999382: step 4847, loss = 2.77 (11.2 examples/sec; 5.704 sec/batch)
2016-04-29 21:46:37.033201: step 4848, loss = 2.82 (10.6 examples/sec; 6.034 sec/batch)
2016-04-29 21:46:42.728608: step 4849, loss = 2.78 (11.2 examples/sec; 5.695 sec/batch)
2016-04-29 21:46:49.476938: step 4850, loss = 2.74 (9.5 examples/sec; 6.748 sec/batch)
2016-04-29 21:47:03.288258: step 4851, loss = 2.83 (10.9 examples/sec; 5.867 sec/batch)
2016-04-29 21:47:09.162097: step 4852, loss = 2.65 (10.9 examples/sec; 5.874 sec/batch)
2016-04-29 21:47:15.169720: step 4853, loss = 2.77 (10.7 examples/sec; 6.008 sec/batch)
2016-04-29 21:47:21.826809: step 4854, loss = 2.61 (9.6 examples/sec; 6.657 sec/batch)
2016-04-29 21:47:27.532739: step 4855, loss = 2.73 (11.2 examples/sec; 5.706 sec/batch)
2016-04-29 21:47:33.710550: step 4856, loss = 2.76 (10.4 examples/sec; 6.178 sec/batch)
2016-04-29 21:47:39.540456: step 4857, loss = 2.71 (11.0 examples/sec; 5.830 sec/batch)
2016-04-29 21:47:45.595142: step 4858, loss = 2.80 (10.6 examples/sec; 6.055 sec/batch)
2016-04-29 21:47:52.266313: step 4859, loss = 2.76 (9.6 examples/sec; 6.671 sec/batch)
2016-04-29 21:47:58.308939: step 4860, loss = 2.58 (10.6 examples/sec; 6.043 sec/batch)
2016-04-29 21:48:12.110544: step 4861, loss = 2.75 (11.0 examples/sec; 5.802 sec/batch)
2016-04-29 21:48:18.209953: step 4862, loss = 2.71 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:48:24.821841: step 4863, loss = 2.73 (9.7 examples/sec; 6.612 sec/batch)
2016-04-29 21:48:30.506269: step 4864, loss = 2.86 (11.3 examples/sec; 5.684 sec/batch)
2016-04-29 21:48:36.452092: step 4865, loss = 2.84 (10.8 examples/sec; 5.946 sec/batch)
2016-04-29 21:48:42.551307: step 4866, loss = 2.72 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:48:48.631213: step 4867, loss = 2.60 (10.5 examples/sec; 6.080 sec/batch)
2016-04-29 21:48:54.538115: step 4868, loss = 2.89 (10.8 examples/sec; 5.907 sec/batch)
2016-04-29 21:49:01.868021: step 4869, loss = 2.96 (8.7 examples/sec; 7.330 sec/batch)
2016-04-29 21:49:07.633711: step 4870, loss = 2.66 (11.1 examples/sec; 5.766 sec/batch)
2016-04-29 21:49:21.266139: step 4871, loss = 2.81 (11.0 examples/sec; 5.841 sec/batch)
2016-04-29 21:49:27.436310: step 4872, loss = 2.80 (10.4 examples/sec; 6.170 sec/batch)
2016-04-29 21:49:33.999756: step 4873, loss = 2.41 (9.8 examples/sec; 6.563 sec/batch)
2016-04-29 21:49:39.914528: step 4874, loss = 2.66 (10.8 examples/sec; 5.915 sec/batch)
2016-04-29 21:49:45.945400: step 4875, loss = 2.76 (10.6 examples/sec; 6.031 sec/batch)
2016-04-29 21:49:51.887312: step 4876, loss = 2.75 (10.8 examples/sec; 5.942 sec/batch)
2016-04-29 21:49:57.567667: step 4877, loss = 2.63 (11.3 examples/sec; 5.680 sec/batch)
2016-04-29 21:50:04.443748: step 4878, loss = 2.80 (9.3 examples/sec; 6.876 sec/batch)
2016-04-29 21:50:10.461435: step 4879, loss = 2.66 (10.6 examples/sec; 6.018 sec/batch)
2016-04-29 21:50:16.329427: step 4880, loss = 2.54 (10.9 examples/sec; 5.868 sec/batch)
2016-04-29 21:50:30.053084: step 4881, loss = 2.94 (11.1 examples/sec; 5.769 sec/batch)
2016-04-29 21:50:36.596930: step 4882, loss = 2.86 (9.8 examples/sec; 6.544 sec/batch)
2016-04-29 21:50:42.588800: step 4883, loss = 2.58 (10.7 examples/sec; 5.992 sec/batch)
2016-04-29 21:50:48.441427: step 4884, loss = 2.83 (10.9 examples/sec; 5.853 sec/batch)
2016-04-29 21:50:54.275387: step 4885, loss = 2.87 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 21:51:00.378060: step 4886, loss = 2.65 (10.5 examples/sec; 6.103 sec/batch)
2016-04-29 21:51:06.415791: step 4887, loss = 2.68 (10.6 examples/sec; 6.038 sec/batch)
2016-04-29 21:51:13.156033: step 4888, loss = 2.70 (9.5 examples/sec; 6.740 sec/batch)
2016-04-29 21:51:18.744743: step 4889, loss = 2.70 (11.5 examples/sec; 5.589 sec/batch)
2016-04-29 21:51:24.656429: step 4890, loss = 2.68 (10.8 examples/sec; 5.912 sec/batch)
2016-04-29 21:51:38.425499: step 4891, loss = 2.62 (11.1 examples/sec; 5.742 sec/batch)
2016-04-29 21:51:45.457524: step 4892, loss = 2.77 (9.1 examples/sec; 7.032 sec/batch)
2016-04-29 21:51:52.769224: step 4893, loss = 2.74 (8.8 examples/sec; 7.312 sec/batch)
2016-04-29 21:51:59.175576: step 4894, loss = 2.56 (10.0 examples/sec; 6.406 sec/batch)
2016-04-29 21:52:05.549568: step 4895, loss = 2.71 (10.0 examples/sec; 6.374 sec/batch)
2016-04-29 21:52:11.724953: step 4896, loss = 2.61 (10.4 examples/sec; 6.175 sec/batch)
2016-04-29 21:52:18.599094: step 4897, loss = 2.70 (9.3 examples/sec; 6.874 sec/batch)
2016-04-29 21:52:24.479320: step 4898, loss = 2.63 (10.9 examples/sec; 5.880 sec/batch)
2016-04-29 21:52:30.516339: step 4899, loss = 2.72 (10.6 examples/sec; 6.037 sec/batch)
2016-04-29 21:52:36.755237: step 4900, loss = 2.73 (10.3 examples/sec; 6.239 sec/batch)
2016-04-29 21:52:51.608831: step 4901, loss = 2.73 (9.8 examples/sec; 6.536 sec/batch)
2016-04-29 21:52:57.600522: step 4902, loss = 2.65 (10.7 examples/sec; 5.992 sec/batch)
2016-04-29 21:53:04.055259: step 4903, loss = 2.73 (9.9 examples/sec; 6.455 sec/batch)
2016-04-29 21:53:10.048449: step 4904, loss = 2.73 (10.7 examples/sec; 5.993 sec/batch)
2016-04-29 21:53:16.098019: step 4905, loss = 2.71 (10.6 examples/sec; 6.049 sec/batch)
2016-04-29 21:53:23.071762: step 4906, loss = 2.45 (9.2 examples/sec; 6.974 sec/batch)
2016-04-29 21:53:29.319179: step 4907, loss = 2.62 (10.2 examples/sec; 6.247 sec/batch)
2016-04-29 21:53:35.185182: step 4908, loss = 2.76 (10.9 examples/sec; 5.866 sec/batch)
2016-04-29 21:53:41.297613: step 4909, loss = 2.81 (10.5 examples/sec; 6.112 sec/batch)
2016-04-29 21:53:47.404144: step 4910, loss = 2.69 (10.5 examples/sec; 6.106 sec/batch)
2016-04-29 21:54:02.223240: step 4911, loss = 2.62 (10.5 examples/sec; 6.083 sec/batch)
2016-04-29 21:54:08.306257: step 4912, loss = 2.71 (10.5 examples/sec; 6.083 sec/batch)
2016-04-29 21:54:14.396960: step 4913, loss = 2.80 (10.5 examples/sec; 6.091 sec/batch)
2016-04-29 21:54:20.137319: step 4914, loss = 2.61 (11.1 examples/sec; 5.740 sec/batch)
2016-04-29 21:54:26.511597: step 4915, loss = 2.68 (10.0 examples/sec; 6.374 sec/batch)
2016-04-29 21:54:32.697867: step 4916, loss = 2.50 (10.3 examples/sec; 6.186 sec/batch)
2016-04-29 21:54:38.880926: step 4917, loss = 2.79 (10.4 examples/sec; 6.183 sec/batch)
2016-04-29 21:54:44.910525: step 4918, loss = 2.55 (10.6 examples/sec; 6.030 sec/batch)
2016-04-29 21:54:50.761731: step 4919, loss = 2.61 (10.9 examples/sec; 5.851 sec/batch)
2016-04-29 21:54:56.861303: step 4920, loss = 2.67 (10.5 examples/sec; 6.099 sec/batch)
2016-04-29 21:55:11.518510: step 4921, loss = 2.72 (11.3 examples/sec; 5.642 sec/batch)
2016-04-29 21:55:17.559203: step 4922, loss = 2.79 (10.6 examples/sec; 6.041 sec/batch)
2016-04-29 21:55:23.466803: step 4923, loss = 2.55 (10.8 examples/sec; 5.908 sec/batch)
2016-04-29 21:55:29.550605: step 4924, loss = 2.69 (10.5 examples/sec; 6.084 sec/batch)
2016-04-29 21:55:36.004069: step 4925, loss = 2.65 (9.9 examples/sec; 6.453 sec/batch)
2016-04-29 21:55:42.164448: step 4926, loss = 2.82 (10.4 examples/sec; 6.160 sec/batch)
2016-04-29 21:55:48.007597: step 4927, loss = 2.69 (11.0 examples/sec; 5.843 sec/batch)
2016-04-29 21:55:53.783802: step 4928, loss = 2.79 (11.1 examples/sec; 5.776 sec/batch)
2016-04-29 21:55:59.780299: step 4929, loss = 2.66 (10.7 examples/sec; 5.996 sec/batch)
2016-04-29 21:56:06.435319: step 4930, loss = 2.51 (9.6 examples/sec; 6.655 sec/batch)
2016-04-29 21:56:20.249915: step 4931, loss = 2.70 (11.2 examples/sec; 5.730 sec/batch)
2016-04-29 21:56:26.221105: step 4932, loss = 2.70 (10.7 examples/sec; 5.971 sec/batch)
2016-04-29 21:56:32.189835: step 4933, loss = 2.72 (10.7 examples/sec; 5.969 sec/batch)
2016-04-29 21:56:38.255576: step 4934, loss = 2.55 (10.6 examples/sec; 6.066 sec/batch)
2016-04-29 21:56:44.629842: step 4935, loss = 2.56 (10.0 examples/sec; 6.374 sec/batch)
2016-04-29 21:56:50.675344: step 4936, loss = 2.72 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 21:56:56.715622: step 4937, loss = 2.84 (10.6 examples/sec; 6.040 sec/batch)
2016-04-29 21:57:02.760342: step 4938, loss = 2.72 (10.6 examples/sec; 6.045 sec/batch)
2016-04-29 21:57:09.203953: step 4939, loss = 2.69 (9.9 examples/sec; 6.444 sec/batch)
2016-04-29 21:57:15.901155: step 4940, loss = 2.66 (9.6 examples/sec; 6.697 sec/batch)
2016-04-29 21:57:29.785218: step 4941, loss = 2.91 (11.3 examples/sec; 5.677 sec/batch)
2016-04-29 21:57:35.835195: step 4942, loss = 2.70 (10.6 examples/sec; 6.050 sec/batch)
2016-04-29 21:57:42.022819: step 4943, loss = 2.72 (10.3 examples/sec; 6.188 sec/batch)
2016-04-29 21:57:48.701411: step 4944, loss = 2.59 (9.6 examples/sec; 6.678 sec/batch)
2016-04-29 21:57:54.771140: step 4945, loss = 2.65 (10.5 examples/sec; 6.070 sec/batch)
2016-04-29 21:58:00.939644: step 4946, loss = 2.82 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 21:58:06.874152: step 4947, loss = 2.55 (10.8 examples/sec; 5.934 sec/batch)
2016-04-29 21:58:12.806093: step 4948, loss = 2.55 (10.8 examples/sec; 5.932 sec/batch)
2016-04-29 21:58:19.748051: step 4949, loss = 2.93 (9.2 examples/sec; 6.942 sec/batch)
2016-04-29 21:58:25.676428: step 4950, loss = 2.70 (10.8 examples/sec; 5.928 sec/batch)
2016-04-29 21:58:39.394379: step 4951, loss = 2.86 (11.3 examples/sec; 5.653 sec/batch)
2016-04-29 21:58:45.366251: step 4952, loss = 2.55 (10.7 examples/sec; 5.972 sec/batch)
2016-04-29 21:58:51.982616: step 4953, loss = 2.86 (9.7 examples/sec; 6.616 sec/batch)
2016-04-29 21:58:57.862155: step 4954, loss = 2.74 (10.9 examples/sec; 5.879 sec/batch)
2016-04-29 21:59:03.968904: step 4955, loss = 2.44 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 21:59:09.873013: step 4956, loss = 2.59 (10.8 examples/sec; 5.904 sec/batch)
2016-04-29 21:59:15.761938: step 4957, loss = 2.67 (10.9 examples/sec; 5.889 sec/batch)
2016-04-29 21:59:21.596523: step 4958, loss = 2.61 (11.0 examples/sec; 5.834 sec/batch)
2016-04-29 21:59:28.003602: step 4959, loss = 2.64 (10.0 examples/sec; 6.407 sec/batch)
2016-04-29 21:59:33.903075: step 4960, loss = 2.61 (10.8 examples/sec; 5.899 sec/batch)
2016-04-29 21:59:47.856338: step 4961, loss = 2.57 (11.1 examples/sec; 5.782 sec/batch)
2016-04-29 21:59:53.772547: step 4962, loss = 2.77 (10.8 examples/sec; 5.916 sec/batch)
2016-04-29 22:00:00.398942: step 4963, loss = 2.65 (9.7 examples/sec; 6.626 sec/batch)
2016-04-29 22:00:06.567315: step 4964, loss = 2.71 (10.4 examples/sec; 6.168 sec/batch)
2016-04-29 22:00:12.746082: step 4965, loss = 2.65 (10.4 examples/sec; 6.179 sec/batch)
2016-04-29 22:00:18.603981: step 4966, loss = 2.58 (10.9 examples/sec; 5.858 sec/batch)
2016-04-29 22:00:24.413777: step 4967, loss = 2.72 (11.0 examples/sec; 5.810 sec/batch)
2016-04-29 22:00:30.986400: step 4968, loss = 2.52 (9.7 examples/sec; 6.573 sec/batch)
2016-04-29 22:00:36.928459: step 4969, loss = 2.53 (10.8 examples/sec; 5.942 sec/batch)
2016-04-29 22:00:42.932190: step 4970, loss = 2.59 (10.7 examples/sec; 6.004 sec/batch)
2016-04-29 22:00:56.644524: step 4971, loss = 2.78 (11.1 examples/sec; 5.752 sec/batch)
2016-04-29 22:01:03.340176: step 4972, loss = 2.62 (9.6 examples/sec; 6.696 sec/batch)
2016-04-29 22:01:09.196102: step 4973, loss = 2.69 (10.9 examples/sec; 5.856 sec/batch)
2016-04-29 22:01:15.318078: step 4974, loss = 2.52 (10.5 examples/sec; 6.122 sec/batch)
2016-04-29 22:01:21.257410: step 4975, loss = 2.59 (10.8 examples/sec; 5.939 sec/batch)
2016-04-29 22:01:27.192936: step 4976, loss = 2.71 (10.8 examples/sec; 5.935 sec/batch)
2016-04-29 22:01:33.310983: step 4977, loss = 2.51 (10.5 examples/sec; 6.118 sec/batch)
2016-04-29 22:01:39.986903: step 4978, loss = 2.92 (9.6 examples/sec; 6.676 sec/batch)
2016-04-29 22:01:46.168934: step 4979, loss = 2.69 (10.4 examples/sec; 6.182 sec/batch)
2016-04-29 22:01:52.215342: step 4980, loss = 2.76 (10.6 examples/sec; 6.046 sec/batch)
2016-04-29 22:02:06.170343: step 4981, loss = 2.56 (10.9 examples/sec; 5.850 sec/batch)
2016-04-29 22:02:12.782126: step 4982, loss = 2.66 (9.7 examples/sec; 6.612 sec/batch)
2016-04-29 22:02:18.643540: step 4983, loss = 2.72 (10.9 examples/sec; 5.861 sec/batch)
2016-04-29 22:02:24.700855: step 4984, loss = 2.55 (10.6 examples/sec; 6.057 sec/batch)
2016-04-29 22:02:30.820474: step 4985, loss = 2.48 (10.5 examples/sec; 6.120 sec/batch)
2016-04-29 22:02:36.648753: step 4986, loss = 2.82 (11.0 examples/sec; 5.828 sec/batch)
2016-04-29 22:02:43.605245: step 4987, loss = 2.59 (9.2 examples/sec; 6.956 sec/batch)
2016-04-29 22:02:49.593670: step 4988, loss = 2.58 (10.7 examples/sec; 5.988 sec/batch)
2016-04-29 22:02:55.482041: step 4989, loss = 2.53 (10.9 examples/sec; 5.888 sec/batch)
2016-04-29 22:03:01.432351: step 4990, loss = 2.59 (10.8 examples/sec; 5.950 sec/batch)
2016-04-29 22:03:15.790328: step 4991, loss = 2.57 (10.1 examples/sec; 6.310 sec/batch)
2016-04-29 22:03:21.904748: step 4992, loss = 2.59 (10.5 examples/sec; 6.114 sec/batch)
2016-04-29 22:03:27.683064: step 4993, loss = 2.79 (11.1 examples/sec; 5.778 sec/batch)
2016-04-29 22:03:33.611699: step 4994, loss = 2.60 (10.8 examples/sec; 5.929 sec/batch)
2016-04-29 22:03:39.483443: step 4995, loss = 2.52 (10.9 examples/sec; 5.872 sec/batch)
2016-04-29 22:03:45.590814: step 4996, loss = 2.57 (10.5 examples/sec; 6.107 sec/batch)
2016-04-29 22:03:52.387434: step 4997, loss = 2.52 (9.4 examples/sec; 6.797 sec/batch)
2016-04-29 22:03:58.474937: step 4998, loss = 2.71 (10.5 examples/sec; 6.087 sec/batch)
2016-04-29 22:04:04.571665: step 4999, loss = 2.71 (10.5 examples/sec; 6.097 sec/batch)

Process finished with exit code 0
