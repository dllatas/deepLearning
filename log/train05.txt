/usr/bin/python2.7 /home/neo/projects/dl/cifar10_train.py
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-04-27 23:39:44.572221: step 0, loss = 164.46 (0.3 examples/sec; 225.992 sec/batch)
2016-04-27 23:41:14.276922: step 1, loss = 164.44 (1.6 examples/sec; 40.652 sec/batch)
2016-04-27 23:41:51.790637: step 2, loss = 164.16 (1.7 examples/sec; 37.511 sec/batch)
2016-04-27 23:42:24.895050: step 3, loss = 164.03 (1.9 examples/sec; 33.104 sec/batch)
2016-04-27 23:42:57.931066: step 4, loss = 164.18 (1.9 examples/sec; 33.036 sec/batch)
2016-04-27 23:43:30.981098: step 5, loss = 164.05 (1.9 examples/sec; 33.050 sec/batch)
2016-04-27 23:44:03.638447: step 6, loss = 164.09 (2.0 examples/sec; 32.657 sec/batch)
2016-04-27 23:44:36.598228: step 7, loss = 163.87 (1.9 examples/sec; 32.960 sec/batch)
2016-04-27 23:45:09.282205: step 8, loss = 163.91 (2.0 examples/sec; 32.684 sec/batch)
2016-04-27 23:45:41.755665: step 9, loss = 163.78 (2.0 examples/sec; 32.473 sec/batch)
2016-04-27 23:46:22.026622: step 10, loss = 163.84 (1.6 examples/sec; 40.271 sec/batch)
2016-04-27 23:47:38.316217: step 11, loss = 163.76 (1.9 examples/sec; 34.481 sec/batch)
2016-04-27 23:48:13.067176: step 12, loss = 163.77 (1.8 examples/sec; 34.749 sec/batch)
2016-04-27 23:48:47.168667: step 13, loss = 163.60 (1.9 examples/sec; 34.101 sec/batch)
2016-04-27 23:49:21.795022: step 14, loss = 163.58 (1.8 examples/sec; 34.626 sec/batch)
2016-04-27 23:49:55.762151: step 15, loss = 163.68 (1.9 examples/sec; 33.967 sec/batch)
2016-04-27 23:50:29.939689: step 16, loss = 163.77 (1.9 examples/sec; 34.177 sec/batch)
2016-04-27 23:51:04.408040: step 17, loss = 163.95 (1.9 examples/sec; 34.468 sec/batch)
2016-04-27 23:51:41.169225: step 18, loss = 164.08 (1.7 examples/sec; 36.761 sec/batch)
2016-04-27 23:52:13.952750: step 19, loss = 164.01 (2.0 examples/sec; 32.783 sec/batch)
2016-04-27 23:52:48.350158: step 20, loss = 163.50 (1.9 examples/sec; 34.397 sec/batch)
2016-04-27 23:54:00.774444: step 21, loss = 163.85 (2.0 examples/sec; 32.361 sec/batch)
2016-04-27 23:54:33.647111: step 22, loss = 163.29 (1.9 examples/sec; 32.873 sec/batch)
2016-04-27 23:55:06.373251: step 23, loss = 163.47 (2.0 examples/sec; 32.726 sec/batch)
2016-04-27 23:55:39.127966: step 24, loss = 163.53 (2.0 examples/sec; 32.755 sec/batch)
2016-04-27 23:56:12.082652: step 25, loss = 163.88 (1.9 examples/sec; 32.955 sec/batch)
2016-04-27 23:56:44.950301: step 26, loss = 167.11 (1.9 examples/sec; 32.868 sec/batch)
2016-04-27 23:57:17.736810: step 27, loss = 165.79 (2.0 examples/sec; 32.786 sec/batch)
2016-04-27 23:57:56.589055: step 28, loss = 163.67 (1.6 examples/sec; 38.852 sec/batch)
2016-04-27 23:58:31.219778: step 29, loss = 163.36 (1.8 examples/sec; 34.631 sec/batch)
2016-04-27 23:59:05.625958: step 30, loss = 163.29 (1.9 examples/sec; 34.405 sec/batch)
2016-04-28 00:00:21.636078: step 31, loss = 163.31 (1.9 examples/sec; 34.354 sec/batch)
2016-04-28 00:00:55.880649: step 32, loss = 163.02 (1.9 examples/sec; 34.244 sec/batch)
2016-04-28 00:01:30.066206: step 33, loss = 163.11 (1.9 examples/sec; 34.185 sec/batch)
2016-04-28 00:02:04.745134: step 34, loss = 162.69 (1.8 examples/sec; 34.679 sec/batch)
2016-04-28 00:02:38.214478: step 35, loss = 162.83 (1.9 examples/sec; 33.469 sec/batch)
2016-04-28 00:03:12.092449: step 36, loss = 162.68 (1.9 examples/sec; 33.878 sec/batch)
2016-04-28 00:03:46.172363: step 37, loss = 162.58 (1.9 examples/sec; 34.080 sec/batch)
2016-04-28 00:04:20.456388: step 38, loss = 163.06 (1.9 examples/sec; 34.284 sec/batch)
2016-04-28 00:04:54.269447: step 39, loss = 163.66 (1.9 examples/sec; 33.813 sec/batch)
2016-04-28 00:05:28.438176: step 40, loss = 166.66 (1.9 examples/sec; 34.169 sec/batch)
2016-04-28 00:06:43.393281: step 41, loss = 167.75 (1.9 examples/sec; 33.655 sec/batch)
2016-04-28 00:07:18.012737: step 42, loss = 163.41 (1.8 examples/sec; 34.619 sec/batch)
2016-04-28 00:07:52.303176: step 43, loss = 163.05 (1.9 examples/sec; 34.290 sec/batch)
2016-04-28 00:08:26.819142: step 44, loss = 163.15 (1.9 examples/sec; 34.516 sec/batch)
2016-04-28 00:09:00.973003: step 45, loss = 162.87 (1.9 examples/sec; 34.154 sec/batch)
2016-04-28 00:09:34.997882: step 46, loss = 162.65 (1.9 examples/sec; 34.025 sec/batch)
2016-04-28 00:10:09.336731: step 47, loss = 162.74 (1.9 examples/sec; 34.339 sec/batch)
2016-04-28 00:10:43.091763: step 48, loss = 162.44 (1.9 examples/sec; 33.755 sec/batch)
2016-04-28 00:11:17.593882: step 49, loss = 162.30 (1.9 examples/sec; 34.502 sec/batch)
2016-04-28 00:11:51.359833: step 50, loss = 162.01 (1.9 examples/sec; 33.766 sec/batch)
2016-04-28 00:13:06.512312: step 51, loss = 163.59 (1.9 examples/sec; 34.174 sec/batch)
2016-04-28 00:13:40.949640: step 52, loss = 173.31 (1.9 examples/sec; 34.437 sec/batch)
2016-04-28 00:14:15.043902: step 53, loss = 188.31 (1.9 examples/sec; 34.094 sec/batch)
2016-04-28 00:14:49.445678: step 54, loss = 162.48 (1.9 examples/sec; 34.402 sec/batch)
2016-04-28 00:15:24.922359: step 55, loss = 162.40 (1.8 examples/sec; 35.477 sec/batch)
2016-04-28 00:16:00.437184: step 56, loss = 162.24 (1.8 examples/sec; 35.515 sec/batch)
2016-04-28 00:16:35.323786: step 57, loss = 162.31 (1.8 examples/sec; 34.887 sec/batch)
2016-04-28 00:17:10.480734: step 58, loss = 161.99 (1.8 examples/sec; 35.157 sec/batch)
2016-04-28 00:17:44.991491: step 59, loss = 162.18 (1.9 examples/sec; 34.511 sec/batch)
2016-04-28 00:18:19.163685: step 60, loss = 162.10 (1.9 examples/sec; 34.172 sec/batch)
2016-04-28 00:19:35.252266: step 61, loss = 162.21 (1.9 examples/sec; 34.405 sec/batch)
2016-04-28 00:20:10.059160: step 62, loss = 161.85 (1.8 examples/sec; 34.807 sec/batch)
2016-04-28 00:20:44.680003: step 63, loss = 162.07 (1.8 examples/sec; 34.621 sec/batch)
2016-04-28 00:21:19.767140: step 64, loss = 161.91 (1.8 examples/sec; 35.087 sec/batch)
2016-04-28 00:21:54.798647: step 65, loss = 161.74 (1.8 examples/sec; 35.031 sec/batch)
2016-04-28 00:22:30.154458: step 66, loss = 161.79 (1.8 examples/sec; 35.356 sec/batch)
2016-04-28 00:23:04.996484: step 67, loss = 161.84 (1.8 examples/sec; 34.842 sec/batch)
2016-04-28 00:23:40.595552: step 68, loss = 161.64 (1.8 examples/sec; 35.599 sec/batch)
2016-04-28 00:24:15.726016: step 69, loss = 161.75 (1.8 examples/sec; 35.130 sec/batch)
2016-04-28 00:24:50.819821: step 70, loss = 161.57 (1.8 examples/sec; 35.094 sec/batch)
2016-04-28 00:26:08.697291: step 71, loss = 161.50 (1.8 examples/sec; 35.225 sec/batch)
2016-04-28 00:26:43.606242: step 72, loss = 161.44 (1.8 examples/sec; 34.909 sec/batch)
2016-04-28 00:27:18.696623: step 73, loss = 161.07 (1.8 examples/sec; 35.090 sec/batch)
2016-04-28 00:27:53.602195: step 74, loss = 185.36 (1.8 examples/sec; 34.906 sec/batch)
2016-04-28 00:28:27.945228: step 75, loss = 193.31 (1.9 examples/sec; 34.343 sec/batch)
2016-04-28 00:29:02.741036: step 76, loss = 168.49 (1.8 examples/sec; 34.796 sec/batch)
2016-04-28 00:29:37.959022: step 77, loss = 361.77 (1.8 examples/sec; 35.218 sec/batch)
2016-04-28 00:30:12.102728: step 78, loss = 1085.48 (1.9 examples/sec; 34.144 sec/batch)
2016-04-28 00:30:46.486496: step 79, loss = 2001.38 (1.9 examples/sec; 34.384 sec/batch)
2016-04-28 00:31:20.712559: step 80, loss = 264.29 (1.9 examples/sec; 34.226 sec/batch)
2016-04-28 00:32:35.522252: step 81, loss = 6018.45 (1.9 examples/sec; 34.104 sec/batch)
2016-04-28 00:33:10.244145: step 82, loss = 113374.52 (1.8 examples/sec; 34.722 sec/batch)
2016-04-28 00:33:44.472130: step 83, loss = 179662749696.00 (1.9 examples/sec; 34.228 sec/batch)
2016-04-28 00:34:18.820450: step 84, loss = 801993392128.00 (1.9 examples/sec; 34.348 sec/batch)
2016-04-28 00:34:52.787416: step 85, loss = 5855654707200.00 (1.9 examples/sec; 33.967 sec/batch)
2016-04-28 00:35:28.276042: step 86, loss = 1118152747913314304.00 (1.8 examples/sec; 35.489 sec/batch)
2016-04-28 00:36:04.157779: step 87, loss = 352476054423500488704.00 (1.8 examples/sec; 35.696 sec/batch)
2016-04-28 00:36:39.760481: step 88, loss = 283242567506804881752064.00 (1.8 examples/sec; 35.603 sec/batch)
2016-04-28 00:37:15.713099: step 89, loss = 283174599181228606226432.00 (1.8 examples/sec; 35.953 sec/batch)
2016-04-28 00:37:51.347200: step 90, loss = 283150892232790127935488.00 (1.8 examples/sec; 35.634 sec/batch)
2016-04-28 00:39:10.654908: step 91, loss = 283110341821745283989504.00 (1.8 examples/sec; 36.127 sec/batch)
2016-04-28 00:39:46.599798: step 92, loss = 283042391510567517945856.00 (1.8 examples/sec; 35.945 sec/batch)
2016-04-28 00:40:22.274460: step 93, loss = 282974477228186770866176.00 (1.8 examples/sec; 35.675 sec/batch)
2016-04-28 00:40:58.086630: step 94, loss = 282906598974603042750464.00 (1.8 examples/sec; 35.812 sec/batch)
2016-04-28 00:41:34.322354: step 95, loss = 282838702706620805152768.00 (1.8 examples/sec; 36.236 sec/batch)
2016-04-28 00:42:10.932399: step 96, loss = 282770842467435586519040.00 (1.7 examples/sec; 36.610 sec/batch)
2016-04-28 00:42:49.710556: step 97, loss = 282702964213851858403328.00 (1.7 examples/sec; 38.778 sec/batch)
2016-04-28 00:43:26.046339: step 98, loss = 283521556496521229238272.00 (1.8 examples/sec; 36.336 sec/batch)
2016-04-28 00:44:01.865874: step 99, loss = inf (1.8 examples/sec; 35.819 sec/batch)
2016-04-28 00:44:37.781845: step 100, loss = inf (1.8 examples/sec; 35.916 sec/batch)
2016-04-28 00:45:56.847529: step 101, loss = inf (1.8 examples/sec; 36.000 sec/batch)
2016-04-28 00:46:32.289778: step 102, loss = inf (1.8 examples/sec; 35.437 sec/batch)
2016-04-28 00:47:07.983062: step 103, loss = inf (1.8 examples/sec; 35.693 sec/batch)
2016-04-28 00:47:43.928522: step 104, loss = inf (1.8 examples/sec; 35.945 sec/batch)
2016-04-28 00:48:20.142288: step 105, loss = inf (1.8 examples/sec; 36.214 sec/batch)
2016-04-28 00:48:55.831109: step 106, loss = inf (1.8 examples/sec; 35.689 sec/batch)
2016-04-28 00:49:31.228104: step 107, loss = inf (1.8 examples/sec; 35.397 sec/batch)
2016-04-28 00:50:07.119127: step 108, loss = inf (1.8 examples/sec; 35.891 sec/batch)
2016-04-28 00:50:42.843710: step 109, loss = inf (1.8 examples/sec; 35.722 sec/batch)
2016-04-28 00:51:18.445942: step 110, loss = inf (1.8 examples/sec; 35.602 sec/batch)
2016-04-28 00:52:36.330839: step 111, loss = inf (1.8 examples/sec; 35.118 sec/batch)
2016-04-28 00:53:11.670167: step 112, loss = inf (1.8 examples/sec; 35.338 sec/batch)
2016-04-28 00:53:47.199146: step 113, loss = inf (1.8 examples/sec; 35.529 sec/batch)
2016-04-28 00:54:23.242250: step 114, loss = inf (1.8 examples/sec; 36.043 sec/batch)
2016-04-28 00:54:58.959731: step 115, loss = inf (1.8 examples/sec; 35.717 sec/batch)
2016-04-28 00:55:34.808309: step 116, loss = inf (1.8 examples/sec; 35.848 sec/batch)
2016-04-28 00:56:10.429382: step 117, loss = inf (1.8 examples/sec; 35.621 sec/batch)
2016-04-28 00:56:46.209632: step 118, loss = inf (1.8 examples/sec; 35.780 sec/batch)
2016-04-28 00:57:21.956548: step 119, loss = inf (1.8 examples/sec; 35.747 sec/batch)
2016-04-28 00:57:57.437834: step 120, loss = inf (1.8 examples/sec; 35.481 sec/batch)
2016-04-28 00:59:16.312720: step 121, loss = inf (1.8 examples/sec; 35.660 sec/batch)
2016-04-28 00:59:51.944673: step 122, loss = inf (1.8 examples/sec; 35.632 sec/batch)
2016-04-28 01:00:27.730468: step 123, loss = inf (1.8 examples/sec; 35.786 sec/batch)
2016-04-28 01:01:03.116817: step 124, loss = inf (1.8 examples/sec; 35.386 sec/batch)
2016-04-28 01:01:38.760568: step 125, loss = inf (1.8 examples/sec; 35.644 sec/batch)
2016-04-28 01:02:14.310037: step 126, loss = inf (1.8 examples/sec; 35.549 sec/batch)
2016-04-28 01:02:50.002553: step 127, loss = inf (1.8 examples/sec; 35.692 sec/batch)
2016-04-28 01:03:25.934531: step 128, loss = inf (1.8 examples/sec; 35.932 sec/batch)
2016-04-28 01:04:01.966568: step 129, loss = inf (1.8 examples/sec; 36.032 sec/batch)
2016-04-28 01:04:37.886949: step 130, loss = inf (1.8 examples/sec; 35.920 sec/batch)
2016-04-28 01:05:56.968244: step 131, loss = inf (1.8 examples/sec; 36.103 sec/batch)
2016-04-28 01:06:32.856731: step 132, loss = inf (1.8 examples/sec; 35.888 sec/batch)
2016-04-28 01:07:08.862989: step 133, loss = inf (1.8 examples/sec; 36.006 sec/batch)
2016-04-28 01:07:44.588516: step 134, loss = inf (1.8 examples/sec; 35.725 sec/batch)
2016-04-28 01:08:20.356696: step 135, loss = inf (1.8 examples/sec; 35.768 sec/batch)
2016-04-28 01:08:55.753479: step 136, loss = inf (1.8 examples/sec; 35.397 sec/batch)
2016-04-28 01:09:31.256780: step 137, loss = inf (1.8 examples/sec; 35.503 sec/batch)
2016-04-28 01:10:06.621427: step 138, loss = inf (1.8 examples/sec; 35.365 sec/batch)
2016-04-28 01:10:42.544448: step 139, loss = inf (1.8 examples/sec; 35.923 sec/batch)
2016-04-28 01:11:18.222753: step 140, loss = inf (1.8 examples/sec; 35.678 sec/batch)
2016-04-28 01:12:37.169618: step 141, loss = inf (1.8 examples/sec; 36.007 sec/batch)
2016-04-28 01:13:12.943941: step 142, loss = inf (1.8 examples/sec; 35.771 sec/batch)
2016-04-28 01:13:48.835698: step 143, loss = inf (1.8 examples/sec; 35.892 sec/batch)
2016-04-28 01:14:24.605571: step 144, loss = inf (1.8 examples/sec; 35.770 sec/batch)
2016-04-28 01:15:00.017127: step 145, loss = inf (1.8 examples/sec; 35.411 sec/batch)
2016-04-28 01:15:35.728365: step 146, loss = inf (1.8 examples/sec; 35.711 sec/batch)
2016-04-28 01:16:11.668123: step 147, loss = inf (1.8 examples/sec; 35.940 sec/batch)
2016-04-28 01:16:47.534554: step 148, loss = inf (1.8 examples/sec; 35.866 sec/batch)
2016-04-28 01:17:23.881014: step 149, loss = inf (1.8 examples/sec; 36.346 sec/batch)
2016-04-28 01:17:59.492161: step 150, loss = inf (1.8 examples/sec; 35.611 sec/batch)
2016-04-28 01:19:18.401710: step 151, loss = inf (1.8 examples/sec; 35.768 sec/batch)
2016-04-28 01:19:54.269438: step 152, loss = inf (1.8 examples/sec; 35.868 sec/batch)
2016-04-28 01:20:30.386007: step 153, loss = inf (1.8 examples/sec; 36.116 sec/batch)
2016-04-28 01:21:05.971183: step 154, loss = inf (1.8 examples/sec; 35.585 sec/batch)
2016-04-28 01:21:41.473569: step 155, loss = inf (1.8 examples/sec; 35.502 sec/batch)
2016-04-28 01:22:17.002907: step 156, loss = inf (1.8 examples/sec; 35.529 sec/batch)
2016-04-28 01:22:52.633247: step 157, loss = inf (1.8 examples/sec; 35.630 sec/batch)
2016-04-28 01:23:28.233536: step 158, loss = inf (1.8 examples/sec; 35.600 sec/batch)
2016-04-28 01:24:04.105954: step 159, loss = inf (1.8 examples/sec; 35.872 sec/batch)
2016-04-28 01:24:39.851410: step 160, loss = inf (1.8 examples/sec; 35.745 sec/batch)
2016-04-28 01:25:58.724656: step 161, loss = inf (1.8 examples/sec; 35.756 sec/batch)
2016-04-28 01:26:34.617629: step 162, loss = inf (1.8 examples/sec; 35.893 sec/batch)
2016-04-28 01:27:10.268585: step 163, loss = inf (1.8 examples/sec; 35.651 sec/batch)
2016-04-28 01:27:46.013267: step 164, loss = inf (1.8 examples/sec; 35.745 sec/batch)
2016-04-28 01:28:22.082847: step 165, loss = inf (1.8 examples/sec; 36.069 sec/batch)
2016-04-28 01:28:57.753891: step 166, loss = inf (1.8 examples/sec; 35.671 sec/batch)
2016-04-28 01:29:33.647073: step 167, loss = inf (1.8 examples/sec; 35.893 sec/batch)
2016-04-28 01:30:09.574981: step 168, loss = inf (1.8 examples/sec; 35.928 sec/batch)
2016-04-28 01:30:45.460233: step 169, loss = inf (1.8 examples/sec; 35.885 sec/batch)
2016-04-28 01:31:21.363898: step 170, loss = inf (1.8 examples/sec; 35.904 sec/batch)
2016-04-28 01:32:39.787015: step 171, loss = inf (1.8 examples/sec; 35.561 sec/batch)
2016-04-28 01:33:15.015114: step 172, loss = inf (1.8 examples/sec; 35.228 sec/batch)
2016-04-28 01:33:50.683923: step 173, loss = inf (1.8 examples/sec; 35.669 sec/batch)
2016-04-28 01:34:26.601827: step 174, loss = inf (1.8 examples/sec; 35.918 sec/batch)
2016-04-28 01:35:02.271009: step 175, loss = inf (1.8 examples/sec; 35.669 sec/batch)
2016-04-28 01:35:37.451794: step 176, loss = inf (1.8 examples/sec; 35.181 sec/batch)
2016-04-28 01:36:13.211105: step 177, loss = inf (1.8 examples/sec; 35.759 sec/batch)
2016-04-28 01:36:48.888309: step 178, loss = inf (1.8 examples/sec; 35.677 sec/batch)
2016-04-28 01:37:24.941052: step 179, loss = inf (1.8 examples/sec; 36.053 sec/batch)
2016-04-28 01:38:00.772136: step 180, loss = inf (1.8 examples/sec; 35.831 sec/batch)
2016-04-28 01:39:19.674246: step 181, loss = inf (1.8 examples/sec; 35.624 sec/batch)
2016-04-28 01:39:55.018362: step 182, loss = inf (1.8 examples/sec; 35.344 sec/batch)
2016-04-28 01:40:30.776408: step 183, loss = inf (1.8 examples/sec; 35.758 sec/batch)
2016-04-28 01:41:06.946704: step 184, loss = inf (1.8 examples/sec; 36.170 sec/batch)
2016-04-28 01:41:43.583747: step 185, loss = inf (1.7 examples/sec; 36.637 sec/batch)
2016-04-28 01:42:20.005767: step 186, loss = inf (1.8 examples/sec; 36.422 sec/batch)
2016-04-28 01:42:55.783020: step 187, loss = inf (1.8 examples/sec; 35.777 sec/batch)
2016-04-28 01:43:31.428330: step 188, loss = inf (1.8 examples/sec; 35.645 sec/batch)
2016-04-28 01:44:07.762006: step 189, loss = inf (1.8 examples/sec; 36.334 sec/batch)
2016-04-28 01:44:43.867969: step 190, loss = inf (1.8 examples/sec; 36.106 sec/batch)
2016-04-28 01:46:03.776968: step 191, loss = inf (1.8 examples/sec; 35.998 sec/batch)
2016-04-28 01:46:40.036304: step 192, loss = inf (1.8 examples/sec; 36.254 sec/batch)
2016-04-28 01:47:15.527973: step 193, loss = inf (1.8 examples/sec; 35.492 sec/batch)
2016-04-28 01:47:50.986060: step 194, loss = inf (1.8 examples/sec; 35.458 sec/batch)
2016-04-28 01:48:26.412068: step 195, loss = inf (1.8 examples/sec; 35.426 sec/batch)
2016-04-28 01:49:02.044024: step 196, loss = inf (1.8 examples/sec; 35.632 sec/batch)
2016-04-28 01:49:37.961252: step 197, loss = inf (1.8 examples/sec; 35.917 sec/batch)
2016-04-28 01:50:13.529851: step 198, loss = inf (1.8 examples/sec; 35.568 sec/batch)
2016-04-28 01:50:49.423041: step 199, loss = inf (1.8 examples/sec; 35.893 sec/batch)
2016-04-28 01:51:24.842366: step 200, loss = inf (1.8 examples/sec; 35.419 sec/batch)
2016-04-28 01:52:43.604861: step 201, loss = inf (1.8 examples/sec; 35.764 sec/batch)
2016-04-28 01:53:19.315455: step 202, loss = inf (1.8 examples/sec; 35.711 sec/batch)
2016-04-28 01:53:54.955863: step 203, loss = inf (1.8 examples/sec; 35.640 sec/batch)
2016-04-28 01:54:30.649558: step 204, loss = inf (1.8 examples/sec; 35.694 sec/batch)
2016-04-28 01:55:06.208600: step 205, loss = inf (1.8 examples/sec; 35.559 sec/batch)
2016-04-28 01:55:41.807758: step 206, loss = inf (1.8 examples/sec; 35.599 sec/batch)
2016-04-28 01:56:17.298843: step 207, loss = inf (1.8 examples/sec; 35.491 sec/batch)
2016-04-28 01:56:52.744212: step 208, loss = inf (1.8 examples/sec; 35.445 sec/batch)
2016-04-28 01:57:28.239338: step 209, loss = inf (1.8 examples/sec; 35.495 sec/batch)
2016-04-28 01:58:03.855123: step 210, loss = inf (1.8 examples/sec; 35.616 sec/batch)
2016-04-28 01:59:21.697219: step 211, loss = inf (1.8 examples/sec; 35.034 sec/batch)
2016-04-28 01:59:57.148509: step 212, loss = inf (1.8 examples/sec; 35.451 sec/batch)
2016-04-28 02:00:32.519880: step 213, loss = inf (1.8 examples/sec; 35.371 sec/batch)
2016-04-28 02:01:08.055303: step 214, loss = inf (1.8 examples/sec; 35.535 sec/batch)
2016-04-28 02:01:43.536019: step 215, loss = inf (1.8 examples/sec; 35.481 sec/batch)
2016-04-28 02:02:19.811631: step 216, loss = inf (1.8 examples/sec; 36.276 sec/batch)
2016-04-28 02:02:55.634136: step 217, loss = inf (1.8 examples/sec; 35.822 sec/batch)
2016-04-28 02:03:31.210166: step 218, loss = inf (1.8 examples/sec; 35.576 sec/batch)
2016-04-28 02:04:06.813340: step 219, loss = inf (1.8 examples/sec; 35.603 sec/batch)
2016-04-28 02:04:42.072990: step 220, loss = inf (1.8 examples/sec; 35.260 sec/batch)
2016-04-28 02:05:59.893876: step 221, loss = inf (1.8 examples/sec; 35.191 sec/batch)
2016-04-28 02:06:35.529830: step 222, loss = inf (1.8 examples/sec; 35.636 sec/batch)
2016-04-28 02:07:11.117920: step 223, loss = inf (1.8 examples/sec; 35.588 sec/batch)
2016-04-28 02:07:47.105536: step 224, loss = inf (1.8 examples/sec; 35.988 sec/batch)
2016-04-28 02:08:22.678673: step 225, loss = inf (1.8 examples/sec; 35.573 sec/batch)
2016-04-28 02:08:57.663370: step 226, loss = inf (1.8 examples/sec; 34.985 sec/batch)
2016-04-28 02:09:33.340319: step 227, loss = inf (1.8 examples/sec; 35.677 sec/batch)
2016-04-28 02:10:08.793519: step 228, loss = inf (1.8 examples/sec; 35.453 sec/batch)
2016-04-28 02:10:44.211715: step 229, loss = inf (1.8 examples/sec; 35.418 sec/batch)
2016-04-28 02:11:20.173038: step 230, loss = inf (1.8 examples/sec; 35.961 sec/batch)
2016-04-28 02:12:38.882038: step 231, loss = inf (1.8 examples/sec; 35.528 sec/batch)
2016-04-28 02:13:14.837431: step 232, loss = inf (1.8 examples/sec; 35.955 sec/batch)
2016-04-28 02:13:50.408115: step 233, loss = inf (1.8 examples/sec; 35.571 sec/batch)
2016-04-28 02:14:26.030225: step 234, loss = inf (1.8 examples/sec; 35.622 sec/batch)
2016-04-28 02:15:01.448939: step 235, loss = inf (1.8 examples/sec; 35.419 sec/batch)
2016-04-28 02:15:36.727028: step 236, loss = inf (1.8 examples/sec; 35.278 sec/batch)
2016-04-28 02:16:12.327098: step 237, loss = inf (1.8 examples/sec; 35.600 sec/batch)
2016-04-28 02:16:47.977049: step 238, loss = inf (1.8 examples/sec; 35.650 sec/batch)
2016-04-28 02:17:23.830339: step 239, loss = inf (1.8 examples/sec; 35.853 sec/batch)
2016-04-28 02:17:59.325979: step 240, loss = inf (1.8 examples/sec; 35.496 sec/batch)
2016-04-28 02:19:18.494637: step 241, loss = inf (1.8 examples/sec; 35.810 sec/batch)
2016-04-28 02:19:54.104966: step 242, loss = inf (1.8 examples/sec; 35.610 sec/batch)
2016-04-28 02:20:29.930174: step 243, loss = inf (1.8 examples/sec; 35.825 sec/batch)
2016-04-28 02:21:05.371612: step 244, loss = inf (1.8 examples/sec; 35.441 sec/batch)
2016-04-28 02:21:40.680198: step 245, loss = inf (1.8 examples/sec; 35.308 sec/batch)
2016-04-28 02:22:15.950363: step 246, loss = inf (1.8 examples/sec; 35.270 sec/batch)
2016-04-28 02:22:51.678991: step 247, loss = inf (1.8 examples/sec; 35.729 sec/batch)
2016-04-28 02:23:27.416745: step 248, loss = inf (1.8 examples/sec; 35.738 sec/batch)
2016-04-28 02:24:03.003168: step 249, loss = inf (1.8 examples/sec; 35.586 sec/batch)
2016-04-28 02:24:38.379062: step 250, loss = inf (1.8 examples/sec; 35.376 sec/batch)
2016-04-28 02:25:56.601262: step 251, loss = inf (1.8 examples/sec; 35.044 sec/batch)
2016-04-28 02:26:31.821015: step 252, loss = inf (1.8 examples/sec; 35.220 sec/batch)
2016-04-28 02:27:07.454028: step 253, loss = inf (1.8 examples/sec; 35.633 sec/batch)
2016-04-28 02:27:42.970254: step 254, loss = inf (1.8 examples/sec; 35.516 sec/batch)
2016-04-28 02:28:18.652416: step 255, loss = inf (1.8 examples/sec; 35.682 sec/batch)
2016-04-28 02:28:54.684048: step 256, loss = inf (1.8 examples/sec; 36.032 sec/batch)
2016-04-28 02:29:30.434946: step 257, loss = inf (1.8 examples/sec; 35.751 sec/batch)
2016-04-28 02:30:05.968569: step 258, loss = inf (1.8 examples/sec; 35.534 sec/batch)
2016-04-28 02:30:41.563007: step 259, loss = inf (1.8 examples/sec; 35.594 sec/batch)
2016-04-28 02:31:16.687120: step 260, loss = inf (1.8 examples/sec; 35.124 sec/batch)
2016-04-28 02:32:35.477983: step 261, loss = inf (1.8 examples/sec; 35.644 sec/batch)
2016-04-28 02:33:11.257914: step 262, loss = inf (1.8 examples/sec; 35.780 sec/batch)
2016-04-28 02:33:46.461332: step 263, loss = inf (1.8 examples/sec; 35.203 sec/batch)
2016-04-28 02:34:22.339758: step 264, loss = inf (1.8 examples/sec; 35.878 sec/batch)
2016-04-28 02:34:57.715836: step 265, loss = inf (1.8 examples/sec; 35.376 sec/batch)
2016-04-28 02:35:33.075283: step 266, loss = inf (1.8 examples/sec; 35.359 sec/batch)
2016-04-28 02:36:08.603759: step 267, loss = inf (1.8 examples/sec; 35.528 sec/batch)
2016-04-28 02:36:44.382777: step 268, loss = inf (1.8 examples/sec; 35.779 sec/batch)
2016-04-28 02:37:19.620317: step 269, loss = inf (1.8 examples/sec; 35.237 sec/batch)
2016-04-28 02:37:55.420951: step 270, loss = inf (1.8 examples/sec; 35.801 sec/batch)
2016-04-28 02:39:13.201780: step 271, loss = inf (1.8 examples/sec; 35.184 sec/batch)
2016-04-28 02:39:49.021365: step 272, loss = inf (1.8 examples/sec; 35.819 sec/batch)
2016-04-28 02:40:24.463928: step 273, loss = inf (1.8 examples/sec; 35.442 sec/batch)
2016-04-28 02:41:00.026369: step 274, loss = inf (1.8 examples/sec; 35.562 sec/batch)
2016-04-28 02:41:36.212290: step 275, loss = inf (1.8 examples/sec; 36.186 sec/batch)
2016-04-28 02:42:12.600981: step 276, loss = inf (1.8 examples/sec; 36.389 sec/batch)
2016-04-28 02:42:48.378669: step 277, loss = inf (1.8 examples/sec; 35.778 sec/batch)
2016-04-28 02:43:23.672911: step 278, loss = inf (1.8 examples/sec; 35.294 sec/batch)
2016-04-28 02:43:58.670304: step 279, loss = inf (1.8 examples/sec; 34.997 sec/batch)
2016-04-28 02:44:34.697968: step 280, loss = inf (1.8 examples/sec; 36.028 sec/batch)
2016-04-28 02:45:52.709195: step 281, loss = inf (1.8 examples/sec; 35.161 sec/batch)
2016-04-28 02:46:28.256398: step 282, loss = inf (1.8 examples/sec; 35.547 sec/batch)
2016-04-28 02:47:03.928616: step 283, loss = inf (1.8 examples/sec; 35.672 sec/batch)
2016-04-28 02:47:39.599108: step 284, loss = inf (1.8 examples/sec; 35.670 sec/batch)
2016-04-28 02:48:15.457089: step 285, loss = inf (1.8 examples/sec; 35.858 sec/batch)
2016-04-28 02:48:51.491313: step 286, loss = inf (1.8 examples/sec; 36.034 sec/batch)
2016-04-28 02:49:27.320378: step 287, loss = inf (1.8 examples/sec; 35.829 sec/batch)
2016-04-28 02:50:02.477548: step 288, loss = inf (1.8 examples/sec; 35.157 sec/batch)
2016-04-28 02:50:37.979847: step 289, loss = inf (1.8 examples/sec; 35.502 sec/batch)
2016-04-28 02:51:13.577715: step 290, loss = inf (1.8 examples/sec; 35.598 sec/batch)
2016-04-28 02:52:31.357227: step 291, loss = inf (1.8 examples/sec; 35.261 sec/batch)
2016-04-28 02:53:07.498389: step 292, loss = inf (1.8 examples/sec; 36.134 sec/batch)
2016-04-28 02:53:42.757563: step 293, loss = inf (1.8 examples/sec; 35.257 sec/batch)
2016-04-28 02:54:18.354398: step 294, loss = inf (1.8 examples/sec; 35.597 sec/batch)
2016-04-28 02:54:53.366849: step 295, loss = inf (1.8 examples/sec; 35.012 sec/batch)
2016-04-28 02:55:28.969717: step 296, loss = inf (1.8 examples/sec; 35.603 sec/batch)
2016-04-28 02:56:04.783673: step 297, loss = inf (1.8 examples/sec; 35.814 sec/batch)
2016-04-28 02:56:40.471217: step 298, loss = inf (1.8 examples/sec; 35.687 sec/batch)
2016-04-28 02:57:15.952384: step 299, loss = inf (1.8 examples/sec; 35.481 sec/batch)
2016-04-28 02:57:51.154443: step 300, loss = inf (1.8 examples/sec; 35.202 sec/batch)
2016-04-28 02:59:09.336685: step 301, loss = inf (1.8 examples/sec; 35.811 sec/batch)
2016-04-28 02:59:45.146607: step 302, loss = inf (1.8 examples/sec; 35.810 sec/batch)
2016-04-28 03:00:24.085135: step 303, loss = inf (1.6 examples/sec; 38.938 sec/batch)
2016-04-28 03:01:13.143767: step 304, loss = inf (1.3 examples/sec; 49.059 sec/batch)
2016-04-28 03:01:48.739730: step 305, loss = inf (1.8 examples/sec; 35.596 sec/batch)
2016-04-28 03:02:24.210409: step 306, loss = inf (1.8 examples/sec; 35.471 sec/batch)
2016-04-28 03:02:59.550923: step 307, loss = inf (1.8 examples/sec; 35.340 sec/batch)
2016-04-28 03:03:35.562297: step 308, loss = inf (1.8 examples/sec; 36.011 sec/batch)
2016-04-28 03:04:11.160346: step 309, loss = inf (1.8 examples/sec; 35.598 sec/batch)
2016-04-28 03:04:46.942601: step 310, loss = inf (1.8 examples/sec; 35.782 sec/batch)
2016-04-28 03:06:05.123645: step 311, loss = inf (1.8 examples/sec; 35.128 sec/batch)
2016-04-28 03:06:40.683649: step 312, loss = inf (1.8 examples/sec; 35.560 sec/batch)
2016-04-28 03:07:16.716736: step 313, loss = inf (1.8 examples/sec; 36.033 sec/batch)
2016-04-28 03:07:52.388814: step 314, loss = inf (1.8 examples/sec; 35.672 sec/batch)
2016-04-28 03:08:27.930446: step 315, loss = inf (1.8 examples/sec; 35.542 sec/batch)
2016-04-28 03:09:04.146974: step 316, loss = inf (1.8 examples/sec; 36.216 sec/batch)
2016-04-28 03:09:39.861033: step 317, loss = inf (1.8 examples/sec; 35.714 sec/batch)
2016-04-28 03:10:15.143788: step 318, loss = inf (1.8 examples/sec; 35.283 sec/batch)
2016-04-28 03:10:50.505837: step 319, loss = inf (1.8 examples/sec; 35.362 sec/batch)
2016-04-28 03:11:26.058707: step 320, loss = inf (1.8 examples/sec; 35.553 sec/batch)
2016-04-28 03:12:44.726593: step 321, loss = inf (1.8 examples/sec; 35.397 sec/batch)
2016-04-28 03:13:20.378025: step 322, loss = inf (1.8 examples/sec; 35.651 sec/batch)
2016-04-28 03:13:56.057112: step 323, loss = inf (1.8 examples/sec; 35.679 sec/batch)
2016-04-28 03:14:31.706298: step 324, loss = inf (1.8 examples/sec; 35.649 sec/batch)
2016-04-28 03:15:07.424994: step 325, loss = inf (1.8 examples/sec; 35.718 sec/batch)
2016-04-28 03:15:42.992614: step 326, loss = inf (1.8 examples/sec; 35.567 sec/batch)
2016-04-28 03:16:18.499951: step 327, loss = inf (1.8 examples/sec; 35.507 sec/batch)
2016-04-28 03:16:53.595691: step 328, loss = inf (1.8 examples/sec; 35.096 sec/batch)
2016-04-28 03:17:29.231493: step 329, loss = inf (1.8 examples/sec; 35.636 sec/batch)
2016-04-28 03:18:05.316131: step 330, loss = inf (1.8 examples/sec; 36.085 sec/batch)
2016-04-28 03:19:23.287248: step 331, loss = inf (1.8 examples/sec; 35.418 sec/batch)
2016-04-28 03:19:58.469960: step 332, loss = inf (1.8 examples/sec; 35.183 sec/batch)
2016-04-28 03:20:34.624703: step 333, loss = inf (1.8 examples/sec; 36.155 sec/batch)
2016-04-28 03:21:10.918750: step 334, loss = inf (1.8 examples/sec; 36.294 sec/batch)
2016-04-28 03:21:46.401935: step 335, loss = inf (1.8 examples/sec; 35.483 sec/batch)
2016-04-28 03:22:22.206231: step 336, loss = inf (1.8 examples/sec; 35.804 sec/batch)
2016-04-28 03:22:57.701341: step 337, loss = inf (1.8 examples/sec; 35.495 sec/batch)
2016-04-28 03:23:33.281150: step 338, loss = inf (1.8 examples/sec; 35.580 sec/batch)
2016-04-28 03:24:08.777845: step 339, loss = inf (1.8 examples/sec; 35.497 sec/batch)
2016-04-28 03:24:45.047565: step 340, loss = inf (1.8 examples/sec; 36.270 sec/batch)
2016-04-28 03:26:03.673155: step 341, loss = inf (1.8 examples/sec; 35.686 sec/batch)
2016-04-28 03:26:39.609293: step 342, loss = inf (1.8 examples/sec; 35.936 sec/batch)
2016-04-28 03:27:15.413420: step 343, loss = inf (1.8 examples/sec; 35.804 sec/batch)
2016-04-28 03:27:51.190255: step 344, loss = inf (1.8 examples/sec; 35.777 sec/batch)
2016-04-28 03:28:26.991088: step 345, loss = inf (1.8 examples/sec; 35.801 sec/batch)
2016-04-28 03:29:02.655400: step 346, loss = inf (1.8 examples/sec; 35.664 sec/batch)
2016-04-28 03:29:38.475515: step 347, loss = inf (1.8 examples/sec; 35.820 sec/batch)
2016-04-28 03:30:14.575951: step 348, loss = inf (1.8 examples/sec; 36.100 sec/batch)
2016-04-28 03:30:50.541368: step 349, loss = inf (1.8 examples/sec; 35.965 sec/batch)
2016-04-28 03:31:26.475158: step 350, loss = inf (1.8 examples/sec; 35.934 sec/batch)
2016-04-28 03:32:45.334024: step 351, loss = inf (1.8 examples/sec; 35.555 sec/batch)
2016-04-28 03:33:20.838714: step 352, loss = inf (1.8 examples/sec; 35.501 sec/batch)
2016-04-28 03:33:56.828744: step 353, loss = inf (1.8 examples/sec; 35.990 sec/batch)
2016-04-28 03:34:32.869224: step 354, loss = inf (1.8 examples/sec; 36.040 sec/batch)
2016-04-28 03:35:09.000029: step 355, loss = inf (1.8 examples/sec; 36.131 sec/batch)
2016-04-28 03:35:44.871339: step 356, loss = inf (1.8 examples/sec; 35.871 sec/batch)
2016-04-28 03:36:20.685222: step 357, loss = inf (1.8 examples/sec; 35.814 sec/batch)
2016-04-28 03:36:56.376285: step 358, loss = inf (1.8 examples/sec; 35.691 sec/batch)
2016-04-28 03:37:31.898984: step 359, loss = inf (1.8 examples/sec; 35.523 sec/batch)
2016-04-28 03:38:07.118811: step 360, loss = inf (1.8 examples/sec; 35.220 sec/batch)
2016-04-28 03:39:25.770194: step 361, loss = inf (1.8 examples/sec; 36.031 sec/batch)
2016-04-28 03:40:01.574454: step 362, loss = inf (1.8 examples/sec; 35.804 sec/batch)
2016-04-28 03:40:37.029907: step 363, loss = inf (1.8 examples/sec; 35.455 sec/batch)
2016-04-28 03:41:12.977106: step 364, loss = inf (1.8 examples/sec; 35.947 sec/batch)
2016-04-28 03:41:49.206997: step 365, loss = inf (1.8 examples/sec; 36.230 sec/batch)
2016-04-28 03:42:25.290110: step 366, loss = inf (1.8 examples/sec; 36.083 sec/batch)
2016-04-28 03:43:00.632002: step 367, loss = inf (1.8 examples/sec; 35.342 sec/batch)
2016-04-28 03:43:36.067912: step 368, loss = inf (1.8 examples/sec; 35.436 sec/batch)
2016-04-28 03:44:11.995932: step 369, loss = inf (1.8 examples/sec; 35.928 sec/batch)
2016-04-28 03:44:47.762916: step 370, loss = inf (1.8 examples/sec; 35.762 sec/batch)
2016-04-28 03:46:06.156171: step 371, loss = inf (1.8 examples/sec; 36.070 sec/batch)
2016-04-28 03:46:41.807445: step 372, loss = inf (1.8 examples/sec; 35.651 sec/batch)
2016-04-28 03:47:17.682924: step 373, loss = inf (1.8 examples/sec; 35.875 sec/batch)
2016-04-28 03:47:53.113080: step 374, loss = inf (1.8 examples/sec; 35.430 sec/batch)
2016-04-28 03:48:28.556990: step 375, loss = inf (1.8 examples/sec; 35.444 sec/batch)
2016-04-28 03:49:04.098322: step 376, loss = inf (1.8 examples/sec; 35.541 sec/batch)
2016-04-28 03:49:39.864712: step 377, loss = inf (1.8 examples/sec; 35.766 sec/batch)
2016-04-28 03:50:15.466734: step 378, loss = inf (1.8 examples/sec; 35.602 sec/batch)
2016-04-28 03:50:51.146386: step 379, loss = inf (1.8 examples/sec; 35.680 sec/batch)
2016-04-28 03:51:27.003400: step 380, loss = inf (1.8 examples/sec; 35.857 sec/batch)
2016-04-28 03:52:46.566143: step 381, loss = inf (1.8 examples/sec; 35.631 sec/batch)
2016-04-28 03:53:22.258101: step 382, loss = inf (1.8 examples/sec; 35.692 sec/batch)
2016-04-28 03:53:57.672615: step 383, loss = inf (1.8 examples/sec; 35.414 sec/batch)
2016-04-28 03:54:33.593413: step 384, loss = inf (1.8 examples/sec; 35.921 sec/batch)
2016-04-28 03:55:09.509989: step 385, loss = inf (1.8 examples/sec; 35.916 sec/batch)
2016-04-28 03:55:44.868022: step 386, loss = inf (1.8 examples/sec; 35.358 sec/batch)
2016-04-28 03:56:20.129394: step 387, loss = inf (1.8 examples/sec; 35.261 sec/batch)
2016-04-28 03:56:55.127294: step 388, loss = inf (1.8 examples/sec; 34.998 sec/batch)
2016-04-28 03:57:30.914314: step 389, loss = inf (1.8 examples/sec; 35.787 sec/batch)
2016-04-28 03:58:06.870655: step 390, loss = inf (1.8 examples/sec; 35.956 sec/batch)
2016-04-28 03:59:25.530211: step 391, loss = inf (1.8 examples/sec; 36.035 sec/batch)
2016-04-28 04:00:01.124490: step 392, loss = inf (1.8 examples/sec; 35.594 sec/batch)
2016-04-28 04:00:36.392026: step 393, loss = inf (1.8 examples/sec; 35.267 sec/batch)
2016-04-28 04:01:12.047423: step 394, loss = inf (1.8 examples/sec; 35.655 sec/batch)
2016-04-28 04:01:47.775924: step 395, loss = inf (1.8 examples/sec; 35.728 sec/batch)
2016-04-28 04:02:23.834302: step 396, loss = inf (1.8 examples/sec; 36.058 sec/batch)
2016-04-28 04:02:59.141213: step 397, loss = inf (1.8 examples/sec; 35.307 sec/batch)
2016-04-28 04:03:35.277504: step 398, loss = inf (1.8 examples/sec; 36.136 sec/batch)
2016-04-28 04:04:10.494584: step 399, loss = inf (1.8 examples/sec; 35.217 sec/batch)
2016-04-28 04:04:46.376630: step 400, loss = inf (1.8 examples/sec; 35.882 sec/batch)
2016-04-28 04:06:04.673748: step 401, loss = inf (1.8 examples/sec; 35.459 sec/batch)
2016-04-28 04:06:40.312171: step 402, loss = inf (1.8 examples/sec; 35.638 sec/batch)
2016-04-28 04:07:15.502915: step 403, loss = inf (1.8 examples/sec; 35.191 sec/batch)
2016-04-28 04:07:50.944895: step 404, loss = inf (1.8 examples/sec; 35.442 sec/batch)
2016-04-28 04:08:26.111375: step 405, loss = inf (1.8 examples/sec; 35.166 sec/batch)
2016-04-28 04:09:01.522773: step 406, loss = inf (1.8 examples/sec; 35.411 sec/batch)
2016-04-28 04:09:37.000698: step 407, loss = inf (1.8 examples/sec; 35.478 sec/batch)
2016-04-28 04:10:12.689538: step 408, loss = inf (1.8 examples/sec; 35.689 sec/batch)
2016-04-28 04:10:47.991556: step 409, loss = inf (1.8 examples/sec; 35.302 sec/batch)
2016-04-28 04:11:23.263062: step 410, loss = inf (1.8 examples/sec; 35.271 sec/batch)
2016-04-28 04:12:42.306699: step 411, loss = inf (1.8 examples/sec; 35.412 sec/batch)
2016-04-28 04:13:17.680737: step 412, loss = inf (1.8 examples/sec; 35.374 sec/batch)
2016-04-28 04:13:52.947655: step 413, loss = inf (1.8 examples/sec; 35.267 sec/batch)
2016-04-28 04:14:28.440707: step 414, loss = inf (1.8 examples/sec; 35.493 sec/batch)
2016-04-28 04:15:03.953126: step 415, loss = inf (1.8 examples/sec; 35.512 sec/batch)
2016-04-28 04:15:39.753582: step 416, loss = inf (1.8 examples/sec; 35.800 sec/batch)
2016-04-28 04:16:15.333247: step 417, loss = inf (1.8 examples/sec; 35.580 sec/batch)
2016-04-28 04:16:50.446711: step 418, loss = inf (1.8 examples/sec; 35.113 sec/batch)
2016-04-28 04:17:25.809456: step 419, loss = inf (1.8 examples/sec; 35.363 sec/batch)
2016-04-28 04:18:01.357102: step 420, loss = inf (1.8 examples/sec; 35.548 sec/batch)
2016-04-28 04:19:20.815925: step 421, loss = inf (1.8 examples/sec; 36.283 sec/batch)
2016-04-28 04:19:56.747032: step 422, loss = inf (1.8 examples/sec; 35.928 sec/batch)
2016-04-28 04:20:32.268359: step 423, loss = inf (1.8 examples/sec; 35.521 sec/batch)
2016-04-28 04:21:08.083546: step 424, loss = inf (1.8 examples/sec; 35.815 sec/batch)
2016-04-28 04:21:43.943310: step 425, loss = inf (1.8 examples/sec; 35.860 sec/batch)
2016-04-28 04:22:19.809555: step 426, loss = inf (1.8 examples/sec; 35.866 sec/batch)
2016-04-28 04:22:55.238569: step 427, loss = inf (1.8 examples/sec; 35.429 sec/batch)
2016-04-28 04:23:30.821686: step 428, loss = inf (1.8 examples/sec; 35.583 sec/batch)
2016-04-28 04:24:06.107092: step 429, loss = inf (1.8 examples/sec; 35.285 sec/batch)
2016-04-28 04:24:41.503385: step 430, loss = inf (1.8 examples/sec; 35.396 sec/batch)
2016-04-28 04:26:00.743143: step 431, loss = inf (1.8 examples/sec; 35.348 sec/batch)
2016-04-28 04:26:36.645734: step 432, loss = inf (1.8 examples/sec; 35.903 sec/batch)
2016-04-28 04:27:12.664881: step 433, loss = inf (1.8 examples/sec; 36.019 sec/batch)
2016-04-28 04:27:48.471089: step 434, loss = inf (1.8 examples/sec; 35.806 sec/batch)
2016-04-28 04:28:24.158190: step 435, loss = inf (1.8 examples/sec; 35.687 sec/batch)
2016-04-28 04:28:59.603136: step 436, loss = inf (1.8 examples/sec; 35.445 sec/batch)
2016-04-28 04:29:35.317008: step 437, loss = inf (1.8 examples/sec; 35.714 sec/batch)
2016-04-28 04:30:11.370301: step 438, loss = inf (1.8 examples/sec; 36.053 sec/batch)
2016-04-28 04:30:47.368293: step 439, loss = inf (1.8 examples/sec; 35.998 sec/batch)
2016-04-28 04:31:22.709204: step 440, loss = inf (1.8 examples/sec; 35.341 sec/batch)
2016-04-28 04:32:41.516054: step 441, loss = inf (1.8 examples/sec; 35.789 sec/batch)
2016-04-28 04:33:17.365194: step 442, loss = inf (1.8 examples/sec; 35.849 sec/batch)
2016-04-28 04:33:52.623243: step 443, loss = inf (1.8 examples/sec; 35.258 sec/batch)
2016-04-28 04:34:28.184742: step 444, loss = inf (1.8 examples/sec; 35.561 sec/batch)
2016-04-28 04:35:03.894494: step 445, loss = inf (1.8 examples/sec; 35.710 sec/batch)
2016-04-28 04:35:39.321197: step 446, loss = inf (1.8 examples/sec; 35.427 sec/batch)
2016-04-28 04:36:15.115281: step 447, loss = inf (1.8 examples/sec; 35.794 sec/batch)
2016-04-28 04:36:51.187757: step 448, loss = inf (1.8 examples/sec; 36.072 sec/batch)
2016-04-28 04:37:26.765696: step 449, loss = inf (1.8 examples/sec; 35.578 sec/batch)
2016-04-28 04:38:02.368262: step 450, loss = inf (1.8 examples/sec; 35.602 sec/batch)
2016-04-28 04:39:20.267596: step 451, loss = inf (1.8 examples/sec; 35.638 sec/batch)
2016-04-28 04:39:55.607371: step 452, loss = inf (1.8 examples/sec; 35.340 sec/batch)
2016-04-28 04:40:31.541243: step 453, loss = inf (1.8 examples/sec; 35.934 sec/batch)
2016-04-28 04:41:07.079843: step 454, loss = inf (1.8 examples/sec; 35.539 sec/batch)
2016-04-28 04:41:42.585819: step 455, loss = inf (1.8 examples/sec; 35.506 sec/batch)
2016-04-28 04:42:18.749865: step 456, loss = inf (1.8 examples/sec; 36.164 sec/batch)
2016-04-28 04:42:54.707059: step 457, loss = inf (1.8 examples/sec; 35.957 sec/batch)
2016-04-28 04:43:30.548458: step 458, loss = inf (1.8 examples/sec; 35.841 sec/batch)
2016-04-28 04:44:07.783926: step 459, loss = inf (1.7 examples/sec; 37.235 sec/batch)
2016-04-28 04:44:43.056878: step 460, loss = inf (1.8 examples/sec; 35.273 sec/batch)
2016-04-28 04:46:01.204871: step 461, loss = inf (1.8 examples/sec; 35.356 sec/batch)
2016-04-28 04:46:36.751020: step 462, loss = inf (1.8 examples/sec; 35.546 sec/batch)
2016-04-28 04:47:12.423041: step 463, loss = inf (1.8 examples/sec; 35.672 sec/batch)
2016-04-28 04:47:48.850252: step 464, loss = inf (1.8 examples/sec; 36.254 sec/batch)
2016-04-28 04:48:24.676852: step 465, loss = inf (1.8 examples/sec; 35.826 sec/batch)
2016-04-28 04:49:00.904711: step 466, loss = inf (1.8 examples/sec; 36.228 sec/batch)
2016-04-28 04:49:36.752316: step 467, loss = inf (1.8 examples/sec; 35.848 sec/batch)
2016-04-28 04:50:12.514230: step 468, loss = inf (1.8 examples/sec; 35.762 sec/batch)
2016-04-28 04:50:47.825094: step 469, loss = inf (1.8 examples/sec; 35.311 sec/batch)
2016-04-28 04:51:23.549363: step 470, loss = inf (1.8 examples/sec; 35.724 sec/batch)
2016-04-28 04:52:41.594349: step 471, loss = inf (1.8 examples/sec; 35.109 sec/batch)
2016-04-28 04:53:17.707506: step 472, loss = inf (1.8 examples/sec; 36.112 sec/batch)
2016-04-28 04:53:53.472495: step 473, loss = inf (1.8 examples/sec; 35.765 sec/batch)
2016-04-28 04:54:29.089129: step 474, loss = inf (1.8 examples/sec; 35.617 sec/batch)
2016-04-28 04:55:05.159485: step 475, loss = inf (1.8 examples/sec; 36.070 sec/batch)
2016-04-28 04:55:40.665212: step 476, loss = inf (1.8 examples/sec; 35.506 sec/batch)
2016-04-28 04:56:16.309851: step 477, loss = inf (1.8 examples/sec; 35.645 sec/batch)
2016-04-28 04:56:51.843002: step 478, loss = inf (1.8 examples/sec; 35.533 sec/batch)
2016-04-28 04:57:27.525071: step 479, loss = inf (1.8 examples/sec; 35.682 sec/batch)
2016-04-28 04:58:03.307076: step 480, loss = inf (1.8 examples/sec; 35.782 sec/batch)
2016-04-28 04:59:21.591943: step 481, loss = inf (1.8 examples/sec; 35.564 sec/batch)
2016-04-28 04:59:57.240869: step 482, loss = inf (1.8 examples/sec; 35.649 sec/batch)
2016-04-28 05:00:33.329595: step 483, loss = inf (1.8 examples/sec; 36.089 sec/batch)
2016-04-28 05:01:09.404932: step 484, loss = inf (1.8 examples/sec; 36.075 sec/batch)
2016-04-28 05:01:45.063946: step 485, loss = inf (1.8 examples/sec; 35.659 sec/batch)
2016-04-28 05:02:20.678291: step 486, loss = inf (1.8 examples/sec; 35.614 sec/batch)
2016-04-28 05:02:56.051025: step 487, loss = inf (1.8 examples/sec; 35.373 sec/batch)
2016-04-28 05:03:31.306411: step 488, loss = inf (1.8 examples/sec; 35.255 sec/batch)
2016-04-28 05:04:07.250124: step 489, loss = inf (1.8 examples/sec; 35.944 sec/batch)
2016-04-28 05:04:42.852519: step 490, loss = inf (1.8 examples/sec; 35.602 sec/batch)
2016-04-28 05:06:01.482429: step 491, loss = inf (1.8 examples/sec; 36.086 sec/batch)
2016-04-28 05:06:36.998540: step 492, loss = inf (1.8 examples/sec; 35.516 sec/batch)
2016-04-28 05:07:12.658151: step 493, loss = inf (1.8 examples/sec; 35.660 sec/batch)
2016-04-28 05:07:47.943085: step 494, loss = inf (1.8 examples/sec; 35.285 sec/batch)
2016-04-28 05:08:23.455894: step 495, loss = inf (1.8 examples/sec; 35.513 sec/batch)
2016-04-28 05:08:58.946499: step 496, loss = inf (1.8 examples/sec; 35.491 sec/batch)
2016-04-28 05:09:34.665774: step 497, loss = inf (1.8 examples/sec; 35.719 sec/batch)
2016-04-28 05:10:10.402943: step 498, loss = inf (1.8 examples/sec; 35.737 sec/batch)
2016-04-28 05:10:46.042582: step 499, loss = inf (1.8 examples/sec; 35.640 sec/batch)

Process finished with exit code 0
